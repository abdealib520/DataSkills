employer_name,job_title,job_description,job_city,Python,R,SQL,Java,Scala,C++,JavaScript,Excel,Power BI,Tableau,SAS,Apache Spark,Redshift,BigQuery,Airflow,Snowflake
Lilly,Data Engineer - (DT) Business Insights & Analytics,"At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 35,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world.

Business Insights and Analytics: Data Engineer

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease. We’re looking for people who are determined to make life better for people around the world.

The LCCI (Lilly Capability Center India), BI&A (Business Insights & Analytics) team was started in 2017 with the objective of supporting business decisions for the commercial and marketing functions in the US and ex-US affiliates. This team is part of the LCCI - Commercial Services organization and works very closely with business analytics team based in Indianapolis (HQ). The team currently comprises of more than 100 staff members, with varied backgrounds and skills across data management, analytics and data sciences, business and commercial operations etc.

To better meet the evolving analytics needs, the LCCI BI&A team is ramping up the data engineering pillar. We are looking for data engineers who can be play integral role in developing, maintaining, and testing infrastructures for data generation, processing and storage; work closely with data scientists and help architecting solutions with the objective of driving right KPIs for the business.

Core Responsibilities:
• Create and maintain optimal data pipeline architecture ETL/ ELT into Structured data
• Assemble large, complex data sets that meet functional / non-functional business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.
• Expert level experience creating Fact tables, Dimensional tables and ingest datasets into Cloud based tools. Job Scheduling, automation experience is must.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Setup and maintain data ingestion, streaming, scheduling, and job monitoring automation. Connectivity between Lambda, Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and “big data” technologies like AWS and Google
• Build analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics
• Work with cross-functional teams including external consultants and IT teams to assist with data-related technical issues and support their data infrastructure needs
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

Experience Required
• 4-8 years of in-depth hands-on experience in data warehousing (Redshift or any OLAP) to support business/data analytics, business intelligence (BI)
• Advanced knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouses
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience building and optimizing “big data” data pipelines, architectures and data sets
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores
• Strong analytical and problem-solving skills to be able to structure and solve open ended business problems (pharma experience is highly preferred)

Education
• Bachelor’s/ Master’s degree in Technology OR Computer Sciences

Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response.

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,True,False,False,True
Koch,Data Engineer,"Description

PRIMARY PURPOSE:

Engineering graduate with relevant experience of 4 to 8 years, who is passionate and eager to learn and contribute. The primary requirement would be to build and design Python data pipelines. Experience on writing complex pipeline using Spark & Pandas. Hands-on experience with CICD pipelines system. Strong understanding of RDBMS is required. Familiar with Agile development. Preferred large data processing experience.

DUTIES & RESPONSIBILITIES:

1.

Design Data Pipelines using Spark & Pandas/Polars

2

Design Data Models & high-performance SQL quires to pull data from different sources

3

Create/ maintain / optimize AWS Glue Jobs and data pipeline.

4

Understand Business requirement and deliver solution as expected.

5

Support Analyst in to Extracting data, reporting and analysis to generate business insights

Knowledge

· Knowledge of Manufacturing, production.

· Knowledge of Python Pandas/Polars/Spark with the ability to understand and work with formulas and formats submitted by customer.

· Knowledge Complex ETL Pipelines

Technical Skills

· Python.

· Pandas/Polars/Spark.

· Data Modelling/SQL design.

· Gitlab/Github (CICD).

· ETL Pipeline with any other tool like (Apche Nifi/AWS Glue/EMR).

Preferred Skills

· Agile development

· SOLID principle.

· RDBMS.

· DevOps.

PREFERRED: Previous experience with developing Data pipelines using Python and refactor the existing code to follow the best practices in industry. Maintain existing Data pipelines and AWS ETL Jobs. Have analytical skills to meet business requirements. Hands-on experience on CICD process and database modelling.

“Koch is proud to be an equal opportunity workplace”",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
HuQuo,Data Engineer - Snowflake,"Snowflake data engineers will be responsible for architecting and implementing very large scale data intelligence solutions around Snowflake Data Warehouse.

A solid experience and understanding of architecting, designing and operationalization of large scale data and analytics solutions on Snowflake Cloud Data Warehouse is a must. Need to have professional knowledge of AWS Redshift.

Responsibilities
• Developing ETL pipelines in and out of data warehouse using combination of Python and Snowflakes Snow SQL.
• Writing SQL queries against Snowflake.
• Developing scripts Unix, Python etc. to do Extract, Load and Transform data.
• Working knowledge of AWS Redshift.
• Provide production support for Data Warehouse issues such data load problems, transformation translation problems.
• Translate requirements for BI and Reporting to Database design and reporting design.
• Understanding data transformation and translation requirements and which tools to leverage to get the job done.
• Understanding data pipelines and modern ways of automating data pipeline using cloud based.
• Testing and clearly document implementations, so others can easily understand the requirements, implementation and test conditions.

,

This job is provided by Shine.com",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Mastercard,Software Engineer II | Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Software Engineer II | Data Engineer

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Overview
The Enterprise Data Solutions team is looking for a Big Data Engineer to drive our mission to unlock potential of data assets by consistently innovating, eliminating friction in how users access data from its Big Data repositories and enforce standards and principles in the Big Data space. The candidate will be part of an exciting, fast paced environment developing Data Engineering solutions in the data and analytics domain.

Role
• Develop high quality, secure and scalable data pipelines using spark, Scala/ python on Hadoop or object storage.
• Leverage new technologies and approaches to innovate with increasingly large data sets.
• Drive automation and efficiency in Data ingestion, data movement and data access workflows by innovation and collaboration.
• Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency.
• Perform assigned tasks and support production incidents.

All About You
• 4+ years of experience in Data Warehouse related projects in product or service-based organization
• Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
• Experience of building data pipelines through Spark with Scala/Python/Java on Hadoop or Object storage
• Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
• Experience of working on Nifi will be an added advantage
• Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
• Strong communication skills - both verbal and written
• Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
• Flexibility to work as a member of a matrix based diverse and geographically distributed project teams

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Ericsson,Data Engineer,"About this opportunity

Develop and deliver data solutions to support business needs & requirements by crafting and optimizing data pipelines and data models for analytical solution’s data lake / data warehouses towards specific micro services. The data engineer will also have responsibility for all data ingress into data lake/DWH, maintain the accurate content, format and integrities all through the lifetime of such data!

What you will do
• Business solution owner, process owners and solution users and information stewards, domain guides, subject matter experts!
• Business Analysts, Data Scientists, Data Architects, Automation Architects, Automation Engineers
• Project Steering Group, Operative Steering Group (OSG), Project Management Office (PMO) and enterprise/Regional IT representatives
• Partners and vendors· Acuity for business flow understanding and expertise in data preparation and pre-processing
• SQL knowledge and experience working with relational databases, query authorising (SQL) as well as a variety of other databases/date-sources.
• Big data tools like Hadoop, Spark, Kafka, BigQuery,GCPetc.
• Data and Model pipeline and workflow management tools like Azkaban, Luigi, Airflow, Dataiku, etc.
• Stream-processing systems like Storm, Spark-Streaming, etc.
• Object-oriented/object function scripting languages like Python, Java, C++, Scala, etc.
• SQL, Python/Java, GCP Big Query (Understanding of Cloud Env), Apache Kafka, Cassandra,Postgres, Hadoop, Airflow and hands on experience on Kubernetes

You will bring
• Manage data solution roadmap
• Translate business requirements to data solution features and create solution roadmap options proposals.
• Convert data solution roadmap to backlog items, sprint plans and release plans.
• Participate in backlog prioritization discussions.
• Perform risk analysis and build mitigation plans.
• Create and communicate status and progress reports.
• Create resource plans for realizing solution roadmap and handle resource skills by selection process and/or trainings.
• Design & lead data solution development
• Manage compliance with analytics reference architecture by creating complaint designs and/or transition plans.
• Write test plans and descriptions for component testing. Support system test specification.
• Participate in Design review, and document approval or exemptions from design guidelines.
• Provide feedback on Reference architecture and target architecture to Data Architecture and IT teams.
• Ensure compliance of data solutions within Data Architecture guidelines
• Develop and test data solution components and create detailed data solution design.
• Write technical specifications and create solution interface or Integration specifications.
• Provide guidance on suitable options, designing, and crafting data pipeline for the analytical solution’s data lake / data warehouses to specific micro services
• Develop and improve Solution components: database, schemas, data dictionary, scripts, reports, graphic user interfaces, system roles.
• Perform configuration and/or code walkthroughs, peer reviews, track, and fix defects and perform unit testing of data solution components. ensure that solution component meet acceptance criteria.
• Support and participate in acceptance procedures and tests, Data solution verification after deployment.

Why join Ericsson?

At Ericsson, you´ll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of what´s possible. To build never seen before solutions to some of the world’s toughest problems. You´ll be challenged, but you won’t be alone. You´ll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.

What happens once you apply?

Click Here to find all you need to know about what our typical hiring process looks like.

Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we nurture it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team.

Ericsson is proud to be an Equal Opportunity and Affirmative Action employer, learn more.

Primary country and city: India (IN) || India : Uttar Pradesh : Noida

Job details: Data Engineer Job Stage: Job Stage 6

Primary Recruiter: Priyanka Vaid",Noida,True,False,True,True,False,True,False,False,False,False,False,False,False,True,True,False
Reverate,Senior Data Engineer - Remote,"Reverate Tech is a product and service-based start-up, working with International Clients. Our services include Data Engineering, Web Development, BI/Data Warehousing, Enterprise Application Implementation (ERP/CRM), and NetSuite. Our product portfolio has business apps in the domain of ERP, Auto Service, and Personal Safety.

This is an exciting opportunity to work as Senior Data Engineer for our client SellerX.

SellerX is the 3rd fastest growing company in the EU evaluated at more than 1 Billion Euros. It has an ambitious goal: to become a leading global acquirer and operator of a new generation of eCommerce businesses.

Your Job:
• You are responsible for all types of data management processes (collection, storage, cleansing, preparation, maintenance, accumulation, transfer to business reporting).
• You optimize and develop existing and new data warehouse applications using tools for data ingestion and data modeling
• You design and document new data models and best-practice solutions.
• You are responsible for prototyping and implementing new ETL jobs and modeling approaches.
• As a data engineer, you will deal with python programs and their configurations in order to create or improve automated data engineering tasks.
• In addition to technical project management, you advise our other tech teams in Data Management aspects.
• Ensuring data security (e.g. encryption) and improving the backup strategy.
• You work hand in hand with data architects, data analysts, and data scientists.
• You ensure that quality, stability, and robustness along the entire process chain meet our high standards.

Your Background:
• You have a bachelor's degree with a focus on software engineering.
• 5+ years of experience in data engineering.
• Strong with Algorithms and have worked on scaling pipelines/solutions
• Hands-on experience with data ingestion tools like Fivetran, Daton, Stitch, or Data Virtuality.
• Hands-on experience with ETL and orchestration tools like Apache Airflow or similar.
• Object-oriented Python programming is more than just a plus.
• Expert knowledge in the areas of data modeling and ETL processes on the SQL level (e.g. using DBT), as well as experience working with REST APIs, is beneficial.
• DevOps experience
• In addition to your ""hands-on"" mentality, you score points with a high technical affinity and a strong analytical mindset.
• Your working standards do not suffer in terms of quality, even in hectic times.

Benefits:
• Compensation: up to 40 LPA
• 100% remote-working;
• Flexible working hours;
• Development of your personal strengths in a dynamic environment;
• An attractive and varied job with a high level of personal responsibility;
• A collegial togetherness and a modern management style/startup;

Interested? Join us and start your learning and growth journey.

Reverate focuses on Software Engineering. Their company has offices in India. They have a small team that's between 11-50 employees.

You can view their website at https://reverate.tech/",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Hewlett Packard Careers,Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.
We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

The Data Engineer will develop, test, and maintain Big Data solutions for a company. Gather large amounts of data from multiple sources and ensure that downstream users can access the data quickly and efficiently. Essentially, the company’s data pipelines are scalable, secure, and able to serve multiple users.

Job description
• Meeting with managers to determine the company’s Big Data needs.
• Developing Hadoop systems.
• Loading disparate data sets and conducting pre-processing services using Spark, Hive or Pig.
• Finalizing the scope of the system and delivering Big Data solutions.
• Managing the communications between the internal system and the vendor.
• Collaborating with the software research and development teams.
• Building cloud platforms for the development of company applications.
• Maintaining production systems.
• Training staff on data management.

Big Data Engineer Requirements:
• Bachelor’s degree in computer engineering or computer science.
• Previous experience as a big data engineer.
• In-depth knowledge of Hadoop, Spark, and similar frameworks.
• Knowledge of scripting languages is preferred .
• Knowledge of NoSQL and RDBMS databases including Redis and MongoDB.
• Familiarity with Mesos, AWS, and Docker tools.
• Excellent project management skills.
• Good communication skills.
• Ability to solve complex data, and software issues.

Education and Experience Required:
• Typically, 6+ years of progressive professional experience as a big data engineer.
• Bachelor’s degree in computer engineering or computer science.

We love our work environment. We think you will too:
• It’s a friendly atmosphere with supportive leaders to bring your creativity to the max.
• Work-life balance support including flex-time arrangements and work from home opportunities.
• Corporate Social Responsibility initiatives to help you make an impact to communities at large.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
GE,Senior Data Engineer,"Job Description Summary
GE HealthCare is on a transformational journey leveraging Data and Analytics to drive business growth. GE HealthCare is looking for Senior Data Engineer who will be responsible for building and implementing the data ETL pipelines for Finance function data (from data ingestion to consumption).The Data Engineering team helps solve our customers' toughest challenges leveraging data and analytics. The Senior Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE HealthCare to drive business analytics to a new level of predictive analytics while leveraging On-prem, Cloud Platform, Big data tools and technologies.

GE HealthCare is a leading global medical technology and digital solutions innovator. Our purpose is to create a world where healthcare has no limits. Unlock your ambition, turn ideas into world-changing realities, and join an organization where every voice makes a difference, and every difference builds a healthier world.

Job Description

In this role you will:
• Responsible for building data and analytical engineering solutions with standard end to end design & ETL patterns, implementing data pipelines, data modelling and overseeing overall data quality.
• Responsible to work with cross functional teams in GEHC to make the data usable for functional users, data scientists and application users to enable delivery of business values to customers.
• Responsible to enable access of data in AWS storage layers and transformations in AWS Datawarehouse and further transporting in respective databases, consumers, data marts etc.
• As a Senior Data Engineer, you will be part of a data engineering or cross-disciplinary team on Finance facing development projects, typically involving large, complex data sets. These teams typically include data engineers, data visualization engineers, architects, data scientists, product managers, and end users, working in cohorts with partners in GE business units.
• Implement Data warehouse entities with common re-usable data model designs with automation and data quality capabilities.
• Demonstrate proficiency at industry standard data modeling tools (e.g., Erwin, ER Studio, etc.).
• Integrate domain data knowledge into development of data requirements.
• Develop processing codebase using pySpark and implement medium to complex transformations, business logics.
• Look across multiple systems, understands the purpose of each system and defines data requirements by systems.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Lead other horizontal improvement initiatives to benefit technology and leap further on a problem area or Hackathon etc
• Establish and maintain as a trusted advisor relationship within GE Healthcare Data & Analytics (Finance Function)
• Establish and maintain close working relationships with teams responsible for delivering solutions to the businesses and functions
• Engage collaboratively with project teams to support project objectives through the application of sound data engineering principles
• Identify risks and assumptions for the in scope Data & Analytics solutions
• Work with the contract/vendor resources to deliver the solution and manage the technical resources work

Qualifications
• Bachelor's Degree in Computer Science, Information Technology or equivalent (STEM)
• A minimum of 6 year of similar experience working on Database(s), SQL, Python, Datawarehouse, Java, ETL and AWS cloud platform is required. AWS certifications would be added advantage
• Experienced in Deployment process on-prem and on-cloud using Kubernetes, Dockers, Jenkins
• Ability to drive projects in big data (structured/unstructured/machine/logs/streaming data types)
• 3+ Year of Data modelling & Data warehousing experience with MPP systems (Teradata, Netezza, Greenplum etc.)
• 3+ years in AWS Services Like Redshift, RDS, S3, Glue, Step Function, Lambda etc.
• Hands on experience in delivering analytics in modern data architecture (Massively Parallel Processing Database Platforms and Semantic Modelling)
• Demonstrable knowledge of ETL and ELT patterns and when to use either one; experience selecting among different tools that could be leveraged to accomplish this. (i.e. Informatica, HVR, Talend etc)
• Demonstrable knowledge of and experience with different scripting languages (python, shell)
• Understands data quality and solves for application-level needs
• Understanding of DaaS, Data management tools / solutions
• Strong verbal & written communication
• Experience working with solutions delivery teams using Agile/Scrum ore similar methodologies
• Added advantage if experienced in working on Finance data

Desired skills:
• Delivers results when working on shorter-term (weeks-months), outcome-focused service engagements
• Proactively learning new technology, predicts trends, and identifies new opportunities based on trends
• Leverages knowledge about technology trends, and changing business needs across the broad environment to bring new ideas to the team
• Articulates the value proposition of existing technology capabilities and maps them to customer requirements to minimize incremental cost of development
• Experienced in working with On-prem (Teradata) data warehouse – Dimensional and data modelling. Experienced in one of the ETL like Informatica.
• Identifies the customer’s business and strategic needs, concerns, and desires for the value delivery capabilities of the Product
• Functional understanding of finance - Close Book, Treasury, Cash, Controllership, Credit, Account Payables, Account Receivables, Cash Forecasting, Balance sheet exposure, Debt, Forex etc.

Inclusion and Diversity

GE Healthcare is an Equal Opportunity Employer where inclusion matters. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

We expect all employees to live and breathe our behaviors: to act with humility and build trust; lead with transparency; deliver with focus, and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.
#LI-Hybrid
#LI-GM2

Additional Information

Relocation Assistance Provided: Yes",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,False
DocuSign,Data Engineer [T500-7242],"Basic Qualifications:
• Bachelor’s Degree in Computer Science, Data Analytics, Information Systems, etc.
• Experience developing data pipelines in one of the following languages: Python or Java
• 5+ years dimensional and relational data modeling experience
• Excellent SQL and database management skills

Preferred Qualifications:
• 1+ years in data warehouse engineering (OLAP) Snowflake, BigQuery, Teradata, Redshift
• 3+ years with transactional databases (OLTP) Oracle, SQL Server, MySQL
• 3+ years with big data, Hadoop, Data Lake, Spark in a cloud environment(AWS)
• 3+ years with commercial ETL tools – dbt, Matillion, and SSIS/ADF
• 3+ years delivering ETL solutions from source systems, databases, APIs, flat-files, JSON
• Experience developing Entity Relationship Diagrams with Erwin, SQLDBM, or equivalent
• Experience working with job scheduling and monitoring systems (Airflow, Datadog, AWS SNS)
• Experience building BI Dashboards with tools like Tableau
• Experience in the financial domain, accounts payable, accounts receivable, invoicing
• Experience managing work assignments using tools like Jira and Confluence
• Experience with Scrum/Agile methodologies is a plus.
• Ability to work independently and as part of a team
• Excellent analytical and problem solving skills",Bengaluru,True,False,True,True,False,False,False,True,False,True,False,False,True,True,True,True
TATA Consultancy Services Ltd.,Data Engineer,"Job DescriptionGreetings from TATA Consultancy Services Limited!!Job Description:Data EngineerQualification: 15 years of Full-time educationRequired Skill Sets:Big Data, Hadoop, Hive, Spark, YarnMust Have-Hands-on development experienceExperience leveraging big data technologies (One or more of Hadoop, Python, Spark) is mandatory.Experience working with various data exchange formats (JSON, CSV, XML etc.).Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred.Excellent knowledge of SQL and Linux shell scripting.Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)Good-to-Have-Experience building real-time data pipelines using Kafka or spark streaming is preferred.Exposure to Microsoft Azure (or other cloud) platforms is preferred.Experience with Agile methodologies for project development.Excellent diagnostic, analytical and problem-solving skills are preferred.Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferredDesired Candidate ProfileUndergraduate3.00-8.00 Years",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Robert Bosch,Data Engineer,"Job Description Job Overview: . To work on development, monitoring and maintenance of Data pipelines across AE clusters. Primary responsibilities: . Develop, Monitor and Maintain data pipeline for various AE plants. . Create and maintain optimal data pipeline architecture. . Assemble large, complex data sets that meet functional / non-functional business requirements. . Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designinginfrastructure for greater scalability. . Work with stakeholders including the Data officers and stewards to assist with data-related technical issues and support their data infrastructure needs. . Work on incidents highlighted by the data officers. . Incident diagnosis, routing, evaluation & resolution. . Analyze the root cause of incidents. . Create incident closure report. Qualifications Qualifications . BE Degree in computer science or equivalent . English on business level . Min 3 years of Experience in Data management within the area of software development rollout and maintenance . Min 2 Years of specific experience with data pipeline development, monitoring and maintenance. Additional Information Skills Self-starter and empowered professional with strong execution and project management capabilities Ability to collaborate effectively, well developed inter personal relationships with all levels in the organization and outside contacts. Outstanding written and verbal communication skills. High Collaboration & a perseverance to drive performance & change Additional information Key Competencies- . Experience in developing data pipelines using spark & scala. . Experience in debugging pipeline issues. . Experience in writing python and shell scripts. . In-Depth Knowledge of SQL and Other Database Solutions . Having a strong understanding of Apache Hadoop-based analytics Experience in Azure cloud platform is desired Technical skills: . Hands on experience onSpark sql and Spark streaming. . Hands on experience on Scala language. . In depth knowledge of SQL and NoSQL (HBase). . Good in writing Shell and python script. . Good understanding of Hadoop ecosystem. . Hands on experience on InteliJ, Github /Bitbucket, HUE. . Understanding of ETL process. Soft Skills: . Good Communication Skills . Ability to use extensively the latest IT communication tools ( Skype /MS TEAMS etc.) . Decent Level in English as Business Language",,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Greetings from TCS!!!

Come and join us for an exciting career with TCS!!!

Hiring For: Data engineer

Skills:
• Hadoop,
• Spark

Location - Pan India

Experience: 4 to 8 years

If interested kindly send all the following details to the following email id: r.vijay23@tcs.com
• Name:
• Contact No:
• Email id:
• Current Location:
• Preferred Location:
• Highest Qualification:
• Total Experience:
• Relevant Experience in years:
• Current Organization:
• Notice Period:
• Current CTC:
• Expected CTC:
• Gap in years if any (Education / Career):
• Graduation(Regular/Full time or Correspondence/ part time)and University name :
• Updated CV attached (Yes / No):
• Have you worked with TCS before ( Permanent / Contract ):

Regards,

Vijay.R

Human Resources - Talent Acquisition Group

Tata Consultancy Services

Email: r.vijay23@tcs.com",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Coders Brain Technology,Azure Data Engineer,"Azure Data Engineer

Exp – 6 - 8 yrs

Primary skills: Azure Synapse, Synapse pipeline/ADF, ADLS, Databricks, Scala/Pyspark, Azure DB migration experience

Secondary Skills - Hadoop, HIVE, Talend (Experience in the past)

Skills: Azure Data Architect, ADLS Gen 2, ADF, DataBricks (Intermediate), Synapse (Advanced). Team Management.""

Coders Brain Technology focuses on Information Technology. Their company has offices in Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://www.codersbrain.com",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
TIAA,"Data Engineering, Nuveen","Business Intelligence, Nuveen
Designs, develops and enforces standards and architecture for installing, configuring and using business intelligence (BI) applications for the purpose of directing and managing the organization.

Key Responsibilities and Duties
• Overseeing complex data modeling and advanced project metadata development； ensuring that business rules are consistently applied across different user interfaces to limit the possibility of inconsistent results.
• Designing specifications and standards for semantic layers and multidimensional models for complex BI projects, across all environments.
• Consulting on training and usage for the business community by selecting appropriate BI tools, features, and techniques.
• Setting and facilitating the implementation of BI standards and architecture; aligning BI architecture with enterprise architecture.
Qualifications
• 5+ years
• University (Degree), Preferred
Physical Requirements
• Physical Requirements: Sedentary Work

Career Level
7IC

Position Summary and Impact:

Position Summary : Data Engineering Associates are critical members of the data supply chain for distribution platform. Implements large scale, complex data projects with a focus on collecting, parsing, managing, analyzing large sets of data to make the appropriate data accessible and available for use by variety of users.

Impact : Customer data is used to ensure high quality customer servicing, efficient distribution management and critical inputs to strategic planning and performance measurement. Data accuracy and completeness are critical enablers to a high performing distribution platform that supports all elements of sales, service and support

Functional Responsibilities:

_Collects the data, does large scale processing and delivers to user community

_Data administration and planning - resolves systemic errors, monitors the data ecosystem, develops strategies to scale up, addresses continuous integration, supports data management and governance

_Identifies syntax, structure, and semantics for each data source

_Provides ideas to improve data ecosystem effectiveness and efficiency; participates in the implementation

_Provides adhoc analysis and reporting assistance from various internal departments

People Responsibilities:

None

Job Requirements And Qualifications:

Required Education:

Bachelors Degree in data analytics/computer science or engineering

Required Experience:

5-8 years

Experience, Skills and Abilities:

Experience:

_Strong knowledge on databases and best engineering practices

_Ability to profile data to measure quality, integrity, accuracy, and completeness

_Proven experience with data preparation and blending for reporting and analytical purposes

_Hands-on experience on all aspects of data warehousing and schema. _Proficient in designing efficient and robust ETL workflows

_Knowledge of asset management industry and financial products (mutual funds, managed accounts, ETFs, etc.) and Salesforce CRM desired

Skills:

_Strong MS SQL Server

_Strong knowledge of Data Lake environments & Tools - Hadoop, Spark & Hive

_Advanced Tableau (server and desktop) authoring

_Extensive experience with and strong understanding of relational databases

_Salesforce CRM knowledge Strongly Desired

_VB, Python, R Studio desired

_Exceptionally good with interpersonal and communication skills including ability to communicate complex information and analysis in written and visual forms

_High attention to data accuracy

_Ability to work in an agile team

_Critical thinking to ask questions, determine best course and offer solutions

_Ability to complete work independently

_Continuous improvement mindset

_Ability to understand the big picture

_Teamwork skills within the department and on project teams

_Demonstrated ability to work effectively in a fast-paced, complex, and dynamic business environment

_Demonstrated and dynamic analytical/problem solving skills

_________________

Company Overview

TIAA GBS India was established in 2016 with a mission to tap into a vast pool of talent, reduce risk by insourcing key platforms and processes, as well as contribute to innovation with a focus on enhancing our technology stack. TIAA GBS India is focused on building a scalable and sustainable organization , with a focus on technology , operations and expanding into the shared services business space.
Working closely with our U.S. colleagues and other partners, our goal is to reduce risk, improve the efficiency of our technology and processes and develop innovative ideas to increase throughput and productivity.

We are an Equal Opportunity/Affirmative Action Employer. We consider all qualified applicants for employment regardless of age, race, color, national origin, sex, religion, veteran status, disability, sexual orientation, gender identity, or any other protected status.

Accessibility Support

TIAA offers support for those who need assistance with our online application process to provide an equal employment opportunity to all job seekers, including individuals with disabilities.

If you are a U.S. applicant and desire a reasonable accommodation to complete a job application please use one of the below options to contact our accessibility support team:

Phone: (800) 842-2755

Email: accessibility.support@tiaa.org

Privacy Notices

For residents of California, please click here to access the TIAA CA Applicant Privacy Notice.

For residents of the EU / UK, please click here to access the EU / UK Pre-employment Notice.

For all other residents, click here to access the Applicant Privacy Notice.",Pune,True,True,True,False,False,False,False,False,False,True,False,False,False,False,False,False
deloitte,Consulting- SAMA- A&C-Azure Data Engineer- AD,"JD:
• Location - Mumbai OR Pune
• Experience range:
• 12-15 yrs for AD
• Strong experience in Python programming and related skills like PySpark
• Strong SQL skills
• Strong experience with any of the data engineering platforms like Hadoop, Spark, Synapse, Databricks, Apache Airflow, etc.
• Preferred: Knowledge of any cloud platform like Azure/AWS/Google
• For AD level: Need technology leadership experience in terms of architecture/solutioning and team leading",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
ANI Calls India Private Limited,Data Engineer,"Anicalls Industry:

IT
Total Positions: 2

Job Type:
Full Time/

Permanent Gender:
No Preference Salary: 900000 INR - 1600000 INR (Annually)

Education:
Bachelor′s degree Experience: 5-9

Years Location:
Chennai, India .

Proactively partner with
ITand research staff to maintain and renovate, as needed, the metadata across the data warehouse, research, reporting, and business intelligence environments to maintain consistency, minimize manual and error-prone processes and improve productivity and data quality.
. Design, develop, and maintain reports and dashboards in analytical, statistical, and BI tools. .

Model and refine data pipelines for data flows in collaboration with research staff, IT, and business stakeholders to ensure data quality identify inconsistencies, promote efficiency and automation, and identify/resolve non-compliance with data and security requirements.
.

Monitor availability of new analytics, reporting, and BI solutions to support the modernization of current data and analytics initiatives and develop new data tools that will enhance the matching experience of NRMP constituents.
.

Work to become the in-house expert on existing analytics, reporting, BI tools, and their use Collaborate with
ITon the delivery, implementation, maintenance, and support of tools used by the research team.
. Work to become a data and analytics tool guru displaying expertise is data quality, availability, architecture, knowledge, and consumption.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Hexaware Technologies Limited,Cloud Data Engineer,"Responsibilities:
• Front end the delivery of processes to data extraction, transformation, and load from disparate sources into a form that is consumable by analytics processes, for projects with moderate complexity, using strong technical capabilities and sense of database performance.
• Sound understanding of dimensional data modeling standards and best practices to ensure high quality.
• Batch Processing - Capability to design an efficient way of processing high volumes of data where a group of transactions is collected over a period.
• In-depth understanding of Data modeling concepts & principles.
• Data Integration (Sourcing, Storage and Migration) - Capability to design and implement models, capabilities, and solutions to manage data within the enterprise (structured and unstructured, data archiving principles, data warehousing, data sourcing, etc.). This includes the data models, storage requirements and migration of data from one system to another. Should be able to develop cost efficient and performant data pipelines in the cloud platform.
• Data Quality, Profiling and Cleansing - Capability to review (profile) a data set to establish its quality against a defined set of parameters and to highlight data where corrective action (cleansing) is required to remediate the data.
• Stream Systems - Capability to discover, integrate, and ingest all available data from the machines that produce it, as fast as it is produced, in any format, and at any quality.
• Excellent interpersonal skills to build network with variety of department across business to understand data and deliver business value and may interface and communicate with program teams, management and stakeholders as required to deliver small to medium- sized projects.
• Implement Data integration and Data Warehouse based solutions using Big Data Technologies.
• Should be highly proficient in the use of Big Data / Open Source Technologies and standard techniques of Data Integration, Data Manipulation with hands-on contribution.

The Role offers:
• Great opportunities to learn various tools and technologies used in a sophisticated data architecture within the Business Intelligence and Analytics Data Services.
• Gives an opportunity to showcase candidates’ strong analytical skills and problem-solving ability.
• An outstanding opportunity to re-imagine, redesign, and apply technology to add value to the business and operations.
• Learning & growth opportunities in cloud and Big data engineering spaces.

Essential Skills:
• 4+ years’ experience in developing large scale data pipelines in a at least one Cloud Services-Azure, AWS, GCP.
• Expertise in one or more (data base + ETL /pipeline + Visualization Reporting) of the following skills, Azure: Synapse, ADF, Insights,AWS: Redshift, Glue, EMR.
• Highly Proficient in any or more of market leading ETL tools like Informatica, DataStage, SSIS, Talend, etc.,
• Fundamental knowledge in Data warehouse/Data Mart architecture and modelling.
• Define and develop data ingest, validation, and transform pipelines.
• Fundamental knowledge of distributed data processing and storage.
• Fundamental knowledge of working with structured, unstructured, and semi structured data.
• For cloud data engineer, experience with ETL/ELT patterns, preferably using Azure Data Factory and Databricks jobs.
• Nice to have on premise platform understanding covering one or more of the below skills Teradata, Cloudera, Netezza, Informatica, DataStage, SSIS, BODS, SAS, Business Objects, Cognos, MicroStrategy, WebFocus, Crystal.

Essential Qualification:
• BE/Btech in Computer Science, Engineering or relevant field.",,False,False,False,False,False,False,False,True,False,False,True,False,True,False,False,False
ICIT Computer Institute,Freshers - Full stack .Net II Azure Data Engineer,"Job Description :

Team,

Let me know if you have trained freshers on the skills mentioned in the subject line.

We are looking for hiring trained immediate freshers who can join us.

More about our organization can be viewed here:https://www.neudesic.com/

Industry : IT

Functional Area :

Education :

Experience :

Job Location :

Status : Open",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Referrals Only,Consultant-Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.
Job responsibilities• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will collaborate with Data Scientists in order to design scalable implementations of their models
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Assure effective collaboration between Thoughtworks' and the client's teams, encouraging open communication and advocating for shared outcomes
Job qualificationsTechnical skills• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Professional skills• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more
Other things to knowL&DThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.
About ThoughtworksThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 28+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.

Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
VISA,Staff Data Engineer,"Job Description

We are looking for an expert with deep expertise in big data/data warehousing and who has experience in working with clients in implementation of data engineering pipelines, data exchange mechanisms and data science models.

The role will need design and architecture skills to advice VISA’s clients on best approaches for data engineering and data science model implementations.

The candidate will also play critical role in enabling the internal data platforms using which Data Scientists, Analysts, and BI Users drive solutions for Visa clients. The candidate is expected to act as a bridge between end-users in Asia and Visa Technology colleagues in San Francisco, influencing the development of our global data platforms whilst provisioning local tools and technologies as required.

This is a hands-on role, and the candidate is expected to work hands on in areas such as Hadoop, Spark, Python, Tableau, Unix scripting.

We are looking for a talented, technical, proactive, energetic, and passionate person who embraces challenges and is a proven problem solver.

Principal Responsibilities
• Provide advisory and implementation support to VISA’s clients for deployment of data engineering pipelines and data science models.
• Work with VISA data scientists for optimization and automation of data science models.
• Support data exchange channels integration (Such as APIs, Secured File Transfer, other custom data exchange capabilities provided by VISA) between VISA and clients.
• Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack
• Provide leadership and support in identifying, implementing, and managing new data tools and processes, relevant to data science – on premise and cloud.
• Partner with Technology on quarterly planning cycle and support management with relevant metrics to evaluate performance, stability, and reliability of various tools.
• Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions.
• Develop custom-built packages to support the needs of Data Scientists across the region.
• Continuous focus on improving Infrastructure efficiency by analyzing logs of queries, tuning settings, translating queries if required.
• Review scripts and educate users by building training assets for beginner and intermediate level users.

Policy guidance and administration of tools used by data scientists including git, ETL schedulers, Spark, Tableau etc. to ensure optimum utilization and efficiency

Qualifications

Basic Qualifications
• Minimum of bachelor’s degree or equivalent
• Qualification in Computer Science or Engineering ideal.

Professional Experience
• 8 - 12 years of relevant experience
• Deep knowledge of distributed data architecture, commonly used BI tools, and approaches/packages used in machine learning build
• Expertise in Design and architecture of big data platforms, data science and visualization platforms.
• Expertise in creating production software/systems using Python and/or Scala, and a proven track record of identifying and resolving performance bottlenecks in production systems.
• Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets.
• Good appreciation of machine learning algorithms, feature engineering, validation, prediction, recommendation, and measurement.
• Experience of working in multiple large projects with diverse cross-functional teams.
• Ability to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.
• Understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant
• Demonstrated ability to incorporate new techniques to solve business problems
• Demonstrated resource planning and delivery skills
• Good communication and presentation skills with ability to interact with different cross-functional team members at varying level

Technical Expertise
• Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark preferred.
• Expertise in dashboard and report development in Tableau
• Experience in coming up with data flow patterns and deployment architecture for data engineering pipeline and data science models
• Experience with APIs, container-based software deployments
• Experience in developing production systems incorporating best-in-class software engineering practices such as DevOps, Agile development, CI/CD, and scheduling technologies
• Working knowledge of Hadoop ecosystem and associated technologies such as Hive, Apache Spark, PySpark, MLlib, GraphX.
• Advanced experience in writing and optimizing efficient SQL queries and Python scripts; Scala experience will be an added advantage.
• Good appreciation of cloud-based technologies and data platforms
• Very strong people/technology project management skills and experience

Business and Leadership competencies
• Results-oriented with strong problem-solving skills and demonstrated intellectual and analytical rigor
• Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred
• Team oriented, collaborative, diplomatic, and flexible
• Detailed oriented to ensure highest level of quality/rigor in reports and data analysis
• Proven skills in translating analytics output to actionable recommendations and delivery
• Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels
• Exhibits intellectual curiosity and a desire for continuous learning
• Demonstrates integrity, maturity, and a constructive approach to business challenges
• Role model for the organization and implementing core Visa Values
• Respect for the Individuals at all levels in the workplace
• Strive for Excellence and extraordinary results
• Use sound insights and judgments to make informed decisions in line with business strategy and needs
• Ability to allocate tasks and resources across multiple lines of businesses and geographies.
• Ability to influence senior management within and outside Analytics groups
• Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions

Additional Information

Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.

Job Description

We are looking for an expert with deep expertise in big data/data warehousing and who has experience in working with clients in implementation of data engineering pipelines, data exchange mechanisms and data science models.

The role will need design and architecture skills to advice VISA’s clients on best approaches for data engineering and data science model implementations.

The candidate will also play critical role in enabling the internal data platforms using which Data Scientists, Analysts, and BI Users drive solutions for Visa clients. The candidate is expected to act as a bridge between end-users in Asia and Visa Technology colleagues in San Francisco, influencing the development of our global data platforms whilst provisioning local tools and technologies as required.

This is a hands-on role, and the candidate is expected to work hands on in areas such as Hadoop, Spark, Python, Tableau, Unix scripting.

We are looking for a talented, technical, proactive, energetic, and passionate person who embraces challenges and is a proven problem solver.

Principal Responsibilities
• Provide advisory and implementation support to VISA’s clients for deployment of data engineering pipelines and data science models.
• Work with VISA data scientists for optimization and automation of data science models.
• Support data exchange channels integration (Such as APIs, Secured File Transfer, other custom data exchange capabilities provided by VISA) between VISA and clients.
• Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack
• Provide leadership and support in identifying, implementing, and managing new data tools and processes, relevant to data science – on premise and cloud.
• Partner with Technology on quarterly planning cycle and support management with relevant metrics to evaluate performance, stability, and reliability of various tools.
• Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions.
• Develop custom-built packages to support the needs of Data Scientists across the region.
• Continuous focus on improving Infrastructure efficiency by analyzing logs of queries, tuning settings, translating queries if required.
• Review scripts and educate users by building training assets for beginner and intermediate level users.

Policy guidance and administration of tools used by data scientists including git, ETL schedulers, Spark, Tableau etc. to ensure optimum utilization and efficiency

Qualifications

Basic Qualifications
• Minimum of bachelor’s degree or equivalent
• Qualification in Computer Science or Engineering ideal.

Professional Experience
• 8 - 12 years of relevant experience
• Deep knowledge of distributed data architecture, commonly used BI tools, and approaches/packages used in machine learning build
• Expertise in Design and architecture of big data platforms, data science and visualization platforms.
• Expertise in creating production software/systems using Python and/or Scala, and a proven track record of identifying and resolving performance bottlenecks in production systems.
• Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets.
• Good appreciation of machine learning algorithms, feature engineering, validation, prediction, recommendation, and measurement.
• Experience of working in multiple large projects with diverse cross-functional teams.
• Ability to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.
• Understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant
• Demonstrated ability to incorporate new techniques to solve business problems
• Demonstrated resource planning and delivery skills
• Good communication and presentation skills with ability to interact with different cross-functional team members at varying level

Technical Expertise
• Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark preferred.
• Expertise in dashboard and report development in Tableau
• Experience in coming up with data flow patterns and deployment architecture for data engineering pipeline and data science models
• Experience with APIs, container-based software deployments
• Experience in developing production systems incorporating best-in-class software engineering practices such as DevOps, Agile development, CI/CD, and scheduling technologies
• Working knowledge of Hadoop ecosystem and associated technologies such as Hive, Apache Spark, PySpark, MLlib, GraphX.
• Advanced experience in writing and optimizing efficient SQL queries and Python scripts; Scala experience will be an added advantage.
• Good appreciation of cloud-based technologies and data platforms
• Very strong people/technology project management skills and experience

Business and Leadership competencies
• Results-oriented with strong problem-solving skills and demonstrated intellectual and analytical rigor
• Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred
• Team oriented, collaborative, diplomatic, and flexible
• Detailed oriented to ensure highest level of quality/rigor in reports and data analysis
• Proven skills in translating analytics output to actionable recommendations and delivery
• Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels
• Exhibits intellectual curiosity and a desire for continuous learning
• Demonstrates integrity, maturity, and a constructive approach to business challenges
• Role model for the organization and implementing core Visa Values
• Respect for the Individuals at all levels in the workplace
• Strive for Excellence and extraordinary results
• Use sound insights and judgments to make informed decisions in line with business strategy and needs
• Ability to allocate tasks and resources across multiple lines of businesses and geographies.
• Ability to influence senior management within and outside Analytics groups
• Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions

Additional Information

Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",,True,False,True,False,True,False,False,True,False,True,False,True,False,False,False,False
Thoughtworks Inc.,Senior Consultant - Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
TensorGo Technologies,TensorGo Technologies - Senior Data Engineer - Python,"Skillset : Python, PySpark, Kafka, Airflow, Sql, NoSql, API Integration,Data pipeline, Big Data, AWS/ GCP/ OCI/ AzureRequirements :Understanding our data sets and how to bring them together.Working with our engineering team to support custom solutions offered to the product development.Filling the gap between development, engineering and data ops.Creating, maintaining and documenting scripts to support ongoing custom solutions.Excellent organizational skills, including attention to precise detailsStrong multitasking skills and ability to work in a fast-paced environment3+ years experience with Python to develop scripts.Know your way around RESTFUL APIs.[Able to integrate not necessary to publish]You are familiar with pulling and pushing files from SFTP and AWS S3.Experience with any Cloud solutions including GCP / AWS / OCI / Azure.Familiarity with SQL programming to query and transform data from relational Databases.Familiarity to work with Linux (and Linux work environment).Excellent written and verbal communication skillsExtracting, transforming, and loading data into internal databases and HadoopOptimizing our new and existing data pipelines for speed and reliabilityDeploying product build and product improvementsDocumenting and managing multiple repositories of codeExperience with SQL and NoSQL databases (Casendra, MySQL)Hands-on experience in data pipelining and ETL. (Any of these frameworks/tools: Hadoop, BigQuery, RedShift, Athena)Hands-on experience in AirFlowUnderstanding of best practices, common coding patterns and good practices aroundstoring, partitioning, warehousing and indexing of dataExperience in reading the data from Kafka topic (both live stream and offline)Experience in PySpark and Data framesResponsibilities :You'll :Collaborating across an agile team to continuously design, iterate, and develop big data systems.Extracting, transforming, and loading data into internal databases.Optimizing our new and existing data pipelines for speed and reliability.Deploying new products and product improvements.Documenting and managing multiple repositories of code.",,True,False,True,False,False,False,False,True,False,False,False,False,True,True,True,False
PayPal,"MTS 1, Data Engineer","Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
The MTS 1 ? Data Engineer will directly report to and support Sr. Manger of Finance Technologies in the development and execution of strategic transformation programs & initiatives, strategic engineering architecture design, resource allocation, and platform performance monitoring. Ideal candidate is a technologist who believes that use of technology is in its infancy and the best is yet to come. The Regulatory Reporting Hadoop product owner (Business System Analyst) will be part of the Global Regulatory Reporting, and Merger & Acquisition Integration support. The nature of role is strategic, analytical and highly collaborative, working with team members across World and also as a liaison for Global projects.
• Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
• Undergraduate/ Master degree in Computer Engineering or equivalent from a leading university.
• 11+ years of post-college working experience as a Business System Analyst and leading large scale projects end to end.?
• Minimum 4+ years? experience working with large data sets, experience working with distributed computing a plus (Map/Reduce, Hadoop, Hive, Spark, etc.)
• Experience in Data Analysis, Data Validation.
• Strong knowledge in writing complex queries for validation of ETL process.
• Preferred/Basic understanding of Payments/Finance/Accounting Industry Background.
• Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations.
• Experience with databases, systems integration, application development and reporting.?
• Works independently and able to make decisions quickly when necessary.
• Quick Learner with an ability to ramp up in technologies and modules to meet business needs.
• Works in an Agile environment and continuously reviews the business needs, refines priorities, outlines milestones and deliverables, and identifies opportunities and risks.
• Maintain, track and collaborate with dev teams to ensure project estimation for delivery.
• Experience using JIRA and Confluence, or similar User Story workflow and management tool is a must.
• Highlight the bugs and blockers and coordinate with the development and operations team to come up with the best solutions/fixes and document them.
• Work across internal team in various geo-locations across the world
• ?Drive For Results? - Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.
• ?Priority Setting? - Spends his/her time and the time of others on what’s important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.
• Weekly and Monthly status reporting to leadership.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Helius Technologies,Data Engineer,"Responsibilities
• Build, deploy and manage big data solutions that can adequately handle the needs of a rapidly growing data-driven company
• Participate in the development of systems, architectures, and platforms to scale Volume, Velocity, and Variety
• Help create a data lake
• Streamline data access and security to enable data scientists and analysts to easily access data
• Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources
• Maintain and optimize the performance of our data analytics infrastructure to ensure accurate, reliable and timely delivery of key insights for decision making
• Clean and normalize subsets of data of interest as preparatory step before deeper analysis
• Research, evaluate, and recommend new technology

Must Haves
• An engineering degree with a specialization in Computer Science or related
• Strong computer science fundamentals in data structures
• More than 2 years of hands-on experience, preferably in big data infrastructure
• Understanding of database performance and scaling
• Familiarity with best data engineering practices – including handling and logging errors, monitoring the system, building human-fault-tolerant pipelines, understanding how to scale up, addressing continuous integration, knowledge of database administration, maintaining data cleaning and ensuring a deterministic pipeline
• Possess good communication, sharp analytical abilities with proven design skills, able to think critically of the current system in terms of growth and stability
• Experience with continuous integration and deployment for big data or other projects
• Business-English fluency
• A “get-it-done"" mindset",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Merck Sharp & Dohme Corp,Lead Data Engineer,"Job Description

Data Lead Engineer

At our Company we are leveraging analytics and technology, as we invent for life on behalf of patients around the world. We are seeking those who have a passion for using data, analytics, and insights to drive decision making, that will allow us to tackle some of the world’s greatest health threats

Within our commercial Insights, Analytics, and Data organization we are transforming to better power decision-making across our end-to-end commercialization process, from business development to late lifecycle management. As we endeavor, we are seeking a dynamic talent for the role of Data Engineer

For the Data Engineer role, we are looking for professional with experience in designing, developing, and maintaining data pipelines. We intend to make data reliable, governed, secure and available for analytics within the organization. As part of a team this role will be responsible for data management with a broad range of activities like data ingestion to cloud data lakes and warehouses, quality control, metadata management and orchestration of machine learning models. We are also forward looking and plan to bring innovations like data mesh and data fabric into our ecosystem of tools and processes

Primary Responsibilities:
• Play a key role in the success and growth of the Data Engineering team by mentoring and playing a leadership role within the team
• Drive innovation within Data Engineering by playing a lead role in technology decisions for the future of our data science, analysis, and reporting needs
• Work with business partners and software engineers to gather, understand, and bridge definitions and requirements
• Lead the design and development for highly complex and critical data projects with strict timelines
• Improvements to team efficiency and effectiveness through implementation of data tools (self-service, data quality, etc.)
• Design, develop and maintain data pipelines to extract data from a variety of sources and populate data lake and data warehouse
• Develop the various data transformation rules and data modeling capabilities
• Collaborate with Data Analyst, Data Scientists, Machine Learning Engineers to identify and transform data for ingestion, exploration, and modeling
• Work with data governance team and implement data quality checks and maintain data catalogs
• Use Orchestration, logging, and monitoring tools to build resilient pipelines
• Use test driven development methodology when building ELT/ETL pipelines
• Understand and apply concepts like data lake, data warehouse, lake-house, data mesh and data-fabric where relevant
• Develop data models for cloud data warehouses like Redshift and Snowflake
• Develop pipelines to ingest data into cloud data warehouses
• Understand and be able to use different databases like Relational, Document, Graph and Key/Value
• Analyze data using SQL
• Use serverless AWS services like Glue, Lambda, Step Functions
• Use Terraform Code to deploy on AWS
• Containerize Python code using Docker
• Use Git for version control and understand various branching strategies
• Build pipelines to work with large datasets using PySpark
• Develop proof of concepts using Jupyter Notebooks
• Work as part of an agile team
• Create technical documentation as needed

Education:
• Bachelor’s Degree or equivalent experience in a relevant field such as Mathematics, Computer Science, Engineering, Artificial Intelligence, etc.

Required Experience and Skills:
• 6 to 8 years of relevant experience
• Good experience with AWS services like S3, ECS, Fargate, Glue, StepFunctions, CloudWatch, Lambda, EMR
• SQL
• Proficient in Python, PySpark
• Good with Git, Docker, Terraform
• Ability to work in cross functional teams

Preferred Experience and Skills
• Any AWS developer or architect certification
• Agile development methodology

Our Human Health Division maintains a “patient first, profits later” ideology. The organization is comprised of sales, marketing, market access, digital analytics and commercial professionals who are passionate about their role in bringing our medicines to our customers worldwide.

Who we are …
We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.

What we look for …
Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us—and start making your impact today.

We are proud to be a company that embraces the value of bringing diverse, talented, and committed people together. The fastest way to breakthrough innovation is when diverse ideas come together in an inclusive environment. We encourage our colleagues to respectfully challenge one another’s thinking and approach problems collectively for the common good. We are an equal opportunity employer, committed to fostering an inclusive and diverse workplace.

#HHIADIND

Current Employees apply HERE

Current Contingent Workers apply HERE

Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.

Employee Status:

Regular

Relocation:

Domestic

VISA Sponsorship:

Travel Requirements:

25%

Flexible Work Arrangements:

Remote Work

Shift:

Valid Driving License:

Hazardous Material(s):",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Multi National Company,Data Engineer,"Job Title: Data Engineer - Spark/Scala

Location: Bangalore / Chennai / Hyderabad

Company: MNC Company

Experience: 5.5 Years – 12 Years

Notice Period: Immediate to 15 Days.

We are seeking a highly skilled Data Engineer with experience in Spark and Scala. The ideal candidate will have a passion for solving complex data problems and be comfortable working with large and varied datasets.

Responsibilities:

· Developing Spark-based applications using Scala that can handle large volumes of data efficiently and reliably.

· Collaborate with cross-functional teams to define data requirements and ensure data quality.

· Optimize and tune Spark applications for performance.

· Monitor and maintain the performance of data pipelines and applications

· Develop and maintain documentation for Spark applications and procedures.

· Identify and implement improvements to Spark applications to improve efficiency and reliability.",Bengaluru,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False
Volvo India Limited,Diagnostics Data Engineer,"Diagnostic Engineer Can you help Volvo Group be the most desired, innovative and successful transport solution provider in the world Innovations within transport will reshape the cities of tomorrow. Automated driving, electromobility and connected vehicles will allow for quieter, cleaner and safer mega-cities to grow and prosper. For us in the Volvo Group, technology means the aspiration to move forward, constantly working together to improve and do better. It means creative and innovative thinking, evolutionary and revolutionary problem solving - all in order to reach our vision. We are looking for a Diagnostic Engineer to join our Aftermarket Technology Team in Bengaluru - is it you Diagnostics Engineers are at the forefront of the Aftermarket group's projects and participate in the mid to late design phases, strategic decisions impacting our products. A diagnostics engineer should generally have a basis in systems and vehicle engineering, powertrain engineering & E-Mob, with a focus on EE (HW, SW, schematics, etc.), but also insight in mechanics, pneumatics, etc. may be required. Focus on Automation, Programming, Analytics will be addon. A strong focus on resolving Customer Issues would be needed The person must be Able to design diagnostic strategies and advanced tools for use by our technicians Have experience within heavy duty vehicle repair and diagnostics with Engineering background To drive improvements and implement new concepts within technical projects Should have good people and communication skills needed to develop positive relationships within our team, and with colleagues keeping the customer in mind Needed competences: . The manufacturing and aftermarket processes needs in terms of diagnostics. . How a vehicle system may fail and understand how on- and off-board solutions could efficiently help in identifying the root cause. . The usage of diagnostic objects such as DTCs, DIDs, RIDs, snapshots, parameters, etc., on-board as well as from off-board perspective, and in the process. . Methods to interact with the vehicle in terms of fault tracing, parameters setting, measurements, etc.. Hands-on experience on Diagnostic Fault Tracing in the Vehicle . Methods to analyses system effects/symptoms from faults . Diagnostic communication, protocols and services . Standards and legislation relevant for diagnostics . Overall System requirements and concepts . Complete Vehicle Functions . Commercial Vehicle Knowledge . Working on Symptoms Based Diagnostics & Operations . Data Analysis & Customer Issues Resolution Knowledge on Quality Tools & KPI's & driving the same within the team. . Knowledge on Diagnostics Content authoring & process Knowledge on AGILE Principles & methodology - SAFe WoW is preferable . Knowledge on Programming (For Automations & Scripting's, Python, HTML5 ) is an addon . Knowledge on Vehicle Testing/RiG testing, working knowledge of electronics/component testing (CANoe, CANalyzer) would be an addon You need energy, passion and trust to join us in our journey to become the world leaders in sustainable transport solutions. As a Diagnostics Engineer within Aftermarket Technology, you will be part of a dynamic team. You will have an eye for detail to support project deliveries, analyze information and contribute to the design and strategy for technical solutions. Prioritization and pedagogical skills are also essential as you will be responsible for the aftermarket diagnostic flows during implementation of the diagnostic strategies The Volvo Group is one of the world's leading manufacturers of trucks, buses, and construction equipment, marine and industrial engines. Volvo Group Trucks Technology (GTT) provides Volvo Group Trucks with state-of-the-art research, cutting-edge engineering, product planning and purchasing services, as well as aftermarket product support. For any further details kindly contact: Hiring Manager: Priyanka Satheesh About us The Volvo Group is one of the world's leading manufacturers of trucks, buses, construction equipment and marine and industrial engines under the leading brands Volvo, Renault Trucks, Mack, UD Trucks, Eicher, SDLG, Terex Trucks, Prevost, Nova Bus, UD Bus and Volvo Penta. Volvo Group Trucks Technology provides Volvo Group Trucks and Business Area's with state-of-the-art research, cutting-edge engineering, product planning and purchasing services, as well as aftermarket product support. With Volvo Group Trucks Technology, you will be part of a global and diverse team of highly skilled professionals who work with passion, trust each other and embrace change to stay ahead. We make our customers win.",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
EPAM Systems,Lead Data Engineer,"Data Software Engineering, Amazon Web Services, Apache Airflow, Apache Spark, CI/CD, Python, SQL

India

We are looking for a remote Lead Data Engineer to join our Customer Success and Support (CSS) Data Engineering team and build world-class data solutions and applications that power crucial decisions throughout the organization. The ideal candidate has 5+ years of relevant work experience and 1+ years of relevant leadership experience. You will work remotely and be responsible for building ingestion pipelines, developing data models, and providing mentorship to junior team members.

responsibilities
• Develop and maintain data solutions and applications for the organization
• Design and implement ETL pipelines and ensure data integrity
• Provide mentorship and leadership to junior team members
• Build ingestion pipelines and manage data modeling
• Collaborate with Data Scientists to design and implement data models
• Participate in on-call support as needed
• 12/5 support mode is expected

requirements
• 5+ years of relevant work experience in Data Software Engineering
• 1+ years of relevant leadership experience
• High proficiency in SQL, with experience in Databrick SQL (DataBricks experience is nice to have)
• Extensive knowledge of Spark and Python (PySpark and Airflow)
• Experience with data integration and modeling
• Familiarity with big data, AWS suite, and Airflow
• Some experience with ETL pipeline building
• Experience with REST APIs
• B2+ English level
• Must be a team player with good communication and documentation skills
• Enthusiastic about learning new technology

nice to have
• Experience with varied forms of data infrastructure, including relational databases (e.g. SQL), Spark, and column stores (e.g. Redshift)

benefits for locations

India

For you
• Insurance Coverage
• Paid Leaves – including maternity, bereavement, paternity, and special COVID-19 leaves.
• Financial assistance for medical crisis
• Retiral Benefits – VPF and NPS
• Customized Mindfulness and Wellness programs
• EPAM Hobby Clubs

For your comfortable work
• Hybrid Work Model
• Soft loans to set up workspace at home
• Stable workload
• Relocation opportunities with ‘EPAM without Borders’ program

For your growth
• Certification trainings for technical and soft skills
• Access to unlimited LinkedIn Learning platform
• Access to internal learning programs set up by world class trainers
• Community networking and idea creation platforms
• Mentorship programs
• Self-driven career progression tool",,True,False,True,False,False,False,False,False,False,False,False,True,True,False,True,False
Boston Consulting Group,​IT Senior Data Engineer,"Under the general supervision of senior management and the Data Engineering Chapter Lead in the Enterprise Data Tribe, you will be working with key customers to deliver timely and accurate data engineering pipelines in a secure manner. You are expected to provide guidance on proper engineering design ensuring that our architectural guidelines are met, and the appropriate support model is in place for production deployments. This role will work in a multi-functional agile squad and support the product owner. You will also be supporting the Chapter Lead and other team members of the Data Engineering chapter in proof-of-concept activities and other Data Engineering chapter related work. You have experience in data warehousing, data modelling, and the building of data engineering pipelines. You are well versed in data engineering methods, such as ETL and ELT techniques through scripting and/or tooling. You are good in analysing performance bottlenecks and providing enhancement recommendations; you have a passion for customer service and a desire to learn and grow as a professional and a technologist.
• Viewed as subject matter expert for stakeholders, possessing in-depth knowledge and specialized technical skill set
• Able to work independently with minimal supervision
• Proactively identify and independently solve non-routine problems by applying expertise
• Perform research of viable technical and/or non-technical solutions
• Develop internal network with senior leaders within the chapter and key stakeholders in the tribe.
• Develop strategies for data engineering in Snowflake using DBT and Talend.
• Architect, design, and implement data pipelines to feed data models for subsequent consumption
• Actively monitor and resolve user support issues, working closely with your assigned squad and other squads as part of the chapter.
• Develop and maintain architectural standards, best practices, and measure compliance You bring to us experience in data engineering technologies, database development, and data model design; both in IaaS and PaaS Cloud (AWS and/or Azure) environments.
• Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
• 5-7+ years of project experience, preferably as a Data Engineer/Developer and minimum of 3 years of agile project experience is a must (preferred tool – JIRA)
• Essential: Must have exposure to technologies such as DBT, Talend and Apache airflow
• Essential: SQL is heavily focused. An ideal candidate must have hands-on experience with SQL database design
• Essential: Extremely talented in applying SCD, CDC and DQ/DV framework
• Essential: Experience in data platforms: Snowflake, Oracle, SQL Server, PostgreSQL, and MySQL
• Essential: Lead R&D efforts to find solutions for data engineering requirements not addressed by existing technology standards
• Essential: Demonstrate ability to write new code i.e., well-documented and stored in a version control system (we use GitHub & Bitbucket)
• Essential: Develop metrics that illuminate the flow of data across the organization
• Essential: Experience in data modelling and relational database design
• Preferred: Experience in AWS and Azure data platforms.
• Preferred: Experience in Qlik Compose, Fivetran and HVR
• Preferred: Strong programming/ scripting skills (Python, Powershell, etc.)",New Delhi,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
Brillio,GCP Data Engineer - R01523647,"About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

GCP Data Engineer
Primary Skills

• BigQuery, Cloud Logging, Cloud Storage, Cloud Trace, Composer, Data Catalog, Data Modelling Fundamentals, Data Warehousing, Dataflow, Datafusion, Dataproc, ETL Fundamentals, Modern Data Platform Fundamentals, PLSQL, T-SQL, Stored Procedures, Python, SQL, SQL (Basic + Advanced)

Specialization

• GCP Data Engineering Basic: Senior Data Engineer

Job requirements

• Job description below. • 6 years of experience in software design and development • 5 years of experience in the data engineering field is preferred • 3 years of Hands-on experience in GCP cloud data implementation suite such as Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage, • Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms. • Mandatory 1 year of software development skills using Python • Extensive hands-on experience working with data using SQL and Python • Cloud Functions. Comparable skills in AWS and other cloud Big Data Engineering space is considered. • Experience in DevOps(CI/CD) pipeline facilitating automated deployment and testing • Experience with agile development methodologies • Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions • Bachelor's Degree in Computer Science, Information Technology, or closely related discipline

Know what it’s like to work and grow at Brillio: Click here",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,True,False
ANI Calls India Private Limited,Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Mumbai, India
Candidate should have:
Clear, concise communication abilities - writing, verbal, presentation - to all levels of technical and non-technical audiences with the ability to collaborate
Strong experience in CICD technologies including Python, SQL, Airflow, and Git in a cloud setting
Understanding of ELK stack is a plus for logging and reporting purposes
Experience with pyspark is preferred and building API's
Hands-on Experience in AWS data related services
Strong Python and object-oriented development skills
Strong UNIX skills
Redshift/Netezza DBA expertise and understanding
Advanced competencies in data modeling, modern data architecture, data masking, and manipulation of structured and unstructured data sources using state-of-art cloud computing technologies (AWS)
Candidate should be able to:
.Migrate from one data warehouse to AWS Redshift along with ETL components
.Collaborate with data engineers, software engineers, business leaders, and cross-functional stakeholders to implement data engineering .solutions based on business priorities and technology initiatives
to ensure High Availability at enterprise scale.
.creating rigorous system at enterprise scale in a dynamic work setting
.Create automated data flows for production settings with enterprise-level best practices, transparency, and scalability.

Experience: 5.00-10.00 Years",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
"Atlassian, Inc.",Senior Data Engineer,"Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.",,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
Invsto,Data Engineer Intern (2024/2025 graduates),"You are
• A self-starter who is fueled by a desire to improve customer experiences in the moments that matter most, approaching your work with a bias toward accountability, decision-making and action
• Inquisitive and creative, with an ability to listen to identify customer needs and dig deeper to understand the reasons behind those needs
• Able to transform conceptual thinking into deliverables that generate excitement, feedback and alignment among stakeholders
• Thrive in a collaborative environment, partnering across the company with business experts, software developers, data engineers, and marketers

You have
• Enthusiasm to take the initiative to tackle problems and work with others to expand on your experience and expertise
• Experience with Python, AWS and databases such as Postgres
• Know-how to build data engineering pipelines using services such as Airflow

You will

Work closely with and learn from a team of engineers to contribute to a build a variety of features and infra.

Must-Have skills
• Python, Database experience
• Django (in lieu of Django, an equivalent tech stack)

Qualification and Experience:
• 0-2 Years software engineering
• Education: BE/BTech or equivalent (in lieu of academics, equivalent software developer experience required)

Benefits
• Fully remote, forever
• Annual retreats

About Us

Invsto is building the future of financial engineering.

We believe in hiring the best and providing complete autonomy to our employees to build stuff that they think would make a difference to the world

How to Apply

Does this role sound like a good fit? Email us at [hello@invsto.com].
• Include the role's title in your subject line.
• Send along links that best showcase the relevant things you've built and done (Github, Behance, Dribbble etc)

Invsto focuses on Hedge Funds and Stock Exchanges. Their company has offices in Bangalore Urban. They have a small team that's between 11-50 employees.

You can view their website at https://invsto.com/ or find them on LinkedIn.",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False
Domnic Lewis Private Limited,Big Data Engineer,"we are hiring for Big Data Engineer with the experience in spark , python , SQL , AWS glue Big Data Engineer: Spark, Python, SQL, AWS Cloud (Glue, Lambda, Athena) Hyderabad Location A big data engineer is an information technology (IT) professional who is responsible for designing, building, testing and maintaining complex data processing systems that work with large data sets.",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
10XTD,AWS Data Engineer,"AWS Data Engineer

Contract /Immediate

Must have

• Good Data Engineering pipeline design and implementation experience with AWS

• Very good hands-on experience in Data warehouse design and implementation

• Very good experience with

• AWS –

• AWS Glu –

• AWS RDS, EC2 –

• SQL –

• Python-

• Spark –

• Redshift –

• IAM and IAC -

• VPC -

• SSO, networking -

• DevOps -

• CI/CD -

Desired Persona

• 6 years plus

Considerations
• Immediate
• Monday to Friday
• 2 PM – 11 PM
• 6 Months

Immediate

Interested Profiles,

Do write to frontoffice@10XTD.in with the Subject – AWS Data Engineer – Provide inputs to the following• Expertise Level <<- Beginner / Intermediate / Experienced/ Expert >>-

• AWS –

• AWS Glu –

• AWS RDS, EC2 –

• SQL –

• Python-

• Spark –

• Redshift –

• IAM and IAC -

• VPC -

• SSO, networking -

• DevOps -

• CI/CD -

• Monthly Rate for 160 Hours -

• Availability to engage Immediate /within 1 week -

• C.V for reference

Rating key:

Beginner / Intermediate / Experienced/ Expert Ratings

Beginner - less than 6 months live project working experience

Intermediate - atleast 1 live project experience + 1 Yr Plus

Experienced - atleast 2 live projects experience + 2 Yr Plus

Expert - 5 Yrs Plus experience

Along with the rating, please highlight the live project from their CV. where the skill was leveraged",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
Inimitable Solutions,Azure Data Engineer,"Notice- Immediate to 15 days.

Azure Data Engineer with Azure Synapse Analysis.

Exp in Spark is mandatory. Data Engineer with Azure Synapse Analytics by using spark.
• Assure that data is cleansed, mapped, transformed, and otherwise optimised for storage and use according to business and technical requirements
• * Develop and maintain innovative Azure solutions
• * Solution design using Microsoft Azure services and other tools
• * The ability to automate tasks and deploy production standard code (with unit testing, continuous integration, versioning etc.)
• * Load transformed data into storage and reporting structures in destinations including data warehouse, high speed indexes, real-time reporting systems and analytics applications
• * Build data pipelines to collectively bring together data
• * Other responsibilities include extracting data, troubleshooting and maintaining the data warehouse",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Hewlett Packard Careers,Cloud Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.

We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

Job Description

Responsibilities:
• Works with the software engineering team to identify and implement the most optimal software solutions for the organization.
• Plans, designs, and develops software applications.
• Manages software development projects in accordance with the organization's security guidelines.
• Deploys and debugs software initiatives as needed in accordance with best practices throughout the software development lifecycle.

Knowledge & Skills:
• Strong technical knowledge of complex software systems, including enterprise, middleware, firmware, and embedded systems.
• Adept at collecting and analyzing input from multiple sources (customers, sales, engineering, marketing, competitor, and management).
• Significant experience in software development, systems engineering, software General. Experience specifically in deploying, optimizing, and maintaining software preferred.
• Using software applications design tools and languages.
• Strong analytical and problem solving skills.
• Designing software applications running on multiple platform types.
• Software applications testing methodology, including writing and execution of test plans, debugging and testing scripts and tools.
• Excellent written and verbal communication skills; mastery in English and local language; ability to effectively communicate product architectures, design proposals and negotiate options at management levels.

Scope & Impact:
• Decisions and actions typically impact multiple departments and may have downstream repercussions lasting multiple weeks.
• Advises various internal, partner, and customer teams on the needs and requirements of target markets.
• Works closely with architects and technology leads, directly engaging with internal and external software development teams.

Complexity:
• Moderate: Some senior management interface; frequent cross-functional/cross-organizational interaction on execution of tactics.
• Applies developed subject matter knowledge to solve common and complex business issues within established guidelines and recommends appropriate alternatives. Works on problems of diverse complexity and scope. May act as a team or project.
• Leader providing direction to team activities and facilitates information validation and team decision making process. Exercises independent judgment within generally defined policies and practices to identify and select a solution.

Education & Experience:
• Bachelor's degree in relevant area or demonstrated competence. Typically 4-8 years of General/owner experience.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Impetus Technologies India Pvt. Ltd,GCP Data Engineer,"Role : GCP Data Engineer

Job Description :

- The candidate should have extensive production experience in GCP, Other cloud experience would be a strong bonus.

- Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.

- Exposure to enterprise application development is a must.

Roles & Responsibilities :

- 6-10 years of IT experience range is preferred.

- Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.

- Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOps.

- Good hands on expertise on either Python or Java programming.

- Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.

- Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.

- Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.

- Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.

- Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.

- Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.

- Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.

,",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
ANI Calls India Private Limited,Lead Snowflake Data Engineer,"Anicalls

Industry : IT

Total Positions : 3

Job Type : Full Time / Permanent

Gender : No Preference

Salary : 900000 INR - 1800000 INR (Annually)

Education : Bachelor s degree

Experience : 5-10 Years

Location : Noida, India

Candidate should have :

Knowledge and experience in Big data and Data Vault methodology

Experience in power shell, shell scripting, and python

Exposure to data modeling for Snowflake

Experience in working with agile / scrum methodologies.

Experience in building data pipelines for large volumes of data across disparate data sources

Experience in DBT for Snowflake

Good experience on Azure Databricks

experience in Confluent cloud platform

Experience in building pipelines through Confluent Kafka and Knowledge of Azure Kubernetes Service

Good communication and presentation skills

Expertise in building data pipelines for Snowflake using Snowpipe, Snowpark, SnowSQL's and stored procedures.

Azure experience must be focused on Azure Data Factory, Azure storage solutions (such as Blob and Azure Data lake Gen2), and Azure data pipelines

Good experience on Snowflake and Snowflake architecture

7+ years of total experience in data projects with a focus on data integration and ingestion

3+ years of experience working primarily on Snowflake",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Avalara,Senior Cloud Data Engineer,"Overview:

Overview

We are building cloud-based tax compliance solutions to handle every transaction in the world. Imagine every transaction you make - every tank of gas, cup of coffee, or pair of sneakers, every movie ticket, or streamed song, every sensor-to-sensor ping. Nearly every time you make a purchase, physical or digital, there is an accompanying unique and nuanced tax compliance calculation.

Responsibilities:

Responsibilities

The Senior Cloud Data Engineer will be responsible for gathering business requirements, understanding complex product, business and engineering challenges, compose and prioritize research projects and then execute them in partnership with other data scientists, data analysts as well as leverage the work of our data engineering team.

The individual will have direct ownership of corporate metrics, ensure accuracy, timeliness and intent of all published data products within area of ownership by our team and all other teams within the company. Will also have executive visibility and influence capability.
• Learn the tax business domain knowledge and data.
• Work with the product managers, DBA teams and broader engineering teams build scalable data orchestration, transformation and reporting streams that can capture and prepare billions of transactions per day for customer reporting.
• Build strong and captivating data visualizations in PowerBI/Tableau/R.
• Develop and manage end-to-end project plans and ensure on-time delivery.
• Communicate status and big picture to the project team and management.
• Work with business and engineering teams to identify scope, constraints, dependencies, and risks.
• Identify risks and opportunities across the business and drive solutions.

Qualifications:
• 5 to 8 years of experience in software engineering
• Minimum of 3 years combined work experience in data engineer, business intelligence and in-app embedded analytics
• Strong experience with cloud data engineering teams
• Advanced SQL development, scripting (Python or R) and advanced data visualization experience
• Implement streaming and batch ETL pipelines supporting both internal and external data sources
• Data warehouse modeling and architecture with strong requirements gathering and technical analysis
• Technical expertise with AWS, DBT, and Airflow
• Bachelor's degree in Computer Science or Engineering
• Experience with requirements of data security, data privacy and legal / jurisdictional compliance and governance
• Experience managing efforts in distributed systems and/or developing large scale web applications
• Proven ability to combine business acumen, technical acumen and process expertise to define client (internal/external) engagement and program execution
• Ability to communicate effectively with technical and non-technical stakeholders across multiple business units
• Strong communication skills, data driven product and corporate excellence culture
• Excellent problem-solving skills

Preferred Skills
• Minimum of 5 years work experience with AWS services, data pipeline and Snowflake cloud DW
• Experience building powerful dashboards which are embedded in web and mobile applications

About Avalara:

About Avalara:

We’re building cloud-based tax compliance solutions to handle every transaction in the world. Imagine every transaction you make — every tank of gas, cup of coffee, or pair of sneakers, every movie ticket, meal kit, or streamed song, every sensor-to-sensor ping. Nearly every time you make a purchase, physical or digital, there’s an accompanying unique and nuanced tax compliance calculation. The logic behind calculating taxes — the rules, rates, and boundaries is a global, layered, three-dimensional mess of complexity, with compliance dictated by governments and applied by every business, every day.

Avalara works with businesses of all sizes, all over the world — from corner stores to gigantic global retailers — to calculate tax accurately and automatically, at speeds measured in milliseconds. That’s a massive technical challenge, in terms of scale, reliability, and complexity, and we do it better than anyone. That’s why we’re growing fast. Headquartered in Seattle, Avalara has offices across the U.S. and around the world, in Brazil, Canada, India, U.K, Belgium and across Europe.

Equal Opportunities:

Avalara is an Equal Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, colour, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law.",,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,True
Apple,Cloud Data Engineer,"Summary

The people here at Apple don’t just build products — they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Imagine what you could do here. Are you passionate about handling large & complex data problems, want to make an impact and have the desire to work on groundbreaking big data technologies? Then we are looking for you. At Apple, phenomenal ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Business Intelligence team is looking for passionate, technical savvy, energetic leader who like to think creatively. Someone who is self motivated and ready to lead team of most hardworking engineers building high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse team deals with Petabytes of data catering to a wide variety of real- time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We leverage a diverse technology stacks such as Snowflake, AWS, Teradata, HANA, Vertica, Single Store, Dremio, Hadoop, Kafka, Spark, Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.

Key Qualifications

High expertise in modern cloud data lakes and implementation experience on any of the cloud platforms like AWS/GCP/Azure - preferably AWS.

Good Experience in cloud based data warehouse - Snowflake.

Hands on Experience in developing and building data pipelines on Cloud & Hybrid infrastructure for analytical needs- Preferably having Cloud certifications.

Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.

Database development experience with Relational or MPP/distributed systems such as Teradata/ SingleStore/ Hadoop

Experience working with data at scale (peta bytes) with big data tech stack and sophisticated programming languages is a plus e:g Python, Scala.

Description

As a Cloud Development Engineer you will design, develop and implement modern cloud data warehouse/ datalakes and influence overall data strategy for the organization

Translate sophisticated business requirements into scalable technical solutions meeting data warehousing/analytics design standards

Strong understanding of analytics needs and proactive-ness to build solutions to improve the efficiency along with that help execute leading data practices & standards

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business

Ability to communicate effectively, both written and verbal, with technical and non- technical multi-functional teams

You will engage with many other group’s & internal/external teams to deliver best-in-class products in an exciting constantly evolving environment

Education & Experience

Bachelors or Masters Degree in Computer Science or equivalent in Engineering.

Role Number: 200154652",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
Tata Consultancy Services,GCP Data Engineer,"Role: GCP Data Engineer

Exp: 2 to 10 years

Location: Pan India

Job Description:
• Good Experience in GCP
• GCP BigQuery
• Secondary Skill - DataFlow, Compute Engine, Cloud Fusion",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
Randstad India,Advanced Data Engineer,"• $+years ETL , SQL, pl/SQL, development, Unit testing and deployment
• Need to be expert in SQL and ETL mapping design and development, follows best practices for coding and performance tuning and iterative development approach (Must Have)
• Strong technical skills in Database and scripting ( PL/SQL, Oracle, SQL, and Stored procedures )
• Very Strong understanding and exposure on Data warehousing concepts. The developer needs to be familiar with agile development practice.
• Should have good experience in application of standard software development principles.

experience

6",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Encora Innovation Labs India Private Limited,Data Engineer,"Job Title : Data Engineer

Experience : 6-15 Years

Location : PAN India

Job Description : Â
• 4+ years of industry experience
• Proficiency in Hadoop Stack (Hadoop, HDFS, Hive, Spark, Scala) and / or other BIG Data technology.
• Demonstrated expertise with SQL Database design and implementations
• Experience working with Apache Hadoop, and ability to perform basic Hadoop Administration tasks
• Ability to design data model and prior expertise in Database (RDBMS), or BI / DW
• Demonstrated expertise with software architecture & design, Data structures and Algorithms
• Demonstrated expertise in Agile software development methodology

Notice period : Immediate to max 1 month

CTC -30LPA-40LPA",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Qualcomm,Sr Lead/Staff Engineer- /Infra Data Engineer,"Company:
Qualcomm India Private LimitedJob Area:
Engineering Group, Engineering Group Software Engineering
General Summary:
General Summary:
Qualcomm is hiring multiple software engineers at Senior and Staff level to help architect and build its next generation data processing platform to support Autonomous Driving R&D efforts. Our goal is to design and build a highly scalable, efficient, and modular data platform. This platform will be used by engineers to run re-simulation pipelines, machine learning workloads, perform in-depth data analysis/analytics, visualize results, and more. Excellent communication and planning skills are critical in this role as we'll be working with internal teams and external partners.
Responsibilities for this position include:
Work with team leads to understand use cases and requirements
Build proof-of-concepts to validate proposed designs and provide feedback
Implement data management systems, data pipelines, and highly scalable distributed processing services used for autonomous driving research and development
Drive software engineering best practices within immediate and external teams
Support users of the platform
Minimum Qualifications:
. Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.
OR
Master's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.
OR
PhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.
. 2+ years of experience with Programming Language such as C, C++, Java, Python, etc.
Preferred Qualifications:
3 to 20 years of relevant experience in a software development role (or equivalent).
Industry experience in designing and implementing scalable solutions used for Autonomous Driving R&D applications.
Experience with building data pipelines and workflow management tools such as Airflow, Prefect, Argo or Cloud native technologies like AWS Batch, Azure Data factory.
Familiarity using a programming language such as Python, Go, C/C++, Java, or Scala
Hands-on experience using managed services from one or more of the major cloud vendors: AWS, GCP, Azure
Experience working with distributed processing frameworks such as Spark, Hadoop, Hive, or other Apache Foundation frameworks.
Understanding of RDBMS, NoSQL DB technologies, and data warehousing solutions and tradeoffs
Experience in Cloud technologies like S3, databases and so on.
Experience in designing, developing, testing, and deploying data integrations in the cloud.
Experience with containerized platforms like docker and Kubernetes.
Minimum Qualifications:. Bachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.
OR
Master's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.
OR
PhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.

. 2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.

Applicants: If you need an accommodation, during the application/hiring process, you may request an accommodation by sending email to

Although this role has some expected minor physical activity, this should not deter otherwise qualified applicants from applying. If you are an individual with a physical or mental disability and need an accommodation during the application/hiring process, please call Qualcomm's toll-free number found for assistance. Qualcomm will provide reasonable accommodations, upon request, to support individuals with disabilities as part of our ongoing efforts to create an accessible workplace.
Qualcomm is an equal opportunity employer and supports workforce diversity.
To all Staffing and Recruiting Agencies:Our Careers Site is only for individuals seeking a job at Qualcomm. Staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. Qualcomm does not accept unsolicited resumes or applications from agencies. Please do not forward resumes to our jobs alias, Qualcomm employees or any other company location. Qualcomm is not responsible for any fees related to unsolicited resumes/applications.
If you would like more information about this role, please contact .

Experience: 4.00-7.00 Years",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,True,False
Visa,Staff Data Engineer,"Job Description We are looking for an expert with deep expertise in big data/data warehousing and who has experience in working with clients in implementation of data engineering pipelines, data exchange mechanisms and data science models. The role will need design and architecture skills to advice VISA's clients on best approaches for data engineering and data science model implementations. The candidate will also play critical role in enabling the internal data platforms using which Data Scientists, Analysts, and BI Users drive solutions for Visa clients. The candidate is expected to act as a bridge between end-users in Asia and Visa Technology colleagues in San Francisco, influencing the development of our global data platforms whilst provisioning local tools and technologies as required. This is a hands-on role, and the candidate is expected to work hands on in areas such as Hadoop, Spark, Python, Tableau, Unix scripting. We are looking for a talented, technical, proactive, energetic, and passionate person who embraces challenges and is a proven problem solver. Principal Responsibilities Provide advisory and implementation support to VISA's clients for deployment of data engineering pipelines and data science models. Work with VISA data scientists for optimization and automation of data science models. Support data exchange channels integration (Such as APIs, Secured File Transfer, other custom data exchange capabilities provided by VISA) between VISA and clients. Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack Provide leadership and support in identifying, implementing, and managing new data tools and processes, relevant to data science - on premise and cloud. Partner with Technology on quarterly planning cycle and support management with relevant metrics to evaluate performance, stability, and reliability of various tools. Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions. Develop custom-built packages to support the needs of Data Scientists across the region. Continuous focus on improving Infrastructure efficiency by analyzing logs of queries, tuning settings, translating queries if required. Review scripts and educate users by building training assets for beginner and intermediate level users. Policy guidance and administration of tools used by data scientists including git, ETL schedulers, Spark, Tableau etc. to ensure optimum utilization and efficiency Qualifications Basic Qualifications Minimum of bachelor's degree or equivalent Qualification in Computer Science or Engineering ideal. Professional Experience 8 - 12 years of relevant experience Deep knowledge of distributed data architecture, commonly used BI tools, and approaches/packages used in machine learning build Expertise in Design and architecture of big data platforms, data science and visualization platforms. Expertise in creating production software/systems using Python and/or Scala, and a proven track record of identifying and resolving performance bottlenecks in production systems. Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets. Good appreciation of machine learning algorithms, feature engineering, validation, prediction, recommendation, and measurement. Experience of working in multiple large projects with diverse cross-functional teams. Ability to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere. Understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant Demonstrated ability to incorporate new techniques to solve business problems Demonstrated resource planning and delivery skills Good communication and presentation skills with ability to interact with different cross-functional team members at varying level Technical Expertise Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark preferred. Expertise in dashboard and report development in Tableau Experience in coming up with data flow patterns and deployment architecture for data engineering pipeline and data science models Experience with APIs, container-based software deployments Experience in developing production systems incorporating best-in-class software engineering practices such as DevOps, Agile development, CI/CD, and scheduling technologies Working knowledge of Hadoop ecosystem and associated technologies such as Hive, Apache Spark, PySpark, MLlib, GraphX. Advanced experience in writing and optimizing efficient SQL queries and Python scripts Scalaexperience will be an added advantage. Good appreciation of cloud-based technologies and data platforms Very strong people/technology project management skills and experience Business and Leadership competencies Results-oriented with strong problem-solving skills and demonstrated intellectual and analytical rigor Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred Team oriented, collaborative, diplomatic, and flexible Detailed oriented to ensure highest level of quality/rigor in reports and data analysis Proven skills in translating analytics output to actionable recommendations and delivery Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels Exhibits intellectual curiosity and a desire for continuous learning Demonstrates integrity, maturity, and a constructive approach to business challenges Role model for the organization and implementing core Visa Values Respect for the Individuals at all levels in the workplace Strive for Excellence and extraordinary results Use sound insights and judgments to make informed decisions in line with business strategy and needs Ability to allocate tasks and resources across multiple lines of businesses and geographies. Ability to influence senior management within and outside Analytics groups Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions Additional Information Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,True,False,True,False,False,False,False,True,False,True,False,True,False,False,False,False
Virtusa,Data Engineer - Informatica BDM,"Role: Data Engineer

Experience: 5+ Years

Job Location: Dubai

JOB Description:

· Should have Strong expertise of Extraction, Transformation and Loading (ETL) mechanism using Informatica Big Data Management 10.2.X and various Push down mode using Spark, Blaze and Hive execution engine.

· Should have Strong expertise of Dynamic mapping Use case, Development, Deployment mechanism using Informatica Big Data Management 10.2.X.

· Should have experience on transforming and loading various Complex data sources types such as Unstructured data sources ,No SQL Data Sources.

· Should have Strong expertise of Hive Database including Hive DDL, Partition and Hive Query Language.

· Should have Good Understanding of Hadoop Eco system (HDFS, Spark, Hive).

· Should have Strong expertise of SQL/PLSQL.

· Should have Good knowledge on working with Oracle/Sybase/SQL Databases.

· Should have Good knowledge of Data Lake and Dimensional data Modelling implementation.

· Should be able to understand the requirements and write Functional Specification Document, Design Document and Mapping Specifications.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Bajaj Finance Limited,Senior Data Engineer,"Job Summary

. Senior Data Engineer
Job Purpose

'This position is open with Bajaj Finance ltd.'
Job Duties & Key Responsibilities

. Duties and Responsibilities - . Would be required to understand technical logic from Sr. Manager and perform development accordingly. . Carry out all development related to portals and other applications. . Identify and resolve production issues. . Follow the processes given and make sure junior team members follow the same. Suggestive Technology Must Have: Microsoft SQL Server 2012 or above Nice to have: Oracle PL/SQL Competencies Agility Strong Result orientation Customer focus Good Team player

Required Qualifications and Experience

. . Minimum qualification required is Science Graduate with minimum of 2-4 years of experience in Microsoft SQL Server. . Should have good analytical and problem solving skills. . Should have good communication skills. . Should have strong code debugging qualities.

Experience: 2.00-4.00 Years",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Varite India Private Limited,Data Engineer,"Job Description: Basic qualifications Degree in Computer Science, Engineering, Mathematics, or a related field or 5+ years of industry experience. 5+ years of experience with demonstrated strength in ETL/ELT, modeling, warehouse technical architecture, infrastructure components and reporting/analytic tools. 5+ years of hands-on experience in writing complex, highly optimized SQL queries across large sets. 3+ years of experience in scripting languages like Python etc. Preferred Qualifications Industry experience as an Engineer or related specialty (e.g., Software Business Intelligence Scientist) with a track record of manipulating, processing, and extracting value from large sets. Experience building/operating highly available, distributed systems of extraction, ingestion, and processing of large sets. Experience building products incrementally and integrating and managing sets from multiple sources. Experience leading large-scale warehousing and analytics projects, including using AWS technologies Redshift, S3, EC2, etc. Experience providing technical leadership and mentor other engineers for the best practices on the engineering space . Linux/UNIX. Experience with AWS . A desire to work in a collaborative, intellectually curious environment.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
Prodapt,GCP - Data Engineer,"• Primary Skills :

Google Cloud Dataflow, Big Data, Hadoop Ecosystem
• Secondary Skills :

SQL
• Degree :

B.E-Bachelor Of Engineering, MCA - Master Of Computer Application, MS-Master Of Science

Responsibilities
• Build and maintain data management workflows. Build and maintain Data ingestion pipelines for batch, micro-batch and real time streaming on big query with Google Cloud
• GCP Certified developer on Biq Query and Data Proc
• Experience in building Data ingestion pipelines for batch, micro-batch and real time streaming on big data/Hadoop platforms
• Hands on experience on Hadoop big data tools – HDFS, Hive, Presto, Apache Nifi, Sqoop, Spark, Log Stash, Elastic Search, Kafka & Pulsar
• Experience in collecting data from Kafka/Pulsar message bus and transporting the data to public/private cloud platform using NiFi, Data Highway and log stash technologies
• Experience in building CI/CD pipeline & Dev Ops is preferred
• Development experience with Agile Scrum/Safe methodology

Requirements

Bachelor's degree in computer science, or any engineering discipline",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
ANI Calls India Private Limited,Azure ADF Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Chennai, India
Candidate should be able to:
Create and review architecture and solution design artifacts
Enforce adherence to architectural standards/principles, global product-specific guidelines, usability design standards, etc.
Design and implementation of modern data solutions( like Data Lake, Data warehouse, Real-time data ingestion, and analytics)
Evaluate and recommend tools, technologies, and processes to be used by the engineering team, to meet both functional non-functional requirements in the product. Running and managing these spikes within the sprint.
analyze and understand complex data
Participate and help in creating Coding standards and Best practices for a robust solution.
Identify, communicate and mitigate Risks, Assumptions, Issues, and Decisions throughout the full lifecycle
Architect and design solutions to meet functional and non-functional requirements
Candidate should have:
Excellent interpersonal/communication skills (both oral/written) with the ability to communicate at various levels with clarity & precision.
Knowledge of Azure data lake is required
In-depth skills with Azure Data Factory, Azure Synapse, ADLS with the ability to configure and administrate all aspects of SQL Server.
Demonstrated experience delivering multiple data solutions as an architect.
Excellent technical architecture skills, enabling the creation of future-proof, complex global solutions
Experience in Architecture, Azure Storage, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service.
known at least one programming language
Expert level knowledge of SQL DB & Data warehouse
Expert-level knowledge of Azure Data Factory.

Experience: 5.00-10.00 Years",,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
KPMG India,Data Engineer- Azure,"About KPMG in India
• KPMG entities in India are professional services firm(s). These Indian member firms are affiliated with KPMG International Limited. KPMG was established in India in August 1993. Our professionals leverage the global network of firms, and are conversant with local laws, regulations, markets and competition. KPMG has offices across India in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.
• KPMG entities in India offer services to national and international clients in India across sectors. We strive to provide rapid, performance-based, industry-focused and technology-enabled services, which reflect a shared knowledge of global and local industries and our experience of the Indian business environment.

Job Description

Data Engineer- Azure

Level: Consultant/Assistant Manager/Manager

Location- Bangalore

Role & Responsibility
• Evaluating, developing, maintaining and testing data engineering solutions for advanced analytics projects.
• Implement processes and logic to extract, transform, and distribute data across one or more data stores from a wide variety of sources
• Distill business requirements and translate into technical solutions for data systems including data warehouses, cubes, marts, lakes, ETL integrations, BI tools or other components.
• Strong knowledge of replication processes, change data capture processes, T-SQL, and Dynamic SQL skills
• Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.
• Optimize data integration platform to provide optimal performance under increasing data volumes
• Support the data architecture and data governance function to continually expand their capabilities
• Support the automation of requirements of database deployments using Azure DevOps
• Experience in development of Solution Architecture for Enterprise Data Lakes (applicable for AM/Manager level candidates)
• Should have exposure to client facing roles
• Strong communication, inter-personal and team management skills

Desired Skills
• Proficient in any object-oriented/ functional scripting languages: Java, Python, C# etc.
• Design and develop new solutions on the Azure Cloud Platform specifically for Azure SQL Data Warehouse/DB with pyspark, Azure Data Factory, Analysis Services, Azure Data Lake Store / Blob
• Hands-on experience working in complex data warehouse implementations using Azure SQL Data warehouse, Azure Data Factory and Azure SQL Database.
• Strong t-SQL skills with experience in Azure SQL DW
• Experience in creating data pipelines using Azure Data Factory, Polybase and U-SQL.
• Experience in developing data pipelines to transform, aggregate and or process data using Azure Databricks platform
• Hands-on experience in implementing Azure Cloud data warehouses, Azure and No-SQL databases and hybrid data ingestion scenarios
• Experience in creating tabular models (DAX) in Visual Studio.

Qualification
• BE/BTech/MCA
• 4+ years- 12 years of strong experience in ADLS, ADF, Databricks with PySpark, SQL. Synapse (Good to have)",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Icicle Technologies,Big Data Engineer,"Icicle is looking for an experienced Data Engineer proficient with AWS Cloud infrastructure for developing, deploying, and supporting our internal business applications. In this project, you get the opportunity to work for a large Japanese enterprise.

We as a team consist of bright young designers, hackers, senior developers, startup specialists. We build Web & Mobile products for Startups, Brands, Digital Agencies and Enterprises, helping in bringing their ideas to life.

We are distributed with offices in Mumbai, Bangalore and New York.

What will you do
• Work for managing the successful design, execution, and measurement of data initiatives
• Define problems and opportunities in a complex business area
• Create and develop end-to-end data driven solutions
• Measure the impact of the products developed
• Track all details in the issue tracking system (JIRA)
• Implementing data enrichment and cleansing routines
• Executing features, preparing modelling data sets, feature selection, etc.
• Ensuring proper runtime deployment of models
• Attend meetings, submit work progress reports, and perform related duties as required.
• Work with the team to evaluate and make strategic decisions that will address specific technology design needs

What are we looking for
• Overall 4+ years of experience of IT experience with an Engineering degree
• Developing database scripts using MySQL/Postgres/DynamoDB etcProgramming language experience with Python or Java.
• Knowledge of cloud environments such AWS S3, CloudWatch, CloudTrail, Lambda etc is preferred.
• Debugging skills on JAVA, experience on DevOps tools, CICD pipelines would be an added advantage
• Proven ability to solve complex quantitative business challenges
• Excellent written and verbal communication skills

Technical Expertise

Languages: Python /NodeJS/Java

AWS Cloud services (EMR, Kinesis, S3, Lambda, API GW, Advanced IAM etc)

Why Icicle
• 100% work-life balance
• Competitive salary
• Exposure to futuristic technology, mentally stimulating work environment.
• Opportunity to work with a big team of experienced, committed, talented people with a great sense of humour.
• Stability of a legacy company and agility of a startup - best of both the world.
• A healthy work environment with no politics, no bureaucracy.
• Encouraged to learn new skills, grow together, not just maintain the status quo

So, if you have 5+ years of experience as a Big Data Engineer, apply today!

Icicle Technologies focuses on CRM, Sales Automation, Web Development, and Mobile Application. Their company has offices in India and United States. They have a small team that's between 11-50 employees.

You can view their website at https://www.icicletech.com/ or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Tata Group,Data Engineer with Celonis Tool,"Job description
• Primary focus on Process mining and providing analysis
• Assist in the delivery of quality solutions
• Work collaboratively with team members including Project Manager, Process/domain consultant and IT consultants
• Leverage database systems including data warehouses, data marts, and models needed for business, financial, and operational analysis
• Connecting to data sources, importing data and transforming data for Data Mining process
• Develop high performing, reliable, and scalable solutions

Skills
Technical Skills:
• Minimum 4-5 years in any of Database technologies (preferably from Software Development project) (Must)
• Experience with one or more data technologies (e.g., Oracle, SQL Data Warehouse, SQL Server, and Peoplesoft etc.) and willingness to learn how to leverage these technologies to improve process data mining skills
• Excellent Hands-on experience in MSSQL
• Good knowledge of Python/ R Programming

Soft Skills:
• Excellent communication and customer management abilities
• Eager to contribute to a team-oriented environment
• Good communication and ability to independently handle technical discussions with stakeholders
• Able to work creatively and analytically in a problem-solving environment

Good to Have:
.

Experience: 4.00-5.00 Years",,True,True,True,False,False,False,False,True,False,False,False,False,False,False,False,False
BT Group,2nd Line Data Engineer,"Job Req ID:

15452

Posting Date:

02-May-2023

Function:

Service

Location:

Candor TechSpace, Tikri, Secto, Gurugram, India

Salary:

within range

Why this job matters

The Network Engineering Professional supports the delivery of engineering activities that contributes client technical requirements, deploying optimal networking and connectivity solutions that enable clients and the enterprise to achieve their operational and business goals.

What You’ll Be Doing

1 - Supports the delivery of routine daily activities and is accountable for system design, build, testing, validation, maintenance, and ongoing support of all network infrastructure components.

2 - Assists the implementation of enhanced network technology that aligns to customers' strategic direction for enterprise and remote site connectivity.

3 - Follows standards for global network infrastructure, including wireless, LAN and SD-WAN networks and connectivity.
• Supports the installation, testing, and setup of new network hardware both physical and virtual (firewalls, routers, switching, monitoring) hardware and software.
• Executes engineering efforts to ensue currency and supportability of networking technology.
• Collates data, reports and information supporting technology lifecycle planning, including contributing to the development of the technology roadmaps and Network Health Assessments.
• Organises material and data for documentation, knowledge transfer and training to successfully land new solutions into the support organisation.
• Undertakes activities that contribute to the implementation of core and cloud infrastructure security to manage risks and exposure.
• Supports in the implementation of ways to improve working processes within Network Engineering.

The Skills You’ll Need

Troubleshooting

Customer Service

Escalation Management

Continuous Improvement

Health & Safety

Network Delivery

Network Security

Network Testing

Network Configuration

Technical Documentation

Network Integration

Event Management

Network Implementation

Requirements Management

Incident Management

Our leadership standards

Looking in:

Leading inclusively

I inspire and build trust through self-awareness, honesty and integrity.

Owning outcomes

I take the right decisions that benefit the broader organisation.

Looking out:

Delivering for the customer

I execute brilliantly on clear priorities that add value to our customers and the wider business.

Commercially savvy

I demonstrate strong commercial focus, bringing an external perspective to decision-making.

Looking to the future:

Growth mindset

I experiment and identify opportunities for growth for both myself and the organisation.

Building for the future

I build diverse future-ready teams where all individuals can be at their best.",New Delhi,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Lilly,Senior Data Engineer,"u2022u00A0 Job Summaryu00A0 Business Units Information and Digital Solutions (IDS) is a global organization strategically positioned so that through information and technology leadership and solutions, we create meaningful connections and remarkable experiences, so people feel genuinely cared for. The Business Unit IDS organization is accountable for designing, developing, and supporting commercial or customer engagement services and capabilities that span multipleu00A0Buinessu00A0Units (Bio-Medicines, Diabetes, Oncology, International), functions, geographies, and digital channels. The areas supported by Business Unit IDSu00A0includes:u00A0Customer Operations, Marketing and Commercial Operations, Medical Affairs, Market Research,u00A0Pricing, Reimbursement and Access, Customer Support Programs, Digital Production and Distribution,u00A0Global Patient Outcomes, and Real-World Evidence. u00A0u00A0 This position isu00A0responsibleu00A0for providing supportu00A0foru00A0ETL / ELT / File Movementu00A0of data.u00A0u00A0 Theu00A0key responsibilities will be to processu00A0and move data between different compute and storage services, as well as on-premises data sources at specified intervals.u00A0The employee will also be responsible foru00A0the creation, scheduling,u00A0orchestrationu00A0and management of data pipelines.u00A0 u2022u00A0 Competency Data Engineer Data engineers are responsible for ensuring the availability and quality of data needed for analysis and business transactions. This includes data integration, acquisition, cleansing, harmonization and transforming raw data into curated datasets for data science, data discovery, and BI/analytics. Responsible for developing, constructing,u00A0testingu00A0and maintaining data sets and scalable data processing systems.u00A0 Data engineers work closest with Data Architects and Data Scientists. u00A0They also work with business and IT groups beyond the data sphere, understanding the enterprise infrastructure and the many source systems. Input is raw datasets. u00A0Output is analytics-ready, integrated/curated datasets. u00A0 Key capabilities in this role family include: u2022u00A0Data Acquisitionu00A0- u00A0isu00A0the process of gathering and storing data in a location and format that it can be consumed for data preparation and/or downstream business uses. u2022u00A0Data Preparationu00A0-u00A0isu00A0an iterative process for exploring, integrating, cleaning, validating and transforming raw data into curated datasets u2022u00A0Data Publishingu00A0- u00A0isu00A0the act of releasing data in consumable form for (re)use by others. Note: u00A0All data engineer roles should have a foundational set of knowledgeu00A0in:u00A0u00A0communication, leadership, teamwork, problem solving skills, solution / blueprint definition, business acumen, architectural processes (e.g. blueprinting, reference architecture, governance, etc.), technical standards, project delivery, and industry knowledge. Businessu00A0Analysisu00A0and Technical Leadershipu00A0 u2022u00A0Engages with business and proactively seeks opportunities to deliver business value.u00A0u00A0 u2022u00A0Understands business requirements and effectively translates business needs and process into technical terms, and vice versau00A0 u2022u00A0Elicits and definesu00A0requirementsu00A0 u2022u00A0Ensures appropriate business roles are engaged in solution execution.u00A0 u2022u00A0Participates in design reviews to ensure traceability of requirements.u00A0 u2022u00A0Networks with appropriate IT colleagues to determine solutions to meet business partnersu2019 needs.u00A0 u2022u00A0Seeksu00A0opportunities to reuse existing processes and services to streamline support and implementation of key systems.u00A0u00A0 u2022u00A0Stay abreast of tools and technologies to influence IT strategy so that it provides best usage opportunities for business u2022u00A0Ability to adapt quickly in a constantly changing environment Must Have: u2022u00A0Bacheloru2019s degree in computer science, information technology, management information systems or equivalent work experienceu00A0 u2022u00A07+ years of development experienceu00A0in the core tools and technologies like SQL, Python, AWS (u00A0Lamda, Glue, S3, Redshift, Athena, IAM Roles & Policies)u00A0, PySpark used by the solution services team. u2022u00A0Architect and build high-performance and scalable data pipelines adhering to data lakehouse, data warehouse & data marts standards for optimal storage, retrieval and processing of data. u2022u00A03+ years of experience in Agile Development and code deployment using Github & CI-CD pipelines. u2022u00A02+ years of experience in job orchestration using Airflow. u2022u00A02+ years of experience leading a small team of data engineers. u2022u00A0Expertise in the design, datau00A0modelling,u00A0creation and management of large datasets/data models u2022u00A0Ability to work with business owners to define key business requirements and convert to technical specifications u2022u00A0Experience with security models and development on large data sets u2022u00A0Ensure successful transition of applications to service management team through planning and knowledge transfer u2022u00A0Develop expertise of processes and data used by business functions within the US Affiliate u2022u00A0Responsible for system testing, ensuring effective resolution of defects, timely discussion around business issues and appropriate management of resources relevant to data and integration u2022u00A0Partner with and influence vendor resources on solution development to ensure understanding of data and technical direction for solutionsu00A0as well as delivery Preferred Qualificationsu00A0/ Certificationsu00A0 u2022u00A0Experience working in regulated environments and with internal systems quality policies and proceduresu00A0 u2022u00A0Familiarity with AWS database technologies.u00A0 u2022u00A0Knowledge of the data architectures associated with information integration & data warehousingu00A0 u2022u00A0Experience in development and deployment on cloud infrastructure u2022u00A0Pharmaceutical or healthcare industry experience u2022u00A0Early drug discovery industry experience u2022u00A0Experience with Sales & Marketing Business processes & systems u2022u00A0Defining best practices and developing technical standards, design principals, best practices, and frameworks u2022u00A0Technical curiosity and desire to innovate",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
CirrusLabs,AWS Data Engineer / Data Engineer / Lead AWS Data Engineer,"Job Role: Aws Data Engineer

Location: Bangalore / Hyderabad

Type: Fulltime

JOB DESCRIPTION

Data Engineer/Operational Support with Snowflake

Must-Have:
• 7+ years of experience in an Oracle/Informatica environment with knowledge of views, packages, stored procedures, functions, constraints, cursors, indexes, and table partitions.
• 7+ years of experience with an ETL tool such as SSIS, Azure Data Factory, or AWS Glue
• Strong background in a data warehouse, data management, and data analytics
• Monitor ETL production batch schedules to meet predefined SLAs.
• Resolve functional and system errors as identified by Business Partners
• Coordinate activity between Business Units and EIS to drive open action items to closure.
• Work with other technical teams to resolve infrastructure-related problems.
• Maintain a good relationship with other technology teams within the client enterprise.
• Generate, Control, and Resolve incident tickets relating to Production batch and Data availability issues.
• Serve as senior contact for production support issues and escalations.
• Enterprise L3 support to resolve production support issues in a timely manner.
• Candidate is expected to exude a take-charge attitude toward problems and thrive for excellence. This is a hands-on, delivery-focused role.
• Attempt to isolate, reproduce, and resolve problems using available systems and tools, and investigate potential workarounds for verified defects.
• Participate in the creation of Knowledge Base articles, solutions, and other related support collateral.
• To interface with Subject Matter Experts, where the problem cannot be resolved at a frontline support level.
• Good to have:
• Excellent written and verbal communication skills
• Detail-oriented; Analytical with problem-solving abilities",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
ANI Calls India Private Limited,"Specialist, Quantitative Data Engineer","Anicalls Industry: IT Total Positions: 3 Job Type: Full Time/Permanent Gender: No Preference Salary: 900000 INR - 1600000 INR (Annually) Education: Bachelor?s degree Experience: 5-9 Years Location: Noida, India . Develop and document data quality solutions and index maintenance processes including rebalancing, data publishing and calculation of indexes across different data assets . Analyze large data sets to identify actionable insights for model development Efficiently work with large datasets and conceptualize analytical framework for quant projects . Designing and evaluating statistical modelling experiments related to Labor Market . Understanding technical issues in statistical modelling and applying these skills toward solving business problems . Participate in the development of methodologies, tools and code used by the IT development team . Develop and document data quality solutions and index maintenance processes including rebalancing, data publishing and calculation of indexes across different data assets . Analyze large data sets to identify actionable insights for model development Efficiently work with large datasets and conceptualize analytical framework for quant projects . Designing and evaluating statistical modelling experiments related to Labor Market . Understanding technical issues in statistical modelling and applying these skills toward solving business problems",Noida,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
"GSPANN Technologies, Inc",Azure Data Engineer,"About Company :

GSPANN is a US California Bay Area based consulting services provider focused on implementations in the Enterprise Content Management, Business Intelligence & Mobile Solution initiatives. More than 90% of our current clientele are FORTUNE 1000 organizations. We specialize in strategy, architecture, delivery and support of solutions in the ECM, BI and Mobility space

Required skills : Azure Data factory, Azure Databricks, Azure DevOps, Azure Data Lake storage (ADLS), Synapse data warehouse, Databases (RDBMS/Cosmos DB)

Roles and Responsibilities :
• Build deep knowledge of the business and understand the end-to-end customer journey.
• Use software development approaches to operations. You should have a breadth of experience in software development, operations, and be actively practicing site reliability principles
• Partner with stakeholders to improve the design, visibility, availability, scalability, and performance of services.
• Communicate availability to the team and manager
• Efficiently automate manual processes, deep dive into incidents, and facilitate blameless post-mortems.
• Improve alert management, decision making, analysis, and various optimization techniques by measuring data using standardized telemetry.
• Support planned changes with deployment, post deployment monitoring and create new dashboards or alerts as needed to monitor new changes in system
• Development and the Deployment of new tools to support systems and services in an automated fashion
• ·Adhere to organizational Service Level Objectives

Required Skills :
• 5+ years of experience in software development, technical operations, and running large-scale applications.
• 3+ years of experience working in working in Service Engineering, Support, or Operations.
• 3+ years of experience in developing or supporting Azure Data factory, Azure Databricks, Azure DevOps, Azure Data Lake storage (ADLS), Synapse data warehouse, Cosmos DB
• 2+ years of experience in RDBMS and Cosmos DB, should be able to debug issues around queries
• Should have experience in building pipelines
• Should have a good understanding of container platforms like Docker and Kubernetes.
• Experience in data virtualization products like Denodo
• Azure Data Engineer or Solutions Architect certification is desirable
• Has creative problem-solving skills related to cross-functional issues amidst the changing priorities.
• Should possess excellent communication skills and should take complete ownership drive triage calls and bring logical closure of critical issues",Gurugram,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Mercedes-Benz Research and Development India Private Limited,Data Engineer - Sales,"Aufgaben
• Work with Business & IT teams to gather, understand and define requirements and break down Demands into EPICS and User Stories, where needed.
• Design and build end-to-end CI/CD data pipelines to get the relevant data and for the deliverables
• Reviewing design, code and other deliverables created by your team to guarantee high-quality results
• Define acceptance criteria, manage & perform testing of own pipelines to ensure expected quality
• Own the status of Change requirements throughout the lifecycle & adapt changes to the existing scripts, codes and pipelines.
• Own PoCs and deliver the results in reasonable time
• Ensuring appropriate use of MB IT project management tools, governance, processes, policies and methodologies
• Share required data for regular status meetings with all stakeholders, keeping the stakeholders needs and requirements continuously in view
• Bring value adds by contributing to continuous improvement and innovation, encourage best practices, challenge current practices, provide feedback to colleagues
• Presents user stories in demand concept meetings together with respective stakeholders
• Align with architects if and as needed
• Detect dependencies to other systems/demands
• Conduct regular status meetings with all stakeholders, keeping the stakeholders needs and requirements continuously in view
• Create / Propose Demand Management baselines as and when required.

Qualifikationen
• Total 8-10 Years of Experience with minimum 4 years of experience in data engineering & analytics.
• Bachelor's Degree in computer science or equivalent
• Expertise in Azure and Azure Data Factory, Storage accounts and Notebooks
• Good experience with DWH, ETL/ELT process, hands-on expertise ETL tools, relational DB or MPP
• Expertise in Python, structured query language, PL/SQL, query tuning and performance tuning
• Good knowledge in DevOps, CI/CD including deploying a range of data engineering pipelines into production, and testing techniques
• Expertise in Big Data technologies such as Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases, etc.
• Good knowledge of data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture)
• Agile development experience
• Experienced in Powershell scripting for orchestration",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
HARMAN International,Data Engineer,"What You Will Do :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis What You Need :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis",Bengaluru,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
GSPANN Technologies,Azure Data Engineer,"Should have experience in ADLS (Azure Data Lake storage) Experience implementing Azure Data Factory Pipelines using latest technologies and techniques Experience in working on Azure HDInsight Experience in working with Storage Strategy Azure developer should be able to ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements Expert in Azure Data Factory, Azure Data Lake Azure SQL Data Warehouse, Azure Functions, Databricks · Comfortable working with Spark, Python, and PowerShell Excellent problem solving, Critical and Analytical thinking skills Strong t-SQL skills with experience in Azure SQL DW DevOps, CI/CD, and Automation experience strongly preferred Able to interact with team members collaboratively Experience handling Structured and unstructured datasets Experience in Data Modelling and Advanced SQL techniques",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Bain & Company,"Specialist, Data Engineer, NPSx","Company Overview

Bain & Company is the management consulting firm
that the world’s business leaders come to when they want results. Bain advises
clients on strategy, operations, information technology, organization, private
equity, digital transformation and strategy, and mergers and acquisition,
developing practical insights that clients act on and transferring skills that
make change stick. The firm aligns its incentives with clients by linking
its fees to their results. Bain clients have outperformed the stock market
4 to 1. Founded in 1973, Bain has 58 offices in 37 countries, and its deep
expertise and client roster cross every industry and economic sector.

Position Summary

As a Data Engineer Specialist, you will be a subject
matter expert responsible for developing and maintaining data pipelines to
assist us aggregate number of client data sources into a single view for our
teams to utilize. The data is often incomplete, inconsistent and stored in
silos with access primarily through APIs and FTPs. We are looking for a
creative, experienced and motivated individual to build solutions for
integrating CRM and Marketing data in GCP & BigQuery environments.

Essential Functions
• Build and maintain custom ingestion and integration
pipelines from multiple sources(Digital Marketing and CRM data)
• Establish monitoring systems to give visibility to
pipelines’ status and monitor automated jobs
• Bringing consistency to our Data handling and
schemas across the board
• Developing quality standards to ensure data quality
and integrity across all database systems
• Partner with analysts and data scientists to improve the efficiency and performance of their data products
• Where appropriate, train other team members to support the development of products, tools and technologies

Qualifications:
• Bachelor’s Degree in
Computer Science, Information Systems, or related field as outlined in the
essential duties
• 5+ years’ experience developing data processing and storage
solutions on a cloud architecture; prior knowledge of GCP ecosystem is a major
advantage
• Experience dealing with Digital marketing and CRM data
• Experience in building pipelines from various data sources
especially APIs
• Experience with Python development is preferred
• Experience with using version control via Git and CI/CD systems is preferred
• Demonstrated success as a developer in the full
scope of the development lifecycle; including, but not limited to,
understanding business needs, design, development, and deployment is preferred
• Digital marketing and CRM data pipeline knowledge
• Understands efficient data warehousing designs and schemas
• Comfortable with ETL tools such as Alteryx, Parabola or Talend
• Highly skilled at writing advanced SQL queries and comfortable
with merging complex datasets into functional presentation layers for model
inputs
• Comfortable with working
with orchestration tools such as Airflow, Dataproc, dbt or Cloud Composer for
ETL and ELT
• Knowledgeable about data infrastructure management and
Unix/Linux operating systems is preferred
• Skilled at deploying containerisation solutions
such as Docker or Kubernetes is preferred",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Cardinal Health,"Sr. Data Engineer, Data Engineering","Job function:

IT Quality Control is responsible for owning and implementing software testing and certification strategies for the enterprise. Debugs problems with software through standard tests and recommends solutions. Conducts defect trend analysis and continuous process improvement. Demonstrates knowledge of requirement and risk based testing principles, theories, concepts and techniques. Establishes internal IT service quality control standards, policies and procedures.

Job duties:

Create Test Strategy, define quality standards for Google Cloud Platform and define metrics to measure the efficiency and testing for applications on Google BigQuery, Dataflow and Airflow. Develop and maintain test automation frameworks, build regression test strategy and continuous testing process.

Skills:
• Ability to create test strategy balancing manual and automated testing
• 8+ years of experience with designing and implementing test frameworks in cloud
• Technical skills – SQL, Java/Python
• Good communication and collaboration skills across teams and business SMEs
• Should exhibit continuous testing mindset
• Knowledge on Devops and CI/CD process
• Develop automated test cases to be used in performance testing or as part of testing
• Identify and implement performance metrics to be measured
• Collaborate with functional and technical teams to identify test data or create through UI and database
• Generate automation or performance testing reports from execution
• Maintain record of test discrepancies, using designated QA tools
• Provide feedback of test results to development and infrastructure teams for resolution
• Review Business Requirements Documents and Functional and Technical Specifications towards determining test data scope
• Oversee defects from initial identification through post-deployment analysis
• Coordinate with other QA engineers, leadership, system administrators, architects and developers

What is expected of you and others at this level:
• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
• Participates in the development of policies and procedures to achieve specific goals
• Recommends new practices, processes, metrics, or models
• Works on or may lead complex projects of large scope
• Projects may have significant and long-term impact
• Provides solutions which may set precedent
• Apply design thinking mindset
• Independently determines method for completion of new projects
• Receives guidance on overall project objectives
• Acts as a mentor to less experienced colleagues

Candidates who are back-to-work, people with disabilities, without a college degree, and Veterans are encouraged to apply.

Cardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,True,True,False
Lowe's India,Senior Data Engineer,"At Lowe's, we deliver the right home improvement products with the best service and value. A critical part behind that mission is data, and our team is working to bring that data to life. Easily accessible, highly accurate and reliable data is what our team is striving for, so the business can work toward our common mission. As a Sr Data Engineer, you'll design and develop our data hub to ensure that the associates have the data they need. You'll have the opportunity to design data our highly needed data hub and solve challenging problems on a very fun and energetic team!

Key Responsibilities:

• Executes the development, maintenance, and enhancements of data ingestion into a data hub of varying complexity levels across various data sources like DBMS, File systems (structured and unstructured), APis and Streaming on on-prem and cloud infrastructure; demonstrates strong acumen in Data Ingestion toolsets and nurtures and grows junior members in this capability

• Translates business requirements and specifications into data hub related solutions; partners with Product Manager to understand business needs and functional specifications

• Evaluates project deliverables to ensure they meet specifications and architectural standards

• Provide technical support for the data hub; solutions are extensible; works to simplify, optimize, remove bottlenecks, etc.

• Handles data manipulation (extract, load, transform) and administration of data and systems securely and in accordance with enterprise data governance standards

• Maintains the health and monitoring of the data hub and related activities; ensures high availability of the platform; monitors workload demands

• Participates and coaches' others in end-to-end testing by applying and sharing an understanding of complex company and industry methodologies, policies, standards, and controls

• Understands Computer Science and/or Computer Engineering fundamentals

• Participates in continuous improvement activities including training opportunities; continuously strives to learn data engineering best practices and apply them to daily activities

• Write and review technical documents, including design, development, and revision documents.

Minimum Qualifications:

• Bachelor's Degree in Engineering, Computer Science, CIS, or related field (or equivalent work experience in a related field)

• 5 years of experience in Data, Bl or Platform Engineering, Data Warehousing/Ell, or Software Engineering

• 4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)

• Master's Degree in Computer Science, CIS, or related field

• 5 years of IT experience developing and implementing business systems within an organization

• 5 years of experience working with defect or incident tracking software

• 5 years of experience writing technical documentation in a software development environment

• 3 years of experience leading teams, with or without direct reports

• 5 years of experience working with source code control systems

• Experience working with Continuous Integration/Continuous Deployment tools

• 5 years of experience in systems analysis, including defining technical requirements and performing high level design for complex solutions

• 3 years of experience in Hadoop or any Cloud Bigdata components (specific to the Data Engineering role)

• Experience in Java/Scala/Python, SQL, Scripting, Teradata, Hadoop (Sqoop, Hive, Pig, Map Reduce), Spark (Spark Streaming, MLib), Kafka or equivalent Cloud Bigdata components (specific to the Data Engineering role)",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Ford Motor Company,Senior Data Engineer,"Job Description This role is for experienced Data Engineer that will be responsible for designing and building the foundational components required for our customer. This includes bringing disparate sources into our Hadoop data lake and creating data products that will be used by the analytics and reporting teams. Data engineers will work in small, cross-functional teams. They will collaborate directly and continuously with product managers, designers, and product owners to release early and often. Work with data scientists and software engineers to support data acquisition activities, data solution ideation, and implementation Work with technical and business leads to transfer global business requirements into sound solutions and implementation Share support responsibilities for implemented components Area of professional exposure (technical skills) . Minimum of 8 years experience working with Informatic Big Data Manager (BDM) . Hands on experience developing Data engineering pipelines using Informatica BDM Workflows . Experience in troubleshooting performance issues and performance with Informatica BDM . Minimum of 3+ years of experience working in Data engineering, Big data platform and technologies including Hadoop, Hive, Sqoop, Spark, Kafka . Design data pipelines and data robots, take a vision and bring it to life . Master data engineer teaches others works closely with IT architects . Experience programming in Java and Python is a plus . Experience using Alteryx and data visualization tools especially Qlikview is a plus . Experience working in Agile/XP . Strong analytical and problem-solving skills . Strong oral and written communication skills . Experience with Informatica EDC (Enterprise Data Catalog) and EDP (Enterprise Data Preparation) is a plus",Chennai,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Amazon Music,"Data Engineer, Amazon Appstore","DESCRIPTION Amazon Appstore is a fast-growing Organization and responsible for providing delightful customer experiences across Amazon devices (FireTV, Tablets) with a vast selection of relevant apps, games, and services. The Appstore team is seeking an experienced Data Engineer to join our central Data Engineering and Analytics team. The role will be responsible for closely partnering with Software Development Engineers and Business Intelligence Engineers to build high quality data pipelines and manage Appstore wide central data lake. The Appstore Data Lake powers critical external customer (3P developer) facing reporting as well as self-service analytics for internal stakeholders. This is an exciting opportunity to work on very large datasets and influence products that impact tens of millions of customers on a daily basis. Key job responsibilities In this role, you will: Manage and administer data platform built on AWS services such as EC2, RDS, Redshift, Kinesis, EMR, Lambda etc Develop and improve the current data architecture, data quality, monitoring and data availability Help continually improve ongoing reporting and analysis processes, simplifying self-service support for customers Provide technical and thought leadership for Data Engineering and Business Intelligence within Appstore Create a Data Governance strategy for mitigating disparate data sources where applicable. Establish key relationships which span WW Appstore business units, PMs and Business Intelligence teams BASIC QUALIFICATIONS Bachelor's degree in Computer Science, Engineering, Math, Finance, or related discipline 5+ years of experience with data modeling, data warehousing, and building ETL pipelines Expert-level proficiency in writing and optimizing SQL Knowledge of AWS services including S3, Redshift, EMR, Kinesis and RDS. Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.) Ability to write code in Python, Ruby, Scala or other platform-related Big data technology PREFERRED QUALIFICATIONS Master's degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, or Engineering Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Knowledge of software engineering best practices across the development life cycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy Linux/UNIX including to process large data sets. Experience building data products which directly influence Tier-1 or external customer facing services",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False
Axtria - Ingenious Insights,Data Engineer - Omnichannel,"Position Summary:

To be a Data Engineering expert delivering solutions for multiple project streams with prior expertise in the Pharma domain.

Roles & Responsibilities:
• Effectively manage the clients/onshore stakeholders, as per the business needs, to ensure successful business delivery
• Work closely with project manager to define the algorithm, break down the problem into execution steps and run the analysis
• Work on complex data problems and help the customers in better business decision making
• Build and scale up capabilities to help efficient business delivery
• Mentor and help build the team
• Contribute to Axtria’s development of new solutions and analytical models.

Required Skills and Experience:
• Expertise in the design, data modelling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation, and delivery o Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena o Strong proficiency in SQL and database design, development, and maintenance
• Good understanding of modern architecture patterns like server less and microservices
• Expertise with analytics and business intelligence solutions (e.g., 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation, and Agile Methodology
• Experience working with some of the marketing data sources like - Traditional > TV / Print / Email or Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data

Behavioral:
• Customer Focus - Dedicated to meeting the expectations of internal and external clients
• Problem Solving - Uses rigorous logic and methods to solve difficult problems with effective solutions. Probes all fruitful sources for answers. Is excellent at honest analysis. Looks beyond the obvious and doesn't stop at the first answers
• Learning on the Fly - Learns quickly when facing new problems. A relentless and versatile learner.
• Drive for result - Able to set priorities; pursue tasks tenaciously & with a need to finish. Able to overcome setbacks which may occur along the way

To know more about the company, please visit our website: www.axtria.com

Interested folks may send their applications at piyush.varshney@axtria.com",Bengaluru,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,False
Dish TV Network,Data Engineer,"About DISH: DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world's best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH's largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market Job Duties and Responsibilities: Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production Creation and optimization of data engineering pipelines for analytic projects Strong analytic capability and the ability to create innovative solutions Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues Provide technical guidance to L1 team members and help to resolve ETL related issues Need to work as on call-support Skills, Experience and Requirements: Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment Experience in developing/debugging and fixing data ingestion pipelines both real time and batch Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb Experience in transforming/integrating data in Redshift/Snowflake Strong in writing complex SQLs to ingest data into cloud data warehouses Good hands on experience in shell scripting or python Experience with scheduling tools - ControlM, Airflow , StepFunction Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements Perform root cause analysis (RCA) for failures Good Communication skills - written and verbal with the ability to understand and interact with the diverse range of stakeholders Capable of working without much supervision",Secunderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Briq,Sr. Data Engineer,"Why is working in this department AMAZING!

Working in engineering at Briq puts you squarely in the driver’s seat of building the next generation of construction financial management, planning, and intelligence software. The problems we work to solve every day are deeply complex and rewarding, both for the engineering team and our customers. Engineering at Briq also provides ample opportunities to work closely with product, UX, infrastructure, devops and other stakeholders outside of tech – there are endless ways for you to gain & refine new skills, deliver value, and make an impact.

We are builders first and foremost; We are looking for the best ideas and tools for the job at hand. As such, we are continually evaluating our stack, approaches, and process to deliver performant and scalable solutions for our clients. Working in engineering at Briq, you can expect to leverage some of the latest technologies and software design paradigms available and would be responsible for quality control on application build on:

● Vue.js frontend

● Flutter

● Python/Flask microservice architecture

● Google Cloud Platform (GCP)

● CI/CD with Gitlab

● Kubernetes

● Docker

● MongoDB

● PostgreSQL

● Elastic

● Machine learning

What does the future of this department look like? Where is this going and why do I want to come along for the ride?

As an engineering team within a hyper-growth startup, we are tasked with continuously expanding both the team and the functionalities afforded to our clients by our products and services. This makes for abundant personal and professional growth opportunities within the organization. Regardless of whether you desire to hone engineering skills and deliver amazing features as an individual contributor or take on people/process focused management roles, engineering at Briq offers separate-but-equal tracks with opportunity for vertical career progression.

Given the breadth of our platform, there are a wide variety of problems to solve both today and on the roadmap. If what you are working on today falls out of favor, there is no shortage of new frontiers to conquer. We deeply believe the best solutions are put forward by happy, engaged, and healthily challenged engineers.

If being an integral part of creating a powerful & diverse platform while helping to shape and grow a rapidly expanding engineering organization sounds exciting, Briq is the place for you!

In a nutshell, what will I do every day in this role?

We are looking to hire a skilled Sr. Data Engineer to observe, improve, help design, and fill a data warehousing environment for our company. Your day-to-day duties will include consulting with the data management team/partners, creating extraction and data transformation processes,

filling new and existing warehouses, creating designs and documenta-tion, and testing data upon completion.

To ensure success as an Sr. Data Engineer, you should have extensive knowledge of programming languages and warehouse architecture and be able to clearly communicate your ideas to the company. Ultimately, a top-level Sr. Data Engineer can create a highly efficient and customized

warehouse environment that perfectly suits the data storage and data processing needs of the company.

Why will I love this job within the context of our pillars?

We are Builders:Create new things, improve the existing, and grow with the team every day.

We Evolve and learn every day: Learn new skills, research and prove-out new technology & approaches, and mentor a team of exemplary professionals.

We take Ownership with Accountability: Briq is yours – embrace the entire software development lifecycle for an application area end-to-end and have a significant impact on the value our clients receive.

We Go Fast and Win: Agile with 2 week sprints means we are constantly delivering and iterating over features and functionality with direct and insightful feedback from our users.

We encourage constructive Dialogue:

Briq’s culture was designed to put an emphasis on communication and collaboration.

We never suffer in silence, as we approach struggles as opportunities to learn and

improve. As a Technical Writer, the “Dialogue” pillar gives you an opportunity to not only

leave an impact in the work you directly do, but an impact on the company as a whole

We are Future Positive:

The challenges we face today are opportunities to make something great or to do something better. From sharing an idea to implementing the solution, feel empowered to make an impact on the future of Briq.

We are a Community:

You are part of a community not just at Briq, but in the entire North American

construction industry. You know how to build relationships with colleagues, and can rely

on the teams you work with to be successful in your role.

Responsibilities for this role:
• Manage and structure our Data Warehouse, build ETLs, and architect our data models with the right infrastructure to enable quick, efficient, and scalable business insights
• Collaborate with Product, Engineering, Solution Architects, and Data Analysts to understand the data needs
• Own core source of truth reporting and prepare aggregation layers of data for different reporting and analytics purposes
• Work with key stakeholders to troubleshoot and debug complex technical issues that may arise
• Design and implement new solutions to address any ETL issues or processes that could be improved

Qualifications for this role:
• Bachelor’s degree (Master’s preferred) in computer science, data science, or a related field or equivalent experience
• Strong experience with Python 3.6 or later, Flask or Django, Microservice Architecture, REST APIs, Redis, Docker, Design Patterns, and Unit Testing (5+ years of experience)
• Familiarity with Git for version control and Pub/Sub for messaging (3+ years of experience)
• Expertise in SQL and NoSQL databases, data modeling, and ETL processes (5+ years of experience)
• Experience with database design and architecture (5+ years of experience)
• Proven work experience as a Sr. Data Engineer
• Strong project management and clear communication skills
• Ability to analyze a company’s big-picture data needs
• Highly self-organized and able to meet or plan for tight deadlines
• A plus: familiarity with Sage, Vista, Viewpoint, Procore, and Spectrum accounting systems

Tech Stack

SQL, MongoDB, Postgres, Firebase, BigQuery, Python 3.6 or later, Flask or Django, Microservice Architecture, NoSQL, REST APIs, Redis, Docker, Design Patterns, Unit Testing, Git, Pub/Sub, Database Design",,True,False,True,False,False,False,False,False,False,False,False,False,False,True,False,False
ANI Calls India Private Limited,PySpark Data Engineer,"Anicalls Industry: IT Total Positions: 2 Job Type: Full Time/Permanent Gender: No Preference Salary: 800000 INR - 1400000 INR (Annually) Education: Bachelor?s degree Experience: 5-8 Years Location: Chennai, India . PySpark Developer / PySpark Data Engineer . Should have at least three years of strong experience in PySpark . Strong SQL Expertise . Strong communication and client-facing skills",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Axtria - Ingenious Insights,Data Engineer,"• 5-8 years of experience in data engineering, consulting, and/or technology implementation roles
• Expertise in the design, data modeling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation and delivery
• Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena
• Strong proficiency in SQL and database design, development and maintenance
• Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence
• Good understanding of modern architecture patterns like serverless and microservices
• Expertise with analytics and business intelligence solutions (e.g. 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation and Agile Methodology
• Experience working with some of the following marketing data sources
• Traditional > TV / Print / Email
• Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data
• Experience leading project teams with members with different roles and skills
• Experience working in hybrid onshore/offshore team models
• Strong communication skills",New Delhi,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
TATA Consultancy Services Ltd.,Data Engineer,"Job Description Location - PAN india . Experience - More than 4 years experience into Data Engineering. Following would be the expectations- -Good experience in Big Data , Cloudera,Hadoop,Hive, Kafka, Spark. -Good working experience in Python. -Experience in Ansibke, Jenkins,Maven, Terraform. -Experience in migrating existing services from on - premises to GCP. Desired Candidate Profile Undergraduate",Chennai,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Zupee,Lead Data Engineer,"About Zupee

Zupee is India’s fastest growing Technology backed Behavioral Science company. We are innovating Skill-Based Gaming with a mission to become the most trusted and responsible entertainment company in the world. We have been constantly focusing on innovation of indigenous games to entertain the mass.

Our strategy is to invest in our people & user experience to drive profitable growth and become the market leader in our space. We have been experiencing phenomenal growth since inception and running profitable at EBT level since Q3, 2020. We have closed Series B funding at $102 million, at a valuation $600 million.

The company also announced a partnership with Reliance Jio Platforms, post which Zupee games will distribute its content across all customers using Jio phones. The partnership now gives Zupee the biggest reach of all gaming companies in India, transforming it from a fast-growing startup to a firm contender for the biggest gaming studio in India.

About The Job

Lead Data Engineer

We are looking for someone to develop the next generation of our Data platform

collaborating across functions like product, marketing design, growth, strategy, customer

experience and technology.

Core Responsibilities

●Understand, implement and automate ETL and data pipelines with up-to-date

industry standards

●Hands-on involvement in the design, development and implementation of optimal and

scalable AWS services

What are we looking for?

●S/he must have experience in Python

●S/he must have experience in Big Data – Spark, Hadoop, Hive, HBase and Presto

●S/he must have experience in Data Warehousing

●S/he must have experience in building reliable and scalable ETL pipelines

Qualifications and Skills

●6-12 years of professional experience in data engineering profile

●BS or MS in Computer Science or similar Engineering stream

●Hands-on experience in data warehousing tools

●Knowledge of distributed systems such as Hadoop, Hive, Spark and Kafka etc.

●Experience with AWS services (EC2, RDS, S3, Athena, data pipeline/glue, lambda, dynamodb etc.
•",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
United Airlines,Lead Data Engineer,"Description

United's Digital Technology team designs, develops, and maintains massively scaling technology solutions brought to life with innovative architectures, data analytics, and digital solutions.

Our Values: At United Airlines, we believe that inclusion propels innovation and is the foundation of all that we do. Our Shared Purpose: ""Connecting people. Uniting the world."" drives us to be the best airline for our employees, customers, and everyone we serve, and we can only do that with a truly diverse and inclusive workforce. Our team spans the globe and is made up of diverse individuals all working together with cutting-edge technology to build the best airline in the history of aviation.

With multiple employee-run ""Business Resource Group"" communities and world-class benefits like health insurance, parental leave, and space available travel, United is truly a one-of-a-kind place to work that will make you feel welcome and accepted. Come join our team and help us make a positive impact on the world.

Job Overview And Responsibilities

The United Data Engineering team designs, develops and maintains massively scaling technology solutions that are brought to life with innovative architectures, data analytics and digital solutions. The Data Engineering team is building a modern data technology platform in the cloud with advanced DevOps and Machine Learning capabilities.

The Data Engineering team at United Airlines is on a transformational journey to unlock the full potential of enterprise data, build a

dynamic, diverse and inclusive culture and develop a modern cloud-based data lake architecture to scale our applications, and drive growth

using data and machine learning. Our objective is to enable the enterprise to unleash the potential of data through innovation and agile

thinking, and to execute on an effective data strategy to transform business processes, rapidly accelerate time to market and enable

insightful decision making. United Airlines is seeking talented people to join the Data Engineering team. Data Engineering organization is

responsible for driving data driven insights & innovation to support the data needs for commercial and operational projects with a digital

focus.
• Partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth
• Design, develop, and implement streaming and near-real time data pipelines that feed systems that are the operational backbone of our business
• Utilize programming languages like Java, Scala, Python with RDBMS, NoSQL databases and Cloud based data warehousing like AWS Redshift
• Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution
• Drive the adoption of data processing and analysis using AWS services and help cross train other members of the team
• Leverage strategic and analytical skills to understand and solve customer and business centric questions
• Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies and partners
• Leverage data from a variety of sources to develop data marts and insights that provide a comprehensive understanding of the business
• Develop and implement innovative solutions leading to automation
• Mentor and train junior engineers
• Use of Agile methodologies to manage projects
• Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
• Expand and share your passion by staying on top of tech trends, experimenting with and learning new technologies, and mentoring other members of the engineering community
• Work with a team of developers with deep experience in Digital technology, machine learning, distributed micro services, and full stack systems

This position is offered on local terms and conditions. Expatriate assignments and sponsorship for employment visas, even on a time-limited visa status, will not be awarded.

United Airlines is an equal opportunity employer. United Airlines recruits, employs, trains, compensates, and promotes regardless of race, religion, color, national origin, gender identity, sexual orientation, physical ability, age, veteran status, and other protected status as required by applicable law.

Qualifications

Required
• Bachelor’s Degree in computer science or related STEM field
• Experience with relational database systems like MS SQL Server, Oracle, Teradata
• Excellent O/S knowledge and experience in Linux or Windows with basic knowledge of the other
• MCSE/RHCE or equivalent level of knowledge preferred
• Experience of implementing and supporting AWS based instances and services (e.g. EC2, S3, EBS, ELB, RDS, IAM, Route53, Cloud front, Elastic cache, WAF etc.)
• Scripting ability in one or more of Python, Bash, Perl
• Git for version control useful
• Working with or supporting containerized environments (ECS/EKS/Kubernetes/Docker)
• Agile engineering practices
• Must be legally authorized to work in India for any employer without sponsorship
• Must be fluent in English and Hindi (written and spoken)
• Successful completion of interview required to meet job qualification
• Reliable, punctual attendance is an essential function of the position

Preferred
• Masters in computer science or related STEM field
• Experience with cloud based systems like AWS, AZURE
• Strong experience with continuous integration & delivery using Agile methodologies
• AWS Certified Developer – Associate or Professional
• AWS Certified Solutions Architect - Associate or Professional
• AWS Certified Specialty certification – (Big Data Analytics, Machine Learning)
• Experience with continuous integration & delivery using Agile methods
• Experience with Data Quality tolls including Deequ or Apache Griffin
• Experience building PySpark based services in a production environment

Equal Opportunity Employer - Minorities/Women/Veterans/Disabled/LGBT GGN00001162",Gurugram,True,False,True,True,False,False,False,True,False,False,True,False,True,False,False,False
Embibe,Data Engineer,"Requirements
• Should have knowledge in Coding: Preferred Java.
• Good to have - ( Python / Scala).
• Should have Knowledge in Technologies: Spark, spark streaming, scala spark/py spark.
• Good to have Knowledge of Messaging buses like Apache Kafka/ Rabbit MQ.
• Good to have Knowledge of NoSQL databases like - MongoDB, ElasticSearch, Cassandra, Hive, Impala, ADX, Synapse, Redshift, Athena, etc.
• Should have Knowledge in building Microservices with Spring boot/ Fast-Api.",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Pratt & Whitney,Senior Cloud and Data Engineer/Architect,"Date Posted:

2023-04-27

Country:

India

Location:

North Gate Business Park Sy.No 2/1, and Sy.No 2/2, KIAL Road, Venkatala Village, Chowdeshwari Layout, Yelahanka, Bangalore, Karnataka 560064

Position Role Type:

Unspecified

Pratt & Whitney is working to, once again, transform the future of flight—designing building and servicing engines unlike any the world has ever seen. And because transformation begins from within, we’re seeking the people to drive it. So- calling all curious.

Come ready to explore and you’ll find a place where your talent takes flight—beyond the borders of title, a country or your comfort zone. Bring your passion and commitment and we’ll welcome you into a tight-knit team that takes our mission personally. Channel your drive to make a difference into shaping an organization and an industry that’s evolving fast to the future.

Where the difference you make is on display every day. Just look up.

Are you ready to go beyond?

Pratt & Whitney (P&W) Digital Technology team is seeking a high-energy individual to join our Engineering Applications team where you will have the opportunity to design and build the next generation of Digital Engineering applications, pushing the boundaries on knowledge management modernization and other key initiatives, in support of the Engineering function. You will be part of a highly collaborative team responsible for delivery of software to our engineering customers and partners.

Key responsibilities:
• Perform analysis, design, development, and configuration functions (includes defining technical requirements) for cloud instances and applications
• Perform analysis, assessment and resolution for defects and incidents associated with cloud instances
• Coordinate with cloud provisioning team to setup cloud instances and configure with appropriate security and access controls
• Provide strategic guidance and input to P&W cloud practices
• Work with Cloud architects to design scalable instances that meet business needs.
• Work independently to tackle well-scoped and loosely scoped problems.
• Seek opportunities to expand technical knowledge and capabilities.
• Provide technical guidance and mentorship to less experienced employees.
• Managing and providing support for commercial and government cloud instances
• Participate and coordinate Cloud Infrastructure incident and problem management
• Support and facilitate implementation of security controls for data protection
• Work to drive automation of deployments and maintenance (infrastructure as code, scripting, etc.)
• Focus on designing for an enterprise environment, considering factors such as privacy, resiliency, scalability, cost efficiency.
• Work to implement approved authentication, authorization, connectivity approaches and assist in refining strategies for future evolutions in these areas

Education and Experience:

Bachelor's Degree (required) and 10 years’ experience; or Master’s Degree (preferred) and 7 years’ experience. Preferred fields include Computer Science, Engineering, MIS or related business discipline(s).

Preferred Qualifications:
• 3 years of Azure or AWS cloud-based deployment and support experience
• Experience working in a regulated industry in cloud
• Advanced experience with Azure or AWS infrastructure
• Ability to take a leadership role in installations and provide guidance to other engineers
• Highly skilled at analyzing business process and seeing opportunities for digital solutions
• Experience in Agile methodologies
• Good communication skills, work ethic and a high level of personal integrity and accountability
• Willingness to learn new technologies & take on new projects to grow experience
• Highly self-directed and able to learn quickly with a solid ability to drive and deliver results
• Strong analytical skills, demonstrated organizational and leadership skills, examples of effective teamwork, a track record of consistently meeting deadlines and due dates
• You're comfortable working with minimal daily supervision and to balance numerous priorities
• Ability to remain calm and composed under pressure, in a fast-paced environment of rapidly changing demands

Raytheon Technologies is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.

Privacy Policy and Terms:

Click on this link to read the Policy and Terms

01621772",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
CareerPartner,Data Engineer,"Experience in developing and optimizing ETL pipelines, bigdata data pipelines, and

data-driven architectures

●

Strong knowledge in programming using Python preferably.

●

Strong analytical skills related to working with different types of datasets

●

Build processes supporting data transformation, data structures, metadata, dependency and

workload management

●

Experience supporting and working with cross-functional teams in a dynamic environment

●

Strong knowledge of working with different OS Linux, Windows, etc

●

Additional brownie points for having good

Experience with big data tools: Hadoop, Spark, Hive, etc",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Futurense Technologies,Data Engineer,"• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet business requirements
• Identify, design, and implement internal process improvements
• Optimize data delivery and re-design infrastructure for greater scalability
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
• Work with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs
• Create data tools for analytics and data scientist team members

Skills Required
• Working knowledge of ETL on any cloud (Azure / AWS / GCP)
• Proficient in Python (Programming / Scripting)
• Good understanding of any of the data warehousing concepts (Snowflake / AWS Redshift / Azure Synapse Analytics / Google Big Query / Hive)
• In-depth understanding of principles of database structure
• Good understanding of any of the ETL technologies (Informatica PowerCenter / AWS Glue / Data Factory / SSIS / Spark / Matillion / Talend / Azure)
• Proficient in SQL (query solving)
• Knowledge in Change case Management / Version Control – (VSS / DevOps / TFS / GitHub, Bit bucket, CICD Jenkin) Skills:- Python, SQL, Data engineering, ETL, Data Warehouse (DWH), Informatica, Amazon Web Services (AWS), Snow flake schema and SSIS",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
InVisions Ltd.,Data Engineer,"Hello people,

We are happy to assist the product and service company “Saras Analytics” in welcoming new Data Engineers to the team.

Now, a little bit about the company and the product:

Saras Analytics is a rapidly growing data management and analytics advisory firm with offices in Austin, USA and Hyderabad, India. We are a group of engineers and analysts focused on accelerating growth for e-commerce and digital businesses by setting up or transforming their data (analytics & BI) ecosystems and providing further analytics services. We are laser focused on providing the best ROI for our clients and leave no stone unturned in our quest to provide the best results for our customers.

We are an employee-centric organization and, to meet the ever-growing demand for our services, are looking for individuals who share our passion to make a difference and would be great additions to our analytics and growth consulting practice.

How Saras Analytics describes your role:

As a Data Engineer at Saras Analytics, you will be responsible for building and maintaining large-scale data pipelines as well as create and data pipelines that deal with large volumes of data.

You will deal with:
• Database programming using multiple flavors of SQL and Python.
• Understand and translate data, analytic requirements and functional needs into technical requirements.
• Build and maintain data pipelines to support large scale data management projects.
• Ensure alignment with data strategy and standards of data processing.
• Deploy scalable data pipelines for analytical needs.
• Big Data ecosystem - on-prem (Hortonworks/MapR) or Cloud (Dataproc/EMR/HDInsight).
• Work with Hadoop, Pig, SQL, Hive, Sqoop and SparkSQL.
• Experience in any orchestration/workflow tool such as Airflow/Oozie for scheduling pipelines.
• Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow.
• Understand and execute IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, writing optimized queries in Spark.
• Hands-on experience in using Spark Streaming, Kafka and Hbase.

What you bring with you:
• 4 to 6 years of experience in building data processing applications using Hadoop, Spark and NoSQL DB and Hadoop streaming. Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow is a plus.
• Expertise in data structures, distributed computing, manipulating and analyzing complex high-volume data from variety of internal and external sources.
• Experience in building structured and unstructured data pipelines.
• Proficient in programming language such as Python/Scala.
• Good understanding of data analysis techniques.
• Solid hands-on working knowledge of SQL and scripting.
• Good understanding of in relational/dimensional modelling and ETL concepts.
• Understanding of any reporting tools such as Looker, Tableau, Qlikview or PowerBI.
• Degree: Bachelor of Engineering - BE, Bachelor of Science - BS, Master of Engineering - MEng, Master of Science – MS or equivalent work experience.

Eligibility:
• Significant technical academic course work or equivalent work experience.
• Excellent communication and interpersonal skills.
• Willingness to work under labor contract, B2B contract is an option too.
• Dedicate 40 hours/weekly to Saras Analytics.

Let’s connect and check if we match!

You can state your interest by sending your CV and we will get in touch with the short-listed candidates.

We treat your personal information with respect and confidentiality, guaranteed and protected by the professional ethics, the Bulgarian and European law.

“InVisions” agency license № 2420 from 19.12.2017.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,False
Newell Brands,Cloud Data Engineer,"Job Title: Cloud Data Engineer

Report To: Sr. Manager, Data Engineering

Job Location: Guindy, Chennai, India

Job Duties
• Participates in the full lifecycle of cloud data architecture (Preferably Azure cloud) from gathering, understanding end-user analytics and reporting needs.
• Migrate On-Prem applications and build CI/CD pipeline in Cloud platform.
• Design, develop, test, and implement on Cloud platform (Ingestion, Transformation and export pipelines that are reliable and performant) .
• Ensures best practices are followed and business objectives are achieved by focusing on process improvements.
• Quickly adapt by learning and recommending new technologies and trends.
• Develop and Test Data engineering related activities on Cloud Data Platform.
• Work with dynamic tools within a BI/reporting environment.

Job Requirements
• B.E/B.Tech, M.Sc/MCA.
• 5+ years experience in Rapid development environment, preferably within an analytics environment.
• 3+ years experience with Cloud experience (Preferably Azure cloud but not mandatory).
• DB : T-SQL, SQL Scripts, Queries, Stored Procedures, Functions and Triggers
• Language : Python / C# or Scala
• Cloud: Azure / AWS / Google cloud
• Frameworks: Cloud ETL/ELT framework

Preferred
• Azure Data Factory, Azure Synapse Analytics, Azure SQL, Azure Data lakes, Azure Data bricks, Airflow and Power BI
• Data warehousing principles and frameworks.
• Knowledge in Cloud DevOps and CI/CD pipelines would be an added advantage.

Newell Brands (NASDAQ: NWL) is a leading global consumer goods company with a strong portfolio of well-known brands, including Rubbermaid, FoodSaver, Calphalon, Sistema, Sharpie, Paper Mate, Dymo, EXPO, Elmer's, Yankee Candle, Graco, NUK, Rubbermaid Commercial Products, Spontex, Coleman, Campingaz, Oster, Sunbeam and Mr. Coffee. Newell Brands' beloved, planet friendly brands enhance and brighten consumers lives at home and outside by creating moments of joy, building confidence and providing peace of mind.",Chennai,True,False,True,False,False,False,False,False,True,False,False,False,False,False,True,False
Emerson,Data Engineer,"Job Description
• 4-6 Years of Experience in Embedded Software Design & Development on 16/32 Bit Processor Platform
• Strong Programming Skills in C & C++ required
• Programming Experience in Operating System / RTOS Environment required
• Linux Kernel / OS Internals Knowledge required
• Web Page Development Experience using HTML, CSS and JavaScript desired
• Linux Porting Experience on X86 / ARM based Hardware Architecture desired
• Knowledge of Device Drivers desired
• Python, Shell scripting Knowledge desired
• Python scripting experience on Ubuntu Linux for Graphical Interface is preferred
• Knowledge of Qt / GTK is preferred
• Experience on SQL Database desired",Pune,True,False,True,True,False,True,True,False,False,False,False,False,False,False,False,False
WinZO,Data Engineer,"About WinZO Games

WinZO is India’s largest social gaming platform aiming at building an astronomical tech strong gaming ecosystem in India. WinZO in a short span of time has emerged as the leanest Series C funded gaming startup in the Indian startup ecosystem. WinZO has so far raised over $100MM and handles more than 4+ Bn micro transactions monthly, a number which is fast growing. WinZO with a data driven DNA is working towards becoming the one-stop-shop for online gaming users spread across every household in Bharat. With a vision of becoming a household name for Bharat, catering to their entertainment needs through interactive engagements, Paavan Nanda (Co-Founder, WinZO, Zostel & ZO Rooms) and Saumya Singh Rathore (Co-Founder, WinZO, Ex-Chief of Staff & Growth- ZO Rooms, Zostel, Ex-Times Group), are aggressively building the platform to not just capture market opportunities but also explore and maximize potential of social interactions as consumption drivers. Both of them are putting together WinZO piece by piece using tech and data to create a transparent and unique gaming experience for its users.

WinZO, which hosts 100+ games in 12+ languages, has 80% users consuming the app in vernacular languages. WinZO has always yearned to mentor, guide and onboard games to be culturally relevant for Bharat. It also provides opportunities for housewives to translate and earn which empowers them economically. A 150+ members strong team with stellar professionals coming from global tech giants and companies such as Google, Amazon, Flipkart, McKinsey etc., WinZO is funded and backed by global gaming and entertainment investment funds such as Griffin Gaming Partners, Maker’s Fund, Courtside Ventures, Pags Group and Kalaari.

WinZO is continually working towards revolutionizing the gaming ecosystem by creating a complete entertainment package through a slew of interactive features. Speaking of the larger picture the platform is driving unique initiatives that are constantly attempting to nurture and groom developers.

WinZO Values

Integrity, Excellence Perseverance, Data Orientation, Agility

About The Role

As a part of the Data Engineering team at WinZO, you will create and manage BI and analytics solutions that turn data into knowledge. You will also be responsible to enhance the business intelligence system to help us make better decisions. You will be working in a fast-paced environment which will require you to take initiatives with complete ownership, manage multiple projects, and drive execution with stakeholders.

What You Will Do
• Responsible for building and operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
• Working on designing, performance/scalability tuning of batch and real time stream analytics and large data processing systems
• Research and recommend frameworks and architectural/code design patterns for large scale data processing and identify areas of improvements within the existing code and processes
• Design, build and deploy internal applications to support our product life cycle, data and business intelligence, among others.
• Optimize data delivery and re-design infrastructure for greater scalability
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Apache Spark, Hadoop and AWS technologies
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
• Work with internal and external stakeholders to assist with data-related technical issues and support data infrastructure needs

What You Should Have

We’re looking for people with a hustler mindset, who are curious, eager to learn new things, with a passion for innovation, and work to be a little better every single day. This is not solely based on whether a candidate has previously done similar work or not. We’re looking for someone dynamic with below qualities in generous quantities to perform well in this role –
• B.Tech/B.E in Computer Science, IT, or similar field; a Master’s is a plus
• Experience working with ETL/ELT pipelines, Data Warehousing tools
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL)
• Experience working with big data and database technologies, especially Apache Spark and Hive/Hadoop
• Experience in a cloud environment, preferably AWS
• Good to have proficiency with Linux and systems administration
• Experience working with both structured and unstructured data paradigms
• Experience with a high-level programming language preferably Python/Scala/Java
• Experience troubleshooting data quality issues, analysing data requirements
• Good to have working knowledge of MongoDB/Apache Kafka/Unix shell scripting/Apache Cassandra
• Most importantly – a reliable team player and a strong desire to learn

What We Offer You
• A flat and transparent culture with an incredibly high learning curve
• A swanky informal workspace which defines our open and vibrant work culture
• Opportunity to solve new and challenging problems with a high scope of innovation
• Complete ownership of the product and chance to conceptualize and implement your solutions
• Opportunity to work with incredible peers across departments and be a part of the Tech revolution
• Most importantly, a chance to be associated with big impact early in your career

At our core, we’re a creative company. Ideas is where we live, and we love building magical products. It’s not just about features, it’s also about how they make people feel. So, we build at the intersection of the technical and the romantic. And it all starts with people, the right team that cares deeply about our mission, values, and our users. We value diversity. We are an equal opportunity employer: we do not discriminate based on race, colour, religion, gender, ethnicity, or disability status.

Download our app for a better understanding - https://www.winzogames.com/",,True,False,True,True,False,False,False,True,False,False,False,True,False,False,False,False
NIRA,Senior Data Engineer,"We are looking for an experienced Data Engineer to join our engineering team. The hire will be responsible for building our data and data pipeline architecture. You will optimise our data flow starting with the collection of data for cross functional teams and purposes. You will support our key data science initiatives while maintaining consistency of data delivery architecture throughout ongoing projects. Ideal candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimising or even re-designing our company's data architecture to support our next generation of products and data initiatives.

Responsibilities
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet performance and business requirements
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data' technologies
• Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery, re-designing infrastructure for greater scalability, etc.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, credit risk, operational efficiency and other business KPIs.
• Create data tools for analytics and data scientist team members that assist them in building and scaling our core products
• Work with cross domain data and analytics experts to strive for stronger data driven outcomes for the business.

Requirements
• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimising big data' data pipelines, architectures and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Working knowledge of message queuing, stream processing, and highly scalable big data' data stores.
• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science.
• Experience with big data tools: Hadoop, Spark / PySpark, Kafka.
• Experience with relational SQL and NoSQL databases.
• Experience with object-oriented/object function scripting languages: Python.
• Expertise in data modelling and buiding data driven systems.
• Well versed with Shell Scripting.
• Hands on experience on various AWS services like EMR, EC2 Glue.
• Deploy existing data projects using CICD pieplines.
• Knowledge on Docker is a plus.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Snowflake,Data Engineer,"Build the future of data. Join the Snowflake team.

Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Because traditional on-premises and cloud solutions struggle with this, Snowflake developed an innovative product with a new built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.

In addition, Snowflake’s culture was built on the following values that are even more important to us today:
• Put Customers First. We only succeed when our customers succeed
• Integrity Always. Be open, honest, and respectful
• Think Big. Be ambitious and have big goals
• Be Excellent. Quality and excellence count in everything we do
• Get It Done. Results matter!
• Own It
• Make Each Other the Best
• Embrace each others’ Differences

Job Description
• Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data infrastructure that scales across the company
• Drive the design, building, and launching of new data models and data pipelines in production
• Build data expertise and own data quality for allocated areas of ownership
• Align to Product roadmap in building tools for data platform users
• Mature requirement and follow design develop and communicate model using Agile methodology for data ingestion and data tooling.

MINIMUM QUALIFICATION
• Expertise in SQL statements and modeling concepts.
• Must be aware of the cloud environment from data ingestion and modeling perspective.
• Must be strong in python
• Experience with Apache Airflow is highly desirable.
• schema design and dimensional data modeling.
• custom ETL design, implementation and maintenance.
• object-oriented programming languages.
• Understanding of API and connectors is highly desirable
• analyzing data to identify deliverables, gaps and inconsistencies.
• Experience in data warehouse space.

PREFERRED QUALIFICATION :
• BE/BTECH in Computer Science, IT or other technical field.
• Experience with data ingestions and data analytics.
• 4 years experience using Python and SQL, .",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,True
Turing,Senior Data Engineer,"A U.S.-based company, that is changing the way people consume dairy and revolutionizing vegan protein-rich products, is looking for a Senior Data Engineer. The selected engineer will collaborate with the data science and engineering team as well as provide technical leadership and utilize their expertise with cloud computing to design and deploy data engineering processes. The company is on a mission to change how people consume their food by making the process more transparent and ethical. This company has been successful in raising over $700 Million during their Series D round of funding.

Job Responsibilities:
• Develop data pipelines, databases, and overall data lake architecture
• Design and implement an analytical environment using third-party and in-house tools
• Use Python to improve and automate analytics, ETL, and data quality platform
• Build and deploy complex data models and model metadata
• Create reports and dashboards
• Take ownership of data dashboarding and presentation tools for the end users of our data products and systems
• Conduct and support ingestion and migration of data in multiple types and formats
• Partake in data retrieval, integration, and analysis
• Understand data trends by analyzing collected data to establish facts
• Deploy developed data solutions, user applications, databases, etc
• Work closely with internal and third-party data producers, consumers, and users
• Collaborate with non-technical stakeholders
• Analyze, understand, and document existing workflows
• Conduct integration and data structuring activities

Job Requirements:
• Bachelor’s/Master’s degree in Engineering, Computer Science (or equivalent experience)
• At least 8+ years of relevant experience as a Data Engineer
• Experience with ETL, Data Modeling, and Data Architecture (at least 8-10 years)
• Expertise in writing and optimizing SQL
• Previous experience developing scripting software solutions using Linux/Unix
• Operational experience in large data warehouses or data lakes
• Demonstrable experience with AWS technologies, Databricks, Benchling, and Glue
• Previous understanding of Lambda, and RDS (PostgreSQL, MySQL)
• Expert-level skills in ETL optimization, designing, coding, and using Apache Spark or similar technologies to tune big data processes
• Background developing applications and data pipelines to process and stream datasets at low latencies
• Ability to track data - track data lineage, ensure data quality, and improve data discoverability
• Sound understanding of distributed systems and data architecture (Lambda)
• Ability to design and implement batch and stream data processing pipelines
• Understanding of how to optimize the distribution, partitioning, and MPP of high-level data structures
• Background in life science data a plus
• Familiarity with Docker
• Ability to clearly express ideas, listen, question, and share valuable information with colleagues
• Effective presentation skills and the ability to communicate ideas to technical and non-technical audiences
• Great to possess high energy and enthusiasm to boost productivity
• Strong organization and planning skills, ability to establish a clear course of action to accomplish goals and objectives
• Ability to effectively manage time and utilize resources and systems
• Knowledge of task prioritization, ability to keep track of activities, and task completion
• Self-starter with the know-how to work independently and multi-task while maintaining quality standards
• Fluent in spoken and written English",,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
Mercede,Positions for Data Engineer,"Technical Skills Competencies
• Deep hands-on expertise in Databricks (Scala or Python).
• Experience in Design and implementation of Big Data technologies (Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases) and familiarity with data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture).
• Experience in working as a Big Data Engineer: query tuning, performance tuning, troubleshooting, and debugging Spark and other big data solutions.
• Familiarity with a full range of data engineering approaches, covering theoretical best practices and the technical applications of these methods.
• Experience building and deploying a range of data engineering pipelines into production, including using automation best practices for CI/CD.
• Very good experience in writing SQL queries.
• Hands-on experience with any of the cloud providers such as AWS or Azure.
• Familiarity with databases and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Ability to juggle and prioritize multiple tasks within a collaborative team environment
• Desire to learn and grow both technical and functional skill sets, and drive team s potential
• Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience And Skills

Microsoft Azure and AWS Certifications
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Trained in Data Factory, Delta lake, Data bricks Notebooks
• Working experience in SAFe - Scaled agile framework
• Working experience in an international team environment
,

This job is provided by Shine.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
VOCO Technologies,Hiring for Data Engineer with Skillls Python ETL & Pyspark 3-4 years for work from Home oppurtunity,"Greetings from Voco Technologies

we are Looking Data engineer for Long term Project

Skillls Set Mandatory : Python, ETL and Pyspark

Experience Level 3-4 years Work from oppurtunity

Immediate or 15 days Can Apply for This Role

Powered by Webbtree",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Antal International,Senior Data Engineer,"Role: Senior Data Engineer

Location: Hyderabad, India (requires WFO)

Experience: 7-10 years

Roles and Responsibilities:
• Develop and implement ETL strategy in collaboration with stakeholders.
• Lead and manage ETL projects from initiation to completion, ensuring that project goals and objectives are met within budget and on time.
• Collaborate with stakeholders, including business leaders, data owners, and IT teams, to understand their data requirements and ensure that data meets their needs.
• Establish and maintain data governance policies and procedures to ensure compliance with data privacy and security regulations.
• Willingness to pick up new technologies and develop POCs and MVPs for demonstrations.
• Leverage tools & technologies like Informatica Power Center / IICS
• Designing & developing data loads & transformations, workflows, exception handling
• Integrate Data Quality (IDQ) rules & data reconciliation as part of orchestration.
• Craft SQL queries using joins, aggregates, merge/upsert etc

Required Skills:
• 7+ years of experience in ETL, data warehousing, or related field using Informatica’s suite of products and solutions with knowledge of Agile.
• Experience in Unix shell scripting and Python.
• Expertise on handling structured, semi-structured and un-structured files.
• Proficient in writing complex and efficient SQL queries and tuning them for performance when required.
• Experience with data analytics and data visualization tools, such as Tableau, Power BI, or QlikView, is a plus.
• Experience with data privacy and security regulations, such as GDPR or CCPA, is a plus.",Hyderabad,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
ProCogia,Azure Data Engineer,"ProCogia is a data consulting firm headquartered in Vancouver, BC with employees and clients across the United States and Canada. We specialize in Data Operations, Data Engineering, BI & Analytics, Data Science & Bioinformatics across a broad range of industries including Telecom, Pharma, Biotechnology, Retail, Logistics, Technology, Financial Services, Media & non-profit. We are a technology-agnostic firm giving us a client-first, independent approach towards delivering our Data solutions.

We’re a diverse, close-knit team with a common pursuit of providing top-class, end-to-end data solutions for our clients. In return for your talent and technical expertise, you will be rewarded with a competitive salary, generous benefits along with ample opportunity for personal development. ‘Growth mindset’ is something we seek in all our new hires and has helped drive much of our recent growth across North America. Our distinct approach is to push the limits and value derived from data. Working within ProCogia’s thriving environment will allow you to unleash your full career potential.

ProCogia has experienced considerable growth over the last two years & core to our culture is maintaining a high-level of gender equality throughout the company. Our diversity and differences allow us to create innovative and effective data solutions for our clients.

Our Mission

We enable our customers to make intelligent and agile decisions through strategically tailored data solutions. We attract, develop and retain diverse, motivated and collaborative team players who love what they do.

Our Values

TRUST We build trust in all of our relationships, internally and externally

GROWTH We believe in life-long learning; both personally and professionally

INNOVATION Our technology-agnostic & innovative approach enables cutting-edge data solutions

EXCELLENCE We are committed to delivering excellence in everything we do

ProCogia Overview

At ProCogia we’re passionate about developing data-driven solutions that provide highly informed answers to our clients’ most critical challenges. Our projects are varied, from Data Warehouse builds, deploying Cloud Data Solutions, Dashboarding, & building predictive models.

We work with industry leading clients from various sectors including Pharmaceuticals, Telecommunications, Technology, Financial Services & Retail. Our work environment ensures opportunities to gain valuable experience in various industries enhancing your personal & career development.

ProCogia has doubled in size over the last two years & core to ProCogia’s culture is ensuring we maintain a balanced gender ratio. Our diversity, and differences allow us to create innovative and effective solutions for our clients.

Job Description

The Data Engineering Consultant has the following responsibilities and duties:
• Create and maintain optimal data architecture pipeline.
• Perform root cause analysis on internal and external data, processes and share insights with different stakeholders using reporting and visualizations tools.
• Design infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources into a data warehouse
• Understand existing implementation and should enhance it as needed.
• Work with other data engineers, data ingestion specialists, and experts across cross-functional teams.
• Work on large datasets to meet functional and business requirements

Requirements

The Data Engineering Consultant should have the following skills, education, and experience:

Skills

Required:
• Excellent SQL and Python skills
• Experience with Spark programming language(s)
• Experience with Docker/containerization
• Expertise in designing efficient Data Models
• Experience with ETL/ELT and familiarity of Data Warehouse concepts
• Strong knowledge across Microsoft Azure Tools.
• Data pipeline and workflow management tools: Databricks (Spark + Python), ADF, Dataflow
• Knowledge of API-based integration and security (Restful and SOAP)
• Experience with Data Visualization tools such as Power BI etc.
• Ability to learn fast and translate data into actionable results
• Excellent written and communication skills with client stakeholders

Preferred:
• Experience with JSON Programming
• Experience developing and supporting scalable data pipelines
• Experience with Snowflake Data Warehouse
• Solid Linux programming skills
• Cosmos DB, Teradata, SQL Server (all nice to have)
• Stream-processing systems: Streaming-Analytics, IoT Hub, Event Hub, Kusto Queries (all nice to have)

Education

Required:
• Bachelors in a quantitative field such as computer science, computer engineering other related disciplines.

Preferred:
• Masters or PhD in a quantitative field such as computer science, computer engineering other related disciplines.

Experience
• An experienced professional will have at least 3+ years of professional experience in Data Engineering role with a Masters Degree or 5+ years of experience in a Data Engineering related role with a Bachelors degree.

ProCogia is proud to be an equal opportunity employer. We are committed to creating a diverse and inclusive workspace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status.",,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,True
DISH Network Technologies,Data Engineer,"About DISH:

DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world’s best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH’s largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market

Job Duties And Responsibilities
• Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production
• Creation and optimization of data engineering pipelines for analytic projects
• Strong analytic capability and the ability to create innovative solutions
• Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues
• Provide technical guidance to L1 team members and help to resolve ETL related issues
• Need to work as on call-support

Skills, Experience and Requirements:
• Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment
• Experience in developing/debugging and fixing data ingestion pipelines both real time and batch
• Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb
• Experience in transforming/integrating data in Redshift/Snowflake
• Strong in writing complex SQLs to ingest data into cloud data warehouses
• Good hands on experience in shell scripting or python
• Experience with scheduling tools - ControlM, Airflow , StepFunction
• Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements
• Perform root cause analysis (RCA) for failures
• Good Communication skills – written and verbal with the ability to understand and interact with the diverse range of stakeholders
• Capable of working without much supervision",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Inference Labs,Data Engineer,"Responsibilities for the job Key Responsibilities: - Data Model Designing, Developing and maintaining Data pipelines on cloud (AWS Platform) Translate business needs to technical specifications and framework Maintain and support data mart, data analytics platforms & application. Perform quality assurance to make sure the data correctness Develop sub-marts using SQL and OLAP function to fulfil immediate/ad-hoc need of the business users basis the comprehensive marts Monitoring of the performance of ETL and Mart Refresh processes, understand the problem areas and open a project to fix the performance bottlenecks. Other Responsibilities (If Any):- Availability during month-end Deck generation, may be sometime during week-end/holidays. Eligibility Criteria for the Job Education B.E/B.Tech in any specialization, BCA, M.Tech in any specialization, MCA Work Experience Data Engineer: 4+ years of experience in data engineering on cloud platforms like AWS, Azure, GCP Exposure with working on BFSI domain / big data warehouse project Exposure to manage multiple source of the information, both structured / unstructured data Manage data lake environment for point in time analysis (SCD Type 2), multiple refresh during the day, event based refresh Should have exposure on Managing environment having real time dashboard, data mart requirement. Primary Skill Must have orchestrated using any of the cloud platforms Expert in writing complex SQL Command using OLAP Working experience on BFSI Domain Technical Skills Must have orchestrated at least 3 projects using any of the cloud platforms (GCP, Azure, AWS etc.) is a must. Must have worked on any cloud PaaS/SaaS database/DWH such as AWS redshift/ Big Query/ Snowflake Python/Java Hands - on Exp from data engineering perspective is a must Experience with any of the object-oriented/object function scripting languages: Python, Java, Scala, Shell, .NET scripting, etc. is a must Experience in at least one of the major ETL tools (Talend + TAC, SSIS, Informatica) will be added advantage Management Skills Ability to handle given tasks and projects simultaneously in an organized and timely manner. Soft Skills Good communication skills, verbal and written. Attention to details. Positive attitude and confident.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
HexaQuEST Global,Data Engineer,"We are looking for a Data Engineer with below Skills.

Desired Skills: Snowflake, Mattalion

Experience: 5 + Yrs

Location: Remote",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
dunnhumby,Big Data Engineer - Women Returner,"dunnhumby is the global leader in Customer Data Science, empowering businesses everywhere to compete and thrive in the modern data-driven economy. We always put the Customer First.

Our mission: to enable businesses to grow and reimagine themselves by becoming advocates and champions for their Customers. With deep heritage and expertise in retail – one of the world’s most competitive markets, with a deluge of multi-dimensional data – dunnhumby today enables businesses all over the world, across industries, to be Customer First.

dunnhumby employs nearly 2,500 experts in offices throughout Europe, Asia, Africa, and the Americas working for transformative, iconic brands such as Tesco, Coca-Cola, Meijer, Procter & Gamble and Metro.

Designed to support talented women who’ve taken an extended career break, the ‘Back to her Future’ program by @dunnhumby India is aimed to ease their re-introduction back into the workplace with our internal opportunities. dunnhumby celebrates talent from all areas – so we are looking beyond the traditional CV, for females who are technically-minded and who put customers at the heart of everything they do.

We’re looking for a Big Data Engineer who expects more from their career. It’s a chance to extend and improve dunnhumby’s Data Engineering Team It’s an opportunity to work with a market-leading business to explore new opportunities for us and influence global retailers. Joining our team, you’ll work with world class and passionate people which is part of Innovation Technology. You will be responsible for working with stakeholders in the development of data technology that meet the goals of the dunnhumby technology strategy and data principles. Additionally, this individual will be called upon to contribute to a growing list of dunnhumby data best practices.

What We Expect From You
• 2+ years of experience in Information Technology Experience in managing Big Data space.
• Experience with Spark along with working knowledge of Hadoop/Spark Toolsets.
• Experience with high level programming languages – Python or Scala
• Experience with relational database management systems (RDBMS) and Data Flow Development

What You Can Expect From Us

We won’t just meet your expectations. We’ll defy them. So you’ll enjoy the comprehensive rewards package you’d expect from a leading technology company. But also, a degree of personal flexibility you might not expect. Plus, thoughtful perks, like flexible working hours and your birthday off.

You’ll also benefit from an investment in cutting-edge technology that reflects our global ambition. But with a nimble, small-business feel that gives you the freedom to play, experiment and learn.

And we don’t just talk about diversity and inclusion. We live it every day – with thriving networks including dh Gender Equality Network, dh Proud, dh Family, dh One and dh Thrive as the living proof. Everyone’s invited.

Our approach to Flexible Working

At dunnhumby, we value and respect difference and are committed to building an inclusive culture by creating an environment where you can balance a successful career with your commitments and interests outside of work.

We believe that you will do your best at work if you have a work / life balance. Some roles lend themselves to flexible options more than others, so if this is important to you please raise this with your recruiter, as we are open to discussing agile working opportunities during the hiring process.

For further information about how we collect and use your personal information please see our Privacy Notice which can be found (here)",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Changeleaders.in,Data Engineer - Google Cloud Platform,"Sr./Lead/Specialist - GCP Data Engineer

Experience : 3 - 12 years

Locations : Mumbai, Pune, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore

Primary Skills : Skilled in GCP Services, Data Pipeline, Python, SQL

Desired Skills & Responsibilities
• Possess In depth knowledge and hands on development experience building end to end data pipeline on GCP including ingestion, processing and storage of datasets.
• Strong Python & SQL coding experience [Must].
• Strong experience dealing with JSON strings using Python.
• Good exposure to GCP data services like BigQuery, CoudSQL, Firestore, BigTable.
• Knows Cloud Functions, DataFlow, Cloud Composer, Apache Airflow.
• Exposure to DocAI service using DocAI API is a plus.
• Good exposure to GCP data services like BigQuery, CoudSQL.
• Knows Cloud Functions, DataFlow, Cloud Composer, Apache Airflow
• Deployment, release and handover experience specific to data projects.

(ref:hirist.com)",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Mindera,Data Engineer,"We are looking for an experienced Data Engineer to join our team.

Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.

As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.

This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.

National and international expected travelling time varies according to project/client and organisational needs: 0%-15% estimated

Requirements

You’re great at
• Python
• AWS like (Glue, S3, EMR, Athena and ECS/Fargate)
• SQL
• Airflow
• Data Modelling
• Pyspark

It also would be cool if you have
• Exposure to DBT would be preferable
• Experience working with modern data platforms such as redshift or snowflake would be preferable
• Experience working with Airflow, Docker, Terraform and CI/CD would be preferable
• Experience working with docker, Scala, and Kafka would be an added advantage

What You Will Be Doing
• Implement/support new data solutions in the data lake/warehouse built on the snowflake
• Develop and design data pipelines using python.
• Design and Implement Continuous Integration/Continuous Deployments pipelines.
• Perform Data Modelling using downstream requirements.
• Develop transformation scripts using advanced SQL and DBT.
• Write test cases/scenarios to ensure incident-free production release.
• Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
• Debug production and development issues and provide support to colleagues where necessary.
• Perform data quality checks to ensure the quality of the data exposed to the end users.
• Build strong relationships with team, peers and stakeholders.
• Contributes to overall data platform implementation.

Benefits

We offer
• Flexible working hours (self-managed)
• Competitive salary
• Annual bonus, subject to company performance
• Access to Udemy online training and opportunities to learn and grow within the role

At Mindera we use technology to build products we are proud of, with people we love.

Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.

We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world.

You get to work with a bunch of great people, and the whole team owns the project together.

Our culture reflects our lean and self-organisation attitude.

We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.

Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook

Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Amazon Web Services (AWS),Data Engineer II,"Description

Amazon Web Services (AWS) provides a highly reliable, scalable, and low-cost cloud platform that powers thousands of businesses in over 190 countries. AWS’ Infrastructure Supply Chain & Procurement (ISCaP) organization works to deliver cutting-edge solutions to source, build and maintain our socially responsible data center supply chains. We are a team of highly-motivated, engaged, and responsive professionals who enable the core sustainable infrastructure of AWS. Come join our team and be a part of history as we deliver results for the largest cloud services company on Earth!

Do you love problem solving? Do you enjoy learning new ideas and apply them to problems? Are you looking for real world engineering challenges? Do you dream about elegant high quality solutions? Want to be a part of an amazing team that delivers first class analytical solutions to our business world-wide?

AWS is seeking a highly motivated and passionate Back End Data Engineer who is responsible for designing, developing, testing, and deploying Supply Chain Application and Process Automation. In this role you will collaborate with business leaders, work backwards from customers, identify problems, propose innovative solutions, relentlessly raise standards, and have a positive impact on AWS Infrastructure Supply Chain & Procurement. In this, you will work closely with a team of Business Intelligence Engineers and Data Scientists to architect the application programming interface (API) and user Interface (UI) in context with the business outcomes. You will be using the best of available tools, including EC2, Lambda, DynamoDB, and Elastic Search. You will be responsible for the full software development life cycle to build scalable application and deploy in AWS Cloud.

In This Job, You Will

Work with business leaders, Business Intelligence Engineers, and Data Scientists to ideate business friendly software solutions.

Design client-side and server-side architecture.

Develop visually appealing front end website architecture, including translating designer mock-ups and wireframes into front-end code.

Develop functional databases, applications, and servers to support websites on the back end.

Write effective APIs.

Test software to ensure responsiveness and efficiency.

Troubleshoot, debug and upgrade applications.

Create security and data protection settings.

Build features and applications with a mobile responsive design.

Develop technical specifications and write technical documentation.

Basic Qualifications
• - Bachelor's or Master's degree or PhD in Computer Science, or a related field.
• - 6+ years of non-internship professional software development experience with 3+ years of technical leadership experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systems
• - Experience with client side technologies (HTML, CSS, JavaScript, jQuery, XML, Angular, React)
• - Knowledge of multiple back-end technologies (e.g. C++, C#, Java, Python, Node.js) including object-oriented design
• - Familiarity with SQL and NoSQL databases (e.g. MySQL, MongoDB, PostgreSQL, Dynamo DB), web servers (e.g. Apache, Apache Tomcat)
• - Proficient in mapping the object oriented model to relational database(Hibernate and JPA)
• - Experience with DevOps tools like Docker, Kubernetes with application deployment using CI/CD.
• - Experience with distributed version control such as Git.
• - Basic knowledge of Linux environments including shell scripting and standard Linux command line tools.

Preferred Qualifications
• - Experience working with REST and RPC service patterns and other client/server interaction models and API design.
• - Experience working in NoSQL databases like MongoDB , DynamoDB
• - Experienced with an Agile software development
• - 5+ years of experience as a full stack engineer or back end engineer.
• - Experience developing complex software systems that have been successfully delivered to customers.
• - Knowledge of professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.
• - Experience in communicating with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.

Company - ADSIPL - Karnataka

Job ID: A2327585",Bengaluru,True,False,True,True,False,True,True,False,False,False,False,False,False,False,False,False
RingCentral,Senior Data Engineer,"Say hello to possibilities.

It’s not everyday that you consider starting a new career challenge.

We’re RingCentral, a global leader in cloud-based communications and collaboration software. We are fundamentally changing the nature of human interaction—giving people the freedom to connect powerfully and personally from anywhere, at any time, on any device.

We’re a $2 billion company that’s growing at 30+% annually and we’re expanding our Solutions Engineering Team to make sure we stay ahead of the competition.

We’re currently looking for:

As a Senior Data Engineer, you’ll be a part of our growing engineering team and help our core data layer, which includes data services, core application schema(s), data lake, data connectors, and data transformation. You’ll have the chance to partner closely with our engineering teams, data analysts, product management, business operations and be the glue that connects the dots. We are looking for an experienced Data professional who is a problem solver, logical thinker and passionate about everything relating to data and analytics.

Responsibilities:
• Create and maintain optimal data pipeline architecture.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake, and AWS ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data related technical issues and support their data infrastructure needs.
• Keep our data separated and secure across national boundaries through multiple data centers and Snowflake, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

Desired Qualifications:
• Minimum of 7 years of experience in a Data Engineer role.
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Experience supporting and working with cross-functional teams in a dynamic environment.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Experience with Kafka, Talend, Stream sets.
• Nice to have experience with big data tools: Hadoop, Spark etc.
• Nice to have experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.

What we offer:
• Mediclaim Benefits
• Paid Holidays
• Casual/Sick Leave
• Privilege Leave
• CaRing Days
• Bereavement Leave
• Maternity Leave
• Paternity Leave
• Wellness Coaching
• Employee Referral Bonus
• Professional Development Allowances
• Night Shift Allowances

RingCentral’s Engineering team works on high-complexity projects that set the standard for performance and reliability at massive scale. What kind of scale? Millions of users today and hundreds of millions tomorrow. This is your chance to help imagine, develop and deliver products that raise the technological bar, and power human connections. If you’re a talented, ambitious, creative thinker, RingCentral is the perfect environment to join a world class team and bring your ideas to life.

RingCentral’s work culture is the backbone of our success. And don’t just take our word for it: we are recognized as a Best Place to Work by Glassdoor, the Top Work Culture by Comparably and hold local BPTW awards in every major location. Bottom line: We are committed to hiring and retaining great people because we know you power our success. RingCentral offers on-site, remote and hybrid work options optimized for the ways we work and live now.

About RingCentral

RingCentral, Inc. (NYSE: RNG) is a leading provider of business cloud communications and contact center solutions based on its powerful Message Video Phone™  (MVP™) global platform. More flexible and cost effective than legacy on-premises PBX and video conferencing systems that it replaces, RingCentral® empowers modern mobile and distributed workforces to communicate, collaborate, and connect via any mode, any device, and any location. RingCentral is headquartered in Belmont, California, and has offices around the world.

RingCentral is an equal opportunity employer that truly values diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
CaaStle,Data Engineer,"Role

We are seeking a self motivated Software Engineer with hands-on experience to build sustainable data solutions, identifying and addressing performance bottlenecks, collaborating with other team members, and implementing best practices for data engineering. Our engineering process is fully agile, and has a really fast release cycle - which keeps our environment very energetic and fun.

What You'll Do
• Design and development of scalable applications.
• Collaborate with tech leads to get maximum understanding of underlying infrastructure.
• Contribute to continual improvement by suggesting improvements to the software system.
• Ensure high scalability and performance
• You will advocate for good, clean, well documented and performing code; follow standards and best practices.

We'd Love For You To Have
• Education: Bachelor/Master Degree in Computer Science
• Experience: 1-3 years of relevant experience in BI/Big-Data with hands-on coding experience

Mandatory Skills
• Strong in problem-solving
• Good exposure to Big Data technologies, Hive, Hadoop, Impala, Hbase, Kafka, Spark
• Strong experience of Data Engineering
• Able to comprehend challenges related to Database and Data Warehousing technologies and ability to understand complex design, system architecture
• Experience with the software development lifecycle, design, develop, review, debug, document, and deliver (especially in a multi-location organization)
• Working knowledge of Java, python

Desired Skills
• Experience with reporting tools like Tableau, QlikView
• Awareness of CI-CD pipeline
• Inclination to work on cloud platform ex:- AWS
• Crisp communication skills with team members, Business owners.
• Be able to work in a challenging, dynamic environment and meet tight deadlines

At CaaStle, we pioneered the clothing rental model and are now powering it for everyday apparel and accessories. Our workplace consists of an inspiring community of people from unique and diverse backgrounds, and our culture is built upon a foundation of respect and camaraderie. Join us in changing the face of fashion.

CaaStle is committed to equality of opportunity in employment. It has been and will continue to be the policy of CaaStle to provide full and equal employment opportunities to all employees and candidates for employment without regard to race, color, religion, national or ethnic origin, veteran status, age, sexual orientation, gender identity, or physical or mental disability. This policy applies to all terms, conditions and privileges of employment, such as those pertaining to training, transfer, promotion, compensation and recreational programs.",Bengaluru,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,False
Impetus,GCP Data Engineer,"Qualification
• The candidate should have extensive production experience (3-5 Years ) in GCP, Other cloud experience would be a strong bonus.
• Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
• Exposure to enterprise application development is a must

Role
• 6-10 years of IT experience range is preferred.
• Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.
• Strong experience in Big Data technologies – Hadoop, Sqoop, Hive and Spark including DevOPs.
• Good hands on expertise on either Python or Java programming.
• Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
• Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
• Ability to drive the deployment of the customers’ workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
• Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
• Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
• Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
• Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.",इन्दौर,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
GrenoSearch India Pvt. Ltd.,Data Engineer,"About Team

Our Client Is a Global Product Development Company

We are a data and analytics team who manages the analytics and data related components of our platform which includes:

Developing ETL pipelines using Azure data factory or SSIS.

BI dashboards/reports using power bi

Data migration from client legacy system to Omnipresence

Integration with client systems with Omnipresence

Role Summary

BI developer with 3-5 years of hands-on experience working in a BI environment with good data analysis skills.

Good experience in developing BI reports/dashboards using power-bi.

Experience working on any of the database like SQL Server/MYSQL/Oracle etc.

Good experience in writing complex SQL queries, procedures & functions etc.

Hands on experience working with any one of the ETL tools like SQL Server Integration Services/Azure Data Factory.

Experience working with structured and unstructured data.

Good understanding on entity relationships

Key Responsibilities

Developing BI reports/dashboards using power-bi.

Developing database tables, SQL queries, procedures & functions etc in SQL Server database

required for reporting as well as ETL development (Data migration or integration).

Developing ETL jobs using Server Integration Services/Azure Data Factory.

Must Have

Required Skills and Experience

Power-Bi, Azure Data Factory or SQL Server Integration Services (SSIS) experience

Experience working on any of the database like SQL Server/MYSQL/Oracle etc.

Good experience in writing complex SQL queries, procedures & functions etc.

Translate requirements from the business and convert them into technical code.

Should have data analysis skills to debug, trace and fix problems.

Experience working with structured and unstructured data.

Self-motivated with the ability to work both independently and in a team environment.

Ready to learn new tools and technologies.

Strong analytical and problem-solving skills.

A self-starter & quick learner with high energy-can do attitude.

Strong focus on results and quality.

Excellent Communication skills (written & verbal) are mandatory.

Good To Have

Working on migration/integration project will be a plus

Experience working with any of the cloud platforms like Azure or AWS will be a plus.

Lifesciences domain experience would be a big plus

Understanding of any of the CRM application will be a plus.",,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
NAB,Senior Data Engineer [T500-7047],"Essential Skills:
• Advanced SQL skills (or equivalent database querying language), Database SQL skills (MySQL preferably), Able to write complex queries (subquery, window function, CTE, removing duplication), Able to analyse query bottleneck by “Explain” command.
• Construct RMDB database-Database modelling skills, Able to operate DDL (stored procedure, indexing, trigger, table, materialised view, view), Understand database modelling (normalisation)
• Develop, maintain and Implement Power BI dashboards,
• Understand batch job process, able to modify it, and solve batch job issue- Python (ODBC DB connection, Pandas, XML handling), PowerShell (General PS command), ETL experience.
• Extract meaningful insights out of the data.
• AWS experience -AWS EC2, RDS monitoring, parameter store
• Aware of GitHub skills- Merge/pull request/resolving conflicts, Pull/push.
• Translate business information requirements into a meaningful set of SQL queries.
• Develop and maintain key reporting metrics that drive business performance.
• Creating Views, Stored Procedures and Materialised Views in MySQL.
• Ability to parse XML tags into SQL human readable tables.

Job Requirements:
• Advanced SQL skills (or equivalent database querying language)
• 6+ years’ experience working in a similar role.
• 6+ years of experience working with BI tools such as Power BI
• Proficient in Python language
• Data Science enthusiast
• Understand Jenkin pipeline, Jira & Confluence
• Advanced MS Office skills, including Excel, PowerPoint, Access etc.
• Ability to deal with ambiguity, solve complex problems, and navigate large, global organisations.
• Self-motivated, assertive, analytical, and comfortable working in a fast-paced environment
• Stakeholder management

Department : Group Security

Sub Department : Cyber Security

Job code : AAZA01",Gurugram,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
AECOM,Data Engineer,"Company Description

At AECOM, we’re delivering a better world.

We believe infrastructure creates opportunity for everyone. Whether it’s improving your commute, keeping the lights on, providing access to clean water or transforming skylines, our work helps people and communities thrive.

Our clients trust us to bring together the best people, ideas, technical expertise and digital solutions to our work in transportation, buildings, water, the environment and new energy. We’re one global team – 47,000 strong – driven by a common purpose to deliver a better world.

Here, you will have freedom to grow in a world of opportunity.

We will give you the flexibility you need to do your best work with hybrid work options. Whether you’re working from an AECOM office, remote location or at a client site, you will be working in a dynamic environment where your integrity, entrepreneurial spirit and pioneering mindset are championed.

You will help us foster a culture of equity, diversity and inclusion – a safe and respectful workplace, where we invite everyone to bring their whole selves to work using their unique talents, backgrounds and expertise to create transformational outcomes for our clients.

We will encourage you to grow and develop your career with us through our technical and professional development programs and diverse career opportunities. We believe in leadership at all levels. No matter where you sit in the organization you can make a lasting impact on the projects you work on, the teams and committees you join and our business.

We offer competitive pay and benefits, well-being programs to support you and your family, and the development resources you need to advance your career.

When you join us, you will connect and collaborate with a global network of experts – planners, designers, engineers, scientists, consultants, program and construction managers – leading the change toward a more sustainable and equitable future. Join us and let’s get started.

Job Description

The role will be part of the Data and Analytics Team responsible for expanding and optimizing AECOM’s data and data pipeline architecture, data flow, and collection for cross-functional teams. The role will support software developers, database architects, data analysts, and data scientists on data initiatives and will ensure consistent optimal data delivery architecture throughout ongoing projects.

Responsibilities & Duties
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep AECOM’s data separated and secure across national boundaries through multiple data centres and regions.
• Create data tools for analytics and data scientist team members -to assist them in building and optimizing our product into an innovative industry leader.

Qualifications & Requirements .

Minimum Requirements:
• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or relevant discipline in the quantitative field
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Advanced working SQL/nosql, ADLS, Databricks, ADF, Azure DevOps
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency,and workload management.
• Demonstrated ability to manipulate, process, and extract value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.

Preferred Qualifications
• Experience with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Qualifications

Additional Information

With infrastructure investment accelerating worldwide, our services are in great demand, and there’s never been a better time to be at AECOM! Join us, and you’ll get all the benefits of being a part of a global, publicly traded firm – access to industry-leading technology and thinking and transformational work with big impact and work flexibility.

AECOM provides a wide array of compensation and benefits programs to meet the diverse needs of our employees and their families. We also provide a robust global well-being program. We’re the world’s trusted global infrastructure firm, and we’re in this together – your growth and success are ours too.

As an Equal Opportunity Employer, we believe in each person’s potential, and we’ll help you reach yours.

Join us and let’s get started.

ReqID: J10082453

Business Line: Geography OH

Business Group: DCS

Strategic Business Unit: GBS

Career Area: Digital & Engineering Technology",Bengaluru,True,False,True,True,False,True,False,False,False,False,False,False,True,False,True,False
MSD,Data Engineer,"Job Description

Data Engineer

At Our Company on behalf of patients around the world. We are seeking those who have a passion for using data, analytics, and insights to drive decision making, that will allow us to tackle some of the world’s greatest health threats.

Within our commercial Insights, Analytics, and Data organization we are transforming to better power decision-making across our end-to-end commercialization process, from business development to late lifecycle management. As we endeavor, we are seeking a dynamic talent for the role of Data Engineer

For the Data Engineer role, we are looking for professional with experience in designing, developing, and maintaining data pipelines. We intend to make data reliable, governed, secure and available for analytics within the organization. As part of a team this role will be responsible for data management with a broad range of activities like data ingestion to cloud data lakes and warehouses, quality control, metadata management and orchestration of machine learning models. We are also forward looking and plan to bring innovations like data mesh and data fabric into our ecosystem of tools and processes.

Primary Responsibilities:

· Design, develop and maintain data pipelines to extract data from a variety of sources and populate data lake and data warehouse

· Develop the various data transformation rules and data modeling capabilities

· Collaborate with Data Analyst, Data Scientists, Machine Learning Engineers to identify and transform data for ingestion, exploration, and modeling

· Work with data governance team and implement data quality checks and maintain data catalogs

· Use Orchestration, logging, and monitoring tools to build resilient pipelines

· Use test driven development methodology when building ELT/ETL pipelines

· Develop pipelines to ingest data into cloud data warehouses

· Analyze data using SQL

· Use serverless AWS services like Glue, Lambda, StepFunctions

· Use Terraform Code to deploy on AWS

· Containerize Python code using Docker

· Use Git for version control and understand various branching strategies

· Build pipelines to work with large datasets using PySpark

· Develop proof of concepts using Jupyter Notebooks

· Work as part of an agile team

· Create technical documentation as needed

Education:

· Bachelor’s Degree or equivalent experience in a relevant field such as Mathematics, Computer Science, Engineering, Artificial Intelligence, etc.

Required Experience and Skills:

· 1 to 3 years of relevant experience

· Good experience with AWS services like S3, ECS, Fargate, Glue, StepFunctions, CloudWatch, Lambda, EMR

· SQL

· Proficient in Python, PySpark

· Good with Git, Docker, Terraform

· Ability to work in cross functional teams

Preferred Experience and Skills:

· Any AWS developer or architect certification

· Agile development methodology

Our Human Health Division maintains a “patient first, profits later” ideology. The organization is comprised of sales, marketing, market access, digital analytics and commercial professionals who are passionate about their role in bringing our medicines to our customers worldwide.

Who we are …

We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.

What we look for …

Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us—and start making your impact today.

We are proud to be a company that embraces the value of bringing diverse, talented, and committed people together. The fastest way to breakthrough innovation is when diverse ideas come together in an inclusive environment. We encourage our colleagues to respectfully challenge one another’s thinking and approach problems collectively. We are an equal opportunity employer, committed to fostering an inclusive and diverse workplace.

Current Employees apply HERE

Current Contingent Workers apply HERE

Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.

Employee Status:
Regular

Relocation:
Domestic

VISA Sponsorship:

Travel Requirements:

Flexible Work Arrangements:

Shift:

Valid Driving License:

Hazardous Material(s):

Required Skills:
Business Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Project Management, Software Development, Stakeholder Relationships

Preferred Skills:

Requisition ID:R228867",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.
Responsibilities

• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements

• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Mumbai,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
Mercedes-Benz Research and Development India Private Limited,Big Data Engineer,"AufgabenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team playerQualifikationenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team player",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications

• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Atlassian,Senior Data Engineer,"Working at AtlassianAtlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.Your future teamThe Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company. What you'll do Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.Design and implement real-time data processing pipelines using Apache Spark Streaming.Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.Develop and implement data governance procedures to ensure data security, privacy, and compliance.Implement new technologies to improve data processing and analysis.Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment. Your background A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar roleExperience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools. Experience with Databricks and its APIs.Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.Champion automated builds and deployments using CICD tools like Bitbucket, GitExperience working with large-scale, high-performance data processing systems (batch and streaming) Great to have, not mandatory Experience working for SAAS companiesExperience with Machine LearningCommitted code to open source projectsExperience building self-service tooling and platformsOur perks & benefitsTo support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.About AtlassianThe world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.To learn more about our culture and hiring process, explore our Candidate Resource Hub.",Bengaluru,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
MPOWER Financing,"Data Engineer - Data and Analytics - Bangalore, India","THE COMPANY

MPOWER’s borderless loans and scholarships enable students from around the world to realize their full academic and career potential by attending top universities in the U.S and Canada.

As a mission-oriented fintech/edtech company, we move extremely quickly and leverage the latest technologies, global best practices, and heavy analytics to tackle one of the biggest challenges in financial inclusion. We’re backed by over $150 million in equity capital from top global investors, which enables fast growth and provides our company with financial stability and a clear path to an IPO over the coming years.

Our global team is composed of former management consultants, financial service and technology professionals, and other experts in their respective fields. We work hard, have fun, and believe strongly in our cause. For us, MPOWER’s mission is personal.

As a member of our team, you’ll be challenged to think quickly, act autonomously, and constantly grow creatively in an environment where fast change and exponential growth are the norm. Ideation and implementation happen very quickly. We value feedback and emphasize personal and professional development by providing the resources you need to further your skills and grow with the company. MPOWER is committed to cultivating your strengths and curiosity and helping you make an immediate impact.

MPOWER has been named one of the best fintechs to work for by American Banker for 2018, 2019, 2020, and 2021. We pride ourselves on being a “growth company for grown-ups,” where there are no pool tables but rather great health, education, and maternity/paternity benefits instead. Our team diversity has been recognized as well; we’re one of the most diverse workforces in the world in terms of nationality, gender, religion, age, sexual orientation, and educational background.

THIS IS A FULL-TIME POSITION, BASED IN OUR BANGALORE OFFICE

THE ROLE

You will be tasked with building and maintaining MPOWER’s data infrastructure. You’ll also play a key role in acquiring, organising and analysing data to provide insights that enable the company in making sound business decisions. This includes, but is not limited to:
• Maintaining MPOWER’s database and building on the existing database infrastructure
• Establishing the needs of different users and monitoring user access and security
• Capacity planning and refining the physical design of the database to meet system storage requirements
• Creating efficient queries and tools to obtain data for different business needs
• Building data models to identify, analyze and interpret trends or patterns in data sets that inform business decisions and strategy
• Working with various internal and external stakeholders to maintain and develop enhanced data collection systems
• Performing periodic data analyses, creating and presenting findings and insights
• Performing scheduled data audits in order to locate and correct code errors and maintain data integrity
• Collaborating with MPOWER’s global tech team to build data collection and data analysis tools

THE QUALIFICATIONS
• Undergraduate degree in computer science; advanced degree preferred
• 5+ years of experience in database programming, database administration and data analysis
• Must have prior experience in building high quality databases in accordance with end users information needs and views
• Proficiency in Big Data and Hadoop ecosystems.
• Deep familiarity with database design and documentation
• Hands-on expertise and exposure to at least one database technology (MySQL, PostgreSQL)
• Advanced knowledge of R/Python, PySpark, or Scala is a plus
• Prior experience building data pipelines and data orchestration is a plus.
• Superior analytical and problem solving skills
• Proven ability to create and present comprehensive reports
• Ability to multitask and own several key responsibilities at a given time
• Passion for excellence: constantly striving to improve professional skills and business operations

A passion for financial inclusion and access to higher education is a must, as well as comfortable working with a global team across multiple-time zones and sites!

In addition, you should be comfortable working in a fast growth environment, meaning a small agile team, fast-evolving roles and responsibilities, variable workload and tight deadlines, a high degree of autonomy, and 80-20 everything.

MPOWER Financing focuses on Financial Services, Finance, Finance Technology, Higher Education, and Education Technology. Their company has offices in New York City, Washington DC, and Washington. They have a small team that's between 11-50 employees. To date, MPOWER Financing has raised $7.291M of funding; their latest round was closed on October 2016.

You can view their website at http://www.mpowerfinancing.com/ or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,False,False,False
"6221, Roche",Senior Data Engineer,"The Position

Roche sequencing solution is developing the next generation sequencing based on nanopore technology. This has the potential to make sequencing based diagnostics cheaper, faster and more accurate enabling precision medicine and early diagnosis of many diseases improving the health outcome.

As part of Data Science Automation group, you will get to work on key software technologies enabling research and development of sequencer. You will solve complex problems related to processing terabytes of data coming out sequencer and deriving useful insights from the data. This requires massively parallel computation locally on GPU as well as in the cloud. You will gain exposure to latest and greatest in data engineering and data pipeline tools and technologies. You will also work with advanced data visualization problems involving millions of data points.

You also will get to collaborate with multidisciplinary team of scientists and engineers working in fields ranging from protein engineering, bio chemistry, biophysics, stats modeling, bioinformatics and deep learning.

If you are excited to become part of the next generation sequencing research and development and revolutionize the healthcare, we have a rare opportunity for you to come and work with us.

We need an experienced Data/Workflow Engineer with a strong background in designing and developing highly scalable data management solutions and workflow pipelines. You will work across a variety of problems and application spaces involved in high availability systems, for data management and compute systems, at a very large scale. You will be working with a hardworking team of engineers and data-scientists who are passionate about building creative and novel solutions at the forefront of Sequencing research.

Required Qualifications:
• BS in CS or similar and 10+ years professional experience, or MS with 7+ years of experience in building highly scalable, performant software systems, in a Linux environment.
• Strong, hands on experience building and supporting Enterprise level Workflow management systems such as airflow, nexflow, kubeflow etc. Experience with building performant airflow pipelines with a large number of DAGs and dynamic DAGs is desirable.
• Working experience in deploying and managing airflow platforms, knowledge of Terraform, Kind, Helm etc. is a huge plus.
• At least 3+ years’ experience of developing solutions using container and cloud technologies. Preferably Kubernetes, Docker.
• Experience building with cloud native technologies (e.g. GCP, AWS), blob stores and knowledge of various data compression formats.
• Demonstrated skill with software development following current software engineering best practices using languages such as: Python, Java and BASH scripting.
• Have a strong understanding of modern software development practices and tools, including: version control systems (e.g., Git), issue trackers, and test frameworks.
• Experience building and using automation tools, CI/CD, unit testing.

You 'll go above and beyond our required requirements if you...
• Possess a PhD/MS in Computer Science, Computer Engineering, or another related, technical discipline.
• Have at least ten years of relevant experience in the development of software systems ideally in a Linux environment.
• Have experience using modern frontend and backend software frameworks for software applications.
• Knowledge of challenges involved in large-scale, high-availability data platforms. Experience with designing and implementing platforms providing secured access to large datasets.
• Have experience applying software expertise to full project lifecycles, including requirements analysis, design, implementation, and testing.

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,True,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False
"NTT Ltd., NTT",Data Engineer,"NTT is a leading global IT solutions and services organisation that brings together people, data and things to create a better and more sustainable future.

In today’s ‘iNTTerconnected’ world, connections matter more now than ever. By bringing together talented people, world-class technology partners and emerging innovators, we help our clients solve some of the world’s most significant technological, business and societal challenges.

With people at the heart of our success, NTT is committed to attracting and growing the best talent and providing an environment where everyone feels they can belong and their contribution matters.

Want to be a part of our team?
Compile baseline data sets of internal / client requirements and maintain data trackers required for visualization.
Identifying relevant data sources for business needs
Sourcing missing data
Organising data into meaningful format to the business
Building dashboards for visualisation to arrive at business decisions.
Enhancing the data collection process
Processing, cleansing, and validating the integrity of data to be used for analysis.
Analyse dataset to find patterns and solutions.

Working at NTT

Roles and responsibilities:
• Proficiency with advanced techniques in MS Excel like use of Statistical analysis in Excel, Macros etc
• Demonstrate good analysis skills with handling large data sets
• Experience with Data Visualisation Tools like Microsoft PBI or any other
• Hands-on experience with data science tools
• Have experience in Python scripting
• Good verbal and written communication skills
• Problem-solving aptitude
• Degree in Computer Science, Engineering or relevant field
• 2-5 years of relevant experience

What will make you a good fit for the role?

Join our growing global team and accelerate your career with us. Apply today.

A career at NTT means:
• Being part of a global pioneer – where you gain exposure to our Fortune 500 clients and world-leading global technology partners and work with a network of over 40,000 smart and diverse colleagues across 57 countries, delivering services in over 200 countries.
• Being at the forefront of cutting-edge technology – backed with a 150-year heritage of using technology for good. With 40% of the world’s internet traffic running on our network and where Emoji were first invented, you can be proud of the group’s many new ‘firsts’.
• Making a difference – by doing meaningful work that helps to shape the future for our clients, and across industries and communities around the world.
• Being your best self – in a progressive ‘Connected Working’ environment that promotes flexibility, connection and wellbeing. Where diversity and different perspectives are embraced to ensure equal opportunities for all.
• Having ongoing opportunities to own and develop your career – with a personal and professional development plan and access to the broadest learning offerings in the industry.",Bengaluru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
lululemon India Tech Hub,Data Engineer - SQL & Python,"We are looking to hire dynamic Data Engineers for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.

Responsibilities
• Uses structured tools for analysis and presentation of concepts and models to enhance the BRD
• Develop, maintain and deliver training materials to the supply chain end-users
• Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs
• Support day to day reporting needs where required
• Support production issues as relate to application functionality and integrations
• Excellent spoken and written communication skills (verbal and non-verbal)
• Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)
• Solid scripting capability for analysis and reporting (ANSI SQL)
• Solid experience in RDBMS and NoSql technologies
• Strong analytical skills to support BAs.
• Strong problem-solving skills (Math skills required for data modeling)
• Ability to work as an integration / data engineer.
• Ability to manage and complete multiple tasks within tight deadlines
• Possess expert level understanding of software development practices and project life cycles.
• Working experience with Java batch spring boot/ python.
• Working Experience with cloud-native technologies
• Must have: Working experience in dealing with big data and data manipulation.
• Desired: Familiarity with Retail planning / merchandising systems/ supply chain.
• Desired: Familiarity with DevOps practices like CICD pipeline
• Desired: Retail experience is a plus. (fashion retail experience would be ideal)
• Must Have: Working experience with cloud platforms namely AWS
• Must Have: Working experience with large data sets (at least 80 – 100 GB data)

Requirements
• name : lululemon India Tech Hub
• location : Bengaluru, IN
• experience : 5 - 8 years
• Primary Skills: SQL or RDBMS or NoSQL,Python,AWS,Springboot,ETL",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Cloud Software Group,Senior Data Engineer,"About Cloud Software Group

Cloud Software Group combines the capabilities of both Citrix and TIBCO, creating one of the world’s largest cloud software providers, serving more than 100 million users around the globe. When you join Cloud Software Group, you are making a difference for real people, each of whom count on our suite of cloud solutions to get work done – from anywhere. Members of our team will tell you that we value diverse lived experiences, varied perspectives, and having the courage to take risks. Our teams are encouraged to learn, dream, and build the future of work. We are on the brink of another Cambrian leap - a moment of immense evolution and growth. And we need your expertise and experience to do it. Now is the perfect time to move your skills to the cloud.

Position Summary

This is an individual contributor role with responsibility for supporting all data warehouse processes including technical analysis, design, development, implementation, and support of ETL solutions. The ideal candidate needs to have at least 5 years of experience developing with Microsoft SQL applications in an implementation and support role of a business intelligence organization.

Primary Duties / Responsibilities

Responsibilities will include, but are not limited to:

• Supporting the designs, tasks, and continuous improvements to maintain a scalable data warehouse

• Analyzing and validating data to ensure that business requirements are satisfied

• Creating data flow diagrams to depict business logic relating to data transformations

• Creating conceptual, logical, and physical data models for relational and dimensional solutions

• Breaking down, estimating, and executing increments of work

• Developing ETL packages of high complexity to fulfill all the business requirements

• Supporting deployment and delivery of defined technical solutions

• Communicating accurate and timely project status, issues, risks, and scope changes

• Performing root cause analysis of data discrepancies

• Creating data dictionaries and business glossaries to document data lineages, data definitions and metadata for all business-critical data domains

• Documenting all work (both technical and procedural) and ensuring that co-workers understand how to support system from an operational perspective

• Working in a highly collaborative team environment following the Agile Framework for planning and executing deliverables

Qualifications (include knowledge, skills, abilities, and related work experience)

• Bachelor’s degree in computer science or related field, or equivalent combination of education and recent, relevant work experience

• Minimum 5 years of experience in developing T-SQL Queries, Stored Procedures, and ETL packages with Microsoft SQL databases

• Strong understanding of data warehouse design and report development principles

• Experience in creating data flow diagrams and data models pertaining to business intelligence

• Experience in analyzing and developing reporting output such as Power BI, Tableau, or SSAS

• Strong interpersonal and problem resolution skills

• Strong teamwork and customer support focus

• Strong written (technical documentation) and verbal communication skills

• Ability to handle numerous conflicting priorities in a professional manner

Cloud Software Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all federal, state and local laws that prohibit employment discrimination on the basis of age, race, color, gender, sexual orientation, gender identity, ethnicity, national origin, citizenship, religion, genetic carrier status, disability, pregnancy, childbirth or related medical conditions, marital status, protected veteran status, and other protected classifications.",Bengaluru,False,False,True,False,False,False,False,False,True,True,True,False,False,False,False,False
Magna HR Consultant India Pvt. Ltd.,Data Engineer,"We are hiring for a Swedish MNC for Data Engineer position.

(Apply only having 3+ relevant Exp in Azure)

Must have 3+ years of experience in Azure Ecosystem

Location - Pune (Work from office)

Notice Period: 1 month or less

· Extensive experience with SQL coding and SQL Server databases.

· Extensive experience delivering data solutions using Databricks

· Extensive experience designing and creating Power BI dashboards and reports

· Experience of Extract, Transform and Load technologies and techniques.

· Familiarity with the Azure ecosystyem ( DataBricks, DataFactory, KeyVault, LogicApps )

· Must be able to collaborate with colleagues with and without technical expertise.

· Must possess excellent written and verbal communication skills",Pune,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
CoffeeBeans,Data Engineer,"Job Responsibilities
• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Encouraging open communication and advocating for shared outcomes

Technical Skills
• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Spark (Scala) and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments

Professional Skills
• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more

Skills:- Hadoop, Scala, Spark, Data modeling, Data engineering, Cloudera and Amazon Web Services (AWS)",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Horizontal Talent,GCP DATA ENGINEER,"Position: GCP Data engineer

Experience level: 4-7 years

Location: Pune (Hybrid)

Notice period: IMMEDIATE TO 15 days

Required: GCP,bigquary,data flow,sql

If interested kindly share your CV at mpadhiyar@horizontal.com",Pune,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Medha Analytics,Senior Data Engineer,"About Narayana Health:

Narayana Health is headquartered in Bengaluru, India, and operates a network of hospitals in India and Overseas. Our mission is to deliver high quality, affordable healthcare services to the broader population.

For more details, please refer to our website at: https://www.narayanahealth.org

About Medha Analytics:

Medha Analytics works to simplify healthcare by unleashing the potential of data. With a powerful tech-stack, interesting ideas and inspired co-workers, no dream is too big at Medha Analytics.

For more details, please refer to our website at: https://www.medha.health

Job Description:
• Understanding business processes, understanding the software systems for retrieval, prepping and modelling data to build/maintain modern data platform for supporting reports and Dashboards.
• Create and maintain optimal data pipeline architecture.
• Assemble large, complex data sets that meet functional/non-functional business requirements.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Azure technologies.
• Work with stakeholders including the Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Work with data and analytics experts to strive for greater functionality in our data systems.

Candidate Requirement:

Education: B.E. or B.Tech

Experience: 3 to 6 Yrs as Data Engineer

Knowledge & Skills:
• Experience of working on any of the public cloud AWS/Azure/GCP
• Experience working on Python/Scala/Java, Big Data technologies like spark and Hadoop File System (HDFS)
• Strong hands-on experience in writing code in Pyspark/Python/Scala/Shell language using best practices.
• Experience on Performance tuning of Spark Jobs and cluster configurations
• Any one
• Knowledge of AWS services viz., EC2, EMR, S3, Lambda, EBS, IAM, Redshift, RDS, GLUE ETL is desired.
• Knowledge of Azure services viz., ADF, Databricks, Azure Synapse Analytics, ADLS Gen2,Azure Devops is desired.
• Proficient in writing highly optimized SQL’s and algorithm for data processing
• Sound understanding of the cloud relational and non-relation database, their storage concepts, best practices and use case.
• Knowledge in RESTful Webservice, Microsoft SQL Server, MySQL Server, and Agile methodology is an advantage.
• DevOps toolchain experience is plus (Docker/Kubernetes/Ansible/Git)
• Strong analytical, problem-solving, and communication skills
• Excellent command of both written and spoken English.
• Should be able to Design, Develop, Deliver & maintain Data Infrastructures.",,True,False,True,True,False,False,False,True,False,False,False,False,True,False,False,False
Grow Your Staff,Data Engineer,"Grow Your Staff is looking for a Data Engineer for an e-commerce- tech firm based in Warsaw, Poland. The position is a full-time opportunity.

The role will have excellent growth opportunities. You will work directly with the team based in Poland from 12:30 p.m. to 9:30 p.m. (Monday - Friday).

Experience required: 4-7 years

CTC: INR 9-15 LPA

About the client

The client is an e-commerce- tech company, based out in Poland. They help brands and retailers conquer new markets successfully via online marketplaces like Amazon and eBay. They cater to end-to-end solutions like automated localization services, multi-language SEO & SEM, and logistics so that the product can reach and convert international buyers affordably.

They are looking for a Data Engineer responsible for managing data in AWS and optimizing databases. As a Data Engineer, you will play a key role in designing and implementing scalable, reliable, and efficient data solutions that meet the business needs of the company.

Responsibilities
• Develop and maintain ETL pipelines in Airflow, Python, and Spark.
• Standardize and clean data from various sources for internal reporting.
• Manage the Data Lake and Data Warehouse in AWS.
• Architect and implement scalable Data Streaming solutions.
• Deploy and manage infrastructure using Docker and Kubernetes.
• Optimize SQL queries and ensure version control using Git.
• Monitor and troubleshoot data pipelines to ensure accuracy.
• Stay updated with new technologies and provide recommendations to improve existing systems.

Requirements

Must haves:
• Data Lake (AWS S3, EMR, Glue, Athena, Kinesis, Apache Hudi, Delta Lake)
• Data Warehouse (AWS Redshift)
• ETL (Airflow, Python, Spark)
• Infrastructure (Kubernetes, Docker, CI/CD)
• Python (pandas, numpy, boto3)
• 3rd party APIs (Salesforce, Marketo, Zendesk, Zuora)

Qualifications
• Bachelor’s or Master’s degree in computer science engineering, a related discipline, or equivalent
• Exceptional debugging and problem-solving aptitude
• Excellent verbal, written, and interpersonal skills in English
• Experience working in an Agile environment
• Ability to work individually, within a team, and with other groups

Follow us on Instagram for updates: https://www.instagram.com/growyourstaff/",,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,False
MASTER MIND CONSULTANCY,Principal Data Engineer,"Job Responsibility:

Responsible for providing highly advanced subject matter expertise for the specialism through the comprehensive use of relevant tools and techniques, driving continuous improvement in systems and processes, overseeing the implementation of the relevant standards, and contributing to strategic development for the specialism to ensure that deliverables continue to successfully meet the needs of the context. Specialisms: Business Analysis; Data Management and Data Science; Digital Innovation.

What you will deliver:

-Leads grows and develops a team of data engineers that write, deploy, and maintain software to build, integrate, manage, maintain, and quality-assure data.

- Creates positive engagement and drives an inclusive work environment with team and stakeholders through the quality of interactions and collaboration across multiple business entities.

- Effectively works with cross-disciplinary collaborators and stakeholders across multiple business entities.

- Architects and designs reliable and scalable data infrastructure.

- Advocates for and ensures their team adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),

- Responsible for delivering and deploying secure and well-tested software that meets privacy and compliance requirements.

- Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they oversee, responsible for defining and maintaining SLAs.

- Actively contributes to improving developer velocity.

- Actively mentors others and build internal capability.

- Drives and promotes a data-driven culture and mindset

Requirement:

- BS degree in computer science or a related field

- Experience (typically 6+ years) leading, growing, and developing a data engineering team of around 30-150 people required.

- Deep and hands-on experience (typically 14+ years) designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments

- Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)

- Deep understanding and proven experience of data modeling and information architecture that underpin most Finance, Procurement, ERP, Human Resources data and analytics

- Experience of building data products using Azure Data Services and Databricks

- Experience in a variety of data modeling techniques

- Experience in building a varied portfolio of data products and use cases

- Experience in building data products for machine learning and advanced AI use cases

- Experience in improving data delivery velocity and reliability including through automation

- Experience designing and implementing large-scale distributed systems

- Deep knowledge and hands-on experience in technologies across all data lifecycle stages

- Strong stakeholder management and ability to lead large organizations through the influence

- Continuous learning and improvement mindset

- No prior experience in the energy industry is required",Pune,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False
Conneqt Digital,AWS Data Engineer- Glue,"Total Experience Required: 5 to 8 Years

Location: Across India

Work Mode: Hybrid

Notice Period: Immediate Joiner (we are looking for a candidate with 5+ years of experience in AWS Data Engineer and can join us in the month of May only)

Skills Required:
• Minimum 5+ years’ experience in AWS Data Engineer Glue.
• In-depth knowledge and extensive experience to build batch based workloads on AWS using AWS EMR, AWS GLUE, AWS Athena, AWS Dynamo DB, AWS REDSHIFT, AWS RDS, AWS Aurora.
• Proven practical experience in migrating relational data base from on-prem to cloud using AWS DMS
• Excellent in Written and Verbal Communication
• Experience with building S3 buckets, using S3 and Glacier for storage and backup
• Extensive experience with AWS data services: Redshift, RDS, DynamoDB, Data Pipeline, EMR.
• Extensive experience writing and tuning SQL queries

Interested aspirants can apply or for a quick response please share your resume to the below-mentioned mail ID: Pavithra.tr@conneqtcorp.com",,False,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False
PepsiCo,"Architect,Azure Data Engineer","Overview

The person will deliver and lead development for Azure Data Lake-Data Engineering, Business Intelligence, Azure, and Advanced Analytics. He or She will be playing the role of scrum master across multiple projects
• Data and Digital Solutions Delivery for data and advanced analytics for AMESA and APAC for functional Areas – Supply Chain, Sales & Distribution, GTM, Finance, Equipment Service, Food Service
• Work on Azure data engineering models/pipelines for AMESA/APAC Projects and solution delivery.
• Key skill sets like Azure data engineering, Azure Data bricks, Azure Synapse/SQL.
• New Business Requirements / Demand Management / Estimations / Work Intake / Project delivery & sustain

In-depth technical knowldge of tools like Azure Data factory, data bricks , Azure Synapse , SQL DB, ADLS etc.
• Deliver ETL solution including data extraction, transformation, cleansing, data integration and data management. Implement batch & near real time data ingestion pipelines based on reference architecture.
• Ability to augment with new sources of data including internal/external untapped data. Contribute to the establishment and maintenance of cloud computing platform and big data services.
• Operationalize analytics models for production usage with big data workflows, proper security & access control.
• Nice to have skills Python, Scala, Java & with advance knowledge of SQL

Responsibilities
• Data and Digital Solutions Delivery for data and advanced analytics for AMESA and APAC for functional Areas – Supply Chain, Sales & Distribution, GTM, Finance, Equipment Service, Food Service
• Work on Azure data engineering models/pipelines for AMESA/APAC Projects and solution delivery.

Work on development of Azure Data engineering pipelines and lead projects/ work as a developer.
• Drive solution design and build to ensure scalability, performance and reuse
• Ensure on time and on budget delivery which satisfies project requirements, while adhering to enterprise architecture standards.
• Manage work intake, prioritization and release timing; balancing demand and available resources. Ensure tactical initiatives are aligned with the strategic vision and business needs

Qualifications

Years of Experience:
• Bachelor’s degree in Computer Science, MIS, Business Management, or related field
• 8 - 10 years’ experience in Information Technology or Business Relationship Management
• 3-4 years’ experience in Azure Data engineering(BLOB/DATA bricks/ADF/Synapse/Delta Lake)
• 5-6 years experience in working on Data & Analytics projects.

Mandatory Tech Skills:

The role will leverage & enhance existing technologies in the area of Data and Digital Solutions like Power BI, Azure multiple technologies, Teradata/Snowflake, and other Azure services

The role will be responsible to develop IT products and solutions using these technologies and deploy them for internal business users

Mandatory Non Tech Skills:
• Must be a strong team player with effective teamwork skills
• Assists in successfully integrating new employees into the department
• Provides training and mentoring to less experienced team members
• Builds others skill sets by coaching them (i.e., coaching, mentoring).
• Leverages cooperative relationships to address complex business data issues.
• Handles day-to-day issues within the work team.
• Ability to adapt to change quickly and handle unforeseen requirements effectively.
• Assists lower level employees with resolving unforeseen requirements. Leading and setting team priorities.
• Fully understand data, systems, and end to end business processes within multiple functional areas.
• Sees and coaches others on connections between data and process. Understands business needs and the impact of his/her/team activities.
• In-depth understanding of Pepsi Co products, services, business processes and business function directly related to the functional area.
• Understands business direction and assesses business impact when evaluating data solutions.
• Proactively analyze data to increase quality as it relates to accuracy, completeness, consistency, integrity, timeliness, conformity, and validity; Verify accuracy of data in functional area and for integration/project activities.
• Provide recommendations for correcting issues or problems across both systems and process.
• Able to conduct root cause analysis.
• Accountability for mass data loads including creation of load format, data preparation, load program design, and testing and result verification in collaboration with the business, or lead the design and development of process flows and business metadata documentation.
• Understanding importance of metadata and reinforcing benefits and usage to team and business partners
• Able to research, understand, and determine the relevance of information applying to all degrees of complexity in analytical tasks. Directs research work of more junior colleagues.
• Incorporate the broad picture across business units/functions to foresee all potential impacts of system or project issues taking appropriate action to resolve.
• Able to synthesize separate elements to form a coherent whole using reasoning, logical deduction, and other methods without direction for all complexity levels of analytical/design tasks.
• Able to see connections between data and process across multiple functions and can make suggestions when resolving changes.
• Interacts and has effective partnerships with other team members, business customers and other layers within the business to recommend, design, test, and implement data related system and process improvements.
• Assists in establishing and maintaining a strong partnership with business partners and manage expectations.
• Forms productive networks with external experts to bring solutions to internal customers/partners needs.
• Prepare material and interact with executive level on subject matter expertise.
• Ability to independently prioritize own work and work of team in functional area, applying analytical skills to master data requests.
• Proactively identify and escalate potential issues, offering ideas for solutions.
• Proactively adjusts work assignments and schedule to meet changing team priorities.
• Effectively manage multiple priorities simultaneously.
• Take ownership of critical path tasks and prioritize work of self and others to deliver results within agreed upon deadline.
• Adjust to multiple demands/priorities focusing on the big picture.
• Makes key decisions about data design approach to components of projects.
• Able to explain to senior leaders how arrived at decision process.",Peeramcheru,True,False,True,True,False,False,False,False,True,False,False,False,False,False,False,True
Logic Planet,Data Engineer - Hadoop/Big Data Technologies,"Job Requirements
• In-depth knowledge of Big Data technologies - Spark, HDFS, Hive, Kudu, Impala
• Solid programming experience in Python, Java, Scala, or other statically typed programming language
• Production experience in core Hadoop technologies including HDFS, Hive and YARN
• Strong working knowledge of SQL and the ability to write, debug, and optimize distributed SQL queries
• Having advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases including NOSQL.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Having experience building and optimizing 'big data' data pipelines, architectures and data sets.
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• Excellent attention to detail and communication/presentation skills
• Experience in working in a highly collaborative environment with a heavy emphasis on teamwork
• Experience in working in Agile scrum methodology
• Experience working through all phases of SDLC including Project Management, Business Analysis, Data Analysis, Architecture Design, Development, Testing and Implementation
• Having strong project management and organizational skills.
• Proficient in process modeling, business process flow and data flow diagrams
• Extensive experience working in a data warehouse environment.
• Excellent understanding of data warehouse concepts, data marts, Extraction Transformation and Loading (ETL),Autosys, feed processing, batch processing and data modeling - ER diagrams, RDBMS
• Expert in SQL for data analysis, data integrity, data accuracy, data mining and data mapping across various database platforms like Oracle and SQL Server

,

This job is provided by Shine.com",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Prescience Decision Solutions,PRESCIENCE - Data Engineer /Senior Data Engineer,"# Role: Data Engineer _AWS

Years of Experience: 5+years

Location: Bangalore

Required Skills:

1.5+ years of experience working with Cloud Data Platforms, especially AWS S3/GLUE & Data Pipeline and its services, must be strongly experienced in building data pipelines

2. Experience with big data tools like Python or R.

3. Expertise in using query languages such as SQL, No-SQL, Hive

4. Strong experience using ETL frameworks (e.g. Airflow, Oozie, Jenkins etc.) to build and deploy production-quality ETL pipelines.

5. A continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs.

6. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.

7. Keen to learn new technologies and apply knowledge in production systems.

Interested share your resume to vidya_naik@prescienceds.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Koantek,Sr AWS Data Engineer,"Job Description

The Sr AWS Databricks Data Engineer at Koantek will use comprehensive modern data engineer techniques and methods with Advanced Analytics to support business decisions for our clients. Your goal is to support the use of data-driven insights to help our clients achieve business outcomes and objectives. You can collect, aggregate, and analyze structured/unstructured data from multiple internal and external sources and patterns, insights, and trends to decision-makers. You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators, and other end-user information portals and insight tools. You will be a critical part of the data supply chain, ensuring that stakeholders can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics. You are expected to function as a productive member of a team, working and communicating proactively with engineering peers, technical lead, project managers, product owners, and resource managers.

Requirements

● Strong experience as a AWS Data Engineer and must have AWS Databricks experience.

● Expert proficiency in Spark Scala, Python, and PySpark is a plus

● Must have data migration experience from on prem to cloud

● Hands-on experience in Kinesis to process & analyze Streaming data, and AWS DynamoDB

● In depth understanding of AWS cloud and AWS Data lake and Analytics solutions.

● Expert level hands-on development Design and Develop applications on Databricks, Databricks Workflows, AWS Managed Airflow, Apache Airflow is required.

Qualifications:

● 2+ years of hands-on experience designing and implementing multi-tenant solutions using AWS Databricks for data governance, data pipelines for near real-time data warehouse, and machine learning solutions.

● 5+ years’ experience in a software development, data engineering, or data analytics field using Python, PySpark, Scala, Spark, Java, or equivalent technologies.

● Bachelor’s or Master’s degree in Big Data, Computer Science, Engineering, Mathematics, or similar area of study or equivalent work experience

● Strong written and verbal communication skills

● Ability to manage competing priorities in a fast-paced environment

● Ability to resolve issues

● Self-Motivated and ability to work independently

● Nice to have-

- AWS Certified: Solutions Architect Professional

- Databricks Certified Associate Developer for Apache Spark",,True,False,False,True,False,False,False,False,False,False,False,True,False,False,True,False
Verizon,Manager-Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

As a Manager for Data Engineering team, you will be managing data platforms and implementing new technologies and tools to further enhance and enable data science/analytics, focus to drive scalable data management and governance practices. Leading the team of data engineers & solutions architects to deliver solutions to business teams.
• Driving the vision with leadership team for data platforms enrichment covering the areas like Data Warehousing/Data Lake/BI across the portfolio.
• Defining and executing on a plan to achieve that vision.
• Building a high-quality Data engineering team and continue to drive to scale up.
• Ensuring the team adheres to the standard methodologies on data engineering practices.
• Building cross-functional relationships with Data Scientists, Data Analysts and Business teams to understand data needs and deliver data for insight solution.
• Driving the design, building, and launching of new data models and data pipelines.
• Driving data quality across all data pipelines and related business areas.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You are curious and passionate about Data and highly scalable data platforms. People count on you for your expertise in data management in all phases of the software development cycle. You create environments where teams thrive and feel valued, respected and supported. You enjoy the challenge of managing resources and competing priorities in a dynamic, complex and deadline-oriented environment. Building effective working relationships with other managers across the organization comes naturally to you.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Two or more years of experience in leading the team and tracking the end-to-end deliverables.
• Experience in end-to-end delivery of Data Platform Solutions and working on large scale data transformation.
• Knowledge of Spark, Hive, Scala, Pig, Kafka, Pulsar, Nifi, Python, Shell scripting.
• Knowledge of Google Cloud Platform/BigQuery.
• Knowledge of Teradata.
• Experience in working with DevOps tools like Bitbucket, Artifactory, Jenkins.
• Knowledge of Data Governance and Data Quality.
• Experience in building / mentoring the team.

Even better if you have one or more of the following:
• Master’s degree.
• Experience in data engineering, big data, hadoop and DevOps technologies.
• Certifications in any Data Warehousing/Analytical solutioning.
• Certification in program/project management.
• Experience in technical leadership in architecture, design, implementation and support of large-scale data and analytics solutions that are highly reliable, flexible, and scalable.
• Ability to meet tight deadlines, multi-task, and prioritize workload.
• Experience in collaborating with cross-functional teams and managing stakeholder expectations.
• Experience in working with globally distributed teams.
• Good Communication and Presentation skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
Dell Technologies,"Advisor,  Data Engineering - Alteryx Designer","Advisor, Data Engineering - Alteryx Designer

Data Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What’s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.

Join us to do the best work of your career and make a profound social impact as an Advisor on our Data Engineering team in India.

What you’ll achieve

As an Advisor, Data Engineering, you will be supporting data used by the Accounting organization. You will be responsible for optimizing and ensuring the integrity of the data flow of large datasets between multiple interfaces, delivering data integrity solutions, performing proactive data quality validations, and supporting business teams on data reconciliations. Additionally, you will work closely with multiple IT and Business teams to develop and test roadmap projects to leverage enhanced value from the datasets.

You will:
• Be an effective liaison between Accounting and IT teams to fulfil business driven requirements.
• Identify, design and implement internal process improvements for process & data delivery optimization.
• Create and maintain reporting and validation dashboards for internal business processes.
• Analyze huge datasets in data warehouse to understand relationships between datasets and use it to improve the logical capabilities of business processes.
• Proactively identify defects in the data interface transformations and work with IT teams to rectify them.

Take the first step towards your dream career
Every Dell Technologies team member brings something unique to the table. Here’s what we are looking for with this role:

Essential Requirements
• Strong understanding of data warehousing/big data environments including 2-3 years’ experience working with Teradata and/or Greenplum and/or BW4Hana.
• Alteryx Core Designer Certified with 2+ Years’ experience working with Alteryx.
• Experience in performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytics and reasoning skills to working with structured and unstructured datasets.
• Experience in building processes supporting data transformation, data structures, metadata management.

Desirable Requirements
• Demonstrable knowledge of SAP Analysis for Office and SAP Cloud Analytics.
• Hands on experience with ETL tools.
• Bachelor’s degree or Equivalent work experience.

Here’s our story; now tell us yours

Dell Technologies helps organizations and individuals build a brighter digital tomorrow. Our company is made up of more than 150,000 people, located in over 180 locations around the world. We’re proud to be a diverse and inclusive team and have an endless passion for our mission to drive human progress.

What’s most important to us is that you are respected, feel like you can be yourself and have the opportunity to do the best work of your life -- while still having a life. We offer excellent benefits, bonus programs, flexible work arrangements, a variety of career development opportunities, employee resource groups, and much more.

We started with computers, but we didn’t stop there. We are helping customers move into the future with multi-cloud, AI and machine learning through the most innovative technology and services portfolio for the data era. Join us and become a part of what’s next in technology, starting today.

You can also learn more about us by reading our latest Diversity and Inclusion Report and our plan to make the world a better place by 2030 here.

Application closing date: 17th May 2023

Dell is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Dell are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Dell will not tolerate discrimination or harassment based on any of these characteristics. Dell encourages applicants of all ages. Read the full Equal Employment Opportunity Policy here.",,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Caterpillar,R0000197254 Data Engineer/Project Lead,"Career Area:
Information Analytics Job Description:

At Caterpillar, we work to help our customers build a better world. Without our team of talented, bright, and driven individuals, we wouldn’t be the Caterpillar we are today. Now, this is your chance to join our team and do work that matters. We want you to help us enable customer success, make progress possible around the world and help our communities grow and thrive.

Global Rental, Service & Marketing (GRSM) is a part of the Services, Distribution & Digital (SD&D) segment. GRSM is comprised of Global Rental & Used Equipment Services, Global Service and Global Marketing & Brand. A primary role of GRSM is to support and enhance Caterpillar’s dealer network, which is a competitive strength and the critical way in which we serve our customers.

The Data Engineer/Project Lead is responsible for designing, developing, and maintaining data pipelines and data warehouses. The Data Engineer/Project Lead will work closely with data scientists, business analysts and business stakeholders to assemble large, complex data sets to support services growth, including rebuild growth, service data quality, faster quoting, and service excellence at our dealerships.

This position will be responsible for building scalable, high-performance infrastructure and data driven and predictive analytics applications that provide actionable insights across Caterpillar’s product support businesses.

Job duties include, but are not limited to the following:
• Work with business stakeholders to understand data requirements and develop data solutions that meet those requirements
• Build infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
• Design, develop, and maintain data pipelines and data warehouses
• Develop and implement data quality assurance
• Operationalize the developed jobs and processes
• Monitor and troubleshoot data pipelines and data warehouses
• Create databases and infrastructure to processing data at scale
• Create solutions and methods to monitor systems and solutions
• Work in a scaled Agile environment accountable to deliver results in sprints
• Stay up-to-date on the latest data engineering technologies and best practices

Basic Qualifications:
• 7+ years of professional experience in data engineering, data warehousing
• Bachelor’s degree; preferably in Computer Science, Computer Engineering, Statistics, Economics, Mathematics, Engineering, or a similar field
• Experience with SnapLogic, SQL, Python and/or Snowflake
• The ability to comfortably and confidently working with MS Office as well as industry standard statistics and data visualization packages

Top Candidates Will Also Have :
• Understanding of Data analytics, machine learning algorithms, and data visualization
• Experience with Microsoft Azure DevOps and Agile Methodology

Caterpillar is an Equal Opportunity Employer (EEO).

Not ready to apply? Submit your information to our Talent Network here .",Chennai,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
Omnivio,Data Engineer - Full Time,"Job Profile

Omnivio is a startup in the Supply Chain and Logistics domain. We help retailers deliver an 'Amazon like' shopping experience to their customers. We optimize and manage delivery times and cost, inventory, geo-distributed stores etc. One of the core pieces of our infrastructure is the data engineering required to get data from various upstream systems into a data model that we use for intelligence. If you've worked in data engineering before, you might imagine that this has a good number of challenging engineering problems. We are looking for junior to mid level data engineers to join our team.

Experience / Skills required

We are looking for previous experience in data engineering for this role. Here's a list of tools and technologies that you can expect to be working with in this job. The listed examples are not necessarily all a part of our stack, but they are solid indicators of your skills being a good fit for the job.
• Relational Databases, both OLTP and OLAP, such as MySQL, Postgres, Redshift, BigQuery, CLickhouse etc
• Solid software engineering fundamentals, and experience with one or more general purpose programming languages such as Python, Typescript
• Data engineering programming libraries such as Pandas, NumPy etc
• Building data engineering pipelines using orchestration tools such as Airflow, Airbyte, Temporal, or other commercial offerings
• Transformations using dbt, or a similar alternative
• Experience with AWS's data engineering stack is definitely a plus
• Deployment/operating experience with any of these tools would be really interesting too

Work culture
• No ego anywhere in the team, including higher management.
• Remote, asynchronous, flexible work timings.
• Collaboration and team-thinking. No single person owns the failure.
• Ample time and attention to help developers level up.

Work Ex - 3+ years

Omnivio focuses on Supply Chain Management, Logistics, Cloud Infrastructure, Logistics Software, and Logistics / Transportation / Shipping. Their company has offices in Noida. They have a small team that's between 11-50 employees. To date, Omnivio has raised $400k of funding; their latest round was closed on July 2022 at a valuation of $5M.

You can view their website at https://omnivio.io or find them on LinkedIn.",,True,False,True,False,False,False,False,False,False,False,False,False,True,True,True,False
Bristol Myers Squibb Careers,"Associate Director, Senior Principal Data Engineer - Data Engineering Lead","Working with Us
Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.

Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.

BMS Hyderabad is an integrated global hub where our work is focused on helping patients prevail over serious diseases by building sustainable and innovative solutions. This important science, technology, and innovation center will support a range of technology and drug development activities that will help us usher in the next wave of innovation.

Responsibilities:
• Manage the team of off-shore Data Engineers, including talent hiring and development, demand management and capacity planning to ensure the team can meet the resource requirements of the different line of businesses.
• Maintain close collaboration with the on-shore Center-for-Excellence (C4E) to establish and drive adoption of data engineering best practices and to participate in change management activities such as training, knowledge sharing, etc.
• Maintain close collaboration with the on-shore Data Engineer Leads to ensure resources assigned to line-of-businesses are meeting expectations and to assist with the resolution of delivery issues.
• Lead the team of offshore engineers that design, implement, maintain, and support all data engineering platforms and accelerators including data ingestion/transformation tools, Enterprise Data Lake, Data Lake Houses, Enterprise Data Marketplace and Data Catalogue, metadata management tools, data virtualization platform.
• Manage platform SLAs.
• Establish robust IT service management processes to ensure services requests, incident and problem management are handled properly.
• Participate in innovation activities including proof-of-concepts to evaluate and learn new technologies.
• Provide demand / capacity forecasts for the data engineering community to leadership.
• Provide operational and financial reports for the data engineering platforms to leadership.

Work Experience:
• 10+ years of professional experience.
• 5+ years of Cloud engineering experience delivering cloud-based data solutions, preferably using AWS Cloud technologies.
• 5+ years of experience in managing operations and support of Cloud-based platforms.
• 5+ years of experience in leading a DevOps team.
• Experience in working in the pharmaceutical industry is a plus.

Certifications:
• B.S. in Computer Science or similar.
• Any of following AWS certifications: Cloud Practitioner DevOps Practitioner, SysOps Administrator, Solution Architect
• ITSM and Agile methodology certifications

Skills/abilities:
• Demonstrated ability do lead team of highly technical engineers delivering data services and technologies.
• Demonstrated ability to forecast demand and perform capacity planning to meet demand.
• Demonstrated ability to manage budget.
• Minimum of 8 years of combined hands-on experience with the following techniques and tools:
o Agile software development.
o Large-scale distributed software services and solutions.
o Data Virtualization tools such as Denodo.
o Data Cloud services such AWS Glue, Athena, RDS, Redshift, Kinesis, MKS, etc and developer tools such as Git, Airflow, Jupyter, Lambda, Jenkins, etc.
o Hands-on experience with Cloudera CDP (Impala, Hive, Spark, Nifi, Kafka) and/or SnowFlake, Databricks.
o Enterprise Data Catalogue and Metadata Management technologies such as Informatica Axon and EDC, Collibra, etc.
o Distributed cloud-computing platforms and technologies like Kubernetes and Spark ecosystem.
o Programming languages and framework such as SQL, Scala, Unix Shell Scripts, Python and PySpark, Angular, etc.
o DevOps and CI/CD.

Other required experience and attributes:
o Experience with working in a matrixed organization.
o Ability to quickly learn new technologies.
o Excellent interpersonal skills in areas such as teamwork, influence, facilitation, and negotiation.
o Problem solver with having demonstrated the ability to “think out of the box”.
o Strong written and verbal communication skills. Explain complex technical issues in simple, business-friendly language.
o Excellent planning and organizational skills.
o Experience defining, driving, and executing low-level tasks to completion across multiple simultaneous activities.

#HYDIT

If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.

Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.

On-site Protocol
Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.

BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.

BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.

Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"• Experience with Azure Data Bricks, Data Factory
• Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics
• Experience in Python/Pyspark/Scala/Hive Programming.
• Experience with Azure Databricks/ADB
• Good understanding of SQL queries, joins, stored procedures, relational schemas
• Experience with NoSQL databases, such as HBase, Cassandra, MongoDB",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
IBU Consulting,AWS Data Engineer,"Hi folks,

We are hiring for AWS Data Engineer for one of the top MNCs.

Please find the JD attached:

Location: Remote, India

Years of experience: 5+ years (at least 4+ years of experience in AWS)

Notice Period: Immediate or 1 Week Max

Qualifications
• 5-6 years of working experience on AWS (We need application development experience and not Infrastructure)
• Should be well versed with Orchestration components like – Lambda, Step functions, Event Bridge & Cloudwatch
• Should be well versed with GITHUB and CICD for AWS components deployment
• Should be well versed with AWS Glue, Dynamodb, Redshift, EC2, S3 & Athena
• Willing to work during USA working hours
• Good in communication skills
• Knowledge on EDW/DL/EDM added advantage
• Preferably certified in AWS

Interested candidates can apply directly or can share their updated resumes on aquib@ibuconsulting.com",,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False
Plume Design,Senior Data Engineer,"Plume’s Cloud Platform team is looking for engineers to build and operate data pipelines that power the gamut of Plume products and analytics. Due to the massive scale and performance requirements of many of our use cases, you will be solving challenging problems on a daily basis using a variety of cutting edge technologies.

What you will do:
• Interact with stakeholders to gather and understand data requirements
• Design and implement data pipelines with high data quality goals
• Maintain up-to-date documentation of data warehouse schemas
• Write clean, maintainable code, and perform peer code-reviews
• Refactor code as needed to improve performance and simplify operations
• Provide production support in triaging and fixing issues relating to data quality and availability
• Mentor and assist junior team members and new hires to become successful and productive
• Adhere to data protection requirements including data access, retention, residency and de-identification
• Play an integral role in driving the technology roadmap and enhancing best practices

What You’ll Bring
• Education Requirements: BS/MS/PhD in Computer Science, Electrical Engineering or related technical field
• 5+ years of software development experience with a proven track record of building, scaling, and supporting production data pipelines
• High proficiency in writing idiomatic code, preferably in Java or Scala
• High proficiency in writing SQL in data warehousing technologies
• Strong understanding of large-scale data processing technologies, e.g. Apache Spark (preferred) or Apache Flink
• Strong understanding of data warehousing concepts
• Strong analytical and problem-solving skills
• Strong oral and written communication skills

Plume Design focuses on Internet Service Providers and Cloud Data Services. Their company has offices in Palo Alto. They have a mid-size team that's between 51-200 employees. To date, Plume Design has raised $37.5M of funding; their latest round was closed on June 2017.

You can view their website at https://platform.plume.com or find them on Twitter, Facebook, and LinkedIn.",,False,False,True,True,False,False,False,False,False,False,False,True,False,False,False,False
Vanderlande Careers,Lead Data Engineer,"Lead Data Engineer at DSF

Vanderlande provides baggage handling systems for 600 airports around the globe, capable of moving over 4 billion pieces of baggage around the world per year. For the parcel market our systems handle 52 million parcels per day. All these systems generate data. Do you see a challenge in building data-driven services for our customers using that data? Do you want to contribute to the fast growing Vanderlande Technology Department on its journey to become more data driven? If so, then join our Digital Service Factory team!

Your Position

As a lead data engineer you will be leading the data engineering efforts in a product team. You will work together with product/solution architecture to provide technical necessities to design and develop end-to-end data ingestion pipelines and well tested and monitored data services. You will assess the technical dependency between different functional components and define a resolution. You will also provide technical guidance and coaching to the junior/medior data engineers in the team, set technical standards and best practices.

Your responsibilities:
• You will be designing, developing, testing, and documenting the data collection framework. The data collection consists of (complex) data pipelines with data from (IoT) sensors and low/high level control components to our Digital Service and Data Science platform.
• You will build monitoring solutions for data pipelines which enable data quality improvement.
• You will develop scalable data pipelines to transform and aggregate data for business use, following software engineering and Data Mesh best practices. For these data pipelines you will make use of the best and most applicable frameworks available for data processing.
• You develop our data services and data products for customer sites towards a product, using (test & deployment) automation, componentization, templates, and standardization to reduce delivery time of our projects for customers. The product provides insights in the performance of our material handling systems at customers all around the globe.
• You design and build a CI/CD pipeline, including (integration) test automation for data pipelines. In this process you strive for an ever-increasing degree of automation and high levels of security.
• You will work with infrastructure engineers to extend storage capabilities and types of data collection (e.g. streaming)
• You have experience in developing APIs.
• You will coach and train the junior data engineer with the state of art big data technologies.
• You will lead the Data Engineering Guild where passionate members discuss current trends, short term development, and solutions for ongoing issues that span multiple teams.

Your Profile
• Total experience of 10+ years (with at least 7+ years of programming exp)
• Experience programming in Python and/or Scala (Java programming exp is a plus)
• You are familiar with DevOps practices and have relevant experience in automation (CI/CD), measurement, applying lean practices and what DevOps culture entails
• You know how to achieve high performing secure pipelines, maintain and test them
• You are familiar with different storage formats (e.g. Azure Blob, SQL, noSQL)​
• Experience with scalable data processing frameworks (e.g. Spark)​
• Experience with event processing tools like Splunk or the ELK stack​
• Deploying services as containers (e.g. Docker and Kubernetes)​
• You have experience with streaming data platforms (e.g. Kafka )​ and messaging formats (e.g. Apache AVRO)
• Strong experience with cloud services (preferably with Azure)

Diversity & Inclusion

Vanderlande is an equal opportunity employer. Qualified applicants will be considered without regards to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Pune,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Roche,Data Engineering Manager,"The Position
Engineering Manager is a critical leadership role in our Data Engineering team. This is a people management role that needs the ability to hire and grow top engineering talent and to manage multiple teams. It includes responsibility to deliver and operate high quality, scalable, and extensible products & solutions, including making appropriate design and technology choices. The role requires strong strategic thinking and making build/buy/partner decisions for technical capabilities. Effective Communication is critical, as you will be working closely with a variety of stakeholders to understand and address their needs. A healthcare background with experience in integrating healthcare IT systems would be good to have.

KEY RESPONSIBILITIES
• Manage team of Data Engineers working on multiple data analytics products.
• Work with different agile product teams, understand and fulfill their staffing needs.
• Work with business stakeholders to develop high level project plans and roles and responsibilities.
• Prepare training and development plans for the team.
• Understand and create a career path for the team members.
• Evolve and develop a long-term roadmap for team and projects.
• Apply data engineering best practices in terms of quality, security, scalability and maintainability.
• Participate in how the budget and staff is allocated for the projects.
• Maintain project time frames, budget estimates and status reports.
• Create management, communication plans and processes. Analyze and develop process for management and technical duties.
• Foster team bonding and trust within the team. Responsible for hiring, growing and motivating engineers on your team, ensuring you recruit and retain top talent.

REQUIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• BS degree in Computer Science, Computer Engineer or a related technical discipline with 10+ years of IT industry experience.
• At least 4-6 years of proven managerial experience developing a high-performing team.
• Experience in Agile Solution Delivery and Operations Management and people management.
• Quick learner with the ability to understand complex workflows and develop and validate innovative solutions to solve difficult problems.
• Strong communication, with the ability to explain complex technical problems to non-technical audiences and the ability to translate customer requirements to technical designs.
• Strong interpersonal skills, with proven ability to navigate complex corporate environments and influence stakeholders and partners.

DESIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• Proven work experience in AWS or other cloud related technologies.
• Experience of working in product based organization
• Proven work experience as an Engineering Manager or similar role
• Communication skills for overseeing staff and working with other management personnel
• Organizational skills for keeping track of various budgets, employees, and schedules simultaneously
• Leadership, team-building, and mentoring skills
• Personnel and project management skills
• Ability to work on multiple projects in various stages simultaneously
• Experience in the Healthcare Laboratory domain is a plus.

EDUCATION

Bachelor’s degree in Engineering

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
General Mills,Data Engineer,":

India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.

Job Description:

Job Overview

The Enterprise Data Development team is responsible for designing & architecting solutions to integrate & transform business data into Data Lake to deliver data layer for the Enterprise using cutting edge technologies like Big Data - Hadoop. We design solutions to meet the expanding need for more and more internal/external information to be integrated with existing sources; research, implement and leverage new technologies to deliver more actionable insights to the enterprise. We integrate solutions that combine process, technology landscapes and business information from the core enterprise data sources that form our corporate information factory to provide end to end solutions for the business.

This position will develop solutions for the Enterprise Data Lake & Data Warehouse. You will be responsible for developing data lake solutions for business intelligence and data mining.

Job Responsibilities

70% of time Create, code, and support a variety of Hadoop, ETL & SQL solutions

Experience with agile techniques or methods

Work effectively in a distributed global team environment.

Works on pipelines of moderate scope & complexity

Effective technical & business communication with good influencing skills

Analyze existing processes and user development requirements to ensure maximum efficiency

Participates in the implementation and deployment of emerging tools and processes in the big data space

Turn information into insight by consulting with architects, solution managers, and analysts to understand the business needs & deliver solutions

20% of time Support existing Data warehouses & related jobs.

Job Scheduling experience (Tidal, Airflow, Linux)

10% of time Proactive research into up to date technology or techniques for development

Should have automation mindset to embrace a Continuous Improvement mentality to streamline & eliminate waste in all processes.

Desired Profile

Education:

Minimum Degree Requirements: Bachelors

Preferred Degree Requirements: Bachelors

Preferred Major Area of Study: Engineering

Experience:

Minimum years of Hadoop experience required: 2 years

Preferred years of Data Lake/Data warehouse experience: 2-4+ years

Total Experience required : 4-5 years

Specific Job Experience or Skills Needed

Skills Level: Beginner  Intermediate Expert  Advance

HDFS, Map reduce

Beginner

Hive, Impala & Kudu

Beginner

Python

Beginner

SQL, PLSQL

Proficient

Data Warehousing Concepts

Beginner

Other Competencies:
• Demonstrate learning agility & inquisitiveness towards latest technology
• Seeks to learn new skills via experienced team members, documented processes, and formal training
• Ability to deliver projects with minimal supervision
• Delivers assigned work within given parameter of time and quality
• Self-motivated team player and should have ability to overcome challenges and achieve desired results",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Fidelity India Careers,Lead - Software Engineering - Data Engineering,"Job Description:

Job Title – Lead Data Engineer [Data CoE]

The Purpose of This Role

At Fidelity, we use data and analytics to personalize incredible customer experiences and develop solutions that help our customers live the lives they want. As part of our digital transformation, we have significant investments to create innovative big data capabilities and platforms. One of them is to build various enterprise data lakes by gathering data across Business Units. We are looking for a hands-on data engineer who can help us design and develop our next generation, cloud enabled data capabilities.

The Value You Deliver
• You will be participating in end to end development which includes design, development, testing and deployment.
• You will be working closely with Technical Lead/Architects to ensure that solutions are consistent with IT Roadmap.
• You will be participating in technical life cycle processes, which include impact analysis, design review, code review, and peer testing.
• You will be participating in hands on development of application framework code in Oracle PL-SQL, pySpark, Python, NiFi, Informatica Power Center, along with Control-M and UNIX shell scripts.
• You will be troubleshooting and fixing any issues reported on data issues and performance.
• You will be presenting the findings and outcome to Senior Leadership teams and provide insights from the data to the business.
• You will be helping business teams optimize their current tasks and increase their productivity.

The Skills that are Key to this role

Technical / Behavioral
• You must be an expert in using SQL and PLSQL on Oracle or Netezza with UNIX shell scripting skills.
• You should be having working knowledge in Hadoop, HDFS, Hive, Spark, NoSQL DBs,
• Good knowledge on Python, JavaScript, Java and Scala
• You should have experience of using AWS services like RDS, EC2, S3, EMR and IAM to move data onto cloud platform
• Experience/Knowledge on Kubernetes, Containerization and building applications in Containers
• Knowledge of Logging, Telemetry and Data Security on AWS / Azure
• Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
• Experience of query tuning and optimization in one of the RBMS (oracle or DB2)
• You should be having experience in Control-M or similar scheduling tools.
• You should have proven analytical and problem-solving skills
• You should be strong in Database and Data Warehousing concepts.
• You must be able to work independently in a globally distributed environment
• You should have clear understanding of the business needs and incorporate these into technical solutions.

The Skills that is good to have for this role
• Experience in performance tuning and optimization techniques on SQL (Oracle and Netezza) and Informatica Power Center.
• Having strong inter-personal and communication skills including written, verbal, and technology illustrations.
• Having adequate knowledge on DevOps, JIRA and Agile practices.

How Your Work Impacts the Organization

Cloud Enablement and Data Model ready for Analytics.

The Expertise we’re looking for
• 3+[SE] / 7+ [Lead] years of experience in Data Warehousing, Big data, Analytics and Machine Learning
• Graduate / Post Graduate

Location: Bangalore , Chennai

Shift timings: 11:00 am - 8:00pm

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following:

For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2
For roles based in Ireland: Contact AccommodationsIreland@fmr.com
For roles based in Germany: Contact accommodationsgermany@fmr.com

Fidelity Privacy policy

Certifications:

Company Overview

At Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. Fidelity India has been the Global Inhouse Center of Fidelity Investments since 2003 with offices in Bangalore and Chennai. For information about working at Fidelity, visit India.Fidelity.com.

Fidelity Investments is an equal opportunity employer.",Bengaluru,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Informatica,Data Engineer,"Build Your Career at Informatica We're looking for a diverse group of collaborators who believe data has the power to improve society. Adventurous, work-from-anywhere minds who value solving some of the world's most challenging problems. Here, employees are encouraged to push their boldest ideas forward, united by a passion to create a world where data improves the quality of life for people and businesses everywhere. ((Job title - (Key Skill or Geography)) We're looking for an Data Engineer candidate with experience in IICS, IDQ, ETL to join our team in Bangalore. You will report to the Director, IT. Technology You'll Use IICS, IDQ, ETL Your Role Responsibilities Here's What You'll Do Establish a high trust partnership between the data team, business functions, and other IT groups. Develop & implement extremely complicated mappings, workflows, task-flows, APIs using Informatica products. Write/understand complex queries on different databases such as Azure SQL DB, ADLS Gen 2, to implement Business functionalities. Orchestrate the jobs via Unix scripts & implement perf improvement strategies. Use crontab as scheduler (or similar) to run jobs at different intervals not available out of the box. Understand cloud-based technologies such as SFDC, Oracle Cloud. Oversee the day-to-day of the Data Engineering team that will build the enterprise Data Platform. Retain and improve the data-engineering skills needed to deliver our technology roadmap and establish practices for delivering engineering excellence. Uncover better ways of developing data pipelines by doing it and helping others. Lead agile delivery, and together with aligned Scrum Masters and Product Owners, develop self-directed, innovative, and agile teams. Develop unit vision, strategies, critical success factors. Provide mentorship and coaching for junior members on the team. Perform other duties or special projects as assigned. Excelled debugging and triaging skills including performance tuning and identifying bottlenecks Present design reviews & code walkthroughs to technical team and incorporating review comments and creating KB articles for team, etc. What We'd Like to See Basic knowledge of project planning methodologies including Agile. 3+ years of hands-on experience developing ETL jobs on data lake, data warehouse or analytic solutions. Experience in Python, data science, machine learning, and advanced analytic tools. Experience mentoring teams through re-platform or modernization efforts. Role Essentials Bachelors degree in Computer Science or related field. Strong analytics, negotiation, facilitation and consensus building skills. In-depth Knowledge of Informatica products such as IICS, IDQ & internal architecture of these tools is a must-have hands-on experience of 5 years in ETL. Knowledge of management concepts, practices and techniques, IT & DWH concepts, strategies, and methodologies Knowledge of business functions and operations, objectives and strategies Experience working on Jupyter Notebook, Databricks, Postman is preferred. Critical thinking, problem solving, and decision-making skills. Excellent interpersonal and collaborative skills BA/BS or equivalent educational background, we will consider an equivalent combination of relevant education and experience Minimum 5+ years of relevant professional experience Perks & Benefits Comprehensive health, vision, and wellness benefits (Paid parental leave, adoption benefits, life insurance, disability insurance and 401k plan or international pension/retirement plans Flexible time-off policy and hybrid working practices Tuition reimbursement programme to support your and personal growth Equity opportunities and an employee stock purchase program (ESPP) Comprehensive Mental Health and Employee Assistance Program (EAP) benefit We're guided by our values and we are passionate ab building and delivering solutions that accelerate data innovations. At Informatica, we know diversity drives innovation. We are proud to be an dedicated to maintaining a work environment free from discrimination, one where all employees are treated with dignity. Informatica (NYSE: INFA), an Enterprise Cloud Data Management leader, brings data to life by empowering businesses to realize the transformative power of their most critical assets. We have pioneered the Informatica Intelligent Data Management Cloud (IDMC) that manages data across any multi-cloud, hybrid system, democratising data to advance business strategies. Customers in over 100 countries and 85 of the Fortune 100 rely on Informatica. . Connect with , , and . Informatica. Where data comes to life.",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Data.Ai,DNA Team - Data Engineer,"data.ai is the mobile standard and the trusted source for the digital economy. Our vision is to be the first Unified Data AI company that combines consumer and market data to provide insights powered by artificial intelligence. We passionately serve enterprise clients to create winning digital experiences for their customers.

We care deeply about our values which helps us operate as a high performing global team. We put our 'Customer First' and at the center of every decision we make, we 'Own It & Deliver' by following through with what we say we are going to do, and 'Challenge, Then Commit' by proposing solutions, not just issues to 'Win As A Team'.

What can you tell your friends when they ask you what you do?

As a DNA Team Data Engineer, I’ll be a key contributor to DNA team data services. I’ll help the DNA team to build and enhance internal processes of data production and transaction/transformation, as well as internal tools. And help colleagues from other teams and/or external clients to better experience the DNA team services.

To ensure we execute on our values we are looking for someone who has a passion for:
• Exciting Projects using technical expertise across Python, SQL, Spark, DataBricks
• Build data pipeline across different data sources/databases such as AWS S3, PG database, and Snowflake
• Produce and maintain relevant documentation
• Support internal and external customers
• Becoming better at what you do every day

You should recognize yourself in the following…
• Bachelor’s degree in Computer Science, Engineering, or equivalent experience
• At least 5 years of related work experience in building data pipelines
• Strong skills in Python and PL/SQL
• Deep understanding and experience in building data pipelines across different data sources/databases such as AWS S3, PG database, and Snowflake
• Experience in data processing such as ETL
• Knowledge of machine learning and AI is preferred
• Familiarity with specific app markets (e.g.: Gaming, Entertainment, Finance, etc.) is a big plus
• Strong problem-solving, analytical, and troubleshooting skills
• A self-starter who identifies and solves problems before anyone has noticed
• Fluent in English, both written and oral
• You are driven by passion for innovation that pushes us closer to our vision in everything we do. Centering around our purpose and our hunger for new innovations is the foundation that allows us to grow and unlock the potential in AI.
• You are an Ideal Team Player:
• You are hungry and no, we are not talking about food here.
• You are humble, yet love to succeed, especially as a team!
• You are smart, and not just book smart, you have a great read on people

data.ai is in the process of establishing an entity in India, in the interim all employees will be on the rolls of Leap 29 our Global Employer of Record",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Danske Bank,Senior Data Engineer-ETL Datastage,"Experience 5-8Years

The ideal applicant should have the following skills:

- Strong technical experience in Data Warehousing and Experience in working with ETL tools (Datastage, Informatica etc) for the purpose of creating data marts for analytical purposes

- Strong understanding of relational database concepts & technology. Exposure to Big Data technologies is an added advantage.

- Strong analytical and problem solving skills with the ability to collect, organize, analyse and process large volumes of data in a complex environment

- Good written and verbal communication skills with the ability to communicate and articulate one's thought process clearly.

- Be self driven and work closely with business stakeholders, in a global environment, to gather enough context to translate the business
objective into an analytical solution.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Koch,Senior Data Engineer,"Description

Position Description/ Requirements

The Data Engineer will be a part of an international team that designs, develops and delivers Data Pipelines and Data Analytics Solutions for Koch Industries. Koch Industries is a privately held global organization with over 120,000 employees around the world, with subsidiaries involved in manufacturing, trading, and investments. Koch Global Solution India (KGSI) is being developed in India to extend its IT operations, as well as act as a hub for innovation in the IT function. As KSGI rapidly scales up its operations in India, it’s employees will get opportunities to carve out a career path for themselves within the organization. This role will have the opportunity to join on the ground floor and will play a critical part in helping build out the Koch Global Solution (KGS) over the next several years. Working closely with global colleagues would provide significant international exposure to the employees.

The Enterprise data and analytics team at Georgia Pacific is focused on creating an enterprise capability around Data Engineering Solutions for operational and commercial data as well as helping businesses develop, deploy, manage monitor Data Pipelines and Analytics solutions of manufacturing, operations, supply chain and other key areas.

A Day In The Life Could Include:

(job responsibilities)
• Partner/collaborate with Business stakeholders and build high-quality end-to-end data solutions.
• Build a data architecture for ingestion, processing, and surfacing of data for large-scale applications in the cloud (AWS/ Azure)
• Create and maintain optimal data pipeline architecture.
• Follow best practices of Agile and DevOps focusing on the delivering of high-quality products and providing the ongoing support to meet the customers' needs
• Implement processes for Continuous integration, Test automation and Deployment (CI/CD Pipelines)
• Provide quality documentation of your design (process and workflows) and implementation including experiment tracking / logs.
• Provide on-call support on an as-needed basis
• Handle support cases to ensure issues are recorded, tracked, resolved, and follow-ups finished in a timely manner.

What You Will Need To Bring With You:

(experience & education required)
• Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science). Master’s degrees preferred.
• 6+ years of IT experience.
• In depth knowledge of Data Engineering concepts and platforms - SQL based systems, Hadoop, Spark, Distributed computing, In-memory computing, real time processing, pub-sub, orchestration, etc.
• Expertise of building data pipelines using (Pyspark based) and Databricks utilising techniques in Azure or AWS.
• 4-5 years of experience in DevOps and CI/CD using tools like Git, Terraform, Jenkins, Ansible.
• 5+ year of experience in Data modeling, SQL, Data Warehouse skills are a MUST.
• A passion and fearlessness for learning new technologies and methods in the areas of Administration
• Ability to thrive in a team environment and juggle multiple priorities.
• Excellent written and verbal communication skills.

What Will Put You Ahead:

(experience & education preferred)
• In depth knowledge of entire suite of services in AWS/Azure Cloud Platform.
• Strong coding experience using Pyspark.
• Experience of designing and implementing ETL process using SSIS.
• Cloud Data Anaytics/Engineering certification.

Other Considerations:

(physical demands/ unusual working conditions)
• Some work may involve hours outside of normal KGS works hours.

Koch is proud to be an equal opportunity workplace",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Turing,Senior Data Engineer,"A NASDAQ-listed company that is determined to build a greener future and help customers reduce their carbon footprint, is looking for a Senior Data Engineer. The selected candidate will be in charge of developing, distributing, and updating internal as well as external requirements and documentation. The company is manufacturing and distributing world-class e-vehicles that will help to conserve the remaining reserve of fossil fuel and switch to more eco-friendly transportation solutions. The company has managed to securely raise more than $11mn in funding so far. This position requires a significant overlap with the PST time zone.

Job Responsibilities:
• Automate the analytics platform's access request and provisioning processes for tools like Tableau, Alteryx, and PowerBI
• Establish and preserve collaborative working connections with clients and teams to create cutting-edge analytics platform products and services
• Create a cycle of continuous improvement by gathering end-user and peer feedback and suggestions on self-service deployments
• Facilitate the delivery of new product solutions by organizing workshops, creating release documentation, and offering assistance during launches
• Control the partners'/stakeholders' expectations for deliverables
• Assemble requirements from stakeholders based on the existing situation, come up with a solution, and then put it into practice
• Decide on the process steps that can be automated and those that cannot
• Ensure a positive user experience while upholding compliance and governance for analytics platforms and services by evaluating, analyzing, and communicating system/process needs
• Take the project from requirements gathering to the implementation side
• Automate the Tableau and Altryx access process as well as the entire flow
• Track every step of the process, noting which ones were automated, what changes were made, and where further work needs to be done
• Be flexible and shift in scope from executing projects to day-to-day operations

Job Requirements:
• Bachelor’s/Master’s degree in Engineering, Computer Science (or equivalent experience)
• At least 5+ years of relevant experience as a data engineer at a large corporation
• Extensive working experience with Tableau, Altryx, and PowerBI
• Prior experience utilizing ServiceNow, SailPoint, and RPA (UiPath, Power Platform, and/or scripting)
• Desire to learn ServiceNow App Engine and requirements gathering or demonstrable experience with those products
• Solid understanding of MS Power Platform, AWS, Microsoft Azure, Python, and SQL
• Demonstrable experience working with Git, AWX, Jira, or similar tools
• Must have the engineering skill set to partner with the internal stakeholders
• Ability to use (Read/Review/Infer) to improve business and flows
• Comprehensive understanding of visualization tools and creating dashboards in Tableau
• Prolific experience building solutions using public cloud platforms like AWS and Azure
• Some familiarity with designing and implementing automated solutions
• Prior experience working in a dynamic and agile environment
• In-depth experience with analytics
• Experience deploying solutions across a broad and deep portfolio
• Self-starter with superior leadership, communication, presentation, organizational, and time management abilities
• Nice to have some knowledge of several business industries like Finance, Supply Chain, Order Management, Logistics, Manufacturing, etc.
• Must possess a strong business acumen
• Devoted to providing excellent customer service
• Sharp financial, analytical, and problem-solving capabilities
• Excellent spoken and written English communication skills",,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,False
Randstad India,Data Engineer,"Role: Data Engineer Job Description
• Design, build, and maintain distributed batch and real-time data pipelines and data models.
• Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
• Be curious and eager to work across a variety of engineering specialties (i.e., Data Science, and Machine Learning to name a few).
• Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
• Enforce privacy and security standards by design.
• Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Qualifications
• +3 years experience building complex data pipelines and working with both technical and business stakeholders.
• Experience in at least one primary language (e.g., Java, Scala, Python) and SQL (any variant).
• Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
• Experience creating and maintaining ETL processes.
• Experience designing, building, and operating a DataLake or Data Warehouse.
• Experience with DBMS and SQL tuning.
• Strong fundamentals in big data and machine learning.

Preferred Qualifications
• Experience with RESTful APIs, Pub/Sub Systems, or Database Clients.
• Experience with analytics and defining metrics.
• Experience with measuring data quality.
• Experience productionalizing a machine learning workflow; MLOps
• Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
• Language ability in Japanese and English is a plus (We have a professional translator but it is nice to have language skills).
• Experience with AWS services.
• Experience with microservices.
• Knowledge of Data Security and Privacy.

experience

6",Hyderabad,True,False,True,True,False,False,False,False,False,False,False,False,True,True,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Affine,Data Engineer,"Company Description

About Company

http://www.affine.ai

""AFFINE"" cited by GARTNER as a SPECIALIST MIDSIZE CONSULTANCY in ANALYTICS and MACHINE LEARNING solutions and services. Click to Read More ""

Affine is a provider of high-end analytics services to solve complex business problems with offices in NJ, USA & Bangalore, India. We combine data driven algorithmic analysis with heuristic domain expertise to provide actionable solutions that empower organizations make better and informed decisions. Affine's value proposition is enabling clients to implement and realize ROI of the recommendations.

Affine has a group of people with significant experience in Analytics industry along with solid pedigree, deep business understanding and strong problem solving acumen. Our group primarily consists of Statisticians, Operations Researchers, Econometricians, MBAs and Engineers. Our employees have experience of working for many Fortune 500 companies.

Job Description

What the candidate will do:
• Contribute to adoption of cloud & cloud-based technologies and good design practices, while finding opportunities to simplify and scale
• Resolve problems and roadblocks as they occur with peers and help unblock junior members of the team. Follow through on details and drive issues to closure
• Define, develop, and maintain artifacts like technical design or partner documentation
• Drive for continuous improvement in Data engineering process within an agile development team
• Own and deliver assigned sprint tasks and help drive the team forward.
• Communicate and work effectively with geographically distributed cross functional teams

Experience

4 to 6 Years in Deploying models, Sage Maker or TensorFlow

Required skillset.
• Big Data: Spark, Kafka, Hadoop, Hive, SQL and NoSQL
• Cloud: AWS, EMR, Qubole/Databricks, VPC
• Devops: Docker containers and Jenkins. Spinnaker is preferred but not required.
• Programming languages: Scala and Pyspark is mandatory
• Agile and scrum experience and working with a remote team (nice to have, not required)

Must Have Skills
• Spark, AWS, Scala/Python, SQL, Java
• ML ops tools:Tensorflow or Sagemaker

Additional Information

Others
• Quick learner
• Excellent written and oral communication skills
• Excellent interpersonal & organizational skills
• Good listening and comprehension skills",Bengaluru,True,False,True,True,True,False,False,True,False,False,False,False,False,False,False,False
AXA Group,Data Engineer,"Data Engineer
Gurgaon, Haryana, India

The Application Developer plays a critical role within the Data and Analytics SDC as this person is responsible for designing and implementing data structures to support current and future analytical projects. We are looking for candidates that have experience working with data from a raw, unprocessed state and organizing it intuitively. Building this data pipeline enables our partners to analyze data better and faster – ultimately leading the organization in optimizing the decision-making process.

DISCOVER your opportunity
What will your essential responsibilities include?
• Candidates for this role should have experience developing data processes with source data in a variety of formats (structured / unstructured, databases, APIs) into a target state. This will involve building proper data pipelines to support initial exploration and real-time integration.
• Data development using appropriate tools and techniques to process data required for advanced analytics. A candidate would be expected to interact with Data Engineering Leads and Data Scientists to understand requirements and would be responsible for the development of the solution.
• Providing the right context of data required for a given analysis. This would require the candidate to work with data modelers/analysts to understand the business problems they are trying to solve and create data structures to feed into their analysis.
• Build upon learnings of internal and external data to become more proactive. This includes thinking ahead of what modelers will anticipate with their data needs and designing structures that are intuitive to use.
• Making sure quality and understanding of analytical data. This would require hands-on data experience to look into data issues and seek resolution or acceptance. Create the appropriate amount of documentation, leverage standards, and build upon them. Data should be reconciled and documented at various stages for integrity.
• Take part in developing governance and rigor of data management practice within the Data and Analytics SDC. This will also include partnering with enterprise IT groups and involvement in enterprise data-related functions.
• You will report to Data Manager/Principal Data Engineer.

SHARE your talent
We’re looking for someone who has these abilities and skills:
• Demonstrated ability to work through data complexities which include a variety of sources, formats, and structures. Robust preference for experience in the Insurance domain.
• Ability to see through ambiguous concepts and break down complex problems into manageable components.
• Detail-orientated, proven ability to recognize patterns in data.
• Demonstrated ability to incorporate data quality standards into data development.
• Possesses natural curiosity. Seek to understand the world around you, and question when appropriate.
• Robust SQL Skills required.
• 2-4 years of development experience using data development (visual ETL or coded) / analysis tools (ex. SAS, SPSS, R, Microsoft SSIS/SSAS, Informatica, DataStage, AbInitio).
• Experience in .NET, Python, or Java development is a plus.
• Experience in web extraction, unstructured data, advanced text parsing, machine learning, and NLP a plus.
• Familiarity with developer support tools (TFS/GIT, Jenkins) is a plus.
• College Degree in MIS, Information Technology, Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, or equivalent.

FIND your future

AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals, and even some inspirational individuals we don’t just provide re/insurance, we reinvent it.

How? By combining an effective and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business − property, casualty, professional, financial lines, and specialty.

With an innovative and flexible approach to risk answers, we partner with those who move the world forward.

Learn more at axaxl.com

Inclusion & Diversity

AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic.

At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That’s why we have made a strategic commitment to attract, develop, advance, and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It’s about helping one another — and our business — to move forward and succeed.
• Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability, and inclusion with 20 Chapters around the globe
• Robust support for Flexible Working Arrangements
• Enhanced family-friendly leave benefits
• Named to the Diversity Best Practices Index
• Signatory to the UK Women in Finance Charter

Learn more at axaxl.com/about-us/inclusion-and-diversity. AXA XL is an Equal Opportunity Employer.

Sustainability

At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called “Roots of resilience”, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations.

Our Pillars:
• Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society – are essential to our future. We’re committed to protecting and restoring nature – from mangrove forests to the bees in our backyard – by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans.
• Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions.
• Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We’re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting.
• AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL’s “Hearts in Action” programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day – the Global Day of Giving.

For more information, please see axaxl.com/sustainability",,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,False
AlphaGrep Securities,Data Engineer,"About the Company

AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes (equities, commodities, currencies, fixed income) that trade on global exchanges..

We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.

Roles & Responsibilities
• Build infrastructure tools and applications to support trading teams across the firm.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Coordinate with global teams to understand their requirements and work alongside them.
• Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
• Set up practices on data reporting and continuous monitoring
• Write a highly efficient and optimized code that is easily scalable.
• Adherence to coding and quality standards.

Required Skills
• Strong working knowledge in Python.
• Strong working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have
• Experience with web crawling and scraping, text parsing
• Experience working in Linux Environment
• Experience with Stock Market Data

Why You Should Join Us
• Great People. We’re curious engineers, mathematicians, statisticians and like to have fun while achieving our goals
• Transparent Structure. Our employees know that we value their ideas and contributions
• Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.
• Health & Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fibe India,Data Engineer - SQL,"Responsibilities:
• The candidate is expected to lead one of the key analytics areas end-to-end. This is a pure hands-on role.
• Ensure the solutions are built to meet the required best practices and coding standards.
• Ability to adapt to any new technology if the situation demands.
• Requirement gathering with business and getting this prioritized in the sprint cycle.
• Should be able to take end-to-end responsibility for the assigned task
• Ensure quality and timely delivery.

Requirements:
• Experience: 3- 6 years.
• Strong at PySpark, Python, and Java fundamentals
• Good understanding of Data Structure
• Good at SQL query/optimization
• Strong fundamental of OOPs programming
• Good understanding of AWS Cloud, Big Data.
• Nice to have Data Lake, AWS Glue, Athena, S3 Kinesis, SQL/NoSQL DB",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
MediaMath,Data Engineer,"About Us

MediaMath is the leading technology pioneer on a mission to make advertising better. We deliver outstanding results through powerful ad tech, partnership and a curiosity for what’s next. We help more than 3,500 advertisers solve complex marketing problems so they can deepen their customer relationships across screens and around the world.

Key Responsibilities

MediaMath’s Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by Analytics to discover and deliver insights that drive value for our clients. The Analytics team fulfils customers’ advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. As the Data Engineer on the Analytics Engineering team within the Analytics team, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:
• Become an expert in MediaMath data flows and the Analytics data infrastructure.
• Build, maintain, and own scalable data pipelines to support client data integration.
• Become a team SME in data munging and automated ETL processes.
• Work with Analysts to understand and leverage big data to solve client problems and needs.
• Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
• Be a team player, and bring the team and company forward by solving team and company priorities.

You are:
• Experienced in writing readable, re-usable code SQL and Python (our entire team uses Jupyter Notebook and Pandas!)
• Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
• Experienced in implementing data pipeline health monitoring, alerting
• Experienced with data infrastructure troubleshooting and working with system logs
• Experienced developing data flow schematics/blueprints
• Advocate for automation and building efficient, scalable solutions
• Self-driven, with a hunger to learn and spread knowledge by teaching others
• Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:
• Bachelor’s Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
• 3 - 5 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
• Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
• Experience partnering with client-facing teams to understand client needs and translate them to technical requirements

Nice-to-have’s:
• Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)
• Experience working with REST APIs, web services, object-oriented technologies like Java, C++
• Public GitHub repos or notebooks that illustrate the way you think about data
• Exposure to ad-tech, digital marketing, or e-commerce industries

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are three values that provide a framework for how we approach our work and the world: Win Together, Obsess Over Growth, and Do Good, Better. These values inform how we energize one another and engage with our clients. They get us amped to come to work.

Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath is recognized as a Leader in the Gartner 2020 Magic Quadrant for Ad Tech and has won Best Account Support by a Technology Company for two years in a row in the AdExchanger Awards.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristic as established by law.

MediaMath focuses on Digital Media, Internet, Advertising, Software, and Marketing. Their company has offices in New York City, San Francisco, Chicago, Durham, and Singapore. They have a large team that's between 501-1000 employees. To date, MediaMath has raised $617.877M of funding; their latest round was closed on July 2018.

You can view their website at http://www.mediamath.com or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,False
FairMoney,Senior Data Engineer,"About FairMoney

FairMoney is a credit-led mobile bank for emerging markets. The company was launched in 2017, operates in Nigeria & India, and raised close to €50m from global investors like Tiger Global, DST & Flourish Ventures. The company has offices in France, Nigeria, and India.

Role and responsibilities

At FairMoney, we are making a lot of data driven decisions in real time: risk scoring, fraud detection as examples.

Our data is mainly produced by our backend services, and is being used by data science team, BI team, and management team. We are building more and more real time data driven decision making processes, as well as a self serve data analytics layer.

As a senior data engineer at FairMoney, you will help building our Data Platform:

• Ensure data quality and availability for all data consumers, mainly data science and BI teams.
• Ingest raw data into our DataWarehouse (BigQuery / Snowflake)
• Make sure data is processed and stored efficiently:
• Work with backend teams to offload data from backend storage
• Work with data scientists to build a machine learning feature store
• Spread best practices in terms of data architecture across all tech teams
• Effectively form relationships with the business in order to help with the adoption of data-driven decision-making.

You will be part of the Datatech team, sitting right between data producers and data consumers. You will help building the central nervous system of our real time data processing layer by building an ecosystem around data contracts between producers and consumers.

Our current stack is made of

• Batch processing jobs (Apache Spark in Python or Scala)
• Streaming jobs (Apache Flink deployed on Kinesis Data Analytics - Apache Beam deployed on Google Dataflow)
• REST apis (Python FastApi)

Our tool stack

• Programming language: Python, SQL
• Streaming Applications: Flink, Kafka
• Databases: MySQL, DynamoDB
• DWH: BigQuery, Snowflake
• BI: Tableau, Metabase, dbt
• ETL: Hevo, Airflow
• Production Environment: Python API deployed on Amazon EKS (Docker, Kubernetes, Flask)
• ML: Scikit-Learn, LightGBM, XGBoost, shap
• Cloud: AWS, GCP

Requirements

You will work on a daily basis with the below tools, so you need working experience on

• Languages: Python and Scala.
• Big data processing frameworks: all or one of Apache Spark (batch/streaming) - Apache Flink (streaming) - Apache Beam.
• Streaming services: Apache Kafka / AWS Kinesis.
• Managed cloud services: one of AWS EMR / AWS Kinesis Data Analytics / Google Dataflow.
• Docker.
• Building REST APIs.

Ideally, you have experience with:

• deployment/management of stateful streaming jobs.
• the Kafka ecosystem: Kafka connects mainly.
• infrastructure as code frameworks (Terraform).
• architecture around data contracts: Avro Schemas management, schema registries (Confluent Kafka / AWS Glue).
• Kubernetes.

Overall experience required for this role: 6+ Years.

Benefits

• Training & Development
• Family Leave (Maternity, Paternity)
• Paid Time Off (Vacation, Sick & Public Holidays)
• Remote Work

Recruitment Process • A screening interview with one of the members of the Talent Acquisition team for 30 minutes.
• Takeaway assignment to be done at home.
• Technical design interview for 60-90 minutes.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,True,False,True,True,True
GWC Analytics,Senior Data Engineer-Airflow,"Title: Senior Data Engineer - Airflow

Job Description:

We are looking for an experienced Senior Data Engineer with expertise in Airflow to join our team. As a Senior Data Engineer, you will be responsible for designing, developing, and maintaining scalable data pipelines that support our data-driven business decisions.

Responsibilities:
• Design, develop, and maintain scalable and efficient data pipelines using Airflow.
• Collaborate with data scientists and business intelligence analysts to ensure our data is accurate, reliable, and accessible.
• Manage the ETL process from data ingestion to data transformation and data loading.
• Monitor and optimize the performance of our data pipelines.
• Develop data quality checks and data validation processes.
• Continuously improve our data engineering processes and best practices.
• Participate in code reviews and contribute to the development of our data engineering standards.
• Qualifications:
• Minimum of 5 years of experience in data engineering, with expertise in Airflow.
• Strong knowledge of SQL and experience working with relational databases and data warehouses.
• Experience with cloud-based data platforms. (AWS, GCP, or Azure)
• Knowledge of Python and experience with data processing libraries such as Pandas and NumPy.
• Experience with data modeling, data integration, and data transformation.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Nisum,Data Engineer - AB4649,"Nisum is a leading global digital commerce firm headquartered in California, with services spanning digital strategy and transformation, insights and analytics, blockchain, business agility, and custom software development. Founded in 2000 with the customer-centric motto “Building Success Together®,” Nisum has grown to over 1,800 professionals across the United States, Chile,Colombia, India, Pakistan and Canada. A preferred advisor to leading Fortune 500 brands, Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today’s world, with immersive and seamless experiences across digital and physical channels.

What You'll Do
• Excellent problem-solving skills to perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Proven experience in manipulating, processing, and extracting value from large disconnected datasets.
• Supporting and working with cross-functional teams in a dynamic environment.
• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL), and familiarity with unstructured datasets.
• Perform code reviews, ensure code quality and encourage a culture of excellence.
• Communicate/work effectively in a team environment

What You Know
• 7-10 years of experience in a Data Engineer role
• Experience with Spark, Scala, java, mongoDB
• Experience with Azure Databricks
• Streaming: Kafka Streaming.
• Experience in Hadoop, Spark or PySpark, Java/Scala/Python, Azure Databricks, MongoDB, Apache Kafka

Education
• Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.

Benefits
• In addition to competitive salaries and benefits packages, Nisum India offers its employees some unique and fun extras:
• Continuous Learning - Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on an as need basis. We support our team to excel in their field.
• Parental Medical Insurance - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs. We offer opt-in parental medical insurance in addition to our medical benefits.
• Activities - From the Nisum Premier League's cricket tournaments to hosted Hack-a-thon, Nisum employees can participate in a variety of team building activities such as skits, dances performance in addition to festival celebrations.
• Free Meals - Free snacks and dinner is provided on a daily basis, in addition to subsidized lunch

Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.",Hyderabad,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Citibank India.,Sr. Data Engineer,"• Candidate should have around 6+ years development and system design experience.
• More than 5 years of experience in BigData technologies like spark, hive, python/java
• Strong experience in building ETL/data engineering solutions.
• Proven proactive problem solving & trouble shooting skills

Responsibilities:
• Design & develop Data engineering solutions using Big Data Technologies.
• Provide technical walk-throughs to various stakeholders
• Be able to work independently as well as within a team
• Works directly with end-users or a projects team to translate business requirements into technical specifications to drive Semantic layer and report
• Prioritize technical issue resolution.
• Works closely with management to prioritize business needs and stay up to date with the goals of the organization as they evolve. Also work with other departments to find new ways to improve opportunities
• Requirement gathering & understanding, effort estimation, technical design, project planning and monitoring
• Understand and analyze the data to find patterns and valuable business insights
• Required to review the day to day delivery work with the team, ensuring a robust process implementation, identifying project/program delivery risks and work on risk mitigation.

Mandatory Skills:
• Strong Knowledge in Spark, Python or Core Java
• Strong experience in Hive/SQL, PL/SQL
• Good Understanding of Big data Ecosystem
• Good Understanding of ETL & DW Concepts, Unix Scripting
• Good understanding of SDLC

Desirable Skills:
• Understanding of any ETL tool
• Understanding of any BI tool
• Cloud Computing AWS/Azure

Qualifications:
• Educational requirements: BE/B. Tech
• Language requirements (including proficiency levels for speaking, reading, and writing): English

Competencies
• Ability to work in a team environment
• Flexible and able to manage time effectively
• Ability to learn new skills quickly with little supervision and ensuring the detail is of high priority
• Excellent communication (verbal and written) and interpersonal skills with the ability to communicate well at all levels
• Efficiently and effectively manage work, time, and resources
• Ability to handle high stress and pressure situations
• Strong problem solving and program execution skills while being process orientated
• Self-motivating and delivery focused individual

,",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
deloitte,Consulting - BO - Cloud Engineering - Manger - Azure Data Engineer,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organisations, Deloitte is where you will find numerous opportunities to succeed and realise your full potential.

The team

Deloitte is working with global customers on cloud technologies to help unlock growth, stability, and sustainability by enabling them to spot unseen business trends through curation, transformation, and blending of data. In our endeavors for continued expansion, we’re searching for like-minded individuals to help us ‘take it to the next level’.

In this exciting opportunity for an experienced developer, you will join a team delivering a transformative cloud hosted data platform for some of the world’s biggest organizations. The candidate we seek, needs to have a proven track record in implementing data ingestion and transformation pipelines on Microsoft Azure. Deep technical skills and experience with working on Azure Databricks. Familiarity with data modelling concepts and exposure to Synapse.

You will also be required to participate in stakeholder management, highlight risks, propose deliver plans and estimate for time and team size based on requirements. Hence, adequate levels of communication skills and relevant experience in handling such situations is desired.

Scope of work

Your main responsibilities will be:
• Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
• Delivering and presenting proofs of concept of key technology components to project stakeholders.
• Developing scalable and re-usable frameworks for ingesting and enriching datasets
• Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
• Working with event based / streaming technologies to ingest and process data
• Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
• Evaluating the performance and applicability of multiple tools against customer requirements
• Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.

Qualifications
• Strong knowledge of Data Management principles
• 9+ years of total years of experience
• Experience in building ETL / data warehouse transformation processes
• Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably Databricks).
• Experience using Apache Spark and associated design and development patterns
• Microsoft Azure Big Data Architecture certification is an advantage.
• Hands-on experience designing and delivering solutions using Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
• Experience with Apache Kafka / Nifi for use with streaming data / event-based data (Nice to have but not mandatory)
• Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)
• Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
• Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Terraform etc.

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters.

In addition to living our purpose, managers across our organisation:
• Develop self by actively seeking opportunities for growth, share knowledge and experiences with others, and act as a strong brand ambassadors
• Understand objectives for clients and Deloitte, align own work to objectives and set personal priorities
• Seek opportunities to challenge self
• Collaborate with others across businesses and borders to deliver and take accountability for own and team results
• Identify and embrace our purpose and values and put these into practice in their professional life
• Build relationships and communicate effectively in order to positively influence peers and other stakeholders

Professional growth

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn.From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Our Purpose

Deloitte is led by a purpose: To make an impact that matters.

Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and work—always striving to be an organisation that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
Bloom Consulting Services,Data Engineer,"Data Engineer ( Job ID : 815310498 )

data engineer

NA

Contract

Experience

06.0 - 08.0 years

Offered Salary

10.00 - 14.00

Notice Period

Not Disclosed

Job Description

Total Experience6 to 8 years

Min Relevant Experience: 3 to 5 years

Location :Bangalore

JD: Data Engineer

Role Description:

In this role, you will be part of a growing, global team of data engineers, who collaborate in DevOps mode, in order to enable business with state-of-the-art technology to leverage data as an asset and to take better informed decisions.

The Life Science Data Engineering Team is responsible for designing, developing, testing, and supporting automated end-to-end data pipelines and applications on Life Science’s data management and analytics platform (Palantir Foundry, Hadoop and other components).

The Foundry platform comprises multiple different technology stacks, which are hosted on Amazon Web Services (AWS) infrastructure or own data centers. Developing pipelines and applications on Foundry requires:
• Proficiency in SQL / Java / Python (Python required; all 3 not necessary)
• Proficiency in PySpark for distributed computation
• Familiarity with Postgres and ElasticSearch
• Familiarity with HTML, CSS, and JavaScript and basic design/visual competency
• Familiarity with common databases (e.g. JDBC, mySQL, Microsoft SQL). Not all types required

This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.

Roles & Responsibilities:
• Develop data pipelines by ingesting various data sources – structured and un-structured – into Palantir Foundry
• Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application
• Acts as business analyst for developing requirements for Foundry pipelines
• Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline
• Document technical work in a professional and transparent way. Create high quality technical documentation
• Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)
• Deploy applications on Foundry platform infrastructure with clearly defined checks
• Implementation of changes and bug fixes via change management framework and according to system engineering practices (additional training will be provided)
• DevOps project setup following Agile principles (e.g. Scrum)
• Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java
• Work closely with business users, data scientists/analysts to design physical data models

Education
• Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields

Professional Experience
• 5+ years of experience in system engineering or software development
• 3+ years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.

Required Knowledge, Skills, and Abilities

Data engineer",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Shell,"Senior Data Engineer- Azure (ADF, Data lake)","Join the number One Global Lubricants supplier in the world and be part of the team that helps in shaping up the digital and the IDT strategy which delights our customers in over 100 countries across every sector.

If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

Where you fit

Shell's Projects and Technology (P&T) business exists to make the delivery of our strategies and the growth of our company possible. Our team develops the advanced products and technologies Shell needs to meet customer demand. Our solutions help our partners grow the LNG, Gas and Power businesses, deepen the integration of Manufacturing, Chemicals and Trading, and maximise the competitiveness of our Upstream business.

What's the role?

As a Data Engineer in Shell, you will create and maintain optimal data pipeline architecture and also will a ssemble large, complex data sets that meet functional / non-functional business requirements.

You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

More specifically, your role will include:
• Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.
• Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

What we need from you

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree and at least have a Seniority level in their previous workplace.

They should also have experience using the following software/tools:
• Experience with Azure: ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.
• Experience with relational SQL/NoSQL databases, file handlings and API integrations
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODS

Skills - Nice to Have
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing data pipelines using ADF
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of transforming, processing and extracting value from large disconnected datasets
• Strong team player with organizational and communication skills
• Experience supporting and working with cross-functional teams in a dynamic environment",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fisker Inc.,Data Engineer,"Responsibilities
• Work with leaders, engineering and data scientists to understand data needs.
• Design, build and launch efficient and reliable data pipelines to best utilize connected vehicle data for real-time systems and within data warehouses.
• Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
• Help insure that best practices are followed when storing, retrieving and accessing data.

Qualifications
• 3+ years of Python development experience.
• 3+ years of SQL experience.
• 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
• 3+ years experience with Data Modeling.
• Experience in organizing queries, tables and pipelines with proper indexing, partition and sharding.
• 3+ years experience in custom ETL design, implementation and maintenance.
• Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Clickhouse, Spark, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience with more than one coding language, ideally Go or C++ and java.
• Experience with designing and implementing real-time pipelines.
• Experience with data quality and validation.
• Experience with SQL performance tuning and E2E process optimization.
• Experience with notebook-based Data Science workflow.
• Experience with Airflow.
• Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,True,True,False
ANI Calls India Private Limited,Data Engineer AWS Sage,"Anicalls Industry: IT Total Positions: 3 Job Type: Full Time/Permanent Gender: No Preference Salary: 900000 INR - 1600000 INR (Annually) Education: Bachelor?s degree Experience: 6-9 Years Location: Chennai, India . Strong computer science fundamentals such as algorithms, data structures, multithreading, object-oriented development, distributed applications, client-server architecture. . Design and implement Machine learning models and data ingestion pipelines. . Develop and support a platform that enables data scientists to rapidly develop, train, and experiment with machine learning models. . Expand and optimize data pipelines, data flow, and collection for cross-functional teams. . Create and maintain optimal data pipeline architecture by assembling large, complex data sets to meet functional and non-functional business requirements. . Identify and implement internal process improvements, including automating manual processes, optimizing data delivery, and redesigning infrastructure for greater scalability. . Work with architecture, data, and design teams to assist with data-related technical issues and support data infrastructure needs.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Verizon,Engineer II-Data Engineering,"Job # 621877

When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

At Verizon, we are on a journey to industrialize our Data and AI capabilities. Very simply, this means that Data & AI will fuel all decisions and business processes across the company. With our leadership in bringing the 5G network nationwide, the opportunity for Data & AI will only grow exponentially in going from enabling billions of predictions to possibly trillions of predictions that are automated and real-time.

As an Engineer II-Data Engineering in the Artificial Intelligence and Data organization (AI&D), you will drive various activities including data engineering, data operations automation, data frameworks, and platforms to improve the efficiency, customer experience, and profitability of the company.
• Building high-quality data engineering solutions to create data observability at scale.
• Designing, building and maintaining Enterprise Level Standard Data Pipe-Lines involving data ingestion, preparation, and transformation.
• Developing reusable data management solutions.
• Building and maintaining appropriate technical and reference documents for every program and delivery.
• Using complex algorithms to develop systems & applications that deliver business functions or architectural components.
• Working with business owners across the organization to understand business requirements and align with the organization's priorities and develop them.
• Following best practices on design and implementation to aid in company-wide data governance.
• Improving existing data pipelines by simplifying and increasing performance.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You’ll Need To Have
• Bachelor’s degree or one or more years of work experience.
• Experience in building DevOps Pipeline using Jenkins/GCP Cloud Build.
• Experience in Source code management tools like Git, experience with Binary Life cycle management tools like JFrog, Cloud Artifactory.
• Experience in DevOps tools like Chef/Puppet/Ansible.
• Experience in Linux and Python scripting.
• Experience in various build tools like maven, sbt and gradle.
• Experience in code quality, unit test automation tools like Sonarqube, Junit.
• Experience in working in an agile team.

Even better if you have one or more of the following:
• Master’s degree in Computer Science, Information Systems or related field.
• Experience in working on GCP, AWS and Teradata.
• Experience in cross-team collaboration, interpersonal skills/relationship building.
• Analytical ability to quickly debug application problems and provide short and long-term solutions.
• Ability to effectively communicate through presentation, interpersonal, verbal, and written skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.

Job Family: TEC

Business Unit: VZBIN",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
S&P Global,Data Engineer,"Job Description

Position Summary:

We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. The person will need to coordinate with Product managers, Industry Analysts and Technology experts to develop and deliver the product in accordance with customer requirements and agreed timeline.

Duties and responsibilities:

• Create and maintain optimal data pipeline architecture,

• Assemble large, complex data sets that meet functional / non-functional business requirements.

• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/Python/Spark and AWS ‘big data’ technologies.

• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

• Work with data and analytics experts to strive for greater functionality in our data systems.

• Handle project management and stakeholder management activities and take full ownership of quality and timeliness of product deliverables.

• Gather knowledge about the maritime industry and its interrelationship with various economic and policy decision as relates to clients in Financial markets and Shipping industry

Business competencies

Basic Qualifications :

• Total 3-6 years of data engineering role with 3+ years of experience in Spark and Python

• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

• Build processes supporting data transformation, data structures, metadata, dependency and workload management.

• A successful history of manipulating, processing and extracting value from large, disconnected datasets.

• Ability to work with a team of data engineers and data scientists and to implement new and innovative algorithms.

• Understanding of data visualization principles and experience with data visualization tools

• Master’s or higher degree in a quantitative fields like Computer Science, Statistics, Informatics, Information Systems or another quantitative field from reputed institutions.

Beneficial:

• Experience in leading complex analytical projects and managing stakeholders

• Experience working with geo spatial data and unstructured/semi-structured data

• Experience using big data tools for data manipulation and modeling (Spark, BigQuery, Hive, MongoDB, Cassandra etc.)

Personal competencies:

• Excellent Logical thinking and problem-solving skills

• Ability to confidently present your own ideas and solutions, as well as guide technical discussions.

• Welcoming and approachable attitude and ability to connect with people and build strong professional relationship.

Equal Opportunity Employer

S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.

If you need an accommodation during the application process due to a disability, please send an email to:  EEO.Compliance@spglobal.com  and your request will be forwarded to the appropriate person. 

US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf  describes discrimination protections under federal law.

20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.1 - Middle Professional Tier I (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)

Job ID: 286062

Posted On: 2023-05-04

Location: Bangalore, Karnataka, India",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,False,False
weITglobal - W.IT.G Consulting AB,Expert Data Engineer,"Background

Looking for an Expert data engineer with deep knowledge of data engineering. Preferable has been working parts on Architectual level on enterprise level associated to create data products.

As we are expanding our team towards India. This is targeting roles in this area of the world.

Requirements

Desired knowledge, experience, competence, skills etc
• A GCP cloud associate Engineer who will enable data from our customer support staff planning team.
• Google Cloud platform
• Kubernetes
• DBT
• Dataform
• Data quality",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Confidential,Cyber Security Data Engineer - Data Pipeline,"Company: HushDataAbout CompanyHushData helps business reduce the overall cyber security technology and data ownership costs while improving the mean time to detect threats and handle them through AI & ML based Automated Workflows. We are headquartered in Atlanta, GA and serve customers across the globe. We partner with leading cloud, data and security platforms to help our customer's accomplish their goals: Reduce Costs, Improve Security Posture, Reduce Time to Detect and Improve SOAR Performance. HushData is hiring talented engineers and business development folks to augment our growth plans. If you are passionate intrapreneur ready to change the game in cybersecurity, let us know. Role: Senior CyberSecurity Data Engineer (Snowflake)Location : HybridRole and Responsibilities :Senior CyberSecurity Data Engineer (Snowflake) :1. HushData is looking to onboard a team of Founding Data Engineers with experience working on Security Data. The founding engineers will work on building and automating our data workflows, infrastructure and integrations from a variety of sources. 2. You will form a vital part of our business, contribute to our first-class end-to-end solutions. You will play an active role in defining our practices, standards and ways of working, and apply them to your role. Be open to working across organisation and team boundaries to ensure we bring the best to our customers.What will you work on :1. Building vulnerabilities' data lake and associated orchestration to scale up multiple vulnerabilities' data sources and correlation with other data2. Collaborate with a diverse audience of engineers, data analysts, and business partners on data storage & organization, transformation, and data accessibility solutions3. Automate data extraction, transformation, and accessibility, along with automating hosted infrastructure4. The Data Engineer is responsible for operationalizing data pipelines that support metrics & analytics initiatives for the company.5. The primary responsibilities include designing, building, managing, optimizing and documenting data flows from various sources into our enterprise data lake6. Delivery of high-quality data is a key item of focus7. The data engineer is expected to collaborate with data scientists, data analysts and other data consumers to productionize data models and algorithms developed by those users to improve the overall efficiency of advanced analysis projects8. Additionally, the data engineer is responsible for ensuring data quality, governance and data security procedures are met while curating data for use in the Data Lake9. Design and incorporate error handling & Data Quality processes into pipelines and processes10. Design, implement, and analyze robust test plans and stress tests11. End-to-end Implementation and monitoring of Data Pipelines12. Lead and/or work with cross-disciplinary teams to understand, document and analyze customer needs13. Identify and present a range of potential solution options for any demand, informing stakeholders of advantages and disadvantages of each; assist them in arriving at an optimal solution strategy14.Optimize flexibility, scalability, performance, reliability, and future-proof capacity of IT services, at an optimal cost15.Implement chosen solutions, including infrastructure, scripts, database resources, permissions, source controlSkills and experience we're looking for:1. Bachelor/Master's Degree in Information Technology, Information Security/Assurance, Computer Science, Data Engineering, or related field or equivalent combination of education and experience2. 6+ years as a Python, PySpark, SQL developer; building scalable ETL applications and data warehouses, 5 years of data orchestration/engineering, and 3 years of cloud/automation experience3. Experienced data engineering build event driven data solution within Snowflake on AWS and/or Azure , strong working knowledge of DataOps and CI/CD best practises.4. Experience of building/developing/managing data using the Data Vault architecture built using DBT.5. Strong technical background with a firm grounding in SQL and Python, enabling you to develop our most sophisticated work.6. Strong interpersonal skills with the ability to work with customers to establish requirements, designs and deliver the solution.7. In addition to above essential skills needed include Scripting in Shell, Yaml, PySpark8. Technical knowledge on computer security operational security, threat actors, attack vectors, vulnerabilities, CVEs, and threat actor techniques7. Experience working with SIEM platforms and log management systems, including concepts such as big data management & analysis, dashboarding, data parsing, log message formatting, and data aggregationAdditional Skills we highly value :1. Experience in using Event stream loading using Snowflake connectors, Snowflake data marketplace, CDC capabilities, SnowPark APIs/UDF 2. Exposure to Vulnerability Management tools (i.e., Tenable, Rapid7, Qualys, Prisma Cloud, DivvyCloud, Wiz, etc.)3. Advantageous skills would include Databricks, EMR, & Big Data technologies AWS Glue, AWS Athena, AWS Lakeformation or similar in Azure4. Experienced in maintaining infrastructure as code using Terraform or cloud formation5. Advanced understanding of both SQL and NoSQL technologies6. Comfortable working with Semi-structured formats such as JSON, XML, and Avro7. Understanding of Machine Learning concepts8. Affinity to Cyber Defense/Cyber Security field9. Ability to develop and communicate recommendations to management10. Ability to understand security data and associated transformation11. Ability to communicate complex technical issues simply to different audiences12. Ability to quickly learn new Information Security concepts and adapt to a fast-paced, ever-changing organization13. Must be organized, disciplined, and task/goal oriented14. Able to prioritize and coordinate work through interpretation of high-level goals and strategy15. Effective team player with a positive attitude16. Strong oral and written English language communications skills17. Familiar with Zero Trust (ZT) architectures in addition to zero trust best practices. Familiar and experience with ZT vendor products and solutions such as SOAR, Identity, SIEM. Works with customers to plan and implement complex Zero Trust customer solutions.18. Provides guidance and work leadership to less-experienced staff.19. Experience with development and implementing data standards, data tagging, data formats.20. Experience with implementing complex database, data warehouse, or data analytics solutions. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
DarioHealth,Data Engineer - Hybrid,"About The Position

At Dario, Every Day is a New Opportunity to Make a Difference.

﻿We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career.

DarioHealth is looking for an experienced Data Engineer who will join our team and create new data solutions, maintain existing solutions and be a focal point of all technical aspects of our data activity. As part of this position, you will develop advanced data and analytics solutions to support our analysts and production units with validated and reliable data.

Responsibilities
• Develop and maintain DarioHealth data infrastructure.
• Develop in-house applications for providing self-service tools.
• Develop real-time data applications for production.
• Provide analysts and data scientists technical support related to data infrastructure.
• Design, build and launch new data models and visualizations in production, leveraging common development toolkits.

Requirements
• At least 4 years of proven experience with Python - a must.
• Very high level of SQL and data warehouse modeling.
• Experience with 24/7 systems and real-time analytics.
• Experience developing data pipelines with Airflow or similar - a must
• Experience with big data solutions like Kinesis/Sparks - an advantage
• Experience with NoSQL databases like MongoDB/Redis.
• Experience with web development using Django/javascript/react - an advantage.
• Experience in the online industry.
• B.A./B.Sc. in industrial/information systems engineering, computer science, statistics, or equivalent.
• **DarioHealth promotes diversity of thought, culture and background, which connects the entire Dario team. We believe that every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions. We are passionate about building and sustaining an inclusive and equitable working and learning environments for all people, and do not discriminate against any employee or job candidate.***",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False
Anblicks,Data Engineer,"Data Engineer :

Exp : 4+

Positions : 2

Location : Hyderabad/Ahmedabad/Vijayawda

Bachelor's or Master's degree in Computer Science, Information Systems, or a related field

4+ years of experience in data engineering and data architecture

Strong expertise in Python and SQL

Strong knowledge of data technologies, such as Snowflake, Databricks, Apache Spark, Hadoop, Dbt, Fivetran, Azure Data Factory

Experience designing and developing end-to-end data solutions, data pipelines, and ETL processes

Experience with data modelling is a plus

Excellent problem-solving and analytical skills

Ability to work independently and as part of a team

Experience working in an Agile environment

Knowledge and experience in developing software using agile methodologies.

Experience working with a globally distributed team.

Skilled in building relationships with clients and in practice development activities.

Excellent written and oral communication skills; Ability to communicate effectively with technical and non-technical staff.

Must be open to travel
Employment Type: FULL_TIME",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,True,False,False,False,True
Bosch Group,Azure data engineer,"Company Description

Bosch Global business solutions private limited is a 100% owned subsidiary of Robert Bosch GmbH, one of the world's leading global supplier of technology and services, offering end-to-end Engineering, IT and Business Solutions. With over 19,500 associates, it’s the largest software development center of Bosch, outside Germany, indicating that it is the Technology Powerhouse of Bosch in India with a global footprint and presence in the US, Europe and the Asia Pacific region.
• Responsible for data product development in Azure environment
• Development and Orchestration of automation pipelines that handles mass volume
• Build and maintain data integration from varied sources (SAP, Oracle etc.,)
• Build services and reports based on log analytics
• Implement CI/CD pipelines
• Collaboration with global stakeholders to understand the requirements and provide cost effective solutions
• Consulting to stakeholders in Azure environment
• Overall 8+ years of IT experience out of which 3+ years of working experience in Azure, specifically on Azure data engineering (Synapse,SQL DB, Storage accounts, databricks, snowflake DB,Data factory, delta lake)
• Experience in Azure devops, Git version control
• Experience in data engineering practices with respect to performance, development standards and methodologies
• Good knowledge in Azure cloud infrastructure, security and development
• Good understanding of Agile/Scrum project methodology
• Good oral and written communication skills
• Conscious about quality, business value, performance
• Ability to work and collobarate with global teams
• Added advantage: Azure certification (DP-203)

BE/BTECH/MCA",Coimbatore,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Factspan,Factspan Analytics - Azure Data Engineer - Big Data/Hadoop,"Job Description :- 7+ Years of deep experience with complex data systems and good instincts around data modelling and usage.- Knowledge of data engineering technologies, architecture, and processes. Specifically, Azure Data Lake, Hadoop ecosystem, Kafka, and common third-party integration and orchestration tools.- Good knowledge of multi-cloud data ecosystem and build scalable solutions on cloud (Azure)- Good knowledge of Big Data Ecosystem-Spark, Hadoop, Databricks- Work across 3-4 teams to develop practices which lead to the highest quality products and contribute transformation change within the cloud- Experience building large scale data processing ecosystems with real time and batch style data as input using big data technologies- Experience in any programming language like Scala or Python.- Exposure to agile methodology and proven ability to technically lead a team of engineers across geographies.- Implement Data Quality, Data Governance on Azure Cloud ecosystem- Good instincts around technical architecture, including metadata, Rest API Integrations, Data API and Solution design of NoSQL and File systems.- Willingness and ability to invest in engineering growth.- Strong communication skills and ability to coordinate across a diverse group of technical and non-technical stakeholders.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Revolo Infotech,Data Engineer - SQL/Python,"Job Description :

- Design, develop, and maintain data pipelines and architecture for data storage, processing, and analysis

- Work with cross-functional teams to understand and implement data requirements

- Build and optimize data pipelines using various cloud-based technologies such as AWS, Azure, or Google Cloud Implement data visualization solutions using cloud-based tools such as Tableau, Power BI, or Looker Monitor and troubleshoot data pipeline issues, and implement solutions to improve performance and scalability

- Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis

- Stay up-to-date with the latest technologies and industry trends in data engineering and data visualization

Requirements :

- 2+ years of experience as a data engineer with a focus on cloud-based data pipelines and visualization

- Strong experience with cloud-based technologies such as AWS, Azure, or Google Cloud

- Experience with data visualization tools such as Tableau, Power BI, or Looker

- Strong knowledge of SQL and programming languages such as Python or Java

- Familiarity with big data technologies such as Hadoop, Spark, or Hive

- Strong problem-solving and analytical skills

- Experience working in an Agile development environment Bachelor's degree in Computer Science or related field.

Preferred Qualifications :

- Experience with data warehousing concepts and technologies

- Experience with data governance and data management best practices.

- Experience with machine learning and AI technologies Strong communication and teamwork skills.

Job Types : Full-time, Regular / Permanent, Contractual / Temporary

Salary : 1,000,000.00 - 1,200,000.00 per year

Benefits :

- Health insurance

- Internet reimbursement

- Paid sick time

- Paid time off

Schedule :

- Day shift

- Monday to Friday

Power BI: 2 years (Preferred)

Tableau: 2 years (Preferred)

AWS: 2 years (Preferred)
(ref:hirist.com)",Navi Mumbai,True,False,True,True,False,False,False,False,True,True,False,False,False,False,False,False
Bazzi Games,Data Engineer,"About Baazi Games Established in 2014, Baazi Games set out to give India a flavor of online gaming by introducing a plethora of indigenous apps. Combined with lucrative rewards, trust-building relationships with consumers & rich experienced skill-based games, it was a winning formula for all game lovers. With scintillating, top-of-the-line gaming platforms across the board, it wasn't long before PokerBaazi, CardBaazi, and BalleBaazi became household names. Our player-centric brand value only helped us deliver the best for our 10 million-plus users. Baazi's key differentiators are as follows: Most trusted online real money gaming platform (Launched India's very first 1 Crore, 2 Crore, 5 Crore guaranteed online poker tournaments)Financially sustainable, we are a profitable organization & strongly believe in product & innovation-led growth. Product Leadership across all the products - PokerBaazi is No.1 across the country - global rank, BalleBaazi ranks in Top 5, CardBaazi & Baazi Mobile Gaming are on a hyper-growth trajectory altogether. What key problem are we solving: Reimagine the Consumer Experience holistically (Product + Design + Engineering + Customer Service + User Behaviour & Intelligence + Trust) Drive it at a different scale altogether - Our concurrency & experience is amongst the highest in the industry (several thousand players playing at the same & yet we know, we are just getting started). Build a unique end-to-end ecosystem (Master the skill + Intelligent Practice + Win) Ecosystems for each of these skill-based experiences. About the role: As a Data Engineer at Baazi, you will be focused on delivering data-driven insights to various functional teams enabling them to make strategic decisions that add value to the top or bottom line of the business. Why this role will be a game-changer for you The role will accelerate your growth in terms of working with a fast-paced company, stakeholder management, and learning solid technical designs that sustain a heavy load. You will build on existing experience and better your understanding of what it means to release quality software that stands the test of time. You will directly be working and learning from the organizational leaders which include Head of Engineering, Head of Products, and Business Heads of other departments. Career Path of the Role At Baazi, for every role, we have dual career paths, in addition to the lateral movements giving Baazigars, a holistic well-rounded exposure to be a T-shaped high-impact professional. SME (Subject Matter Expert) career path - Building the Baazi DevOps as a Centres of Excellence with industry pioneering standards. Team career path - Leading teams & Baazigars to higher success & impact Here is an example of the career path (this is for Data Engineer) that we align to across our functions: What you will do Design, build and own all the components of a high-volume data warehouse end to end. Build efficient data models using industry best practices and metadata for ad hoc and pre-built reporting. Interface with business customers, gathering requirements and delivering complete data & reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers. Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources. Own the functional and non-functional scaling of software systems in your own area. Provides input and recommendations on technical issues to BI Engineers, Business & Data Analysts, and Data Scientists. What we look for 3-5 years of experience in data engineering/business intelligence space Strong understanding of ETL concepts and experience building them with large-scale, complex datasets using the traditional or map-reduce batch mechanism. Strong data modeling skills with solid knowledge of various industry standards such as dimensional modeling, star schemas, etc Extremely proficient in writing performant SQL working with large data volumes Experience designing and operating very large Data Warehouses Experience with scripting for automation (e.g., UNIX Shell scripting, Python, Perl, Ruby). Good to have experience working on AWS stack Clear thinker with superb problem-solving skills to prioritize and stay focused on big needle movers Curious, self-motivated & a self-starter with a can-do attitude. Comfortable working in a fast-paced dynamic environment. Key Behaviours that we look for Must have excellent knowledge of Advanced SQL working with large data sets. Must have excellent dimensional modeling skills. Good to have experience with AWS technologies including Redshift, RDS, S3, EMR, EML or similar solutions build around Hive/Spark, etc. Good to have experience with reporting tools like Tableau, OBIEE, or other BI packages. Interview Process Round one - evidence from experience that demonstrates fitment, expertise in the Data Engineer role and technical depth, some behavioral skills. Round two - behavioral skills not covered in round one, expertise in Data Engineer role. Round three - optional third round to go over aspects that may not have been covered in the first 2 rounds. The rounds may not be in a particular order depending on the interviewer's availability. HR Round.",Bengaluru,True,False,True,False,False,False,False,True,False,True,False,False,True,False,False,False
Aroha Technologies,Data engineer,"hi everyone

we have opening with data engineer for python and sql. we are looking for BE,MCA,MSC,Mtech graduates and year passing should be from 2016 to 2020.

interested can share their resume to HIDDEN TEXT",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Gartner, Inc.",Sr Data Engineer,"What You Will do
• Architect, design, and implement high-performing, scalable, and optimized data solutions
• Design, build and automate the deployment of our data pipelines and applications to support data scientists and researchers with their reporting and data requirements
• Ingest / harvest data from various sources including web pages & REST APIs using modern scraper & programming techniques.
• Scale mature ingestion platforms using advanced Python and strong command of AWS cloud services (e.g. Lamba, Step Functions)
• Collaborate with internal business units and data science teams on implementation, technical issues, and training of the datalake ecosystem
• Perform ETL data processing using SQL and dbt (https://www.getdbt.com/) for data ingested from various disparate sources (web scrapers, APIs, manually inputted)
• Ensure data integrity by writing automated tests and work with QA to create monitoring dashboards
• Work with internal infrastructure teams on monitoring, security, and configuration of datalake environment and applications
• Work with team on managing AWS resources and continuously improve deployment process of our applications
• Work with administrative resources and support provisioning, monitoring, configuration, maintenance and cost optimization of AWS tools, Snowflake, & Postgres
• Promote the integration of new cloud technologies and continuously evaluate new tools that will improve the organization’s capabilities while leading to lower total cost of operation
• Support automation efforts across the data analytics team utilizing Infrastructure as Code (IaC) using Terraform, Configuration Management, and Continuous Integration (CI) / Continuous Delivery (CD) tools such as Jenkins
• Work with the team to implement data governance, access control and identify and reduce security risks

What You Will Need:
• Bachelor's or Master’s Degree in Computer Science, Information Systems, Engineering or related technical fields.
• 4-8 years’ experience in software development, including significant experience in Big Data and Cloud Services
• Experience with Python
• Expertise with SQL
• Experience with ETL / data transformation
• Experience with modern Big Data Modelling techniques
• Experience with AWS services
• Experience with unit testing
• A plus if you have experience with:
• Big data tools: Snowflake, Hadoop, Spark
• DBT ( https://www.getdbt.com/ )
• Continuous integration/delivery tools like Jenkins and infrastructure as code using Terraform
• BI tools, specifically Tableau and PowerBI
• Ability to take vague requirements and transform them into deliverables
• Good combination of technical and interpersonal skills with strong written and verbal communication; detail-oriented with the ability to work independently
• Takes initiative on improvements and testing results
• Owner mindset – identify, communicate, and act on issues and initiatives
• Ability to handle multiple tasks and projects simultaneously in an organized and timely manner
• Detail oriented, with the ability to plan, prioritize, and meet deadlines in a fast-paced environment
• Ability to work independently, as well as part of a team
• Experience working with fast-paced operations/dev teams and DevOps

Who are we?

Gartner delivers actionable, objective insight to executives and their teams. Our expert guidance and tools enable faster, smarter decisions and stronger performance on an organization’s most critical priorities. We’ve grown exponentially since our founding in 1979 and we're proud to have nearly 16,000 associates globally that support our 14,000+ clients in more than 100 countries.

What makes Gartner a great place to work?

Our teams are composed of individuals from different geographies, cultures, religions, ethnicities, races, genders, sexual orientations, abilities and generations. We believe that a variety of experiences makes us stronger—as individuals, as communities and as an organization. That’s why we're recognized worldwide as a great place to work year after year. We've been recognized by Fortune as one of the World’s Most Admired Companies, named a Best Place to Work for LGBTQ Equality by the Human Rights Campaign Corporate Equality Index and a Best Place to Work for Disability Inclusion by the Disability Equality Index. Looking for a place to turn your big ideas into reality?

What we offer:

Our people are our most valuable asset, so we invest in them from Day 1. When you join our team, you’ll have access to a vast array of benefits to help you live your life well. These resources are designed to support your physical, financial and emotional well-being. We encourage continued personal and professional growth through ongoing learning and development opportunities. Our employee resource groups, charity match and volunteer programs keep you connected to your internal Gartner community and causes that matter to you.

The policy of Gartner is to provide equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, ancestry, disability, veteran status, or any other legally protected status and to affirmatively seek to advance the principles of equal employment opportunity.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,True
CommerceIQ,Data Engineer SDE2,"The Role:

The team's mission is to maintain the snowflake and schema evolution tool. This involves working with application teams in evolving snowflake schema for various applications, development and maintenance of our in house schema evolution process and tool.

Exploring Snowflake capabilities for role based access, zero copy cloning and resource monitoring to reduce our cost and improve data security.

What You'll Do:
• Enhance and support our in house tool for applying data definition changes to all our snowflake databases.
• Work on enhancing RBAC (Role Based Access Control) and security models in snowflake.
• Enhance and support database access libraries for snowflake.
• Improving and reviewing database schema.
• Reducing snowflake cost by optimizing usage patterns.
• Advocate Snowflake usage and best practices

What You'll Bring:
• Advanced SQL - Beyond just CRUD.
• Able to code in java.
• An understanding of data modeling, data access, data storage, and optimization techniques
• Experience working with cloud based technologies and development processes
• A pragmatic approach and desire to learn new things
• Knowledge on schema model design like STAR schema / Snowflake Snowflake",Bengaluru,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,True
Collabera Digital,Senior Data Engineer,"Mandate Skills:

Hi ALL,

Collabera is hiring Data Engineer

Role - Data Engineer

5+ Years

WFO in hybrid Mode

Location :: Bangalore

Bigdata/Hadoop: 5+ years relevant experience

SQL:

Pyspark:

DWH:

3-5 years’ experience with Bachelor's Degree in Computer Science, Engineering, Technology or related field required. 5 to 8 years of relevant software development experience with sound skills in database modeling (relational, multi-dimensional) & optimization and data architecture - databases e.g. Vertica

● Good understanding of streaming technologies like Kafka, Spark Streaming. Proficiency in one of the programming language preferably Java, Scala or Python. Good knowledge of Agile, SDLC/CICD practices and tools with good understanding of distributed systems

● Experience with Enterprise Business Intelligence Platform/Data platform sizing, tuning, optimization, and system landscape integration in large-scale, enterprise deployments.

● Must have proven experience with Hadoop, Mapreduce, Hive, Spark, Scala programming. Must have in-depth knowledge of performance tuning/optimizing data processing jobs, debugging time consuming jobs.

● Proven experience in development of conceptual, logical, and physical data models for Hadoop, relational, EDW (enterprise data warehouse) and OLAP database solutions.

● Experience working extensively in multi-petabyte DW environments. Experience in engineering large-scale systems in a product environment",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Numerator,Data Engineer,"Responsibilities:
• Develop expertise in the different upstream data stores and systems across Numerator.
• Design, develop and maintain data integration pipelines for Numerators growing data sets and product offerings.
• Build testing and QA plans for data pipelines.
• Build data validation testing frameworks to ensure high data quality and integrity.
• Write and maintain documentation on data pipelines and schemas",Vadodara,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.Job DescriptionDevelops and maintains scalable data pipelines for bulk data movement between systems of record and systems of referenceDevelops and maintains scalable application to application integrationsAligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organizationImplements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processesWrites appropriate unit or integration tests to implement test-driven developmentContinually contributes to and enhances data team documentationPerforms data analysis required to troubleshoot and resolve data related issuesWorks closely with a team of frontend and backend engineers, product managers, and analystsDefines company data assets, artifacts and data modelsQualificationsRequired qualifications:5 years of Data Engineering and Data Integration5 Years of Data Warehousing3 Years of Data Architecture and Modeling2 years of Cloud Data EngineeringAgile MethodologiesPreferred skills:AWS or Azure Data CertificationsExperience with databricks, spark, pythonExperience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)Experience with SalesforceAdditional InformationAll your information will be kept confidential according to EEO guidelines.** At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. ** insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Edu Angels India Private Limited,Data Engineer (PySpark),"Responsibilities
• Develop process workflows for data preparations, modeling, and mining Manage configurations to build reliable datasets for analysis Troubleshooting services, system bottlenecks, and application integration.
• Designing, integrating, and documenting technical components, and dependencies of big data platform Ensuring best practices that can be adopted in the Big Data stack and shared across teams.
• Design and Development of Data pipeline on AWS Cloud
• Data Pipeline development using Pyspark, AWS, and Python.
• Developing Pyspark streaming applications

Eligibility
• Hands-on experience in Spark, Python, and Cloud
• Highly analytical and data-oriented
• Good to have - Databricks",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Aryng,Data Engineer - Cloud - Remote (Ahmedabad),"Welcome! You made it to the job description page!

Aryng is looking for a cloud data engineer with experience in developing
enterprise-class distributed data engineering solutions on the cloud. We are seeking
an entrepreneurial and technology-proficient Data Engineer who is an expert in the
implementation of a large-scale, highly efficient data platform, batch, and real-time
pipelines and tools for Aryng clients. This role is based out of India. You will work
closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength
is creative and effective problem-solving.

Key Responsibilities:

● Should have implement asynchronous data ingestion, high volume stream data
processing, and real-time data analytics using various Data Engineering
Techniques.
● Implement application components using Cloud technologies
and infrastructure.
● Assist in defining the data pipelines and able to identify bottlenecks to enable
the adoption of data management methodologies.
● Implementing cutting edge cloud platform solutions using the latest tools and
platforms offered by GCP, AWS, and Azure.
• 2+ years of data engineering experience is a must.
• 2+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers
• 2+ years’ experience in Python.
• Must be strong in SQL and its concepts.
• Experience in Big Query, Snowflake, Redshift, DBT.
• Strong understanding of data warehousing, data lake, and cloud concepts.
• Excellent communication and presentation skills
• Excellent problem-solving skills, highly proactive and self-driven
• Consulting background is a big plus.
• Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study

Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred

This role requires mandatory overlap hours with clients in the US from 8 am - 1
pm PST.
• Direct Client Access
• Flexible work hours
• Rapidly Growing Company
• Awesome work culture
• Learn From Experts
• Work-life Balance
• Competitive Salary
• Executive Presence
• End to End Problem Solving
• 50%+ Tax Benefit
• 100% Remote company
• Flat Hierarchy
• Opportunity to become a thought leader

Why Join Aryng: Click on the Youtube link",Ahmedabad,True,False,True,False,False,False,False,True,False,True,False,False,True,False,True,True
Zepto,Data Engineer III (Lead Data Engineer),"Responsibilities:
• Collaborate with Tech and Analytics team to build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
• Oversee and govern the expansion of the current data architecture as the business grows and ensure best practices are followed.
• Design and build best-in-class architecture for data tables to ensure optimal querying performance in relational databases.
• Create and maintain connectors that expose the data securely for consumption by downstream systems and services in near real-time.
• Create and maintain data architecture docs to communicate data requirements that are important to business stakeholders and work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that.
• Build data governance and security protocols and ensure adherence from analytics, tech, and business teams.
• Build and mentor the data engineering team, recognize their strengths, and lead them to take ownership of end-to-end data architecture.
• Stay on top of the latest developments in the tech stack and propose potential upgrades to existing systems.

Requirements:
• 6 to 10 years of experience in Data Engineering - Designing databases, building data pipelines, and maintaining data governance protocols in cloud platforms.
• A visionary in technical architecture, with experience building and maintaining Data.
• Engineering Products, along with the demonstrated ability to take accountability for achieving results.
• Hands-on working experience with Python, ETL pipelines, and advanced SQL.
• Strong understanding of AWS Services - Redshift, Lambda, Glue, Athena, and security protocols.
• Experience in any Cloud DW Redshift/Snowflake/BigQuery/Synapse.
• Strong data Modelling and database design experience with Redshift or other relational databases.
• Experience working with Agile methodologies, Test Driven Development, and implementing CI/CD pipelines using Gitlab and Docker.
• Good understanding of ETL/ELT technology and processes.
• Experience in gathering and processing raw data at scale including writing scripts, web scraping, and calling APIs.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,True,False,True
Rightpoint,Data Engineer,"Description

Rightpoint, a Genpact company (NYSE: G) is a global experience leader. Over 12 offices work with clients end-to-end, from defining and enabling vision, to ensuring ongoing market relevance. Our diverse teams lead with empathy, data and creativity—always in service of the experience. From whiteboard to roll-out, we help our clients embed experience across their operations from front to back office to accelerate digital transformation through a human-centric lens. Rightpoint has been recognized among the top customer experience consultancies in The Forrester Wave™: Customer Experience Strategy Consulting Practices, Q4 2020.

Are you someone who wants to inspire change in the way business is done? Do you want to work with encouraged and like-minded intrapreneurs? Us too! We take our work very seriously, but we have fun doing it. And we’re searching for passionate, talented people to join the Rightpoint team.

Our Commitment to You

No matter who you are, where you come from, who you love, what you believe, or what you geek out about, we bring people together to make great work. That's what makes us Rightpoint!!

Brief About Role

The Senior Data Engineer, Analytics role falls into the Data Engineering practice within the Systems and IT department. The Senior Data Engineer focuses on building cutting edge, strategic data and analytics solutions leveraging BI solutions. You will use your experience in data engineering and data visualization to work with RP business units including Finance, IT, Operations, People Potential (HR), Sales, Marketing, and Delivery practices to help them become more data driven by providing timely, trustworthy data and insights. 

What you'll do:
• Responsible for building and creating data visualization, business intelligence reports and dashboards.
• Work with business leadership to define business intelligence requirements, Key performance indicators (KPI), priorities and data solutions.
• Contribute as part of the Systems and IT Data Engineering team to the enterprise data analytics and reports.
• Accountable for facilitating data gathering, requirement analysis, data analysis and building reports.
• Hands-on development of Power BI dashboards
• Ensure proper standards, policies and procedures are followed.
• Leverage data to provide meaningful insights to business leaders and use BI tools to support operational processes by delivering data visualization reports and dashboards.
• Utilizes an advanced understanding of multiple data structures and sources to lead the design, development and implementation of decision support solutions, which may include data visualization, business intelligence, or data collection.

What We’d Like to See:
• 5+ years demonstrated advanced proficiency in data, analytics, and BI development.
• 5+ years demonstrated understanding of database technology, SQL, and data warehousing design patterns
• Track record of creating effective reports and dashboards in Power BI
• Interest in thinking broader than data visualization, curiosity to expand skills to other forms of analytics and data applications.
• At least 4-year hands-on work experience with Microsoft Power BI, experience of working in AAS environment will be Plus.
• Experience in optimizing Microsoft Power BI dashboards with a focus on usability, performance, flexibility, testability, and standardization.
• Solid understanding of the following concepts
• Data visualization
• Data modeling
• Data warehousing
• ETL Design and Development
• Data lineage, integrity, and validation 
• Data quality / clean up 
• Relevant hands-on development experience in the following: 
• Design and develop reports and dashboards using PowerBI, Tableau, Qlik, amongst others.
• Design and develop database stored procedures, functions, views and queries using various SQL standards (e.g. T-SQL, PL/SQL, etc.) 
• Design, develop, tune and maintain ETL packages using ETL tools such as SSIS and Azure Data Factory 
• Hands on experiences with Azure tools or their analogues AWS / Google cloud tools (this is not an exhaustive list) 
•",,False,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
Zensar Technologies,AWS DATA Engineers,"AWS Data Engineer

What's this role about?

Looking for a self-driven individual who has a high quotient for problem solving, technical zeal in learning newer areas and has worked in a high demand work environment. Candidate would be involved in various projects

AWS technical background having extensive experience in Data engineering services and should have knowledge OR worked on Spark, EMR, RedShift, Kinesis, Lambda, Glue, Apache Airflow, Data Lake

Here's How You'll Contribute
• Work with the team in capacity of AWS Data Engineer on day to day activities
• Solve problems at hand with utmost clarity and speed
• Train and coach other team members
• Ability to turn around quickly
• Work with Data analysts and architects to help them solve any specific issues with tooling/processes
• Design, build and operationalize large scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, RedShift, Kinesis, Lambda, Glue, Apache Airflow, Data Lake
• Design and build production data pipelines from ingestion to consumption within a big data architecture, using Python, Spark, Apache Hudi
• AWS RedShift modeling and performance tuning techniques
• RDBMS and No-SQL database experience
• Knowledge on orchestrating workloads on cloud
• Implement Data warehouse & Big/Small data designs, data lake solutions with very good data quality capabilities

Skills Required To Contribute

Hands-on (Must have) 6+ years of working experience:
• AWS - S3, Spark, EMR, RedShift, Kinesis, Lambda, Glue, Apache Airflow, Data Lake

Primary Location: India-Maharashtra-Pune

Job Posting: Feb 28, 2023

Experience Required (In Years)

6",Pimpri-Chinchwad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Recruise India Consulting Pvt Ltd,1700_Medical Affairs Data Engineer,"• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouse like Redshift
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Prior Data Modelling, OLAP cube modelling in SQL Server, SSAS and Power BI experience
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience of building and optimizing “big data” data pipelines, architectures and data sets
• Experience of performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores",Bengaluru,False,False,True,False,False,False,False,False,True,False,True,False,True,False,False,False
Ericsson,People Analytics Data Engineer,"About this opportunity

We are seeking an expert Data Engineer to join our People Analytics Team. The hire will be responsible for expanding and optimising our data landscape using a unified and scalable architecture. They will help to optimise our relational data structures with good data warehouse design methodology and some data modelling outside of the data warehouse. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimising data systems and building them from the ground up. The Data Engineer will support our database architects, data analysts and data scientists on data initiatives and ensure an optimal data delivery architecture consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited about optimising or re-designing our company’s data architecture to support our next generation of products and data initiatives!
• We believe in trust – we trust each other to do the right things!
• We believe in taking decisions as close to the product and technical expertise as possible.
• We believe in creativity – trying new things and learning from our mistakes.
• We believe in sharing our insights and helping one another to build an even better user plane.
• We truly believe in happiness, we enjoy and feel passionate about what we do and value each other’s technical competence deeply.

What you will do

Proactively drive the vision for BI and Data Warehousing across a product lifecycle and define and execute a plan to achieve that vision.
• Define the processes needed to achieve operational excellence in data management, including data quality, system reliability and project management.
• Understanding the business requirements for building a new data model or enhancing the existing ones based on new requirements to ensure that the data models are fit.
• Drive a good schema design for scalable, high-performance data models.
• Drive the design, building, and launching of new data models and data pipelines in production. Create and maintain optimal data pipeline architecture.
• Review the design specification with the Solution Architect and the Development team, bringing in outside-in and best practices perspectives to the solution, validating and quality assuring the Data models, and assessing their fit for purpose.
• Bring knowledge, experience and best practices to data modelling areas such as Slowly Changing Dimension (SCD) so that we can track the history of dimension records.
• Provide standard processes guidance on building normalized and denormalized tables for addressing different use cases such as dashboard development, data science exploration, and other business-centric use cases.

You will bring
• Collaborate with the Data Science and visualisation team to understand the business need for delivering impact dashboards and data visualisations. Transform these business needs into Data Model requirements. Develop use case-specific scalable and optimised data models.
• Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery, and re-designing infrastructure for greater scalability.
• Review and collaborate with the development team to build the infrastructure (on-premise or cloud) required for optimal extraction, transformation, and loading of data from various data sources using SQL, AWS ‘big data’ technologies, Hadoop, Snowflake etc. Experience with SAP technologies such as SAP S/4 HANA, SAP HRMS, Native HANA (BI Tool), and SuccessFactors Employee Central will be preferable. Experience with Dashboarding tools such as Tableau and PowerBI will be preferable.
• Experience building analytics tools that utilise the data pipeline to provide actionable insights into people analytics, operational efficiency, and other key business performance metrics.
• Experience in solving data-related technical issues and supporting their data infrastructure needs.
• Experienced in data storage and transfer (including cross border), adhering to global and local legal/regulatory and compliance needs, such as GDPR. Comfortable in working with diverse teams from Legal and compliance on these matters.

Why join Ericsson?

At Ericsson, you´ll have an outstanding opportunity. The chance to use your skills and imagination to push the boundaries of what´s possible. To build never seen before solutions to some of the world’s toughest problems. You´ll be challenged, but you won’t be alone. You´ll be joining a team of diverse innovators, all driven to go beyond the status quo to craft what comes next.

What happens once you apply?

Encouraging a diverse and inclusive organization is core to our values at Ericsson, that's why we nurture it in everything we do. We truly believe that by collaborating with people with different experiences we drive innovation, which is essential for our future growth. We encourage people from all backgrounds to apply and realize their full potential as part of our Ericsson team.

Ericsson is proud to be an Equal Opportunity and Affirmative Action employer, learn more.

Primary country and city: India (IN) || India : Haryana : Gurgaon

Job details: Business Analytics Specialist

Primary Recruiter: Sudarshan Mandolkar",Gurugram,False,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
Himflax information Technology,Data Engineer - ETL/Snowflake DB,"Job Description :

- Good Communication compulsory(Oral/Verbal)Good Hands-on in Data Warehouse concepts, Dimensional Modeling (Star/snowflake schema) Dimensions, Facts, Partitioning and processing data. Exp of 1-2years.

- Exp. in Azure but compulsory hands on SQL Database(2yrs- 3yrs).

- Minimum 2 years experience in ETL Development - Data Integration, building data pipelines using SSIS/ADF.(2yrs).

- Good Understanding of ETL concepts like SCDs, CDC, Lookup & Scripting etc.

- Should be ok to work in morning shift (starting from 5.30 am)

Good to have :

- Exp. in Streamsets, Exp in using Snowflake, working knowledge in DBT

Secondary Skills :

- Should have good communications skills (Written/ Oral).

- Should be self driven to overcome technical challenges

- Should be able to work independently as well as in a team.

- Should be able to handle client independently.

- Practical exp. in Power BI will be a good add-on(Optional)
(ref:hirist.com)",,False,False,True,False,False,False,False,False,True,False,False,False,False,False,False,True
Tiger Analytics India Consulting Private Limited,Senior Data Engineer - Denodo,"Job Title: Senior Data Engineer – Denodo

Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 2800+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Curious about the role? What your typical day would look like?
· Engage with clients to understand their business context.
· Translate business needs to technical specifications.
· Define Data Virtualization architecture, deployments, and standards.
· Support the development of data architecture principles, standards, and processes and applies these to deliverables
· Involving in data exploitation and the development of (advanced) analytical data models with multiple data sources using Denodo/Tibco or AtScale semantic layer.
· Developing integrated data solutions, modernizing, consolidating, and coordinating business needs across several applications.
· Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Valiance Solutions,Big Data Engineer,"About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc. Our team comprises 150+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire a Senior Data Engineer to help our customers create scalable data engineering pipelines and infrastructure for downstream analytics workloads. You should be good at understanding client data needs, the landscape of various heterogeneous data sources, identifying a set of services for data ingestion & transformation workloads, and timely execution of projects.

Roles & Responsibilities:
• As a data engineer with Pyspark & SQL skills you will be required to highly scalable, robust, and resilient data engineering pipelines .
• You will be working closely with business stakeholders & the data science team to understand their data requirements and underlying business logic.
• Deploy and monitor pyspark jobs on cloud infrastructure.
• Troubleshoot job failures and ensure system recovery at earliest.
• Attending regular client calls, communicating work status and pro-actively highlighting any delays to the product release.

Technical Skills :
• Hands on experience on pyspark for at least 3 years
• Solid programming experience in Python & SQL is required.
• Working experience of any one cloud platform; AWS, GCP or Azure
• Intermediate plus proficiency in shell scripting
• Experience deploying ML algorithms in production is preferred

Personal Skills :
• Excellent communication skills, both written & oral.
• Ability to learn new skills quickly, adjust to the changing needs of the project.
• You are highly enthusiastic about your work
• Ability to multi-task, manage high-pressure release scenarios occasionally.

Valiance Solutions focuses on Financial Services, Cloud Computing, Artificial Intelligence, Internet of Things, and Big Data Analytics. Their company has offices in Noida and Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://valiancesolutions.com or find them on Twitter and LinkedIn.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Niftel Resources,Senior Data Engineer,"Responsibilities:

 Design and build reusable components, frameworks and libraries at scale to support analytics

products

 Design and implement product features in collaboration with business and Technology

stakeholders

 Anticipate, identify and solve issues concerning data management to improve data quality

 Clean, prepare and optimize data at scale for ingestion and consumption

 Drive the implementation of new data management projects and re-structure of the current data

architecture

 Implement complex automated workflows and routines using workflow scheduling tools

 Build continuous integration, test-driven development and production deployment frameworks

 Drive collaborative reviews of design, code, test plans and dataset implementation performed by

other data engineers in support of maintaining data engineering standards

 Analyze and profile data for the purpose of designing scalable solutions

 Troubleshoot complex data issues and perform root cause analysis to proactively resolve product

and operational issues

 Mentor and develop other data engineers in adopting best practices

Qualifications:

Primary skillset:

 Experience working with distributed technology tools for developing Batch and

Streaming pipelines using SQL, Spark, Python [3+ years], Airflow [2+ years], Scala [1+

years].

 Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc.

 Able to quickly pick up new programming languages, technologies, and frameworks.

 Strong skills building positive relationships across Product and Engineering.

 Able to influence and communicate effectively, both verbally and written, with team members and

business stakeholders

 Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed

Spark jobs, build Docker images, etc.

 Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture

Good to have:

 Experience working with Data platforms, including EMR, Airflow, Databricks (Data Engineering &

Delta Lake components, and Lakehouse Medallion architecture), etc.

 Experience working in Agile and Scrum development process

 Experience in EMR/ EC2, Databricks etc.

 Experience working with Data warehousing tools, including SQL database, Presto, and

Snowflake

 Experience architecting data product in Streaming, Server less and Microservices Architecture

and platform.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,True,True
Randstad India,Data Engineer - ETL,"Design, build, and maintain the data infrastructure that supports our data-
driven applications and services.• Develop and maintain ETL (Extract, Transform, Load) processes to ensure
data accuracy and completeness.
• Optimize data pipeline performance and scalability.
• Collaborate with data scientists, analysts, and developers to ensure that our
data pipeline meets their needs.• Implement data governance policies and procedures to ensure data quality
and consistency.
• Design and develop data models that support data-driven applications and
services.
• Troubleshoot and resolve issues related to the data pipeline.
• Participate in the evaluation and selection of data management and analytics
tools and technologies.
• Keep up to date with emerging trends and technologies in data engineering
and big data.

experience

8",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Bloom Energy,Senior Engineer - Data Engineering,"Role and Responsibilities
• Responsible for working on scalable ETL solutions and data pipelines from different types of data sources using python
• Investigate data to identify potential issues within ETL pipelines , notify end-users and implement adequate solutions
• Solve challenging data and architectural problems using cutting edge technology
• Responsible in working with business analysts across different functions to support ad-hoc analysis
• Cross functional collaboration with data science / IT Applications teams

Skills and Experience:
• Engineering or MCA background with 3 – 8 years of experience in data engineering / data warehousing
• 3+ years of relevant experience
• Strong python programming/debugging ability and clear design patterns understanding. Knowledge in C# is a plus
• Should be excellent in the SQL skills
• Hands on experience in Kafka
• Implementing data wrangling, transformation and processing solutions , demonstrated experience of working with large datasets
• Knowhow of cloud computing platforms like AWS
• Exposure to data lakes and data warehousing concepts, SQL (Redshift, PostgreSQL), NoSQL databases
• Exposure to developing/supporting API (REST/SOAP) based integration with various cloud vendors.
• Good analytical skills and high attention to detail
• Ability to be creative, versatile, efficient and productive
• Excellent communication (written and verbal) and interpersonal skills",Mumbai,True,False,True,False,False,False,False,True,False,False,False,False,True,False,False,False
T&N Business Services,Data Engineer (Technical support 3),"Job Details
Full-time

Full Job Description
Job TNS220929065545

Location India-Delhi

Designation Data Engineer ( Technical Support 3)

Experience 1-3 Yrs

Salary 6 Lakh -8 Lakh

Function Engineering ( Senior Operations Support Engineer )

Skills we need a candidate who have a good experience in SQL, Python, Java, C++etc. other Programming Langugaes and have a good experience in Technical Support L3

Job Type Full Time

Description

24*7 technical support

The Contributor handles complex cases that have been escalated by different stakeholders.

Manage technical escalations and blockages in pipelines, provide status updates and drive

them to successful resolution.

The contributor would be the primary escalation point for all issues in the ETL pipelines and

would be the point of contact for all crisis management related to blockages.

Assess and report process issues, attend triage to discuss customer impact and status, and

report back to the Core Team. This includes communication of customer impact to all internal

customer facing teams.

Work as an onCall support for all critical escalations, if needed able to fix code (in absence of

primary developer).

Keep upto date with latest development updates in critical and sensitive pieces of code in

the pipeline that goes into production, and if escalation comes. Be able to roll back

changes.

Read, understand and analyze development logs / daily progress of verticals and share it with

leads.

Scripting and data handling through Internal Google applications.

Development and document support process (SOPS), includes developing support

requirements with Product Management and Engineering.

Responsible for Support Member Training. Serves as the project's technical advisor to

management and represents the team at cross-functional meetings and discussions

Confers with the Team Lead/Manager daily to clarify priorities, processes, assignments and to

discuss any issues therein.

Document all blockages and customer-reported problems in the database/process related

guides, including the nature of the blockage, and the resolution recommended/provided.

Able to work in a competitive environment and meet the KPIs (Productivity, Quality, FRT (First

Response Time)

Regular client/leads interactions to discuss pipeline blockage related issues and progress of

resolutions for escalations

In absence of critical workflows, resources will work on regular pipeline monitoring

Requirements

Minimum Qualifications:

Preferably B.Tech or other graduate in computer science with good academics

Open to work in 24x7 shift environment

Skills and Requirement

Strong analytical, logical reasoning and problem solving skills

Knowledge of any programming language

Has a sound understanding of computer science concepts like OOPS, Database (SQL) and

Data Structure

Good verbal and written communication skills

Good academics and quick learner

Preferences

Job Responsibilities

24*7 technical support

The Contributor handles complex cases that have been escalated by different stakeholders.

Manage technical escalations and blockages in pipelines, provide status updates and drive

them to successful resolution.

The contributor would be the primary escalation point for all issues in the ETL pipelines and

would be the point of contact for all crisis management related to blockages.

Assess and report process issues, attend triage to discuss customer impact and status, and

report back to the Core Team. This includes communication of customer impact to all internal

customer facing teams.

Work as an onCall support for all critical escalations, if needed able to fix code (in absence of

primary developer).

Keep upto date with latest development updates in critical and sensitive pieces of code in

the pipeline that goes into production, and if escalation comes. Be able to roll back

changes.

Read, understand and analyze development logs / daily progress of verticals and share it with

leads.

Scripting and data handling through Internal Google applications.

Development and document support process (SOPS), includes developing support

requirements with Product Management and Engineering.

Responsible for Support Member Training. Serves as the project's technical advisor to

management and represents the team at cross-functional meetings and discussions

Confers with the Team Lead/Manager daily to clarify priorities, processes, assignments and to

discuss any issues therein.

Document all blockages and customer-reported problems in the database/process related

guides, including the nature of the blockage, and the resolution recommended/provided.

Able to work in a competitive environment and meet the KPIs (Productivity, Quality, FRT (First

Response Time)

Regular client/leads interactions to discuss pipeline blockage related issues and progress of

resolutions for escalations

In absence of critical workflows, resources will work on regular pipeline monitoring",Gurugram,True,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False
hyperx hp direct,Senior Data Engineer,"The Company HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations and dreams. We apply new thinking and ideas to create simpler, more valuable and trusted experiences with technology, continuously improving the way our customers live and work. Position background In the GTM analytics COE our mission is to deliver impact by building machine learning products to optimize pricing and marketing investments and provide guidance to our sales organization. As a Big Data Engineer, you will be in a unique position to support the development one of our internal assets. You will work together with the project and asset team to understand the end state in which the data must be delivered and you will model the data using Big Data technologies like Spark. We offer an international experience, collaborative culture, top rate experience in AI and ML and opportunity to create significant real-world impact. What you will do Create / Maintain ETL pipelines. Ensure that processes are optimized. Use Spark to model big volumes of data. Contribute to the database architecture, design and implementation. What you will need: Bachelor's in computer engineering, Computer Science, Electrical Engineering, Robotics or a related field 2+ years on a similar role.Ability to work independently under a fast-paced environment, comfortable to deliver results under pressure. You have strong problem-solving skills.Agile experience. Experience with modern application lifecycle management tools (Git, Visual Studio, Intellij, Code Reviews). Proficient in at least one of the following languages: Python, PySpark, Scala, Spark, SparkR. Experience with SQL & NoSQL databases is preferred (PostgreSQL, MongoDB, Elastic Search). Experience in working with DataIku DSS software. Strong analytical skills with demonstrated problem solving ability. Who We Are At HP, we believe in the power of ideas. We use ideas to put technology to work for everyone. And we believe that ideas thrive best in a culture of teamwork. That is why everyone - at every level in every function, is encouraged to think big, have original ideas and express and share them. We trust anything can be achieved if you really believe in it, and we will invest in your ideas to change lives and the way people work. This vision is what sets us apart as a company. At HP, we work across borders and without limits. Global virtual teams share resources, pool their big ideas to solve our biggest business opportunities. Everyone is valued for the unique skills, experiences and perspective they bring. That's how we work at HP. And this is how ideas and people grow.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Lilly,Senior Data Engineer,"We're looking for people who are determined to make life better for people around the world.

• Job Summary

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We're looking for people who are determined to make life better for people around the globe.

Business Units Information and Digital Solutions (IDS) is a global organization strategically positioned so that through information and technology leadership and solutions, we create meaningful connections and remarkable experiences, so people feel genuinely cared for. The Business Unit IDS organization is accountable for designing, developing, and supporting commercial or customer engagement services and capabilities that span multiple Buiness Units (Bio-Medicines, Diabetes, Oncology, International), functions, geographies, and digital channels. The areas supported by Business Unit IDS includes: Customer Operations, Marketing and Commercial Operations, Medical Affairs, Market Research, Pricing, Reimbursement and Access, Customer Support Programs, Digital Production and Distribution, Global Patient Outcomes, and Real-World Evidence.

This position is responsible for providing support for ETL / ELT / File Movement of data. The key responsibilities will be to process and move data between different compute and storage services, as well as on-premises data sources at specified intervals. The employee will also be responsible for the creation, scheduling, orchestration and management of data pipelines.

• Competency

Data Engineer

Data engineers are responsible for ensuring the availability and quality of data needed for analysis and business transactions. This includes data integration, acquisition, cleansing, harmonization and transforming raw data into curated datasets for data science, data discovery, and BI/analytics. Responsible for developing, constructing, testing and maintaining data sets and scalable data processing systems.

Data engineers work closest with Data Architects and Data Scientists. They also work with business and IT groups beyond the data sphere, understanding the enterprise infrastructure and the many source systems.

Input is raw datasets. Output is analytics-ready, integrated/curated datasets.

Key capabilities in this role family include:

• Data Acquisition - is the process of gathering and storing data in a location and format that it can be consumed for data preparation and/or downstream business uses.

• Data Preparation - is an iterative process for exploring, integrating, cleaning, validating and transforming raw data into curated datasets

• Data Publishing - is the act of releasing data in consumable form for (re)use by others.

Note: All data engineer roles should have a foundational set of knowledge in: communication, leadership, teamwork, problem solving skills, solution / blueprint definition, business acumen, architectural processes (e.g. blueprinting, reference architecture, governance, etc.), technical standards, project delivery, and industry knowledge.

Business Analysis and Technical Leadership

• Engages with business and proactively seeks opportunities to deliver business value.

• Understands business requirements and effectively translates business needs and process into technical terms, and vice versa

• Elicits and defines requirements

• Ensures appropriate business roles are engaged in solution execution.

• Participates in design reviews to ensure traceability of requirements.

• Networks with appropriate IT colleagues to determine solutions to meet business partners' needs.

• Seeks opportunities to reuse existing processes and services to streamline support and implementation of key systems.

• Stay abreast of tools and technologies to influence IT strategy so that it provides best usage opportunities for business

• Ability to adapt quickly in a constantly changing environment

Must Have:

• Bachelor's degree in computer science, information technology, management information systems or equivalent work experience

• 7+ years of development experience in the core tools and technologies like SQL, Python, AWS ( Lamda, Glue, S3, Redshift, Athena, IAM Roles & Policies) , PySpark used by the solution services team.

• Architect and build high-performance and scalable data pipelines adhering to data lakehouse, data warehouse & data marts standards for optimal storage, retrieval and processing of data.

• 3+ years of experience in Agile Development and code deployment using Github & CI-CD pipelines.

• 2+ years of experience in job orchestration using Airflow.

• 2+ years of experience leading a small team of data engineers.

• Expertise in the design, data modelling, creation and management of large datasets/data models

• Ability to work with business owners to define key business requirements and convert to technical specifications

• Experience with security models and development on large data sets

• Ensure successful transition of applications to service management team through planning and knowledge transfer

• Develop expertise of processes and data used by business functions within the US Affiliate

• Responsible for system testing, ensuring effective resolution of defects, timely discussion around business issues and appropriate management of resources relevant to data and integration

• Partner with and influence vendor resources on solution development to ensure understanding of data and technical direction for solutions as well as delivery

Preferred Qualifications / Certifications

• Experience working in regulated environments and with internal systems quality policies and procedures

• Familiarity with AWS database technologies.

• Knowledge of the data architectures associated with information integration & data warehousing

• Experience in development and deployment on cloud infrastructure

• Pharmaceutical or healthcare industry experience

• Early drug discovery industry experience

• Experience with Sales & Marketing Business processes & systems

• Defining best practices and developing technical standards, design principals, best practices, and frameworks

• Technical curiosity and desire to innovate

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
VISA,Staff Data Engineer,"Job Description

We are looking for an expert with deep expertise in big data/data warehousing and who has experience in working with clients in implementation of data engineering pipelines, data exchange mechanisms and data science models.

The role will need design and architecture skills to advice VISA’s clients on best approaches for data engineering and data science model implementations.

The candidate will also play critical role in enabling the internal data platforms using which Data Scientists, Analysts, and BI Users drive solutions for Visa clients. The candidate is expected to act as a bridge between end-users in Asia and Visa Technology colleagues in San Francisco, influencing the development of our global data platforms whilst provisioning local tools and technologies as required.

This is a hands-on role, and the candidate is expected to work hands on in areas such as Hadoop, Spark, Python, Tableau, Unix scripting.

We are looking for a talented, technical, proactive, energetic, and passionate person who embraces challenges and is a proven problem solver.

Principal Responsibilities
• Provide advisory and implementation support to VISA’s clients for deployment of data engineering pipelines and data science models.
• Work with VISA data scientists for optimization and automation of data science models.
• Support data exchange channels integration (Such as APIs, Secured File Transfer, other custom data exchange capabilities provided by VISA) between VISA and clients.
• Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack
• Provide leadership and support in identifying, implementing, and managing new data tools and processes, relevant to data science – on premise and cloud.
• Partner with Technology on quarterly planning cycle and support management with relevant metrics to evaluate performance, stability, and reliability of various tools.
• Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions.
• Develop custom-built packages to support the needs of Data Scientists across the region.
• Continuous focus on improving Infrastructure efficiency by analyzing logs of queries, tuning settings, translating queries if required.
• Review scripts and educate users by building training assets for beginner and intermediate level users.

Policy guidance and administration of tools used by data scientists including git, ETL schedulers, Spark, Tableau etc. to ensure optimum utilization and efficiency

Qualifications

Basic Qualifications
• Minimum of bachelor’s degree or equivalent
• Qualification in Computer Science or Engineering ideal.

Professional Experience
• 8 - 12 years of relevant experience
• Deep knowledge of distributed data architecture, commonly used BI tools, and approaches/packages used in machine learning build
• Expertise in Design and architecture of big data platforms, data science and visualization platforms.
• Expertise in creating production software/systems using Python and/or Scala, and a proven track record of identifying and resolving performance bottlenecks in production systems.
• Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets.
• Good appreciation of machine learning algorithms, feature engineering, validation, prediction, recommendation, and measurement.
• Experience of working in multiple large projects with diverse cross-functional teams.
• Ability to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.
• Understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant
• Demonstrated ability to incorporate new techniques to solve business problems
• Demonstrated resource planning and delivery skills
• Good communication and presentation skills with ability to interact with different cross-functional team members at varying level

Technical Expertise
• Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark preferred.
• Expertise in dashboard and report development in Tableau
• Experience in coming up with data flow patterns and deployment architecture for data engineering pipeline and data science models
• Experience with APIs, container-based software deployments
• Experience in developing production systems incorporating best-in-class software engineering practices such as DevOps, Agile development, CI/CD, and scheduling technologies
• Working knowledge of Hadoop ecosystem and associated technologies such as Hive, Apache Spark, PySpark, MLlib, GraphX.
• Advanced experience in writing and optimizing efficient SQL queries and Python scripts; Scala experience will be an added advantage.
• Good appreciation of cloud-based technologies and data platforms
• Very strong people/technology project management skills and experience

Business and Leadership competencies
• Results-oriented with strong problem-solving skills and demonstrated intellectual and analytical rigor
• Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred
• Team oriented, collaborative, diplomatic, and flexible
• Detailed oriented to ensure highest level of quality/rigor in reports and data analysis
• Proven skills in translating analytics output to actionable recommendations and delivery
• Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels
• Exhibits intellectual curiosity and a desire for continuous learning
• Demonstrates integrity, maturity, and a constructive approach to business challenges
• Role model for the organization and implementing core Visa Values
• Respect for the Individuals at all levels in the workplace
• Strive for Excellence and extraordinary results
• Use sound insights and judgments to make informed decisions in line with business strategy and needs
• Ability to allocate tasks and resources across multiple lines of businesses and geographies.
• Ability to influence senior management within and outside Analytics groups
• Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions

Additional Information

Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.

Job Description

We are looking for an expert with deep expertise in big data/data warehousing and who has experience in working with clients in implementation of data engineering pipelines, data exchange mechanisms and data science models.

The role will need design and architecture skills to advice VISA’s clients on best approaches for data engineering and data science model implementations.

The candidate will also play critical role in enabling the internal data platforms using which Data Scientists, Analysts, and BI Users drive solutions for Visa clients. The candidate is expected to act as a bridge between end-users in Asia and Visa Technology colleagues in San Francisco, influencing the development of our global data platforms whilst provisioning local tools and technologies as required.

This is a hands-on role, and the candidate is expected to work hands on in areas such as Hadoop, Spark, Python, Tableau, Unix scripting.

We are looking for a talented, technical, proactive, energetic, and passionate person who embraces challenges and is a proven problem solver.

Principal Responsibilities
• Provide advisory and implementation support to VISA’s clients for deployment of data engineering pipelines and data science models.
• Work with VISA data scientists for optimization and automation of data science models.
• Support data exchange channels integration (Such as APIs, Secured File Transfer, other custom data exchange capabilities provided by VISA) between VISA and clients.
• Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack
• Provide leadership and support in identifying, implementing, and managing new data tools and processes, relevant to data science – on premise and cloud.
• Partner with Technology on quarterly planning cycle and support management with relevant metrics to evaluate performance, stability, and reliability of various tools.
• Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions.
• Develop custom-built packages to support the needs of Data Scientists across the region.
• Continuous focus on improving Infrastructure efficiency by analyzing logs of queries, tuning settings, translating queries if required.
• Review scripts and educate users by building training assets for beginner and intermediate level users.

Policy guidance and administration of tools used by data scientists including git, ETL schedulers, Spark, Tableau etc. to ensure optimum utilization and efficiency

Qualifications

Basic Qualifications
• Minimum of bachelor’s degree or equivalent
• Qualification in Computer Science or Engineering ideal.

Professional Experience
• 8 - 12 years of relevant experience
• Deep knowledge of distributed data architecture, commonly used BI tools, and approaches/packages used in machine learning build
• Expertise in Design and architecture of big data platforms, data science and visualization platforms.
• Expertise in creating production software/systems using Python and/or Scala, and a proven track record of identifying and resolving performance bottlenecks in production systems.
• Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets.
• Good appreciation of machine learning algorithms, feature engineering, validation, prediction, recommendation, and measurement.
• Experience of working in multiple large projects with diverse cross-functional teams.
• Ability to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.
• Understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant
• Demonstrated ability to incorporate new techniques to solve business problems
• Demonstrated resource planning and delivery skills
• Good communication and presentation skills with ability to interact with different cross-functional team members at varying level

Technical Expertise
• Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark preferred.
• Expertise in dashboard and report development in Tableau
• Experience in coming up with data flow patterns and deployment architecture for data engineering pipeline and data science models
• Experience with APIs, container-based software deployments
• Experience in developing production systems incorporating best-in-class software engineering practices such as DevOps, Agile development, CI/CD, and scheduling technologies
• Working knowledge of Hadoop ecosystem and associated technologies such as Hive, Apache Spark, PySpark, MLlib, GraphX.
• Advanced experience in writing and optimizing efficient SQL queries and Python scripts; Scala experience will be an added advantage.
• Good appreciation of cloud-based technologies and data platforms
• Very strong people/technology project management skills and experience

Business and Leadership competencies
• Results-oriented with strong problem-solving skills and demonstrated intellectual and analytical rigor
• Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred
• Team oriented, collaborative, diplomatic, and flexible
• Detailed oriented to ensure highest level of quality/rigor in reports and data analysis
• Proven skills in translating analytics output to actionable recommendations and delivery
• Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels
• Exhibits intellectual curiosity and a desire for continuous learning
• Demonstrates integrity, maturity, and a constructive approach to business challenges
• Role model for the organization and implementing core Visa Values
• Respect for the Individuals at all levels in the workplace
• Strive for Excellence and extraordinary results
• Use sound insights and judgments to make informed decisions in line with business strategy and needs
• Ability to allocate tasks and resources across multiple lines of businesses and geographies.
• Ability to influence senior management within and outside Analytics groups
• Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions

Additional Information

Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",,True,False,True,False,True,False,False,True,False,True,False,True,False,False,False,False
RGBSI,Lead Data Engineer,"• 8+ years of experience in architecture, design, implementation, and support of data solutions
• Hands on development experience in creating tables, partitioning, loading, and aggregating data using Spark SQL/pyspark in Azure Databricks
• Advanced SQL experience working with relational databases
• Able to build ingestion to ADLS and enable BI layer for Analytics.
• In depth understanding of Spark Architecture including Spark Core, Spark SQL, Data Frames, Spark Streaming, Azure cloud, data lake and analytics solutions on Azure
• Understanding of database principles",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
TransUnion,Lead Data Engineer,"TransUnion's Job Applicant Privacy Notice

What We'll Bring

Data Pipeline Engineer at Orion project are embedded within our engineering teams and support the development and operation.

What You'll Bring

Lead Data Engineer

What We Offer

We are looking for an individual to be part of an autonomous, cross-functional agile/scrum team where everyone shares responsibility for all aspects of the work.

The ideal candidate will have a strong interest to join our growing Data Engineering and Analytics track of GFS Core services who will drive building next generation suite of products and platform by designing, coding, building and deploying highly scalable and robust solutions. We are looking for enthusiastic professionals who are excited to learn, love a good challenge, and are always looking for opportunities to contribute. Finally yet importantly, we look for dedicated team players who enjoy collaboration and can work effectively with others to achieve common goals.

TransUnion is currently seeking a Lead Data Engineer with 7+ years’ experience to work in our Chennai office, India. You will be working with some of the latest tools and a great team of cross-functional engineers. We work with multiple technologies. This will be an opportunity to work on core services of an industrial strength Identity and Risk solution by streamlining design and collaborating with the team to build orchestration platform in cloud.

Who We Are

At TransUnion, we are dedicated to finding ways information can be used to help people make better and smarter decisions. As a trusted provider of global information solutions, our mission is to help people around the world access the opportunities that lead to a higher quality of life, by helping organizations optimize their risk-based decisions and enabling consumers to understand and manage their personal information. Because when people have access to more complete and multidimensional information, they can make decisions that are more informed and achieve great things. Every day TransUnion offers our employees the tools and resources they need to find ways information can be used in diverse ways.

What You’ll Bring
• Bachelor's Degree in a quantitative field, plus 7+ years of work experience or equivalent practical experience.
• 5+ years of experience in Big Data technologies
• Experience designing and implementing data pipelines
• Experience with SQL, PostgreSQL and/or Redshift, or other data management, reporting and query tools.
• Big Data Technologies – Hadoop HDFS, Hive, Spark, Kafka, Sqoop
• Designing Logical Data Model and Physical Data Models including data warehouse and data mart designs.
• Expertise in writing complex, highly optimized queries across large data sets to write data pipelines and data processing layers.
• Cloud System experience on AWS, Azure, GCP –Preferably GCP
• Coach / Mentor / Lead a team of Data Engineers
• Design, build, test and deploy cutting edge Big Data solutions at scale
• Extract, Clean, transform, and analyze vast amounts of raw data from various Data Sources
• Build data pipelines and API integrations with various internal systems
• Proactively monitor, identify, and escalate issues or root causes of systemic issues
• Evaluate and communicate technical risks effectively and ensure assignments delivery in scheduled time with desired quality
• Work across Data Engineering, Data Architecture, Data Visualization functions

What We’ll Bring
• At TransUnion, we have a welcoming and energetic environment that encourages collaboration and innovation we’re – consistently exploring new technologies and tools to be agile. This environment gives our people the opportunity to hone current skills and build new capabilities, while discovering their genius.
• Come be a part of our team – you will work with great people, pioneering products and cutting-edge technology.
• This role is for a Lead Data Engineer that will operate as a lead for Data Pipeline track and responsible for development of the global fraud solutions of TransUnion.
• We pride ourselves in working in a collaborative cross-functional manner where all engineers are expected to contribute to design, build, deployment and operation of our cloud platform.

Location: Chennai

Job Type: Full-time day job

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law.

Impact You'll Make

N/A

TransUnion Job Title

Lead Developer, Software Development",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
Terrascope,Senior Data Engineer,"Join us on a mission to save the planet!

Reversing the impact of climate change is one of the world’s biggest challenges. And businesses have a responsibility to lead the way. While individual consumer choices are important, over 80% of all the emissions reductions necessary for the world to reach Net-Zero, require business-level action. But despite the growing momentum and ambition from companies around the world to set Net-Zero goals, there are significant challenges to delivering on these ambitions. Business leaders don’t really know how they will get there. And the very first step, of getting emissions measurement right, is hard.

Terrascope is a smart carbon management and accounting platform that empowers corporations to decarbonise their operations, portfolios and supply chains in a trusted, confident, and secure manner. We are on a journey to build digital tools and analytics, datasets and algorithms, and an ecosystem of technical expertise and partnerships needed for companies to optimise their climate strategy.

Terrascope is backed by one of the world’s largest food and agri companies and global leader in climate action and sustainability. With their significant strategic advantage and secure funding, the venture is uniquely positioned to drive profit with purpose; driving decarbonization in supply chains while generating outsized financial returns.

We are looking for a Senior Data Engineer who will work closely with our engineering and data teams to build efficient tools, design data architecture structure, and maintain our data warehouse and analytics environment. This role will report into the Head of Engineering (Data Platform) and will be crucial in shaping the future of climate-tech SaaS products.

In This Role You Will
• Enhance data collection, processing and cleansing procedures to include information that is relevant for building analytic systems.
• Develop and support automated and interactive visualisation and BI tools.
• Normalise and develop new data sources. This includes developing and implementing automated cleansing routines and procedures to improve data quality.
• Develop standalone functions and routines to enhance data, either via third party tools or scripts.
• Extend company’s data with third party sources of information when needed.
• Interpret and analyze patterns and present data findings to stakeholders to help support decision making.
• Collect and interpret data, including logical mapping for the purposes of data modellers/scientists etc
• Identify, analyse and escalate enterprise data quality issues, facilitate the determination of issue impact, root cause and solution options.
• Maintain appropriate level of documentation for new and existing initiatives.
• Design and build data ingestion/integration solutions to receive data from these prioritized external data sources. Develop data pipelines for the delivery of structured, semi-structured and unstructured data to the required standards. This might relate to internal applications, data marts/data warehouses or other systems.

You Should Have
• At least 6 years experience in similar role in a tech company/Saas company
• Data frameworks like: Spark, CDAC, CDAC APIs, batch pipelines, schedules Data Fusion on GCP etc.
• Data visualisation skills in any tools like Tableau etc.
• Database querying competency using SQL, PostgreSQL, Hive, Pig, BigQuery etc.
• Experience in writing data pipelines in Java or Python leveraging Spark libraries.
• Keen interest in acquiring knowledge in dimensional modeling, data warehousing design and data management best practices.
• Keen interest in developing solid scripting skills in Spark, Python, SQL, Scala, Dataflow etc.
• Knowledge of Data Factory in Azure or Dataflow in GCP is an advantage.

Even Better If You Are
• An entrepreneurial problem solver comfortable in managing risk and ambiguity
• A self-starter with a growth-mindset and proactiveness in working independently to drive toward results

We're committed to creating an inclusive environment for our strong and diverse team. We value diversity and foster a community where everyone can be his or her authentic self.",Bengaluru,True,False,True,True,False,False,False,False,False,True,False,False,False,True,False,False
Cortex Consultants LLC,Principal - Data Engineer,"Roles And Responsibilities

Must be extremely proficient with python core libraries and Object-Oriented Programming concepts.

Lead the development of software applications using Python and related technologies, including Multidimensional,

Multiprocessing, and Web Development Framework.

Develop and maintain Python applications that work with large, multi-dimensional arrays and matrices.

Must have solid understanding of data structures and algorithms.

Must have experience working with REST API.

Maintenance and optimization of existing processes

Defining project and program management process

Prioritizing task within project: Task allocation across resources capable of evaluating resources by their skills and

improvement areas - prioritizes resource development.

Contributes to org-level activities such as sales collaterals, designing proposals, guiding teams through Proof of Concepts

and training.

Experience with any RDBMS preferably SQL Server and must be able to write and understand complex SQL queries.

Design software architecture, write code, and review code written by other developers.

Expertise in requirement gathering, technical design and functional documents.

Troubleshoot and solve complex problems that arise during the software development process.

Good to have knowledge on GCP Data Flow, Big Query, Google cloud storage, cloud function, Cloud composer, etc.

Strong understanding of containerization technologies such as Docker and Kubernetes, with experience in deploying and

managing containerized applications.

Strong experience in writing unit tests, integration tests, and end-to-end tests, with experience in automated testing

frameworks.

Experience in leading conversations with clients and onsite counterparts

Strong analytical and logical skills.

Must be able to comfortably tackle new challenges and learn.

Must have strong verbal and written communication skills.

Expertise And Qualifications

Must Have:

Python

OOPS

Web framework like Flask/ FastAPI/ Django

REST API

SQL (PostgreSQL / MS Server)

Docker

Multiprocessing / Dask / Multithreading

Hands-on experience on AWS/Azure/GCP/ Any Cloud

Git/similar version control tool

Good To Have

GCP PubSub, Cloud Run, Google cloud storage, cloud function, Cloud composer

knowledge of CI/CD pipelines and DevOps

Apache Airflow

Terraform

Good to have experience with Azure Devops or Jira or other tools utilized in an agile development environment.

Microservices

Linux

fsspec / gcsfs

Pytest",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Encora Inc.,Python Data Engineer,"• 4+ years of experience in software engineering
• 4+ years of experience wrangling data in SQL used for BI Reporting & Analytics
• 2+ years of experience in domain modeling / data warehousing modeling
• Fluent in Python or Java
• Experience working with Databricks, Spark (PySpark / Spark SQL), dbt, Jupyter

Notebooks, AWS Glue is preferred",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
HuQuo,Urgent Requirement for Data Engineer - Hive/Hadoop,"We are looking to hire a skilled hadoop developer to help build big data infrastructure and storage software. Your primary responsibility will be to design, build, and maintain hadoop infrastructure. You may also be required to evaluate existing data solutions, write scalable ETLs, develop documentation, and train staff.

To ensure success as a hadoop developer, you should have in-depth knowledge of Hadoop API, high-level programming skills, and the ability to project manage. Ultimately, a top-class Hadoop developer designs and implements bespoke hadoop applications to manage current and future Big Data infrastructures.

Hadoop Developer Responsibilities
• Meeting with the development team to assess the company's big data infrastructure.
• Designing and coding hadoop applications to analyze data collections.
• Creating data processing frameworks.
• Extracting data and isolating data clusters.
• Testing scripts and analyzing results.
• Troubleshooting application bugs.
• Maintaining the security of company data.
• Creating data tracking programs.
• Producing Hadoop development documentation.
• Training staff on application use.

Hadoop Developer Requirements
• Bachelor's degree in software engineering or computer science.
• Previous experience as a hadoop developer or big data engineer.
• Advanced knowledge of the hadoop ecosystem and its components.
• In-depth knowledge of Hive, HBase, and Pig.
• Familiarity with MapReduce and Pig Latin Scripts.
• Knowledge of back-end programming languages including JavaScript, Node.js, and OOAD.
• Familiarity with data loading tools including Squoop and Flume.
• High-level analytical and problem-solving skills.
• Good project management and communication skills.

,

This job is provided by Shine.com",Noida,False,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False
Cortex Consultants LLC,Data Engineer,"Hi,

Welcome to Cortex

Job Title: Data Engineer

Job Description

2+ years of Data Engineer experience in Snowflake (on Azure Cloud Preferred).

Strong knowledge of SQL to build queries and Optimization techniques.

Strong Knowledge of the ETL process using SSIS / ADF (Azure Data Factory) / Matillion

Experience of Python programming is an added advantage.

Location: Chennai

Work type-Hybrid

Immediate joiners

Interested candidates share your resume to

Deepak.g@cortexconsultants.com

Contact No: 9080100600",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Milestone Inc,Staff Data Engineer,"Job Summary

As a Staff Data Engineer, you will be responsible for designing and implementing complex data processing systems and pipelines that enable the analysis and interpretation of large heterogenous datasets. You will collaborate closely with other members of the data team, including data scientists, analysts, and developers, to ensure that our data infrastructure is robust, scalable, and optimized for performance. You will be responsible for running POCs to solve business problems and productionize the concepts. We are looking for a highly motivated individual who is passionate about data engineering and has a deep understanding of data processing, storage, and management with strong collaboration skills.

Responsibilities
• Develop and maintain scalable data pipelines and architectures for our data infrastructure.
• Analyze large and complex data sets to derive insights and provide recommendations.
• Collaborate with cross-functional teams to understand business requirements and develop solutions that meet those requirements.
• Implement and maintain data security and privacy standards in compliance with relevant regulations.
• Develop and maintain web APIs to enable data access and integration with other systems.
• Stay up to date with industry trends and emerging technologies in big data engineering and AI.

Requirements
• At least 5 years of experience in big data engineering with 2 to 3 years of expose in ML/AI area. In total 8 to 12 years of experience.
• Should have experience working with Azure stack like Azure Synapse, Data Lake, Azure analytics services, ADF pipelines, eventing frameworks, functions and webapps and common open-source tech stocks such as Flink or Spark, Kafka, Blob store, Elastic search, etc.
• Excellent Python skills and experience in building scalable web API services. Having .NET skill is an added advantage.
• Should have a very good knowledge of SQL and distributed document DB like Mongo or Casandra or Elasticsearch.
• Knowledge of analytics and data visualization tools like Power BI or Tableau will be an added advantage.
• Experience in Productionizing ML/AI model and hosting them as part of web services is necessary.
• Having experience in training and deploying machine learning and AI is a Plus.
• Strong problem-solving skills and the ability to think creatively.
• Ability to convert concepts to working PoCs and productionizing them.
• Excellent communication skills and ability to work well in a team environment.

Education

Should have a bachelor’s or master’s degree in computer science, Software Engineering, or a related field.

Certifications
• Relevant certifications from Microsoft Azure or other relevant certifications are a plus.
• Relevant certifications in data analytics/AI/ML areas are welcome.

Top Reasons to work for Milestone.
• SPEED: We have the fastest and only SEO-first CMS on the market 
• VOICE: A seat at the executive table and a significant remit of responsibility 
• COMPANY POSITION: Joining a company that is poised to capitalize on the accelerating digital transformation
• GROW: Great place to learn with phenomenal growth opportunity
• IMPACT: Create an impact through the thought leadership you bring to the team
• ETHICS: A principled and values-based culture
• STIMULATION: Exciting and innovative work environment",Bengaluru,True,False,True,False,False,False,False,True,True,True,False,False,False,False,False,False
Notified,Data Engineer,"Exp: 3-6 years; Responsible for serving as the primary escalation point for assigned area of responsibility, and creating and document ETL Processes to feed our Data Warehouse from the different company data sources. Will also analyze and evaluate applications and tasks and determine areas that need improvement within the scope of departmental responsibility MAJOR JOB ACCOUNTABILITIES Data Engineering Functions
• Responsible for serving as the primary escalation point for assigned area of responsibility, and creating and document ETL Processes to feed our Data Warehouse from the different company data sources while adhering to corporate and departmental policies and procedures Develop ETL procedures to maintain our reporting systems Maintenance and support for the difference user of our back-end reporting system. (Snowflake and Informatica Cloud) Create documentation to be entered into a centralized knowledgebase for case resolutions to be utilized by department staff Maintain and update centralized knowledgebase to ensure information is accurate and relevant Analyze and evaluate applications and tasks and determine areas that need improvement within the scope of departmental responsibility Collaborate with internal teams (e.g. Global Billing Support Services, Billing, Development, etc.) to complete design phase and initiate implementation Participate in large-scale projects, completing tasks under the guidance of senior staff (e.g. Sr. Data Engineer, etc.) Serve as subject-matter-expert (SME) for smaller projects requiring knowledge of specific systems or methods Provide assistance to senior staff when conducting complex data analysis to resolve management inquiries Problem Resolution
• Proactively oversee the activities involved in quality resolution of problems related to area of responsibility Respond with a sense of urgency to problems escalated to employee's level Escalate to supervisor any situation outside the employee's control that could adversely impact the services being provided Place the highest priority on providing quality service by ensuring the unique needs of internal/external customers are met Ensure quality resolution and thorough and accurate documentation of issues Provide analysis and feedback to management staff and appropriate departments regarding recurring problems, recommending improvements aimed at reducing future occurrences of problems Participate in creating, administering, and continuously updating procedures for resolution of all related issues Project Assistance
• Participate in the activities associated with a variety of departmental projects ensuring established timelines are met Prepare reports, material and documentation as requested May update project plan Create and maintain organized project files Provide project lead with status reports throughout assigned projects detailing project status and timeline Mental and Physical Requirements - -
• This position will be exposed mainly to an indoor office environment and will be expected to work near or around computers, telephones, and printers
• The nature of the work in this position is sedentary and the incumbent will be sitting most of the time
• Essential physical functions of the job include typing, grasping, pulling hand over hand, and repetitive motions to utilize general computer software/hardware continuously throughout the workday
• Essential mental functions of this position include concentrating on tasks, reading information, and verbal/written communication to others continuously throughout the workday Related Duties as Assigned -
• The job description documents the general nature and level of work but is not intended to be a comprehensive list of all activities, duties, and responsibilities required of job incumbents Consequently, job incumbents may be asked to perform other duties as required Also note, that reasonable accommodations may be made to enable individuals with disabilities to perform the functions outlined above Please contact your local Employee Relations representative to request a review of any such accommodations Minimum Qualifications Applicant for this job will be expected to meet the following minimum qualification Bachelor's degree from an accredited college or university with major course work in computer science, MIS, or a related field is required Equivalent work experience in a similar position may be substituted for education requirements Experience Minimum 3 years of experience with data analysis and migration to include experience in the analysis or design of applications or systems to store and extract data Minimum 1 year of experience with telecommunications billing preferred Minimum 1 year of experience writing detailed test plans for small to medium sized projects preferred Technical Knowledge Minimum 2 years of experience on Relational Database design Minimum 2 years of experience with SQL required Minimum 2 years of experience with Informatica Cloud Data Integration Minimum 2 years of experience working with Data Warehousing and dimensional modeling Minimum 1 year of experience with Python programming language Minimum 1 year experience with requirements analysis and the software development life cycle required Minimum 1 year experience with Microsoft OneNote and Project preferred Experience working with Snowflake is a plus Other Intermediate knowledge of Word, Excel, and PowerPoint required",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
NatWest Group,"Data Engineer, AVP","Our people work differently depending on their jobs and needs. From hybrid working to flexible hours , we have plenty of options that help our people to thrive.

This role is based in India and as such all normal working days must be carried out in India.

Join us as a Data Engineer, AVP
• We're seeking a talented Data Engineer to build effortless, digital first customer experiences and simplify the bank through developing innovative data driven solutions
• You’ll inspire the bank to be commercially successful through insights, while at the same time keeping our customers’ and the bank's data safe and secure
• This is a chance to hone your expert programming and data engineering skills in a fast paced and innovative environment

What you'll do

As a Data Engineer, you’ll partner with technology and architecture teams to build your data knowledge and data solutions that deliver value for our customers. Working closely with universal analysts, platform engineers and data scientists, you’ll carry out data engineering tasks to build a scalable data architecture, including data extractions and data transformation.

As well as this, you’ll be:
• Loading data into data platforms
• Building automated data engineering pipelines
• Delivering streaming data ingestion and transformation solutions
• Participating in the data engineering community to deliver opportunities to support the bank's strategic direction
• Developing a clear understanding of data platform cost levers to build cost effective and strategic solutions

The skills you'll need

You’ll have at least four years' experience in Java, J2ee and web services. You'll be an experienced programmer and data engineer, with a BSc qualification or equivalent in Computer Science or Software Engineering.

Along with this, you’ll have a proven track record in extracting value and features from large scale data, and a developed understanding of data usage and dependencies with wider teams and the end customer.

You’ll also demonstrate:
• Expertise knowledge of RESTful API, microservices and event driven architecture such as Lambda and Spring Frameworks
• Exposure to CI/CD and messaging layers such as Kafka or any other
• Good knowledge of container technologies such as Docker and Kubernetes
• Experience with MongDB architecture, patterns and a good understanding of NoSQL technologies such as MongoDB
• Experience of ETL technical design, automated data quality testing, QA and documentation, data warehousing and data modelling capabilities

Apply for this job",Bengaluru,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer: Data Integration,"633913BR

Introduction

As Senior Talend Developer, you will serve as a liaison among business partners, technical resources, and project stake holders to identify, articulate and facilitate business process and systems changes related to document digitization and document- driven business processes.

Your Role and Responsibilities

As Data engineer, you will develop and move data from the operational and external environments to the business intelligence environment using Ab Initio software. Skills include designing and developing extract, transform and load (ETL) processes.

Responsibilities
• Coordinate with multiple technical teams to ensure apt integration of functions to identify and define necessary system enhancements to deploy new products and process improvements
• Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
• Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
• Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
• Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
• Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions,

Required Technical and Professional Expertise
• Minimum 4+ years of experience in ETL Datastage development
• Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting
• Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers

Preferred Technical And Professional Expertise
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications,

About Business Unit

IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBM

In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

When applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False
ShyftLabs,Senior Data Engineer,"Position Overview:

We are looking for an experienced and versatile Data Engineer to join our dynamic and fast-growing team. We are looking for an Experienced person who has hands-on with data modeling, data warehousing, and building ETL pipelines. If you are passionate about data and solving complex problems, this role could perfectly fit you!

ShyftLabs is a growing data product company that was founded in early 2020 and works primarily with Fortune 500 companies. We deliver digital solutions built to help accelerate the growth of businesses in various industries, by focusing on creating value through innovation.

Job Description:

Design, implement, and operate stable, scalable, low-cost solutions to flow data from production systems into the data lake and into end-user-facing applications.

Design automated processes for in-depth analysis of databases.

Design automated data control processes.

Collaborate with the software development team to build and test the designed solutions.

Learn, publish, analyze, and improve management information dashboards, operational business metrics decks, and key performance indicators.

Improve tools, and processes, scale existing solutions, and create new solutions as required based on stakeholder needs.

Provide in-depth analysis to management with the support of accounting, finance, and transportation teams.

Perform monthly variance analysis and identify risks & opportunities.

Building data products incrementally and integrating and managing datasets from multiple sources

Qualifications:

BS in Computer Science or related fields

4+ years of experience as a Data Engineer

3+ years of Experience in SQL and Snowflake

Good hands-on experience in Airflow and DBT

Experience with AWS Tools and Technologies (Redshift, S3, EC2, Glue)

Expertise with Data modeling skills, Advanced SQL with Oracle, MySQL, and Columnar Databases

Strong customer focus, ownership, urgency, and drive.

Excellent communication skills and the ability to work well in a team.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Bread Financial,Senior Data Engineer - AWS,"Every career journey is personal. That's why we empower you with the tools and support to create your own success story.

Be challenged. Be heard. Be valued. Be you ... be here.

Job Summary

The Senior Data Engineer will lead the evolution of our data infrastructure and data analytics capabilities. This role is responsible for building high quality, architecturally sound systems that will transform how we can take advantage of our data, including data infrastructure and pipelines to support analytics and product, as well as our various business units. The Senior Data Engineer will own a variety of areas in our data stack and lead us forward. This role will have a major impact on the direction of the organization and be a part of building infrastructure from the ground up.

Job Description

Essential Job Functions:

Infrastructure Design - Design, develop and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources. Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines. Evaluate new technologies for continuous improvements in Data Engineering. Collaborate closely with the product team to build out new data features. Work with the data analysts and scientists to implement descriptive, forecasting and predictive algorithms and models using latest technologies. Make technology decisions for our data infrastructure.

Quality and Testing - Consistently write production-ready code that is easily testable, easily understood by other developers, and accounts for edge cases and errors. Understand when it is appropriate to leave comments, but biases towards self-documenting code.

Debugging - Use systematic debugging to diagnose all issues located to a single service. Use systematic debugging to diagnose cross service issues, sometimes with help from more senior engineers.

Observability - Aware of the organizations monitoring philosophy. Tune and change the monitoring on the team accordingly. Aware of the operational data for their team’s domain and uses it as a basis for suggesting stability and performance improvements.

Security - Approach all engineering work with a security lens. Actively look for security vulnerabilities both in the code and when providing peer reviews.

Prioritization - Ensure tasks are prioritized correctly, and that dependencies are noted.

Managing Ambiguity - Manage risk, change, and uncertainty within scope of work effectively. Make decisions and act responsibly without having the total picture during routine business and when in high-pressure situations.

Reliability, Delivery and Accountability - Ensure commitments are realistic, manage priorities with a sense of urgency, and deliver upon them accordingly. Anticipate and communicate blockers, delays, and cost ballooning for work before they require escalation. Ensure expectations within team are clarified between all parties involved.

Delivering Feedback - Deliver praise and constructive feedback to the team, peers, and manager in a useful manner. Deliver feedback to their team's business stakeholders when opportunities arise.

Effective Communication - Communicate effectively, clearly and concisely in written and verbal form, both technical and non-technical subjects, and in an audience-oriented way. Actively listens to others and ensures they are understood. Pays attention to nonverbal communication.

Reports To

Lead / Manager

Direct Reports

None

Working Conditions/ Physical Requirements
• Hybrid office environment.
• Some travel may be required.
• Shift Timing: 1pm – 10pm

Minimum Qualifications
• Bachelor’s degree in computer science or engineering; or equivalent technical training and experience.
• 5+ years of experience building and deploying large-scale data processing pipelines.
• 5+ years of experience with latest databases technologies in the industry including Data Warehousing/ETL and Relational Databases (PostgreSQL, Snowflake, Databricks Delta Lake, etc.).
• 5+ years of experience with AWS.
• 5+ years of experience with Spark with Python/Scala.
• 5+ years of experience with Workflow & pipeline systems with Airflow.
• 5+ years of experience with SQL with Redshift, PostgreSQL and Columnar Databases.
• Flexible to work on Maintenance & Support activities with 20% bandwidth.
• Strong data modeling skills, including data warehouse schema design.
• Experience on GIT & CI/CD.

Preferred Qualifications
• Experience with message-based, loosely coupled architectures (e.g. Kafka).
• Experience developing systems intended for cloud deployments (AWS, EKS, lambda’s, etc).
• DevOps experience.
• Experience with production support and an ability to drive issues with basic requirements.
• Experience with analytics platforms like Looker or Tableau.

Knowledge, Skills And Abilities
• AWS
• Spark with Python/Scala
• Tableau
• EKS
• Lambda’s
• Kafka
• GIT
• CI/CD
• Snowflake
• Databricks
• Delta Lake

Education Requirements

Bachelor’s Degree (Required)

Certifications

Work Experience

Five or more years

Skills

AWS Data Pipeline, AWS Organizations, Bandwidth Optimization, Columnar Databases, Data ETL, Data Modeling Tools, Data Warehouse Design, Data Warehousing (DW), GitLab CI/CD, PostgreSQL, Relational Databases, Snowflake Schema, Spark SQL, Structured Query Language (SQL)

About Bread Financial

At Bread Financial, you’ll have the opportunity to grow your career, give back to your community, and be a part of our award-winning culture. We’ve been consistently recognized as a best place to work in many markets and we’re proud to promote an environment where you feel appreciated, accepted, valued, and fulfilled—both personally and professionally. Bread Financial supports the overall wellness of our associates with a diverse suite of benefits and offers boundless opportunities for career development and non-traditional career progression.

Bread Financial is a tech-forward financial services company providing simple, personalized payment, lending and saving solutions. The company creates opportunities for its customers and partners through digitally enabled choices that offer ease, empowerment, financial flexibility and exceptional customer experiences. Driven by a digital-first approach, data insights and white-label technology, Bread Financial delivers growth for its partners through a comprehensive product suite, including private label and co-brand credit cards, installment lending, and buy now, pay later (BNPL). Bread Financial also offers direct-to-consumer solutions that give customers more access, choice and freedom through its branded Bread Cashback American Express® Credit Card and Bread Savings products.

Headquartered in Columbus, Ohio, Bread Financial is powered by its 7,500+ global associates and is committed to sustainable business practices.
• All job offers are contingent upon successful completion of credit and background checks.
• Bread Financial is an Equal Opportunity Employer.

Job Family

Data and Analytics

Job Type

Regular",,True,False,True,False,False,False,False,False,False,True,False,False,True,False,True,True
Codalyze Technologies,Data Engineer,"Job Overview

Your mission is to help lead team towards creating solutions that improve the way our business is run. Your knowledge of design, development, coding, testing and application programming will help your team raise their game, meeting your standards, as well as satisfying both business and functional requirements. Your expertise in various technology domains will be counted on to set strategic direction and solve complex and mission critical problems, internally and externally. Your quest to embracing leading-edge technologies and methodologies inspires your team to follow suit.

Responsibilities And Duties
• As a Data Engineer you will be responsible for the development of data pipelines for numerous applications handling all kinds of data like structured, semi-structured &

unstructured. Having big data knowledge specially in Spark & Hive is highly preferred.
• Work in team and provide proactive technical oversight, advice development teams fostering re-use, design for scale, stability, and operational efficiency of data/analytical solutions

Education Level
• Bachelor's degree in Computer Science or equivalent

Experience
• Minimum 5+ years relevant experience working on production grade projects experience in hands on, end to end software development
• Expertise in application, data and infrastructure architecture disciplines
• Expert designing data integrations using ETL and other data integration patterns
• Advanced knowledge of architecture, design and business processes

Proficiency In
• Modern programming languages like Java, Python, Scala
• Big Data technologies Hadoop, Spark, HIVE, Kafka
• Writing decently optimized SQL queries
• Orchestration and deployment tools like Airflow & Jenkins for CI/CD (Optional)
• Responsible for design and development of integration solutions with Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions
• Knowledge of system development lifecycle methodologies, such as waterfall and AGILE.
• An understanding of data architecture and modeling practices and concepts including entity-relationship diagrams, normalization, abstraction, denormalization, dimensional

modeling, and Meta data modeling practices.
• Experience generating physical data models and the associated DDL from logical data models.
• Experience developing data models for operational, transactional, and operational reporting, including the development of or interfacing with data analysis, data mapping,

and data rationalization artifacts.
• Experience enforcing data modeling standards and procedures.
• Knowledge of web technologies, application programming languages, OLTP/OLAP technologies, data strategy disciplines, relational databases, data warehouse development and Big Data solutions.
• Ability to work collaboratively in teams and develop meaningful relationships to achieve common goals

Skills

Must Know :
• Core big-data concepts
• Spark - PySpark/Scala
• Data integration tool like Pentaho, Nifi, SSIS, etc (at least 1)
• Handling of various file formats
• Cloud platform - AWS/Azure/GCP
• Orchestration tool - Airflow Skills:- Hadoop, Scala, Spark, Amazon Web Services (AWS), Java, Python, Apache Hive and Big Data",Mumbai,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
Citi,Sr Oracle Data Engineer,"Job Id: 23638869

About Citi:

Citi, the leading global bank, has approximately 200 million customer accounts and does business in more than 160 countries and jurisdictions. Citi provides consumers, corporations, governments, and institutions with a broad range of financial products and services, including consumer banking and credit, corporate and investment banking, securities brokerage, transaction services, and wealth management.

As a bank with a brain and a soul, Citi creates economic value that is systemically responsible and in our clients’ best interests. As a financial institution that touches every region of the world and every sector that shapes your daily life, our Enterprise Operations & Technology teams are charged with a mission that rivals any large tech company. Our technology solutions are the foundations of everything we do from keeping the bank safe, managing global resources, and providing the technical tools our workers need to be successful to designing our digital architecture and ensuring our platforms provide a first-class customer experience. We reimagine client and partner experiences to deliver excellence through secure, reliable, and efficient services.

Our commitment to diversity includes a workforce that represents the clients we serve from all walks of life, backgrounds, and origins. We foster an environment where the best people want to work. We value and demand respect for others, promote individuals based on merit, and ensure opportunities for personal development are widely available to all. Ideal candidates are innovators with well-rounded backgrounds who bring their authentic selves to work and complement our culture of delivering results with pride. If you are a problem solver who seeks passion in your work, come join us. We’ll enable growth and progress together.

About Our Team:

Citi Technology Infrastructure (CTI) provides the critical technical foundation for Citi’s operations and is responsible for delivering reliable IT solutions, scalable infrastructure services, and secure capabilities while creating a trusted customer experience and enabling Citi’s workforce to be the best for our clients. Making the bank simpler, greener, and better connected while powering it with trusted, well-secured data, and automating policy enforcement through code are all at the heart of our refreshed global strategy. Data Quality, Simplification, Environmental Stability, Automation, and Service Excellence are the key pillars and priorities on our strategic journey.

In CTI, we are focused on delivering the best for our clients, and we know that to do this we need a talented team with diverse experiences, backgrounds and skills.

The Senior Engineer for Tech Reference Inventory is a senior level position responsible for leading a variety of engineering activities including the design, acquisition and deployment of hardware, software and network infrastructure in coordination with the Technology team. The overall objective of this role is to lead efforts to ensure quality standards are being created and met for Citi’s technology inventory within our Big Data and RDBMS Warehouse systems. Responsibilities include analyzing logical and physical data models & interpreting them into data warehousing structures (Kimble and Snowflake are the standards we hold to). The Senior Engineer will accomplish this by ensuring the integrity of data flowing through environments, as well as leading work efforts within the Engineering Pod and being responsible for the cross functional teams of Big Data, Data Warehouse, and Analytics & Visualization team to ensure the delivery of Pristine, Automated, Accessible, Accurate, Covered and Complete, Timely and Traceable, technology inventory data. The role will also be responsible for managing the engineering of outbound data requests from RDBMS Warehouse, as well as the monitoring of data flow utilizing analytics and reporting tools available. The overall objective is to use infrastructure technology knowledge and identified policies to process data, resolve issues and execute enhancement projects.

Responsibilities:
• Serve as a technology subject matter expert for internal and external stakeholders and provide direction for the creation, implementation, and lifecycle management of the Data Reference Inventory Fabric for Technology (DRIFT)
• Work within current framework of projects and domains created today and identify enhancements and the creation of new domain roadmaps
• Ensure that all integration of functions meet business goals and prepare engineer solutions for architecture design forums for review and approval
• Define necessary system enhancements to deploy new products and enhancements current framework with well designed, architected, and approved processes.
• Recommend product/process customization for additions domain data integration as well as improve relationships among current and inflight domains.
• Identify problem causality, business impact and root causes for production support staff to easily provide 24x7 support with limited engagement with development teams.
• Exhibit knowledge of how own specialty area contributes to the business and apply knowledge of competitors, products, and services
• Impact the engineering function by influencing decisions through advice, counsel or facilitating services
• Appropriately assess risk when business decisions are made, demonstrating consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.
• Integrate Big Data and Warehouse teams within the Engineering pod to create well documented and version-controlled code, ensuring that teams continue to operate in a CI/CD framework by improving interactions and polices to ensure success.

Qualifications:
• 6-10 years of relevant experience in an Senior Engineering role
• 6-10 years of experience with Big Data (Hadoop), Data Warehousing (Oracle / MS SQL), Data Modeling (Erwin / Magic Draw)
• Proven track record of managing multiple streams of architecture and engineering groups, driving the success of data ingestion, processing, and production
• Experience with code repos such as Git or Bitbucket
• Can operate in an Agile team with experience in Scrum
• Project Management experience to drive engineering and architecture efforts within our data platforms
• Excellent written and verbal communication skills required to lead engineering design and implementations
• Comprehensive knowledge of design metrics, analytics tools, benchmarking activities and related reporting to identify best practices
• Ability to work in a matrix environment and partner with virtual teams across our global infrastructure and support groups
• Ability to drive individuals in the implementation of designs, multi-task, and take ownership of various parts of initiatives
• Ability to work under pressure and manage to tight deadlines or unexpected changes in expectations or requirements
• Proven track record of operational process changes and improvement

Education:
• Bachelor’s degree/University degree or equivalent experience

-------------------------------------------------

Job Family Group:

Technology

-------------------------------------------------

Job Family:

Systems & Engineering

------------------------------------------------------

Time Type:

Full time

------------------------------------------------------

Citi is an equal opportunity and affirmative action employer.

Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review Accessibility at Citi.

View the ""EEO is the Law"" poster. View the EEO is the Law Supplement.

View the EEO Policy Statement.

View the Pay Transparency Posting",Chennai,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
NTT DATA Services,Data Developer / Architect (Data Engineer),"Data Developer/Architect

We are looking for a savvy Data Developer to join our growing team. The successful candidate for this role will design and implement data models and segmentation techniques in support of fellow application developers. In addition to optimizing data flow and collection for cross functional teams, this role will have the opportunity to understand, optimize and potentially expand our data and data pipeline architecture. The ideal candidate is an experienced data pipeline builder and data wrangler. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and applications.

Responsibilities
• Design and implement data models in existing data stores.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Product Management, Development and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product.
• Work with data and analytics experts to strive for greater functionality in our data systems.

Qualifications
• We are looking for a candidate with 5+ years of experience in a Data Developer role.
• Good listening, explanation and writing skills in English.
• Advanced working SQL knowledge and demonstrable experience working with both relational and NoSQL databases, preferably MongoDB among them.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of manipulating, processing and extracting value from large disconnected datasets.
• Working knowledge of message queuing, stream processing and highly scalable ‘big data’ data stores.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fusion Plus Solutions Inc,GCP Data Engineer,"• Experience working with large datasets and solving difficult analytical problems is a primary task of a Google data engineer.
• Conducting end-to-end analyses, including data collection, processing, and analysis.
• Building prototype analysis pipelines to generate insights.
• Developing comprehensive abilities for Google data structures and matrix for upcoming product development and sales activities is also a crucial task for Google data engineers.
• Designing, building, operationalizing, securing, and monitoring data processing systems on Google Cloud Platform.
• Deploying, leveraging, and continually training and improving existing machine learning models.
• Identifying, designing, and implementing internal process movements
• Automating manual processes to enhance delivery.
• Meeting business objectives in collaboration with data scientist teams and key stakeholders.
• Creating reliable pipelines after combining data sources.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications
• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Peoplefy,Sr.Data Engineer,"Role: Sr.Data Engineer

Years of experience: 8 to 16Years

Work mode: Hybrid

Location: Pune, Kharadi

Prerequisites :
• Practical knowledge on Microsoft Azure infrastructure components
• Experience with data processing platforms: Databricks, Data Factory
• Development experience on Python and Spark (pyspark/snowpark)
• Practical knowledge about data crunching techniques
• Database experience on Snowflake, PostgreSQL, MongoDB, DeltaLake
• Used to work with GIT as version control system
• Basic knowledge with any GIS usage (ArcGIS, kepler.gl, Carto.com, …)

Primary skills:

· Python,SQL,Pyspark

· Pandas

· Snowflake

· Azure/GCP

Interested candidates can share their resume to Saniya.ga@peoplefy.com",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
CME Group,Senior Cloud Data Engineer,"Description

2023 is an exciting year for technology professionals to join CME Group following the recent $1bn equity investment from Google. CME Group has also agreed a long-term partnership with Google to migrate our technology infrastructure to Google Cloud in a visionary partnership to transform the global derivatives markets through technology.”

Overview Of Role

CME Group is seeking a Sr Data Engineer to join our Data Engineering team. The successful candidate will have excellent technical and problem-solving skills, a passion for data, and experience in GCP data services.

Responsibilities

Data Ingestion, Storage, Processing and Migration
• Acquire, cleanse, and ingest structured and unstructured data on the cloud platforms (in batch or real time) from internal and external data sources
• Combine data from disparate sources in to a single, unified, authoritative view of data (e.g., Data Lake)
• Create, maintain and provide test data to support fully automated testing
• Enable and support data movement from one system / service to another system / service.

Ensure Data Resiliency And Integrity
• Define and implement processes and procedures to ensure resiliency of data, software and platforms
• Design, build, operationalize, secure, and monitor data processing systems with emphasis on security, compliance; scalability, reliability and portability

Skills & Software Requirements
• GCP data services (BigQuery; Dataflow; Data Fusion; Dataproc; Cloud Composer; Pub/Sub; Google Cloud Storage)
• Programming languages e.g., Python, Java, SQL

CME Group: Where Futures Are Made

CME Group (www.cmegroup.com) is the world's leading derivatives marketplace. But who we are goes deeper than that. Here, you can impact markets worldwide. Transform industries. And build a career shaping tomorrow. We invest in your success and you own it, all while working alongside a team of leading experts who inspire you in ways big and small. Problem solvers, difference makers, trailblazers. Those are our people. And we're looking for more.",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,True,False,False
P99SOFT Pvt Ltd,Data Engineer,"P99SOFT is hiring DATA ENGINEERS. please join the network and reach us if looking for any job opportunities.

Requirements:

Qual : B.Tech/M.Tech/BCA/MCA/BE/B.Sc

Skills required : Python,Snowflake,ETL,AWS,MySql

Exp : 3+ years of exp is required.

Job Location : Hyderabad

Mode : Hybrid

IMMEDIATE JOINERS PREFERRED.",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Holcim Global Digital Hub,AWS Data Engineer - Navi Mumbai (Hybrid),"About Holcim

Holcim is the global leader in building materials and solutions and active in four business segments: Cement, Aggregates, Ready-Mix Concrete and Solutions & Products. It is our ambition to lead the industry in reducing carbon emissions and accelerating the transition towards low-carbon construction. With the strongest R&D organization in the industry and by being at the forefront of innovation in building materials we seek to constantly introduce and promote high-quality and sustainable building materials and solutions to our customers worldwide - whether they are building individual homes or major infrastructure projects. Holcim employs over 70,000 employees in over 70 countries and has a portfolio that is equally balanced between developing and mature markets.

About The Role

The Data Engineer will play an important role in enabling business for Data Driven Operations and Decision making in Agile and Product-centric IT environment.

Education / Qualification
• BE / B. Tech from IIT or Tier I / II colleges
• Certification in Big Data Technologies
• Certification in Cloud Platforms AWS or GCP

Experience
• Total Experience of 3-10 years
• Hands on experience in python coding is must.
• Experience in data engineering which includes laudatory account
• Hands-on experience in Big Data cloud platforms like AWS(redshift, Glue, Lambda), Data Lakes, and Data Warehouses, Data Integration, data pipeline.
• Experience in SQL, writing code in spark engine using python.
• Experience in data pipeline and workflow management tools ( such as Azkaban, Luigi, Airflow etc.)

Key Personal Attributes
• Business focused, Customer & Service minded
• Strong Consultative and Management skills
• Good Communication and Interpersonal skills",Navi Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
LTIMindtree,Data Engineer,"Experience:- 5 years to 9 years

Location:- Coimbatore/ Bangalore/Mumbai/Pune/Hyderabad/Chennai/Delhi/Kolkata/Odisha

Primary Skills- ADF, Data bricks, PySpark

Secondary Skill- Azure data services

Job Description:-

- Experience on the Azure Data Bricks, Azure Data platform specifically Azure Data Lake , and Spark SQL Well versed in Data Architecture - Azure Data Bricks ,Data Lake, Should be familiar with Agile ways of working Should have hands on experience in streaming analytics and data integration using Azure streaming. Integrating different Source systems like Salesforce, Google Analyze

JOB SUMMARY

Software Engineers are skilled in solving various technical problems by effectively leveraging the collaborative platforms in an ecosystem. This opportunity allows one to independently implement diverse and complex engineering tasks through analytical thinking and problem solving. Software Engineers will get to exhibit a deep understanding of the end-to-end software life cycle and show high levels of collaboration, customer centricity and effective communication to understand requirements, analyze, provide support, and deliver technical results and drive excellence.",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Citius Communications Pvt. Ltd.,Data Engineer - Loglogic Engineer,"Position: Data Engineer/ Loglogic Engineer

Skill:
• 4+ years of experience on Log monitoring tools like LogLogic, Splunk, etc.
• Experience with automating monitoring workflows through Loglogic stack.
• Excellent Syslog Administration knowledge.
• Approximately 4+ years of experience in Loglogic and Syslog
• Advanced experience in Troubleshooting Loglogic, Infrastructure, Dashboards/Search issues
• Deep knowledge in Loglogic or Spotfire Dashboard development
• looking for Developer - TIBCO

Immediate Joiner required

Location - Pune",Pune,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
winwebhire,Senior Data Engineer,"About the Role:

We require candidate with a strong designing sense in product & specialisation in Hadoop and Spark technologies.

Requirements:

3-5 years of exp in Big Data tech

Experience with any scripting language such as Python, Bash etc.

Experience in SQL, and in developing streaming applications e.g. Spark Streaming with AWS and cloud technologies such as S3",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Syren Technologies Private Limited,Syren Technologies - Lead Data Engineer/Data Engineer,"We do have a very immediate Requirement for Lead Data Engineer/Data Engineer with us.

Experience : 8-20 Years

Location : Hyderabad (Remote for 6 months initially)

Notice Period : Immediate to 15 Days MAX please.

Please Find Below JOD DescriptionSyren is a specialist supply chain, data and software engineering with PRODUCT solutions company.
• Azure Data,
• Pyspark
• SQL is mandatory.
• Good Programming skills in Pyspark/python.
• Strong SQL/PLSQL knowledge, Datamodelling
• Gather and elicit requirements from business users, and translate them into technical
• requirements
• Understand source systems and perform data profiling
• Document source table details and appropriate transformation rules that needs to be applied on the target tables/reports
• Work closely with the business, IT teams, project managers, Architects, developers, and Modelers.
• Experience in Translating Business requirements into Code and deliver the required result
• Experience working with Azure Cloud Data Platform solutions (Azure data factory, Azure Data Lake, Azure Synopsis, Azure Databricks)
• Good communication skill and able to work on multiple projects.

(ref:hirist.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Mobile Programming LLC,Data Engineer,"Day-to-day Activities

Develop complex queries, pipelines and software programs to solve analytics and data mining problems

Interact with other data scientists, product managers, and engineers to understand business problems, technical requirements to deliver predictive and smart data solutions

Prototype new applications or data systems

Lead data investigations to troubleshoot data issues that arise along the data pipelines

Collaborate with different product owners to incorporate data science solutions

Maintain and improve data science platform

Must Have

BS/MS/PhD in Computer Science, Electrical Engineering or related disciplines

Strong fundamentals: data structures, algorithms, database

5+ years of software industry experience with 2+ years in analytics, data mining, and/or data warehouse

Fluency with Python

Experience developing web services using REST approaches.

Proficiency with SQL/Unix/Shell

Experience in DevOps (CI/CD, Docker, Kubernetes)

Self-driven, challenge-loving, detail oriented, teamwork spirit, excellent communication skills, ability to multi-task and manage expectations

Preferred

Industry experience with big data processing technologies such as Spark and Kafka

Experience with machine learning algorithms and/or R a plus

Experience in Java/Scala a plus

Experience with any MPP analytics engines like Vertica

Experience with data integration tools like Pentaho/SAP Analytics Cloud Skills:- Python, SQL, Linux/Unix, Shell Scripting, DevOps, CI/CD, Docker, Kubernetes, Java, Scala, Data integration, Spark, Google Cloud Platform (GCP), Kafka and Pentaho",Pune,True,True,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Compere HR Solutions,Data Engineer,"Data Engineer
• Key responsibilities and expected “output”: Expertise in developing HANA Modelling artifacts with strong PL/SQL skills.
• Required tech stack and other related details: PL/SQL, SAP HANA, Performance Optimization(HANA), SAP BO
• Good to have tech stack and other related details: Hadoop, Hive,HQL,Spark, PySpark, HDFS ,PostgreSql ,Informatica",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Peopletech,Peopletech - Data Engineer,"Data Engineer with below Skills

Skills : SQL, Advanced SQL, Joins, Performance Tuning, ETL.

Exp : 6+ Years

Notice : Max 45 Days.

Work Location : Hyderabad.

Job Description
• Having strong written and verbal communication, and the ability to communicate with end users in non-technical terms, is vital to your long-term success.
• The ideal candidate will have experience working with large datasets, distributed computing technologies and service-oriented architecture.
• The candidate should relish working with large volumes of data, and enjoys the challenge of highly complex technical contexts.
• He/she should be an expert with data modelling, ETL design and business intelligence tools and has hand-on knowledge on columnar databases.
• He/she is a self-starter, comfortable with ambiguity, able to think big and enjoys working in a fast-paced team.

Responsibilities
• Design, build and own all the components of a high-volume data warehouse end to end.
• Build efficient data models using industry best practices and metadata for ad hoc and pre-built reporting
• Provide wing-to-wing data engineering support for project lifecycle execution (design, execution and risk assessment)
• Interface with business customers, gathering requirements and delivering complete data reporting solutions owning the design, development, and maintenance of ongoing metrics, reports, dashboards, etc. to drive key business decisions
• Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
• Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources
• Own the functional and non-functional scaling of software systems in your ownership area.
• Implement big data solutions for distributed computing.
• Willing to learn and develop strong skillset in AWS technologies.

,

This job is provided by Shine.com",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Rudhra Info Solutions,Senior Data Engineer,"Primary Required Technical Skill Set : Python, Spark, SQL, AWS (Must have)

Secondary Technical Skill set with below combination for different positions along with primary skill set:-
• AWS must not Glue but S3 & Redshift, Data bricks (Good to have)
• AWS with Lambda, Glue, EMR and RedShift
• AWS DE with Scala
• AWS DE with snowflake
• Lead DE - Java, AWS, Python, NodeJS
• GCP Python

Desired Competencies

Good hands on experience in Python programming

Data Engineering experience using AWS core services

Responsibility Will be accountable for build/test complex data pipelines (batch and near real time)

Expectation Readable documentation of all the components being develop

Experience in writing SQLs and stored procedures

Skills and Qualifications:-

Bachelor degree in related subject area or equivalent experience

Experience Detailing Technical Requirements.

Should have 5 to 8 years of experience in java back-end development.

Proficient at multitasking and proactive in work responsibilities.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
MLOPS Solutions Private Limited,Data Engineer,"Hi All, Looking to connect Data Engineer from 5 to 8 yrs exp for Bangalore, Pune, Chennai location with relevant skills and experience in Pyspark, Scala (Flow Master), Kafka (API Master), Apache Spark, Dremio, Spark SQL, jupyter, python, Hue, Hive, Tableau, Power BI. Banking Domain would be an added advantage. If anyone with matching skills reach me to vijayadurga@mlopssol.com, appreciate references if any ( Immediate joiners / Serving Notice Period.)",Bengaluru,True,False,True,False,True,False,False,False,True,True,False,True,False,False,False,False
Tech Mahindra,Sr. Azure Data Engineer (max 30days joiners),"Hi,

We are hiring Azure Data Engineer

Experience:- 3.5yrs relevant in DE

Location:- Hyderabad and Bangalore

Skills:- Azure, ADF, ADL, ADB, SQL, Pyspark, Python, PLSQL, Data Warehouse, API Development.

If interested kindly share your updated resume on ms00883967@techmahindra.com

Below is the Job Description:

· Designed Azure ADF pipelines to move data from Open hubs to Azure Data Lake Gen2 and then to Azure Data Warehouse.
• Developing notebooks and applying transformation using spark SQL in Azure data bricks.
• Testing the notebook and validating the scripts using SQL.
• Implemented activities Copy activity, Execute Pipeline, Get Meta data, If Condition, Lookup, Set Variable, Filter, For Each pipeline Activities for On-cloud ETL processing.
• Load the files to stage tables and then to data warehouse tables in dedicated SQL pool in synapse. Writing views, stored procedures for transformations.
• Converting views to external tables for direct query for faster retrieval.
• Preparing views data, data models for PBI reports Designed dashboards visuals with slicers, filters, drill through options.
• Assure that data is cleansed, mapped, transformed, and otherwise optimised for storage and use according to business and technical requirements Develop and maintain innovative Azure solutions Solution design using Microsoft Azure services and other tools The ability to automate tasks and deploy production standard code (with unit testing, continuous integration, versioning etc.)
• Load transformed data into storage and reporting structures in destinations including data warehouse, high speed indexes, real-time reporting systems and analytics applications Build data pipelines to collectively bring together data Other responsibilities include extracting data, troubleshooting and maintaining the data warehouse",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Innova Solutions,Bigdata Developer / Data Engineer @ Innova Solution,"Position Overview

Come work as a Senior - Big Data Engineer at a growing company that offers great benefits with opportunities to advance and learn alongside accomplished leaders. Innova Solutions is a global information technology company combining a global reach with a local touch. Headquartered in Santa Clara, California, Innova employs 17000 technology professionals worldwide, with field offices in New York, Chennai, Bangalore, Hyderabad, Pune, and Taipei. From Cloud Transformation to Data Services to Managed IT Operations, Innova provides a broad array of proven, tested, cost-effective and enterprise-scale technologies and services that leverage the latest technology and delivery models to deliver high value in the cloud, in the data center, and across complex interconnected environments. What you will be doing?

We are looking for a Spark developer who knows how to fully exploit the potential of our Spark cluster. You will clean, transform, and analyze vast amounts of raw data from various systems using Spark to provide ready-to-use data to our feature developers and business analysts. This involves both ad-hoc requests as well as data pipelines that are embedded in our production

Requirements

The candidate should be well-versed in the Scala programming language Should have experience in Spark Architecture and Spark Internals Exp in AWS is preferable Should have experience in the full life cycle of at least one big data application

Interested candidates, share your updated profile

Current CTC

Expected CTC

Notice Period or LWD

Present & Preferred Location

Skills:- undefined and undefined",Hyderabad,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
GrenoSearch India Pvt. Ltd.,Director – Data Engineering,"Associate Director Data Engineering

Position: - Associate Director Data Engineering

Experience: Must have 12-14 years of experience with a minimum of 9 years of experience in building BigData applications at scale.

About Role

We are looking for experienced Data engineers with excellent problem-solving skills to develop machine learning-powered Data Products design to enhance customer experiences.

About The Team

Data Platform Team is a horizontal function to support various LOBs and works heavily on streaming datasets that power personalized experiences for every customer from recommendations to in-location engagement.

There Are Two Key Responsibilities Of Data Engineering Team

One to develop the platform for data capture, storage, processing, serving and querying.

Second is to develop data products starting from;
• personalization & recommendation platform
• customer segmentation & Intelligence
• data insights engine for persuasion and
• the customer engagement platform to help marketers craft contextual and personalized campaigns over multi-channel communications to users

We developed Feature Store, an internal unified data analytics platform that helps us to build reliable data pipelines, simplify featurization and accelerate model training. This enabled us to enjoy actionable insights into what customers want, at scale, and to drive richer, personalized online experiences.

Role Responsibilities
• Solution Architecture in Big Data and Advanced Analytics domains
• Build 'Reference Architecture' for Data / Big Data technology domain
• Define and own end-to-end Architecture from definition phase to go-live phase for large and complex engagements
• Build scalable architectures for data storage, transformation and analysis
• Designing platforms as consumable data services across the organization using Big Data tech stack
• Think of solutions as scalable generic reusable organization-wide platforms
• Architect and build near real-time (low latency) platforms for segmentation, personalized recommendations, reporting etc.
• Build and execute data mining and modeling activities using agile development techniques
• Leading big data projects successfully from scratch to production
• Appreciate and understand the cloud delivery model and how that affects application solutions both delivery and deployment
• Solve problems in robust and creative ways and demonstrate solid verbal, interpersonal and written communication skills
• Work in an environment and get clarity about unknowns both technically and functionally

Technology Experience
• Extensive experience working with large data sets with hands-on technology skills to design and build robust data architecture
• Extensive experience in data modeling and database design
• At least 5+ years of hands-on experience in Spark/BigData Tech stack
• Stream processing engines Spark Structured Streaming/Flink
• Analytical processing on Big Data using Spark
• At least 9+ years of experience in Java/Scala
• Hands-on administration, configuration management, monitoring, and performance tuning of Spark workloads, Distributed platforms, and JVM based systems
• At least 2+ years of cloud deployment experience AWS | Azure | Google Cloud Platform
• At least 2+ product deployments of big data technologies Business Data Lake, NoSQL databases etc
• Awareness and decision making ability to choose among various big data, no sql, and analytics tools and technologies
• Should have experience in architecting and implementing domain centric big data solutions
• Ability to frame architectural decisions and provide technology leadership & direction
• Excellent problem solving, hands-on engineering, and communication skills

Leadership Experience
• Strong people management skills Ability to build high performing teams, mentoring team members, building a strong second line, ability to attract & retain talent
• Seen as a thought-leader in the industry in the areas of Data / Big Data, Analytics prescriptive and predictive, and insights visualization for effective story telling.
• Experience in building strong partnerships with leaders in the Big Data technologies providers like Databricks, Cloudera, Cassandra, AWS, etc.",Gurugram,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Kadel Labs,Azure Data Engineer,"Senior Software Engineer – Azure (Job ID ZR_000_JOB)

Kadel Labs is a software development company with a synergetic blend of IT Services and SaaS products under its portfolio. Since 2017, KL has grown organically (150+ employees and expanding) and served 200+ Customers. At the core, KL is a people's company. We serve clients by practicing an employee-centric culture and creating an environment where ideas, creativity, free-flowing communication and innovation is encouraged to create better products and services.

Role: Senior Software Engineer – Azure

Experience: 4+ years

Location: Udaipur/Jaipur

Job Description:

The Azure Data Engineer is responsible for building, implementing and supporting Microsoft BI solutions to meet market and/or client requirements. They apply knowledge of technologies, applications, methodologies, processes and tools to support a client, project or entity.

Responsibilities:

· Understand business requirement and actively provide inputs from Data perspective

· Understand the underlying data and flow of data.

· Build simple to complex pipelines & dataflows.

· Should be able to implement modules that have security and authorization frameworks.

· Recognize and adapt to the changes in processes as the project evolves in size and function.

· Analyze data requirements, complex source data, data model and determine the best methods in extracting, transforming and loading the data.

· Writing clean and concise code that is well-tested, easy to maintain, and consistent.

· Clearly communicate technical information to both technical resources and business stakeholders.

Requirements/Skills:

· 3-5 years of total work experience in the development of analytics-based data solutions that produce quantitative and qualitative business insights

· Should have good experience in Azure services.

· Fully conversant with big-data processing approaches and schema-on-read methodologies are a must and knowledge of Azure Data Factory, Azure Data Lake, PySpark, and SQL is a must.

· Hands-on development & coding experience in PySpark, Python, SQL

· Experience implementing robust data pipelines within Microsoft Azure stack

· Experience in developing with Microsoft SQL server or equivalent database backend

Education and Experience

BTech or relevant educational field required

Visit us:

https://kadellabs.com/

https://in.linkedin.com/company/kadel-labs

https://www.glassdoor.co.in/Overview/Working-at-Kadel-Labs-EI_IE4991279.11,21.htm",Jaipur,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"GE Appliances, a Haier company",Data Engineer,"Job Location

Hyderabad (SAL) IN

Job Posting Title

Data Engineer

The Challenge

Business Intelligence skillset to be insourced to ensure long term knowledge retention, as per Viren's guidelines. Demand for Data and Insights to support every different business function has been exponentially growing. Business Intelligence team is responsible for creation of new data assets to measure and optimize business processes. With the current FTE headcount of Business Intelligence team, there has been a constant need to hire contract resources to be able to serve the demand. Aligned with DT Organization’s goal of insourcing to retain the critical knowledge, leadership has approved this additional headcount to be added in 2021.

What you will Do

Data Analyst is responsible for designing, developing and implementing data assets as a part of Modernizing Data warehouse across the business Intelligence platforms. The primary platform targeted to modernize is Oracle Business Intelligence Application (OBIA), with a target to build a single source of truth platform with high performance catering to critical data needs across the enterprise. Functional knowledge of one of the enterprise Appliances business processes encompassing Supply Chain, Order, Ship, Bill for both Finished goods and Parts business essential to translate the business logic and requirements to technical specifications.

What you need to Succeed

Data Analyst interacts with business users to understand the requirement and partner with various cross functional teams. They are responsible for design, extend, customize ETL modules and develop technical solution components including interactive dashboards, Scorecards, adhoc queries, KPIs and reports independently. They are the point of contacts for their solutions and ensure that requirements are clear/executable, that solutions are built to technical standards and meet customer needs, and that solutions get built and launched per project schedules. Additionally, Systems Analyst is responsible for migrating solutions to support the project schedules and documenting the solutions after production launches. Systems Analyst participates in design discussions, generate design alternatives, provide development time estimates, and build solutions that match the given solution specifications. Data analyst develops solutions independently or in collaboration with other team members for larger projects, and in both cases should ensure that solutions are built efficiently and with very high quality.
• 3+ years IT experience, preferably in DW Business Intelligence DW area
• Experience with SQL and data analysis
• Ability to elicit requirements and perform data analysis independently
• Experience in dashboarding concepts
• Passionate about data and analyzing business needs
• Capable of prioritizing multiple projects while still achieving deadlines

Bachelor’s degree in Engineering; Any Analytics certification will be a good to have

GE Appliances is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Wipro Technologies,Data Engineer,"Share resume to akshara.raju@wipro.com

JD :
• Azure data factory
• Azure data bricks with PySpark coding experience
• Experience in Snowflake
• Good to have knowledge in data visualization tool Tableau or PowerBI
• Good to have knowledge in data warehousing & data analytics

Location - Bangalore , Chennai , Pune , Hyderabad

Data Analyst / Data Engineer
• 5 yrs. experience in working with large, complex data sets
• Create reports for internal teams and/or external clients
• Hands on Experience on Azure Data Factory and Azure
• Need to be able to code the data pipelines from ADF standpoint / Data Ingestion.
• Collaborate with team members to collect and analyze data
• Knowledge of Snowflake will be a big plus
• Need to ensure they can work on Data Mapping / Data Enrichment and Data Transformations.
• Use graphs and other methods to visualize data
• Establish KPIs to measure the effectiveness of business decisions
• Structure large data sets to find usable information
• Reporting and Data Visualization skills
• Experience in Data Mapping, data cleansing.",Bengaluru,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
weITglobal - W.IT.G Consulting AB,Senior Data Engineer (GCP),"We are looking for an Expert Data Engineer (7+ experienced at least) with deep knowledge preferably on Architectual level on enterprise level associated to create data products.

Desired Experience:

A GCP cloud associate Engineer who will enable data from our customer support staff planning team.
• Google Cloud platform
• Kubernetes
• DBT
• Dataform
• Data quality

We do accept remote 100% for this time but we might have gatherings around Bangalore 2-3 times a tertial.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
TMF Group,Data Engineer,"We never ask for payment as part of our selection process, and we always contact candidates via our corporate accounts and platforms. If you are approached for payment, this is likely to be fraudulent. Please check to see whether the role you are interested in is posted here, on our website.

About TMF Group

TMF Group helps its clients operate internationally and ‘belong’ wherever they are in the world. Our work includes helping companies of all sizes with business services such as HR and payroll, accounting and tax, corporate secretarial, global governance and administration and fiduciary services for structured finance, private equity and real estate investments.

TMF India is a Great Place to Work, ISO & ISAE certified organization.

Job Purpose

TMF provides a range of services for Corporates to assist them in: a) running a local business, b) preparing to enter a new market or c) operating across many borders. These services include Corporate Secretarial, Accounting & Tax, and HR & Payroll.

In addition to this, TMF Group is a recognized global player in the administration of Capital Markets transactions and PERE investments, providing a complete range of fund and business services.

The role is Data Engineer for TMF Group and the role will report to the Head of Data & Insights.

Primarily role is to understand business processes, how business performance is being measured, understand & analyze data, deliver insights to Business users using PowerBI or any other reporting tool. Experience in statistics, data analysis, extracting insights and how to present them to Business users. Development experience on SQL, PowerBI, Tableau, Python, Machine Learning, Deep Learning, Regression, Classification, Forecasting. Experience in UX designing is preferred. The Analyst is expected to work within the security standards and code quality requirements of the group including full documentation and use of source control.

Key Responsibilities
• Understanding Business processes and existing ecosystem
• Understand Business KPIs, metrics and their calculations
• Analyze, understand, and model data being used in the process and (re)define process flows, reporting and data visualization that improves efficiency in how we deliver outputs and insights to Business users
• Perform Descriptive, Diagnostic and Predictive analytics.
• Identify key insights, relations from data in the context of the Business scenario
• Share analysis with Business users
• Work collaboratively with Business users to deliver insights in an agile manner;
• Design high quality reports and dashboards selecting right types of visuals and presentation considering best user experience
• Develop BI dashboards– to share final analysis result with business stakeholders
• Understand data sources for KPIs and how data is stored in source systems
• Work collaboratively with Data Platform team to get data from Source in DWH
• Understand Dimension model design and how to use for reporting
• Create reports and dashboards using Power BI or any other reporting tool
• Implement report level and data level security in reports/dashboards
• Publish reports to PowerBI Service and maintain code in repository
• Maintain user access to reports & dashboards
• Maintain reports and amend changes based on stakeholders’ requirements
• Identify areas of process improvements and propose solutions including ML models to improve Customer Experience or optimize Processes
• Perform statistical analysis, Hypothesis Testing
• Build ML models using Python and evaluate models for accuracy.
• Interact with multiple stakeholders to present results in PPT or Jupyter Notebook
• Deploy models and integrate insights into reports
• Focus on developing end-to-end analytical solution
• Ownership and oversight of data quality and identify potential enhancements
• Ensure process compliance
• Provide Daily / Weekly update on progress of work

Qualifications
• Have a passion for data analysis and drawing insights from data
• Has a strong problem solving and analytical mindset
• Ability to interpret & ask questions to understand business requirements, to ensure successful business outcomes.
• Ability to learn new software and technologies quickly & intuitively.
• Ability to follow instructions and work in a team environment, adapting to situation & need.
• Ability to operate using Agile Methodology and DevOps tools, to coordinate, deliver & track work.
• Can work independently under limited supervision & apply subject knowledge in Data Analysis.
• Self-motivated and able to proactively learn good practice through research, peer Q&A & experiments.
• Appreciation of business case fundamentals an advantage, to contribute throughout solution lifecycle.
• Excellent verbal and written communication skills with the ability to effectively advocate solutions to engineering teams and business stakeholders
• Strong interpersonal skills and ability to work in a collaborative environment
• Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.

Candidate profile
• Degree in business analytics / data science / statistics / economics and / or degree in Engineering, Mathematics or Computer Science
• Bachelor’s degree with minimum 4 years or Master’s degree with minimum 2 years of Data Science experience from a recognized institute
• Candidates with domain knowledge and relevant experience in Customer Analytics would be highly preferred
• Strong data understanding inference of patterns, root cause, statistical analysis, understanding forecasting/predictive modelling.
• Hands-on experience in implementation of various machine learning algorithms (e.g. SVM, Random Forests, Gradient Boosting, Log-Log regression, XGBoost, Lasso, Ridge, Clustering techniques, Neural Networks and others) and time-series algorithms (e.g. ARIMA, ARIMAX, UCM, Holt-Winters and others)
• Excellent coding skills in - SQL, Python
• Able to support conclusions with analytical evidence using descriptive stats, inferential stats and data visualizations
• Candidate having UX design experience will be preferred
• Strong analytical, problem solving, and conceptual skills.
• Demonstrated ability to work with ambiguous problem definitions, recognize dependencies and deliver impact solutions through logical problem solving and technical ideations
• Excellent communication skills with the ability to speak to both business and technical teams, and translate ideas between them
• Intellectually curious, high energy and a strong work ethic

What's in it for you?

Pathways for career development
• Work with colleagues and clients around the world on interesting and challenging work.
• We provide internal career opportunities, so you can take your career further within TMF.
• Continuous development is supported through global learning opportunities from the TMF Business Academy.

Making an impact
• You’ll be helping us to make the world a simpler place to do business for our clients.
• Through our corporate social responsibility program, you’ll also be making a difference in the communities where we work.

A supportive environment
• Strong feedback culture to help build an engaging workplace.
• Our inclusive work environment allows you to work from our offices around the world, as well as from home, helping you find the right work-life balance to perform at your best.",Bengaluru,True,False,True,False,False,False,False,True,True,True,False,False,False,False,False,False
MinionLabs,Data Engineer,"MinionLabs is looking for a Data Engineer professional to join their team and help them improve their Energy Monitoring product and create new applications. If you are looking for learning opportunities (formal and informal) to improve your role-specific skills and expertise in a fast-growing early-stage company, they want you! They are looking for an analytical, results-driven candidate looking to build their skills as Data Engineer and have a passion for creating a better world. As a Data Engineer, you’ll work closely with IoT & Data Science Teams and Clients to build new features and improve on existing features and processes.

Your background 
• A Bachelor's or Master’s degree in Computer Science or related studies
• Between 1-2 years of working experience.
• Deep and hands-on experience in designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.
• Development experience in one or more object-oriented programming languages (Python).
• Advanced SQL knowledge.
• Must Have Good Hands-On Experience in Excel And Presentation.
• An excellent communicator and good personal skills.
• Continuous learning and improvement mindset.
• Demonstrated client management and relationship skills.
• Fluent in English (both spoken and written).
• Experience supporting and working with cross-functional teams in a dynamic environment.
• Experience in the energy industry (Added Advantage).

Job Responsibilities:
• Visiting customer sites for data collection.*
• Working with structured and unstructured datasets requires Strong analytic skills.
• Analyze, organize raw data, and perform EDA
• Build data systems and pipelines
• Interpret trends and patterns
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Creating Reports with the help of Python and Excel.
• This Job profile requires the candidate to visit client locations (Commercial & Industrial Facilities to acquire datasets) across PAN India when required which includes flexible transport arrangements that suit your personal situation without compromising on health and safety, expense allowances, flexible work hours, etc.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
MOURI Tech,Sr. Data Engineer,"Hi Folks,

Greetings from Mouritech!!

We are hiring for Sr. Data Engineer

Mandatory Skill: Data Engineer, Python, SQL, DWH & GCP

Location: Gurgaon (Hybrid)

Exp: 4+ Yrs to 12 Yrs

Notice Period: Immediate to 30 Days Serving.

If interested pls share your profile on below mail id

surabhim.in@mouritech.com.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Valiance Analytics .,Valiance Solutions - Data Engineer - Azure Data Factory/SQL,"Title : Azure Data Engineer

Location : Bangalore (Hybrid)

About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc.

Our team comprises 100+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire data engineers who will be working with our global clientele to help architect & develop data ingestion, transformation, and storage solutions on on-premise and cloud infrastructure. Below are important responsibilities and skills for this role.

Required Skillsets
• 1-6 years of total IT experience with 2+ years in big data engineering and Microsoft Azure
• Experience in implementing Data Lake with technologies like Azure Data Factory (ADF), PySpark, Databricks, ADLS, Azure SQL Database
• Strong hands-on experience on PySpark, SQL, Python
• A comprehensive foundation with working knowledge of Azure Full stack, Event Hub & Streaming Analytics.
• A passion for writing high-quality code and the code should be modular, scalable, and free of bugs (debugging skills in SQL, Pyspark or Python)
• Enthuse to collaborate with various stakeholders across the organization and take complete ownership of deliverables.
• Experience in understanding of different file formats like Delta Lake
• Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) are a valuable Skills :
• Strong communication skills, both written and verbal.
• Ability to manage stakeholders' expectations and work in a high-pressure environment.
• Demonstrate strong ownership & commitment to the cause of customer success.
• A team player capable of high performance, flexibility in a dynamic working environment and the ability to lead.

(ref:hirist.com)",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
PepsiCo,Data Engineer,"Overview

This role is with the Global business services arm of Media Center of Excellence (CoE) at PepsiCo.

In this role, you will play a key role in shaping the future of media measurement strategy for PepsiCo, esp. with focus on building an understanding role of media as key Growth Driver thru ROI and related analytics.

You will be laying down strong data foundation through implementation of process & technology.

This role is the backbone of media measurement agenda at Pepsico and is responsible for building data systems and pipelines to feed into prescriptive and predictive modeling by establishing and enhancing process around data capture, storage, quality and reliability.

You will work with Media, Data engineering, Data Science and IT teams globally and within AMESA sector to identify best practices in the industry and across all PepsiCo’s brands, providing support to codify and scale best in-class methods that inspire continuous improvement in marketing effectiveness and ROIs.

Responsibilities
• Collect, structure, analyze, organize and maintain RAW data from various data sources needed for creating predictive models in structured databases in order to ensure faster model building
• Partner with PepsiCo functional teams, agencies and third parties to build seamless process for acquiring, tagging, cataloging and managing all media, Nielsen and internal data periodically in structured format as needed for measurement statistical models
• Design, build and codify data structures in efficient way to periodically feed in raw data from various internal and external sources and also manage and house model outputs for quick input to businesses;
• Build data systems and pipelines as per business needs and objectives, in this case prepare data to feed specifically to MMM and media measurement models or any descriptive or prescriptive analysis
• Promote data consistency globally to support common standards and analytics
• Establish periodic data verification processes to ensure data accuracy
• Build new technologies and algorithms to optimize any business process around creation and maintenance of databases/data lakes running of batch processes for data updation

Qualifications
• 3-6 years of experience in the field of data structures, building and managing data lakes
• Hands on experience in building database/Data Management Solutions
• Mandatory experience Python, Data modelling, data pipeline set up and meta data management
• Experience in relational databases as well as unstructured data streams
• Experience with schema design and dimensional data modeling (for ex: using data vault/snowflake/star schema)
• Hands on experience, Airflow (or any other Orchestration tools)
• Hands on experience in AWS (or any other cloud operator)
• Good to have experience on technologies like, DBT
• Good to have experience in data engineering teams in consulting or ecom/telecom sector
• Desirable - Experience optimizing larger applications to increase speed, scalability, and extensibility
• Educational Background- BE/B T ECH/ MS in computer science or related technical field",Peeramcheru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,True,True
DataTheta,Azure Data Engineer,"Experience Required: 5-10 Years

Location: Noida/Chennai

Azure Data Engineer | Job Description
• Dev / Architect
• 2+ years of experience in Azure cloud data stack such as Synapse/DW, Azure SQL DB, Azure Blob Storage
• 1+ years of experience in Logic Apps
• 3+ years of experience in Python
• 5+ years of experience in SQL
• 3+ years of experience in Databricks
• 2+ years of experience in Azure/AWS Lambda Functions
• 2+ years of experience in Microservices (REST) architecture
• Ability to project manage and work within an agile, flexible environment.
• Performs peer reviews for other data engineers’ work.
• Ensuring adherence to programming and documentation policies, software development, testing and release.
• Develop modeling, design, and coding practices.
• Experience with Lean / Agile development methodologies
• Positive attitude with great collaboration and communication skills",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Biocon Biologics,Info Tech - Azure Data Engineer - Manager,"The Azure Data Engineer will focus specifically on developing solutions leveraging cloud-based data platforms and associated technologies based on Modern Data Platform (MDP) Architecture. You will be responsible for expanding and optimising data and pipeline architectures, and for optimising data collection and flow across functional teams. Your responsibilities include assisting software developers, database architects, data analysts, and data scientists with data initiatives and ensuring a consistent data delivery architecture is put in place throughout ongoing and future projects.

More specifically you will have the following responsibilities:
• Analyses current business practices, processes, and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics Services
• Provide technical and thought leadership in areas such as data access & ingestion, data processing, data integration, data modelling, database design & implementation
• Engage and collaborate with IT Business Partners to understand business requirements/use cases and translate them into detailed technical specifications
• Develop best practices including reusable code, libraries, patterns, and consumable frameworks for cloud-based data warehousing and ETL
• Ensure that data is cleansed, mapped, transformed, and otherwise optimised for storage and use according to business and technical requirements
• Maintain best practice standards for the development of cloud-based data solutions including naming standards
• Deliver successful outcomes in projects, identify value propositions for internal customers, provide inputs for strategic development in specific area(s) of expertise, and to maintain knowledge and educate junior team members as required

YEAR ONE CRITICAL SUCCESS FACTORS
• Successfully complete project implementations within time, cost and quality standards, in line with business and IT expectations
• Successfully deploy data pipelines to build Azure data lake
• Successfully develop data sourcing techniques that ensure data accuracy, integrity and accessibility

PROFESSIONAL EXPERIENCE / SKILLS / QUALIFICATIONS
• Bachelor’s or Master’s degree in in Computer Science or Engineering OR equivalent years of work experience
• 5+ years of proven experience in designing and hands-on development in Azure cloud-based data and analytics solutions
• Expert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required
• Designing and building of data pipelines using API ingestion and Streaming ingestion methods
• Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential
• Experience in Modern Data Platforms related technologies such as Snowflake, Matillion, BryteFlow is a distinct advantage
• A comprehensive understanding of the principles of and best practices behind data engineering, and the supporting technologies such as RDBMS, NoSQL, Cache & In-memory stores
• Knowledge in Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Stream Analytics, Power BI is desirable
• Experience of mapping key Enterprise data entities to business capabilities and applications
• Working knowledge of Python is desirable
• Familiarity with Agile / Scrum methodologies

SOFT SKILLS
• Excellent verbal and written English communications skills
• Strong interpersonal skills, with the demonstrated ability to engage well internally with colleagues who have both technical and non-technical backgrounds to develop shared solutions
• Excellent creative problem-solving skills, including undertaking research to inform problem definitions
• Be a team player liaising with business and technology teams across Biocon Biologics.",,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,True
Edu Angels India Private Limited,Big Data Engineer,"8+ years of Experience including –

MS SQL Server

Big Data – Hadoop Stack (HDFS, Hive)

Big Data – Basics of Hadoop Cluster Set Up / Job Monitoring / Troubleshooting

Data Pipeline Design – Experience in designing data processing flows / pipelines

Complex Query Writing (Joins, aggregations, analytical functions, etc.)

Exposure to Stored Procedure Development

Basics of Unix Shell Scripting

Excellent Written and Verbal Communication Skills (Regular Client Interaction Expected)

Team-leading Experience Is Required

Ability to work independently along with leading the team, take technical ownership, mentor team members

Good Problem solving skills

Good To Have

Experience / Exposure to Apache Spark

Knowledge of Programming (any language Java / Scala / Python)

Team Lead Role – Take Technical Ownership

Design & Maintain Data Pipeline Solutions in SQL Server / Big Data technologies as per customer requirement

Mentor and groom Team Members

Perform individual tasks along with the lead role

Regularly interact with Customer and their partners over e-mails and meetings etc.

Skills:- Big Data, Apache Hive, SQL and SQL server",Pune,True,False,True,True,True,False,False,True,False,False,False,True,False,False,False,False
Synergy Consultants,Sr data Engineer/ Data Engineer,"Roles and Technical Skills

• Bachelor’s degree in computer science, information technology, management information

systems or equivalent work experience

• 5+ years of development experience in the core tools and technologies like SQL, Python, AWS

( Lamda, Glue, S3, Redshift, Athena, IAM Roles & Policies) , PySpark used by the solution

services team.

• Architect and build high-performance and scalable data pipelines adhering to data lakehouse,

data warehouse & data marts standards for optimal storage, retrieval and processing of data.

• 1+ years of experience in Agile Development and code deployment using Github & CI-CD

pipelines.

• 1+ years of experience in job orchestration using Airflow.

• Expertise in the design, data modelling, creation and management of large datasets/data

models

• Ability to work with business owners to define key business requirements and convert to

technical specifications

• Experience with security models and development on large data sets

• Ensure successful transition of applications to service management team through planning and

knowledge transfer

• Develop expertise of processes and data used by business functions within the US Affiliate

• Responsible for system testing, ensuring effective resolution of defects, timely discussion

around business issues and appropriate management of resources relevant to data and

integration

• Partner with and influence vendor resources on solution development to ensure understanding

of data and technical direction for solutions as well as delivery

Preferred Qualifications / Certifications

• Experience working in regulated environments and with internal systems quality policies and

procedures

• Familiarity with AWS database technologies.

• Knowledge of the data architectures associated with information integration & data

warehousing

• Experience in development and deployment on cloud infrastructure

• Pharmaceutical or healthcare industry experience

• Early drug discovery industry experience

• Experience with Sales & Marketing Business processes & systems

• Defining best practices and developing technical standards, design principals, best practices,

and frameworks

• Technical curiosity and desire to innovate",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Predera,Predera Technologies - Data Engineer - Big Data Technologies,"Job Description For The Position

Predera Technologies is a US based startup which is building AI based big data solutions for Healthcare, Finance and Retail clients. We are looking for a great Big Data Engineer that is eager to wrangle big data in creative ways to build scalable pipelines.
• Design architecture, implement and deploy to production new end-to-end services, which rely on large amounts of data
• Work on data warehousing on big data platform, which may include both batch data processing and real time stream processing
• Build ETL pipelines for structured data, operational logs and unstructured data like social, email, etc
• Build tools, infrastructure and share knowledge with software engineers, enabling them to gather and use vast quantities of data in features they build

Requirements
• Strong experience with object-oriented design, coding and testing patterns
• Experience in architecting, building and maintaining (commercial or open source) software platforms and large-scale data infrastructures
• Experience building big data solutions using Hadoop technologies
• 2+ years of Java/Scala development experience
• Extensive knowledge of UNIX/Linux
• Solid understanding of SQL and experience
• Hands on experience using Map/Reduce and Hive
• Experience using Scoop and Flume is a plus
• Understanding of MPP systems, HDFS, Map/Reduce, HBase
• Experience with stream processing technologies such as Storm/Spark is a plus
• Experience with other technologies such as amazon ec2 is a plus
• A strong team player
• Ability to quickly triage and troubleshoot complex problems
• Very strong in at least one object-oriented language (knowledge of functional paradigms is a plus)
• Understanding of NoSQL and distributed databases like MongoDB, ElasticSearch, Cassandra or HBase
• Shell Scripting
• SQL (Postgres & MSSql)
• Apache NiFi
• Informatica
• SSIS, Power BI
• BigQuery
• Minimum 3+ years of experience into bigdata technologies.

(ref:hirist.com)",Hyderabad,False,False,True,True,False,False,False,False,True,False,False,False,False,True,False,False
Corporate Ladder Consultants Private Limited,Data Engineer Python,"A leading UK based data and market intelligence company that is setting up an India center of excellence. It has database products on a platform and sells it on a subscription fee.

Package - 7-8 LPA

Experience required - 3+ yrs using Python.

Must Have Experience - Snowflake, TSQL, Tableau Server, SQL server, OLAP,

Purpose - The role of Data Engineer is to design and implement data flows to connect operational systems and make data available for data analytics and business intelligence (BI) systems. You will be competent in data assessment and profiling, source system analysis and be able to provide a clear vison on how the data can be managed to support the end use of the data. You will be competent in data modelling and integration of data from multiple sources to fulfil specific business needs. You will require strong data management skills to store and make data available for data analysts. A data engineer will possess both programming and data transformation skills and can complete data projects from end to end where required. The Technology Data remit covers both internal data as well as product and client data so you will be comfortable working with a variety of internal and external clients. The data team is a relatively new unit and so you will keep abreast of data technologies and trends and recommend areas for improvement and innovation to increase data maturity within the business, and you will promote the growth and advancement of the Technology Data Team by presenting high quality insights to business leaders.

The person in this role:

The ideal candidate for this role will be a self-starter who is confident to take on projects with minimal support and deliver to deadlines. The role requires someone who is an assured communicator as often they will need to work with stakeholders throughout the project rather than just at the start. The ideal candidate will be able to communicate technical concepts and express data management in plain language to not technical and data immature audiences. Problem solving is a key part of the role, as when you are working with multiple systems and/or multiple data sources the number of points of possible errors increases so you will be able to both anticipate and resolve problems when they occur. The data team continually looks to drive for efficiencies and evolve their work, so the right candidate will follow innovations and trends in the industry and look to apply them where relevant. This role requires someone who is passionate about data and how data insight can affect change and is naturally inquisitive when it comes to exploring data assets. The ideal candidate demonstrates strong business acumen and an understanding of how data technology can support the business ambitions.

The skills required:

Essential
Ability to undertake a variety of activities with a substantial degree of personal responsibility and autonomy. It is important to be able to work independently within a team.
Experience of working with Tableau/PowerBI server infrastructure
Strong Organisational skills and must have the ability to manage multiple projects, as well as meet time constraints and expectations.
Strong data modelling and SQL/database design skills.
Programming skills – specifically those used in managing data and integration data systems and data sources.
Strong conceptual and problem-solving skills
Excellent numerical and analytical skills
Expertise in Tableau Server, T-SQL, Python, SQL Server, Online Analytical Processing (OLAP) and Tabular data models.
Development experience with Python, C#/.Net, SnowFlake, SQL Server Integration Services (SSIS) packages.
Effective verbal communication skills; strong analytical and technical writing skills
Knowledge/experience of data mining and segmentation techniques.
Excellent interpersonal skills; ability to work as part of a multi-disciplinary team with minimal supervision. This is especially important as the team are a mix of office-based and home workers.
Having worked in a cloud data environment (e.g. AWS, Azure, Snowflake) would be beneficial.
Desirable
Knowledge/Experience using data science technologies including predictive and prescriptive modelling techniques
Intermediate to advanced knowledge of Tableau, Tableau Server, Snowflake
Knowledge of common statistical methods and data science and AI techniques.
Project management experience, as a minimum the ability to manage delivery of work within stated timeframes
Experience working with common business systems that hold internal data e.g. Salesforce, Marketo, Google Analytics etc.
Experience using APIs to interface with external systems.
Experience configuring scheduled jobs to run out of business hours.
Experience in CSS Styling

If interested, please email your Resume on rajesh@corporateladder.in

PLEASE MENTION CURRENT CTC AND NOTICE PERIOD.

Thanks & Regards
Rajesh
9987583505",Mumbai,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
SID Global Solutions,Senior Data Engineer(8+yrs),"Skillset: SQL, AWS Stack, Python, Redshift/MYSQL

Roles & Responsibilities:

Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift

Hands on Python programming.

Working knowledge on S3

AWS certification is a nice to have

Must be punctual and follow deadlines and deliver on time.

Must be able to clearly communicate with stakeholders and team",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
Rently,Data Engineer,"Must have:
• Overall experience of 4+ years
• Experience in AWS Cloud services & solutions
• Experience working with enterprise data warehouse
• Experience as an ETL/ELT Developer using various ETL/ELT tools
• Experience in SQL/NoSQL/DWH databases across SQL DB, Managed instance & Data warehouse
• Experience in AWS platform services such as S3, EMR, RedShift, Glue, Kinesis, OpenSearch, Athena, QuickSight
• Working on SnowFlake and pipeline tools like Fivetran or Matillion.
• Experience in Apache Spark, Databricks
• Experience in creating data structures optimized for storage and various query patterns like Parquet
• Experience in building secured visualization reports and dashboards with access controls
• Experience in working in an Agile SDLC methodology
• Experience in DevOps Services using Git Repos, deployment artifacts and release packages for Test & production environment
• Experience in building end-end scalable data solutions, from sourcing raw data, and transforming data to producing analytics reports
• Should have experience in developing a complete DWH ETL lifecycle
• Experience in Data Analysis, Data Modelling and Data Mart design
• Should have experience in developing ETL processes - ETL control tables, error logging, auditing, data quality, etc. - using ETL tools.
• Experience in Data Integrator Scripts, workflows, Dataflow, Data stores, Transforms, and Functions.
• Should have worked on at least 2 end-to-end implementations
• Worked on Change Data Capture on both SOURCE and TARGET levels and a good understanding of Slowly changing Dimension (SCD)
• Should be able to implement reusability, parameterization, workflow design
• Should have experience in interacting with customers in understanding business requirement documents and translating them into ETL specifications and Low/High-level design documents
• Strong database development skills like complex SQL queries, complex stored procedures
• Able to work in Agile Framework Should have exposure to Scrum meetings.

Good to have:
• Exposure to other ETL/ELT, DWT technologies
• Hands-on with Data visualization tools like Power BI, Tableau, Qlik, QuickSight etc.
• Exposure to Python on ETL and Data Visualization libraries

Additional Skills:
• Good Communication Skills.
• Able to deliver independently.
• Team player.

Professional Commitment:

Being a product based company we heavily invest in developing functional/ technology/ leadership skill sets in our team members. So candidates who are willing to commit to a minimum of 2 years need to apply.",Coimbatore,True,False,True,False,False,False,False,False,True,True,False,True,True,False,False,True
vcw,Azure data Engineer,"Job description

Greetings from Ashra technologies

We are hiring

Job role: Azure data engineer

Experience: 6+years

Job description:

Location Pune Bangalore Chennai Mumbai

Role Description:

Azure Data Engineer

Azure Data Engineer:Core Skills required – Azure Data Bricks, PySpark, Spark SQL, PL/SQL, Python•Minimum 7+ years of client service delivery experience on Azure•Minimum 3 years of experience in developing data ingestion, data processing through Data bricks and analytical pipelines for relational databases, NoSQL and data warehouse solutions•Strong experience in data curation/aggregation/preparation using Azure Data Lake, Azure Data Factory, DBT and distilling data into meaningful insightswith hands-on expertise on Azure and Databricks•Extensive experience providing practical direction within the Azure Native services•Good experience in Spark SQL is preferred with Azure services•Extensive hands-on experience implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Azure Functions, Synapse/DW, Azure SQL DB, Databricks Azure Data Catalog, etc.•Cloud migration methodologies and processes including tools like Azure Data Factory, Data Migration Service, SSIS, Event Hub, Kafka, etc.•Minimum of 3 years of RDBMS experience•Strong SQL Server 2012 and above with MSBI (SSIS, SSRS)•Experience in using various File Formats and compression techniques•Experience working with Developertools such as Azure DevOps, GitLab•Experience with private and public cloud architectures, pros/cons, and migration considerations•Experience developing and deploying ETL solutions on Azure•Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.•Experience of working in Agile delivery •Proven ability to work creatively and analytically in a problem-solving environment•Excellent communication (written and oral) and interpersonal skills•Excellent organizational, multi-tasking, and time-management skills•Proven ability to work independently

If interested contact 8688322632
• or share your resume to akshitha@ashratech.com",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Attrillion Services Private Limited,Data Engineer,"Hiiiii,

We have openings with our Tier 1 client.

Job Title: Data Engineer

Location – Bangalore

Client: Apexon

Job Description:

1. In-Depth hands-on technical expertise with Microsoft Analytics, Reporting, and Database (Primary and Mandatory skill- Azure SQL; Secondary skill: Azure Synapse)

2. ETL stack & Tuning - SSIS & Azure Data Factory, SSRS.

3. In-depth experience in Power BI and SQL database programming

4. Advanced performance tuning experience– Stored Procedures, Views, and Functions.

5. Skilled in analyzing and assessing query execution plans and DMVs.

6. Ability to assess and implement indexes and partitioning to address performance issues.

7. Demonstrated track record of keeping up to date with the latest advancements in the above tech stack.

8. Understand Business Requirements and Domain to own and do hands-on development and delivery of end-to-end solutions for assigned projects (covering data model, design, and code). Duties include but are not limited to:

a. Estimate projects.

b. Develop data model, design, and approach; consider options and validate pros and cons.

c. Develop/Code using the technologies mentioned above.

d. Thoroughly Test (From Functional and Performance perspectives) the developed code and document test results

e. Review the code of other developers and identify areas needing remediation for functional and non-functional reasons such as query performance and track all these to closure.

f. Take proactive actions to address functional/technical issues and ensure delivery on time and within budget with quality.

g. Own identification of problems and drive solutions of those to closure as assigned.

h. Address production issues as necessary

9. Interact with various stakeholders – Business, BAs, Production Support, Testing team, and other development teams to assess and understand any impact to them and drive coordination between them as necessary to ensure successful delivery

If interested please share your updated resume to souparnika@attrillion.com

Regards,

Souparnika.",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,False,False,False,False
e-Hireo,Data Engineer - AWS Cloud,"Requirements
• Experience with at least one Cloud platform: AWS, Azure, Google etc. AWS is preferred.
• Experience to AWS Data analytics (Redshift, Kinesis, MKS, Athena ,Glue ,Lambda, EMR, Aurora, DynamoDB services).
• Experience with Python.
• Experience with RDBMS databases - PostgreSQL, MySQL, Oracle etc.
• Experience with NoSQL database - MongoDB, Cassandra etc.
• Exposure to Java will add an advantage.
• Exposure to Data warehouse environment.
• Demonstrate the ability to reduce complex ideas and problems into clear concepts and solutions.
• Experience in presenting technical information, documenting data inventory and data flows.

What We Appreciate About Your Background
• Undergraduate degree in Information Technology, Computer Science, Engineering, or a related field required, with a graduate degree preferred.
• 5+ years of overall experience in Database technologies.
• Strong hands-on understanding of scalability, security, high availability, and operational requirements.
• Experience with full product lifecycle.
• Experience with Atlassian suite: Jira, Confluence, etc.
• Excellent verbal and written communication skills.

(ref:hirist.com)",Pune,True,False,True,True,False,False,False,True,False,False,False,False,True,False,False,False
Fusion Plus Solutions Inc,Data Engineer (Deepika Jain),"Requirement- Data Engineer

Exp- 5 years to 9 Years

Budget- 20 LPA

Location- Pan India

Mandatory-

They know Azure or some similar platform.

Scala and Spark good knowledge ( Experience from 2 or 3 years).",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
LTIMindtree,Data Engineer- Bigdata Pyspark,"• Job Title- Specialist Data Engineer
• Primary skill- Bigdta Python Pyspark
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 8 Years
• Notice Period- 0 to 30 Days
• Job Description-

""Data Engineer :

• Deep expert level knowledge of Python, Scala Spark

• Application coding in Scala, Python, pySpark , SQL, and Relational Databases

• Experience on developing with Apache Spark streaming and batch framework with Scala and PySpark

• Good to have experience with DevOps practices and deployment models using Jenkins.

• Good to have experience with Databricks

• Strong familiarity with agile methodologies

• Experience working in an agile scrum environment

• Strong analytical, problem-solving and decision making capabilities.

• Excellent verbal, interpersonal and written communication skills

""",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,True,False,False,False,False
Advance Auto Parts,Lead Data Engineer,"Job Description

Job Description

Proficient in all aspects of Product Information Management (PIM). Ensures completeness of all existing and impending data within the corporate PIM solution software and subscribing systems. Possesses a detailed understanding of import and export functions within PIM solution software. Works closely with other key stakeholders such as Merchandising, Information Technology, Marketing, e-Commerce, Accounting, and Logistics. Working knowledge in data quality, metadata management, and end-to-end data lineage. Has technical understanding of PIM solution software including ability to modify software configurations.

Job Description

Reviews, analyzes, and evaluates current business needs to create system solutions to support overall strategies. Documents system requirements, defines scope and objectives, and assists in the creation of system specifications that drive system development and implementation. Functions as a liaison between IT and business.

Job Duties
• Develop an ownership approach to all assigned tasks and projects
• Work closely with the Merchandising Team to ensure complete and accurate product data information is collected from Vendors and other relevant sources
• Communicate with cross-functional teams and internal and external stakeholders to resolve data issues
• Accountable for data residing within PIM solution software and subscribing systems
• Ensure complete, accurate, and timely publication of data to downstream subscribing systems
• Review current product data issues and recommend plans for data corrections
• Recommend and develop continuous improvements to PIM processes
• Identify missing business critical data elements and determine best approach to obtain information
• Develop and maintain appropriate documentation as required
• Develop/conduct training on new/existing processes
• Prepare and present periodic progress reports to identified stakeholders of the process

Qualifications
• Bachelor's Degree or Master level qualification
• 4+ Years’ of Data Stewardship experience
• 2+ years’ experience in handling data quality issues
• 1+ years of experience in using data management tool
• Strong experience in data analysis & troubleshooting
• Exposure to the data quality tool
• Exposure to Data Governance processes
• Metadata Management tool experience is preferred
• Must demonstrate good organizational and follow-up skills and work independently with minimal supervision, including ability to balance multiple tasks
• Demonstrated strong problem-solving capabilities and excellent customer service
• Ability to operate in a complex, rapidly changing environment while adhering to tight schedules
• Proven analytical skills
• Working knowledge of SQL and data query tools
• Advanced proficiency in Microsoft Office Software
• Must demonstrate good written and verbal communication skills
• Ability to speak effectively to groups of customers or team members.
• Strong interpersonal skills, ability to interact effectively with team members
• Ability to cultivate relationships across functions to achieve business objectives
• Working knowledge of STIBO PIM solution software including data model and workflows preferred

Education And Experience
• Bachelor's Degree or Master level qualification
• 2+ Years PIM Administration
• 4+ Years Data Stewardship
• 2+ years Data Quality

California Residents Click Below For Privacy Notice

http://www.worldpaccareers.com/uploads/2/4/0/4/24047148/advance_auto_parts_--_california_candidate_privacy_notice.pdf

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age national origin, religion, sexual orientation, gender identity, status as a veteran and basis of disability or any other federal, state or local protected class.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Technologies,Data Engineer,"Job Title : Data Engineer

Job Location : Thane(Mumbai)

Domain Knowledge:

Should be capable of carrying out the following operations on the data with any application.

• Familiarity with data loading tools like Flume, Sqoop.

• Analytical and problem-solving skills applicable to Big Data domain

• Proven understanding with Hadoop, PySpark, Hive, Hadoop

• Good aptitude in multi-threading and concurrency concepts",Thane,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Acme Services,AWS Data Engineer,"Person should have experience & expertise on Data Modelling by discussion with various stakeholders from different

departments in DV-Salesforce, HR, Marketing, Finance, Engineering, Account Development Teams etc

Python programming skills for ETL Jobs Be in a position to model the data, propose the end to end pipelines and then

implement the same

Experience on the systems or platforms highlighted in scope section

Scope Data Sources-Salesforce, Netsuite, HR Systems of DV, Finance Systems, Marketing Systems,

Good Data Warehouse and Reporting and Snowflake Data Extraction-Rivery Data Loading and Warehouse-Snowflake.

Data Aggregation using Snowpipes Reporting and Visualization-Tableau.",Mumbai,True,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
TransUnion,Data Engineer,"TransUnion's Job Applicant Privacy Notice

What We'll Bring

We are looking for Data Engineer to bring big data, marketing analytics, and database technology together to deliver best-in-class insights. You must be a hands-on data guru who’s passionate about data, database product development, and play well with others. If you have a curiosity for different industries and companies, a passion for data analytics, SQL enthusiast, and skills to create effective and repeatable data transformation and profiling methods, we would like to hear from you.

Data is rarely perfect. We are looking for data analysts who will ensure we have the best data for client marketing insights. You will work in a cross functional team that spans Strategy, Data Management, Analytical Insights, and Product Solutions. You will use a keen eye, an understanding of the client’s industry and business practice, and common sense.

What You'll Bring

Responsibilities:
• Analyze data and identify data pattern to bring insight out of data.
• Build story around the data findings and explain in simple words.
• Data discovery and validation.
• Follow best practices and document the processes.
• Pay attention to Detail.

Skills And Experience
• Bachelor in computer science/engineering/statistics/economics (master degree preferred).
• 3-5 years of experience in data analysis and exploratory analysis.
• Have strong analytical thinking and soft skills.
• Hands-on experience in Hive SQL, Spark SQL or any ANSI SQL.
• Strong in MS Excel, Tableau for data crunching.
• Good knowledge of Hadoop platform, Python, Spark.
• Should be team person and strong interpersonal skills.
• Experience in GCP / AWS .

Impact You'll Make
• Experience in digital marketing, campaigning and advertising.

TransUnion Job Title

Acquisition Non-Exempt, Acquired Associate",,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,False
Sanofi,SBS DPA CoE Data Engineer,"At Sanofi diversity and inclusion is foundational to how we operate and embedded in our Core Values. We recognize to truly tap into the richness diversity brings we must lead with inclusion and have a workplace where those differences can thrive and be leveraged to empower the lives of our colleagues, patients and customers. We respect and celebrate the diversity of our people, their backgrounds and experiences and provide equal opportunity for all.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer,"638573BR

Introduction

""We are looking for a Data Engineer to join the IBM Watson Media engineering team and help us maintain and improve the key aspects of our leading SaaS video streaming platform.

As a Data Engineer at IBM Watson Media, you will be responsible for building and operating the data pipeline of our unit. You will have the opportunity to learn and work with tools like Kubernetes, Terraform, Helm, Airflow, Athena, Elasticsearch, Pinot, Kafka, Flink, Python or Tableau to get the most out of our most valuable resource, all data from different areas of our platform. With this we want to make sure we have all critical insights of our platform's usage and we give meaningful information to our customers whenever they need it.""

Your Role and Responsibilities
• design, implement, maintain and operate efficient and reliable data pipelines to move data across systems
• develop and deploy changes via infrastructure as code
• support multiple teams (product, engineering, marketing, finance etc) in high visibility roles
• implement data workflow to aggregate and structure data for high-performance analytics
• monitor performance of the data platform and optimize as needed
• provide on-call duty for the supported systems

Required Technical and Professional Expertise
• Experience with query languages (eg. SQL, Hive etc.)
• Experience with a scripting language (eg. Python) and a statically typed programming language (eg. Java)
• Experience with relational databases
• Strong collaboration and problem-solving skills
• Working experience with container orchestration solutions (e.g. Kubernetes), Helm and infrastructure as code systems (e.g. Terraform)
• Curiosity to understand the business use of data tools
• Fluency in English

Preferred Technical And Professional Expertise
• Experience with functional programming (example: Scala)
• You have a bachelor’s degree in Computer Science or equivalent training, education and experience
• Experience in monitoring systems (eg. Prometheus)
• System administration and automation skills

About Business Unit

IBM Watson Media is the leading Live Video Streaming, AI and Video Hosting Solution for businesses. Our comprehensive video streaming platform is built for scale and reliability, to optimize video quality and workflow, and powered by IBM Watson AI for video search and automated closed captioning. Our products are used by thousands of enterprises, service providers, educators, and media companies worldwide from startups to Fortune 500 companies like Salesforce, Pinterest, SONY, Mazda, Herman Miller and Roland. With our help, they create large scale live streams, host video entertainment venues with pay-per-view access for subscribers, conduct video marketing activities like product launches or webinars, stream executive townhalls and trainings and create powerful virtual and hybrid events.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBM

In a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you’ll be able to learn and develop yourself and your career, you’ll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

When applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Ahmedabad,True,False,True,True,False,False,False,False,False,True,False,False,False,False,True,False
Deloitte,Data Engineer-Hyderabad/Bengaluru,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration and high performance. As the undisputed leader in professional services, Deloitte is where you’ll find unrivalled opportunities to succeed and realize your full potential

Deloitte is where you’ll find unrivalled opportunities to succeed and realize your full potential.

The Team

Deloitte Digital has created a new model for a new age: the creative digital consultancy. We're transforming the digital journey in a way an agency or traditional consultancy alone cannot–now leaders across their entire organization can come to one place to have their ambitions brought to life. We combined the creative and digital capabilities of our studios and the broad reach of an advertising agency with the technical experience, deep business strategy, and relationships of one of the world's largest consultancy to create something that is so much more than the sum of its parts. We can help you imagine bigger and scale as your business grows. Learn more about Digital

Level: Consultant / Senior Consultant

Experience Band: 4-9 Years

Location: Hyderabad (preferred), Bangalore

Job Description:

Looking for competent big data engineer with the following technical and functional skillsets :
• 4+ years of overall experience with hands-on development experience in big data stack
• Must have experience to implement complex data pipeline and data processing jobs using Py-Spark
• Must have strong technical fundamentals with SQL and hands-on experience with the same
• Excellent communication skills and solution oriented approach in problem solving
• Experience to work with a large team (preferably cross-geography), in client facing roles

Notice Period : Immediate joiners with <= 60 days of notice period

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society and make an impact that matters.

How you’ll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help build world-class skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Centre.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Everyday Health Group,Data Engineer,"Description

Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.

Health eCareers, a property of Everyday Health Group, is looking for a Data Engineer to support our growing business.

Key Responsibilities
• Understand overall data collection strategies and implement them in data tables and connections.
• Program Python-based APIs and web services per implementation schema, such as creating applications to access Facebook, Twitter, Salesforce, and other data sources.
• Use SQL Server, MySQL, HDFS and AWS to design, develop and deploy data processing.
• Analyze and organize raw data from various ETL tools.
• Monitor/Forecast computing resources usage (data lakes, AWS, EC2, etc.).
• Collaborate with ETL developers located at various offices (US and India)
• Implement coding standards, procedures and techniques, concluding writing technical code base.
• Detect, repair, prevent data pipeline failures; identify systematic weaknesses and provide pre-emptive remedies with programming or processes.
• Build and optimize data storage systems.
• Prepare data for prescriptive and predictive modeling.
• Recommending and implement emerging database technologies.
• Create automation for repeating database tasks.

Job Qualifications
• 4+ years of programming experience with an emphasis on data processing.
• Coding skills: Python, SQL.
• Ability to create object-oriented programming and data architectures
• Knowledge of new, leading technology strategy in data engineering and management
• Understanding of industry technologies in scalability, performance, delivery pipeline and maintenance of these tools and systems.
• Hands-on experience with SQL database design, AWS or other cloud systems.
• 2+ year’s experience in Linux environments.
• Experience with SQL Server, MySQL or other popular database management tools.
• Good/Expert knowledge of API services
• Familiarity with Agile process
• Clear documentation requirements and specifications
• Bachelors or Advanced Degree in Information Management, Computer Science or related field.

Desirables:
• Experience in AWS, Tableau
• Experience in developing custom data pipelines in HDFS
• Experience working with social, Double Click, Adobe APIs
• Working with teams across multiple locations
• Skills: Pig; Hive; Spark; Impala; EMR

Our Culture and Values

We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.

Life at Everyday Health

At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work.

#HealtheCareers",,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Prevaj Consultants Pvt Ltd,Data Engineer,"• 5+ years of experience building real-time and distributed system architecture, from whiteboard to production
• Strong programming skills in Python, Scala and SQL.
• Versatility. Experience across the entire spectrum of data engineering, including:
• Data stores (e.g., AWS RDS, AWS Athena, AWS Aurora, AWS Redshift)
• Data pipeline and workflow orchestration tools (e.g., Azkaban, Airflow)
• Data processing technologies (e.g., Spark, Pentaho)
• Deployment and monitoring large database clusters in public cloud platforms (e.g., Docker, Terraform, Datadog)
• Creating ETL or ELT pipelines that transform and process petabytes of structured and unstructured data in real-time
• Industry experience building and productionizing innovative end-to-end Machine Learning systems is a plus.
Skills:- Google Cloud Platform (GCP)",Chennai,True,False,True,False,True,False,False,False,False,False,False,False,True,False,True,False
Tata Consultancy Services,Data Engineer,"Greetings From TCS!!!

Role: Data Engineer

Experience : 3+ years

Location : Pan India

Required Skill set- Spark, Scala, Glue, Hadoop, EMR redshift

Roles and Responsibilities

Data Engineer to build data pipelines to support implementation of data science and analytics use cases.

Candidate needs to be able to develop code in the corresponding language (see technical skills), test it, and follow up with the implementation into Production environment.

Candidate will also take part in the solution design phase, so experience in analysis requirements is desirable.

Technical Skills: Hadoop, Hive, Impala, SQL, Spark, Python, OS (Unix), CI/CD Tools (Git, Jenkins, Nexus), Agile (Jira, Confluence).
•",इन्दौर,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
LTIMindtree,Specialist Data Engineer- AZURE-ADF/ADB,"• Job Title- Specialist Data Engineer
• Primary skill- Azure+Databricks (ADF+ADB+Pyspark)
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 12 Years
• Notice Period- 0 to 30 Days
• Job Description-

Primary Skills

• 5+ years of experience in Python and Databricks.

• Deep understanding of data modelling techniques for analytical data (i.e. facts, dimensions, measures)

• Experience developing and managing reporting solutions, dashboards, etc. Design and architecture experience in data transformation.

• Should have experience with data platforms and in data transformation and extraction: some combination of ETL/ELT, table and database design, query design, performance analysis and optimization

• Worked as a data engineer or related specialty (Software Engineer/Developer, BI Engineer/Developer, DBA)

Secondary Skills

• Experience in Azure Data Factory and Azure Storage

• Hands on experience with handling of large amount of data using SQL, Azure Data Factory, Spark, Azure Cloud architecture

• Knowledge of cloud architecture and data solutions

• Proficiency in Snowflake would be added advantage.

• Excellent written and verbal communication skills",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
YesOrYes,Senior Data Engineer. Experience: 6+years,"Experience:6+years.

Description:
• 6-8 years of IT experience with 6+ years of experience as a Google Cloud Platform Data Engineer with hands-on experience on Big Query & Java is a must.
• Should be an Expert in Core & Advanced Java. Python is a value-added.
• Hands-on experience in building Apache Beam/Cloud Dataflow pipelines for batch and Streaming using Java & Python.
• Hands-on experience in Apache Airflow/Cloud Composer to create end-to-end pipelines.
• Design and implement data engineering, ingestion, and curation functions.
• Understanding of Data warehousing concepts.
• Experience in implementing SQL Standards and implementing various scenarios.",Chennai,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
TalentOne Consulting Pvt Ltd,Looking for Lead Data Engineers for a Global Healthcare company,"Role : Lead Data Engineer

Exp : 7-10 Yrs

Location : Bangalore/Mumbai/Pune/Hyderabad

Position :Permanent Position

Skills :Lead Data Engineer// Python// AWS// SQL// Data streaming// Data pipeline // Team leading exp

Responsibilities for Data Engineer
• Create and maintain optimal data pipeline architecture for sourcing data from multiple structured and unstructured data sources.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Perform data cleaning, exploration and statistical analysis to ensure data quality
• Work alongside a Data Scientist to develop modeling techniques for various product capabilities

Qualifications For Data Engineer
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing big data data pipelines, architectures, and data sets.
• Knowledge of sprint and scrum
• 5+ Years of Experience in ETL Applications ...like Apache Airflow, pipelines like Glue etc.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of manipulating, processing and extracting value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable big data data stores.
• Experience with big data tools: Hadoop, Spark, Kafka, is a plus.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

If you are interested please send your updated CV @ sangita.talentone@gmail.com

Regards,

Sangita

https://in.linkedin.com/company/hireatease-consulting-private-limited

This job is provided by Shine.com",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,False,True,False
Visa,Sr. Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. You will be an integral part of the Payment Products Development team focusing on design and development of software solutions that leverage data to solve business problems. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and testing of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Responsible for the design, development, and implementation
• Work on development of new products iteratively by building quick POCs and converting ideas into real products
• Design and develop mission-critical systems, delivering high-availability and performance
• Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology
• Develop code to ensure deliverables are on time, within budget, and with good code quality
• Have a passion for delivering zero defect code and be responsible for ensuring the team's deliverables meet or exceed the prescribed defect SLA
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
• Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.

Qualifications

We are seeking team members that are passionate, visionary and insatiably inquisitive. Successful candidates frequently have a mix of the following qualifications:
• Bachelor’s Degree or an Advanced Degree (e.g. Masters) in Computer Science/ Engineering, Information Science or a related discipline
• Minimum of 3 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies
• Extensive experience with SQL and Big Data technologies (Hadoop, Java, Spark, Kafka, Hive, Python) for large scale data processing and data transformation
• Deep knowledge of Unix/Linux
• Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
• Familiar with software design patterns
• Experience working in an Agile and Test-Driven Development environment
• Strong knowledge of API development is highly desired
• Strategic thinker and good business acumen to orient data engineering to the business needs of internal and external clients
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
• Previous exposure to financial services is a plus, but not required

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
Speriti Solutions,Interesting Job Opportunity: Speriti Solutions - Data Engineer - Python/Pandas,"Job Description
• Deep, expert level knowledge of Python, Pandas
• Application coding in Python, Pandas , SQL, and Relational Databases
• Experience developing and implementing data models for various domains.
• Experience developing hybrid automation/interactive solutions for SMEs
• Experience developing with containers and linux.
• Experience with Spark
• Some experience with Modeling techniques, possibly include linear regression, timeseries (ARIMA, etc) and other econometric techniques.
• Experience with modern DevOps practices .
• Good to have: experience with Databricks
• Good to have: experience in the energy sector, Refining is a big plus
• Strong familiarity with agile methodologies
• Ability to work with non-technology stakeholders to elicit and implement requirements.
• Strong analytical, problem-solving and decision-making ability
• Excellent verbal, interpersonal and written communication skills

(ref:hirist.com)",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Position: Data Engineer

Experience- 4+

Location- Bangalore, Hyderabad, Chennai , Pune

Must to Have

.Good understanding of Hadoop concepts including file system and Map Reduce. 2. Hands on experience on Spark framework, Unix scripting, Hive queries, wriring UDF in Hive. Theortical knowledge and POC alone will not suffice. 3. Good knowledge in Software Development Life Cycle and Project Development Lifecycle. 4. Associate should be able to work independently and should have strong debugging skill in both Hive and Spark. Associate should have experience developing large scale systems, experience debugging and performance tuning, excellent software design skills, communication skills and ability to work with client partners.

Good-to-Have

1. Experience in Banking and Finance Domain. 2. Experience in Agile Methodology 3. Knowledge in job scheduling tools like Autosys. 4. Knowledge in Kafka

·",,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Neudesic Technologies Private Limited,Azure Data Engineer,"Must Have Skills:
• Prior experience in ETL, data pipelines, data flow techniques using Azure Data Services
• Working experience in Python, Scala, PySpark, Azure Data Factory, Azure Data Lake Gen2, Databricks, Azure Synapse and file formats like JSON & Parquet
• Experience in creating ADF Pipelines to source and process data sets.
• Experience in creating Databricks notebooks to cleanse, transform and enrich data sets.
• Good understanding about SQL, Databases, NO-SQL DBs, Data Warehouse, Hadoop and various data storage options on the cloud.
• Development experience in orchestration of pipelines
• Experience in deployment and monitoring techniques
• Working experience with Azure DevOps CI/CD pipelines to deploy Azure resources.
• Experience in handling operations/Integration with source repository
• Must have good knowledge on Datawarehouse concepts and Datawarehouse modelling
• Power BI and Data Catalog experience
• Zeal to learn new tool/technologies

Neudesic is an Equal Opportunity Employer

Neudesic provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by local laws.

Neudesic is an IBM subsidiary which has been acquired by IBM and will be integrated into the IBM organization. Neudesic will be the hiring entity. By proceeding with this application, you understand that Neudesic will share your personal information with other IBM companies involved in your recruitment process, wherever these are located. More Information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here: https://www.ibm.com/us-en/privacy?lnk=flg-priv-usen",Bengaluru,True,False,True,False,False,False,False,False,True,False,False,False,False,False,False,False
HackerTrail,Senior Data Engineer - II,"This posting is on behalf of the client of HackerTrail - a leading Fintech Product Company

Responsibilities
• Work with cross functional teams across the organization to help support business decisions.
• Designing and building scalable cloud-based data pipelines, optimized for processing terabytes of data (both batch and stream)
• Collaborate with Product owners and Architects
• Design, build and support data processing pipelines and APIs;
• Write Clean testable code and practice code quality .
• Mentor other team members in development best practice, and methodologies
• Tune and Improve product performance
• Use Agile/Scrum methodologies to iterate quickly on product changes, developing user stories and working through backlogs;

Requirements and Skills
• 4+ years of experience with detailed knowledge of data warehouse technical architectures, ETL/ ELT, reporting/analytic tools, and data security;
• Hands on knowledge of at least two of these programming languages like Scala, Java, Python Must have AWS/Azure hands on experience (EMR, lambda, s3, redshift, glue, etc.) or (VM, Databricks, App functions, etc.)
• Advanced knowledge of relational database systems, columnar databases and NoSQL;
• Strong communication and ability to work with cross functional team
• Experience in mentoring other team members in development best practice, and methodologies.
• Experience with distributed computing frameworks such as spark
• Experience with build tools like Maven/SBT is must
• Experience with implementation, integration and administration with at least one of the following Streaming Data-Streaming frameworks or services: Apache Kafka, Kinesis Data Streams, Apache Spark Streaming, Apache Flink, Apache Storm, Apache NiFi, Azure Stream Analytics, or Confluent.

Good to have
• Experience in Containerization (docker/Heroku)
• Exposure to microservices
• Exposure to DevOps practices

P.S. Even if this job is not for you, perhaps you have a friend who’d be a perfect fit. Send them this link. Thanks!

NOTE: When we receive your job application for any of our clients at HackerTrail we create an account for you on HackerTrail.com so we can keep track of your application, and progress your candidature. You can choose to have your HackerTrail account deleted at any time by sending an email to support@hackertrail.com from the email address listed on your resume",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,True,True,False,False,False
Best Infosystems Ltd.,Data Engineer,"Job Description
• Experience with integration of different data sources with Data Lake is required
• Experience in Perform Design, Hands on development Deployment using Hadoop, Spark, Scala, Hive, Kafka, SQL, Oozie
• Experience in optimal extraction, transformation, and loading of data from a wide variety of data sources
• Experience in working with Big Data eco-system including tools such as Hadoop, Spark, Scala, Hive, Kafka, SQL, Oozie
• Develop and maintain scalable data pipelines and build out new Data Source integrations to support continuing increases in data volume and complexity.
• Experience building and optimizing Big Data pipelines and data sets
• Extensive Experience around SQL
• Experience in solving Streaming use cases using Spark, Kafka

Mandatory Skills

Mandatory Skills : Hadoop, Spark, Scala, Hive, Kafka, SQL, Oozie

Nice to have skills: Python, Airflow

This job is provided by Shine.com",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
HackerTrail,SDE 2 - Data Engineer,"This posting is on behalf of the client of HackerTrail - a leading Retail Product Company

THE IDEAL CANDIDATE

The ideal candidate is a self-starter, problem-solver and successful in combining technology and data into best-in-class outcomes. The candidate is energized by solving complex business problems and consistently effective in making high-judgement decisions at rapid pace amidst the frequent ambiguity that comes with charting a course of action with no precedent. Moreover, the ideal candidate is energized by an environment where strategy, innovation and decision-making are intentionally distributed, where candor, speed and data are highly valued and colleagues at all levels hold each other to unusually high standards on behalf of Quince customers.

Year of experience: 2 - 5 years

Responsibilities:
• Build and deploy scalable data engineering solutions on the cloud for analytics & data-science consumers.
• End-to-end module ownership of your functional area.
• Sharp focus on performance optimizations & continuous effort towards improving both read & write latencies.
• Researching on and integrating any big data tools and frameworks required to provide requested capabilities.
• Time split - 90% hands-on programming/execution/research and 10% stakeholder management depending on experience & exposure.
• Ensuring operational efficiency of all DE deliverables.
• Nice to have skills:
• Good understanding of data warehousing fundamentals and prior experience of building data models to meet business requirements.
• Experience in big data technologies(Apache Hadoop/Spark/Hive) and relational databases(mssql server/oracle/mysql/postgres) either on-premise or on the cloud.
• Proficiency in at least one of the following programming languages - Python, Java or Scala.
• Advance expertise in SQL (T-SQL/PL-SQL/SPARK-SQL/HIVE-QL).
• Experience with integrating data across multiple data sources.
• Good understanding of distributed computing principles.
• Strong analytical/quantitative skills and comfortable working with very large & varied sets of data.

Good To Have
• Working knowledge of cloud native (AWS preferred) services/tools.
• Experience with MPP data-warehouses(e.g.Snowflake/Redshift).
• Proficiency in Apache Spark.
• Experience with Message Queues(e.g.
• Apache Kafka/Kinesis)Experience with any NoSQL storage(e.g. Redis/DynamoDB/Memcache).

P.S. Even if this job is not for you, perhaps you have a friend who’d be a perfect fit. Send them this link. Thanks!

NOTE: When we receive your job application for any of our clients at HackerTrail we create an account for you on HackerTrail.com so we can keep track of your application, and progress your candidature. You can choose to have your HackerTrail account deleted at any time by sending an email to support@hackertrail.com from the email address listed on your resume",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,True,True,False,False,True
Prajosh Technologies,Azure data engineer,"Good Programming skills in Scala/Pyspark/python.

Strong SQL/PLSQL knowledge, Datamodelling

Gather and elicit requirements from business users, and translate them into technical

Requirements

Understand source systems and perform data profiling

Document source table details and appropriate transformation rules that needs to be

applied on the target tables/reports

Work closely with the business, IT teams, project managers, Architects, developers, and

Modelers.

Experience in Translating Business requirements into Code and deliver the required result

Experience working with Azure Cloud Data Platform solutions(Azure Data Lake, Azure Data Factory, Azure Databricks, Azure Synapsis)

Good communication skill and able to work on multiple projects",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LTI - Larsen & Toubro Infotech,Data Engineer,"LTIMindtree is looking for a Data Engineer with DBT experience

Experience- 4 to 6 Years

Job location- Pan India

Notice Period- Immediate to 30 Days

If interested please share resume on t.boopathi@lntinfotech.com with below details.

Total expereince:

Relevant Hands on Experience in DBT:

Current CTC:

Expected CTC:

Notice Period:

Job Description:

Data Engineer - DBT

Skills Required -
• 4-6 years of experience as Data engineer.
• Strong knowledge on DBT tool and its features and capabilities
• Min 1 year of hands-on experience in DBT (Data build tool) is a must.
• Experience working on Data warehouses like Snowflake, redshift or equivalent.
• Experience on working on any ETL tools like Informatica, Matillion, etc
• Hands on experience with SQL and database design
• Strong understands basic data warehouse and ETL concepts.
• Should be an excellent team player, strong communication and ownership skills.

Responsibilities –

• Candidate should be able to perform and deliver under large and complex env.

• Involve in Design of pipelines

• Develop DBT models to transform data into useful, actionable information

• Create Models and Identify Patterns.

• Build, test, and maintain database pipeline architectures

• Collaborate with management to understand company objectives

• Create new data validation methods and data analysis tools

• Ensure compliance with data governance and security policies",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,True,False,False,True
Peoplefy,Data Engineer,"Greetings!! We have an opening for Data Engineer role with one of our MNC client, based out in Pune- Kharadi

Role: Data Engineer

Location: Kharadi, Pune

General Shift: 9AM-6PM

Notice Period: upto 60 days

Experience: 4-11 Years

Required Skills:

Azure DataBricks

Azure datafactory

Should have basic concepts on Data warehousing

Interested candidates can share their resumes on deepti.ta@peoplefy.com",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
"Prudent Technologies and Consulting, Inc.",Senior Data Engineer,"Senior Data Engineer

Required Work From Office ( Hyderabad )
• 10 years of experience in analyzing, extracting data which could be in various formats and sources.
• Experience in building dimensional models – OLAP.
• Ability to articulate insights from the data and help business teams make decisions.
• Strong knowledge of and experience with reporting, databases (SQL etc), programming ( ETL frameworks)
• Experience in understanding the source data from various platforms and mapping them into Entity relationship model(ER) for data integration and reporting.
• Develop and maintain data architecture & data modeling best practices and guidelines for different data and analytic platforms
• Strong in writing complex SQL queries.",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Quess IT Staffing,Azure Data Engineer,"Hello All,

We are Hiring for Product based company(permanent opportunity) ----Data engineer

Location: Kharadi, Pune

Mode: Hybrid

exp:6-10 years

Notice period-45-60 days

Job Description:-

• Team performance : Demonstrates on a daily basis Agile & Lean values like cooperation, transparency, courage and humility to foster teamwork. Continuously reflect on how to become more effective then tunes and adjusts team norms and helps to update IT standards

• Right Product : A solution creating business value is build and deployed by · Understanding (hands on business activities) and challenging Business requirements · Translating Business requirements into functional requirements · Designing, developing the solution by taking into account DGSI principals and guidance’s (EA rules like security and integration, Group methodologies,…) · Validating the global solution · Deploying the solution by contributing to change management, communication and training

• Deliver Right : A stable, performing, scalable, easy to use, secure, easy to maintain & to operate Solution is designed and developed (this includes non-functional requirements) The solution provides a great user experience.

• Deliver Fast : An incremental approach (Do & Learn) is used during design and development to manage the lead time, by per example : - Implementing automation for testing and deployment of components - Being able to deliver incremental functionalities quickly

• Solution Maintenance : The Developer Analyst maintains and optimizes a working solution in « marche courante » by providing support level 3

Skill set:

01 : DataBricks + Datafactory + strong Datawarehousing concepts (PowerBI exposure will be an advantage)

Skill set

02 : DataBricks + any alternative tools + strong Datawarehousing concepts (PowerBI exposure will be an advantage)

(Azure Databricks is mandatory) (Hands on:: Pyspark /python/scala)

(relevant experience 2.5yrs)

Interested candidates can share their resume on: ramanjaneyareddy.m@quesscorp.com",Pune,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Uplers,Solutions Architect (Data Engineer),"Profile: Technical Architect - Data Engineering

Experience: 9+ years

Salary: INR 3,20,000/month (based on experience)

Expected Notice Period: 2 to 4 Weeks

Shift: 10:30 AM to 7:30 PM IST

Opportunity Type: Onsite (Hyderabad)

Placement Type: Direct Placement

(*Note: This is a requirement for one of Uplers' clients)

About the Client/Company:

Our Vision is to build a company of world-class people that helps our clients optimize business performance through data, technology and analytics. The company has two divisions: Data Science Solutions: We work at the intersection of data, technology and analytics. Talent Solutions: We live and breathe the digital and talent marketplace.

Our Client is Looking for:

We are looking for Technical Architect - Data Engineering who are passionate about their work, eager to learn and grow, and who are committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you!

Roles & Responsibilities

Practice Development
• Work with DE Practice Lead and clients to understand business problems, industry context, data sources, potential risks, and constraints
• Collaborate with Leadership – provide meaningful and credible feedback on Data Engineering capabilities, data availability, and customer trend information
• Actively mentor and coach the team and help them realize the best solution to a problem
• Facilitating, guiding, and influencing the clients and teams towards right information technology architecture and becoming interface between Business leadership, Tech leadership and the delivery teams
• Provide best practice advice to customers and team members
• Create an ecosystem that fosters innovation and encourages members of the CoE to build innovative solutions and publish papers/content in the public domain

Project Delivery
• Lead our DE team performing standard-to-advanced and the highest complexity Data and API engineering in modern ways on modern platforms and technologies
• Works with and directs all resources on requirements and reviewing the quality of work prior to move to production
• Get the stakeholder feedback, get alignment on approaches, deliverables, and roadmaps
• Develop a project plan including milestones, dates, owners, and risks and contingency plans
• Create and maintain efficient data pipelines, often within clients’ architecture; typically, data are from a wide variety of sources, internal and external, and manipulated using SQL, spark, and Cloud big data technologies

People Management
• Provide strategic leadership, development and talent management activities for direct reports and their organizations, which may include forecasting resource needs, recruiting, hiring, performance management, training and budgeting. You will also collaborate with managers and supervisors in your organization to ensure staff selections align with current and future needs
• Executes Performance Management cycle responsibilities (reviews, salaries and incentives). Formulates development plans and roadmaps for new and incumbent staff to help them succeed at their job responsibilities
• Assists with the development and/or monitoring of workflow, procedures, and metrics to track employee and department productivity, gathers and analyzes statistics and makes recommendations for performance improvements.

This Role Requires:

Required Skills and Experience
• 8+ years of hands-on Experience with crafting and building large scale data pipelines in distributed environments with technologies such as Hadoop, Spark, Kafka, Hive etc.
• Experience with NoSQL datastores like Cassandra, Elasticsearch, HBase, MongoDB.
• Proven skills in designing, tuning & optimization scalable, highly available distributed systems which can handle high data volumes.
• Strong understanding of software engineering principles and fundamentals including data structures and algorithms.
• Strong experience of working with API’s and integrating multiple applications together.
• Proficient & hands-on in Python is a must.
• Good Experience with modern engineering practices and technologies including CICD, Cloud native development, social coding (github), chatbots and more.
• Good data modeling experience to address scale and read/write performance.
• Excellent written and oral communication skills on both technical and non-technical topics.
• Excellent general analytical & problem solving skills.
• Experience with cloud computing platforms like AWS or GCP is a plus.

Educational Qualifications
• Bachelor/Master’s degree in Computer Science, Computer Engineering, quantitative studies, such as Statistics, Math, Operation Research, Economics and Advanced Analytics

About Uplers

Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant direct placement onsite opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. You will also be assigned to a dedicated Talent Success Coach during the engagement.

How to apply for this opportunity?
• Register or login on our portal & fill out the application
• Clear the assessment(s) required and apply for the opportunity
• Once completed, our team will contact you with an interview with our SME/Matcher
• Once it’s all done, your profile will be shared with the client
• When selected, just meet the client and get your exciting career started!

(Note: There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Verizon,Senior Engineer Consultant-Data Engineering,"Job # 621579

When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

At Verizon, we are on a journey to industrialize our Data and AI capabilities. Very simply, this means that Data & AI will fuel all decisions and business processes across the company. With our leadership in bringing the 5G network nationwide, the opportunity for Data & AI will only grow exponentially in going from enabling billions of predictions to possibly trillions of predictions that are automated and real-time.

As a Lead Data Engineer in the Artificial Intelligence and Data organization (AI&D), you will drive various activities including data engineering, data operations automation, data frameworks, and platforms to improve the efficiency, customer experience, and profitability of the company.
• Working with stakeholders to understand requirements and effectively prioritizing for implementation.
• Building high-quality Data engineering solutions using data storage technologies, distributed file systems, data processing, and business intelligence best practices.
• Designing, building, and launching new data engineering solutions and data pipelines.
• Working Data Ingestion, Preparation, and Transformation.
• Developing reusable Data Management solutions.
• Processing large amounts of structured and unstructured data, including integrating data from multiple sources.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You’re curious about data and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise in Data Engineering to solving business problems. You thrive in a fast-paced, innovative environment working as a phenomenal teammate to drive the best results and business outcomes.

You’ll Need To Have
• Bachelor’s degree or four or more years of work experience.
• Four or more years of relevant work experience.
• Experience in working on GCP Storage, Big Query, Data Proc, Data Flow, and Composer
• Experience in complex SQL, and DevOps.
• Experience in designing, building, and deploying production-level data pipelines using tools from the Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, Oozie, etc.) and programming in Scala/Python.

Even better if you have one or more of the following:
• Master’s degree in Computer Science, Information Systems.
• Database experience in Teradata SQL, Teradata Utilities, and NoSQL.
• Big Data engineering certifications on Google Cloud.
• Knowledge of telecom architecture.
• Experience in cross-team collaboration, interpersonal skills/relationship building.
• Good analytical ability to quickly debug application problems and provide short and long-term solutions.
• Ability to effectively communicate through presentation, interpersonal, verbal and written skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.

Job Family: TEC

Business Unit: VZBIN",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LTIMindtree,GCP Data Engineering POD Lead,"Primary Skill – GCP Data Engineering POD Lead

Total Exp – 3 to 14 Years

Notice Period – 0 to 30 Days

Job Location – Kolkata, Bangalore, Mumbai, Pune, Chennai, Hyderabad

Job Description:

Job Description:

TPrimary Skill – GCP

Secondary Skill – Python, Big query

Overall, more than 8+ Yrs of experience in Data Science Statistical Modeling and Projects to Develop and Deliver Data Science work Strong understanding of Machine Learning Statistics fundamentals Technology Skill Set Python R Pandas Scikit Learn R s

Desired Candidate Profile Technology & Engineering Expertise

• 5+ years of experience in implementing data solutions using GCP/SQL programming

• Proficient in dealing data access layer, RDBMS | NO-SQL.

• Experience in implementing and deploying Big data applications with GCP Big Data Services.

• Good to have SQL skills.

• Experience with different development methodologies (RUP | Scrum | XP) Soft skills

• Able to deal with diverse set of stakeholders

• Proficient in articulation, communication, and presentation

• High integrity

• Problem solving skills & learning attitude

• Team player Key Responsibilities

• Implement data solutions using GCP and need to be familiar in programming with SQL/python.

• Ensure clarity on NFR and implement these requirements.

• Work with Client Technical Manager by understanding customer’s landscape & their IT priorities

• Lead performance engineering and capacity planning exercises for databases",,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Gartner,Sr. Software Engineer - Data Engineering,"About The Role

Gartner is looking for a Sr. Software Engineer focusing on data engineering. This person will be a part of team developing and supporting Gartner ‘s client facing experience. This includes supporting all the data related operations within the team. This position will provide an opportunity to work with on several different technologies focusing on data modelling , data services and data science. This includes building and consuming web services, integrating search technologies, and building personalization , recommendation engines, data engineering best practices integration.

What You Will Do
• Participate in architecture design and implementation of high-performance, scalable, and optimized data solutions.
• Good Sql understanding with ability to create the data model from scratch.
• Help write and optimize in-application SQL statements.
• Ensure performance, security, and availability of databases.
• Prepare documentations and specifications
• Handle common database procedures such as upgrade, backup, recovery, migration, etc.
• Profile server resource usage, and optimize and tweak as necessary
• Design, build and automate the deployment of data pipelines and applications to support data scientists and researchers with their reporting and data requirements.
• Integrate data from a wide variety of sources, including on premise databases and external data sources with rest APIs and harvesting tools.
• Collaborate with internal business units and data science teams on business requirements, data access, processing/transformation and reporting needs and leverage existing and new tools to provide solutions.
• Effectively support and partner with businesses on implementation, technical issues, and training on the datalake ecosystem.
• Work with team on managing AWS resources (EMR, ECS clusters, etc.) and continuously improve deployment process of our applications
• Work with administrative resources and support provisioning, monitoring, configuration, and maintenance of AWS tools.
• Promote the integration of new cloud technologies and continuously evaluate new tools that will improve the organization’s capabilities while leading to lower total cost of operation.
• Support automation efforts across the data analytics team utilizing Infrastructure as Code (IaC) using Terraform, Configuration Management, and Continuous Integration (CI) / Continuous Delivery (CD) tools such as Jenkins.
• Work with the team to implement data governance, access control and identify and reduce security risks.

What You Will Need
• 5-7 years of experience in Solution, Design and Development of Cloud based data models, ETL Pipelines and infrastructure for reporting, analytics, and data science.
• Experience working with both structured and unstructured data.
• Strong proficiency with SQL and its variation among popular databases
• Experience with some of the modern relational databases
• Skilled at optimizing large complicated SQL statements
• Knowledge of best practices when dealing with relational databases
• Capable of configuring popular database engines and orchestrating clusters as necessary
• Ability to plan resource requirements from high level specifications
• Capable of troubleshooting common database issues.
• Experience working with Spark, Hive, HDFS, MR, Apache Kafka/AWS Kinesis
• Experience with version control tools (Git, Subversion)
• Experience using automated build systems (CI/CD)
• Experience working in different programming languages (Java, python, scala)
• Experience of Data Structures and algorithms
• Knowledge of different databases technologies (Relational, NoSQL, Graph, Document, Key-Value, Time Series, etc…). This should include building and managing scalable data models.
• Knowledge of ML model deployment
• Knowledge of Cloud based platforms (AWS)
• Knowledge of TDD/BDD
• Strong desire to improve upon their skills in software development, frameworks, and technologies

Who are we?

Gartner delivers actionable, objective insight to executives and their teams. Our expert guidance and tools enable faster, smarter decisions and stronger performance on an organization’s most critical priorities. We’ve grown exponentially since our founding in 1979 and we're proud to have nearly 16,000 associates globally that support our 14,000+ clients in more than 100 countries.

What makes Gartner a great place to work?

Our teams are composed of individuals from different geographies, cultures, religions, ethnicities, races, genders, sexual orientations, abilities and generations. We believe that a variety of experiences makes us stronger—as individuals, as communities and as an organization. That’s why we're recognized worldwide as a great place to work year after year. We've been recognized by Fortune as one of the World’s Most Admired Companies, named a Best Place to Work for LGBTQ Equality by the Human Rights Campaign Corporate Equality Index and a Best Place to Work for Disability Inclusion by the Disability Equality Index. Looking for a place to turn your big ideas into reality? Join #LifeAtGartner

What we offer:

Our people are our most valuable asset, so we invest in them from Day 1. When you join our team, you’ll have access to a vast array of benefits to help you live your life well. These resources are designed to support your physical, financial and emotional well-being. We encourage continued personal and professional growth through ongoing learning and development opportunities. Our employee resource groups, charity match and volunteer programs keep you connected to your internal Gartner community and causes that matter to you.

The policy of Gartner is to provide equal employment opportunities to all applicants and employees without regard to race, color, creed, religion, sex, sexual orientation, gender identity, marital status, citizenship status, age, national origin, ancestry, disability, veteran status, or any other legally protected status and to affirmatively seek to advance the principles of equal employment opportunity.

Gartner is committed to being an Equal Opportunity Employer and offers opportunities to all job seekers, including job seekers with disabilities. If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access the Company’s career webpage as a result of your disability. You may request reasonable accommodations by calling Human Resources at +1 (203) 964-0096 or by sending an email to ApplicantAccommodations@gartner.com.

Job Requisition ID:76665

By submitting your information and application, you confirm that you have read and agree to the country or regional recruitment notice linked below applicable to your place of residence.

Gartner Applicant Privacy Link: https://jobs.gartner.com/applicant-privacy-policy

For efficient navigation through the application, please only use the back button within the application, not the back arrow within your browser.",Gurugram,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
