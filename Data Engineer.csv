employer_name,job_title,job_description,job_city,Python,R,SQL,Java,Scala,C++,JavaScript,Excel,Power BI,Tableau,SAS,Apache Spark,Redshift,BigQuery,Airflow,Snowflake
Mastercard,Software Engineer II | Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Software Engineer II | Data Engineer

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Overview
The Enterprise Data Solutions team is looking for a Big Data Engineer to drive our mission to unlock potential of data assets by consistently innovating, eliminating friction in how users access data from its Big Data repositories and enforce standards and principles in the Big Data space. The candidate will be part of an exciting, fast paced environment developing Data Engineering solutions in the data and analytics domain.

Role
• Develop high quality, secure and scalable data pipelines using spark, Scala/ python on Hadoop or object storage.
• Leverage new technologies and approaches to innovate with increasingly large data sets.
• Drive automation and efficiency in Data ingestion, data movement and data access workflows by innovation and collaboration.
• Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency.
• Perform assigned tasks and support production incidents.

All About You
• 4+ years of experience in Data Warehouse related projects in product or service-based organization
• Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
• Experience of building data pipelines through Spark with Scala/Python/Java on Hadoop or Object storage
• Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
• Experience of working on Nifi will be an added advantage
• Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
• Strong communication skills - both verbal and written
• Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
• Flexibility to work as a member of a matrix based diverse and geographically distributed project teams

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Plume Design,Senior Data Engineer,"Plume’s Cloud Platform team is looking for engineers to build and operate data pipelines that power the gamut of Plume products and analytics. Due to the massive scale and performance requirements of many of our use cases, you will be solving challenging problems on a daily basis using a variety of cutting edge technologies.

What you will do:
• Interact with stakeholders to gather and understand data requirements
• Design and implement data pipelines with high data quality goals
• Maintain up-to-date documentation of data warehouse schemas
• Write clean, maintainable code, and perform peer code-reviews
• Refactor code as needed to improve performance and simplify operations
• Provide production support in triaging and fixing issues relating to data quality and availability
• Mentor and assist junior team members and new hires to become successful and productive
• Adhere to data protection requirements including data access, retention, residency and de-identification
• Play an integral role in driving the technology roadmap and enhancing best practices

What You’ll Bring
• Education Requirements: BS/MS/PhD in Computer Science, Electrical Engineering or related technical field
• 5+ years of software development experience with a proven track record of building, scaling, and supporting production data pipelines
• High proficiency in writing idiomatic code, preferably in Java or Scala
• High proficiency in writing SQL in data warehousing technologies
• Strong understanding of large-scale data processing technologies, e.g. Apache Spark (preferred) or Apache Flink
• Strong understanding of data warehousing concepts
• Strong analytical and problem-solving skills
• Strong oral and written communication skills

Plume Design focuses on Internet Service Providers and Cloud Data Services. Their company has offices in Palo Alto. They have a mid-size team that's between 51-200 employees. To date, Plume Design has raised $37.5M of funding; their latest round was closed on June 2017.

You can view their website at https://platform.plume.com or find them on Twitter, Facebook, and LinkedIn.",Hyderabad,False,False,True,True,False,False,False,False,False,False,False,True,False,False,False,False
Lilly,Data Engineer - (DT) Business Insights & Analytics,"At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 35,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world.

Business Insights and Analytics: Data Engineer

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease. We’re looking for people who are determined to make life better for people around the world.

The LCCI (Lilly Capability Center India), BI&A (Business Insights & Analytics) team was started in 2017 with the objective of supporting business decisions for the commercial and marketing functions in the US and ex-US affiliates. This team is part of the LCCI - Commercial Services organization and works very closely with business analytics team based in Indianapolis (HQ). The team currently comprises of more than 100 staff members, with varied backgrounds and skills across data management, analytics and data sciences, business and commercial operations etc.

To better meet the evolving analytics needs, the LCCI BI&A team is ramping up the data engineering pillar. We are looking for data engineers who can be play integral role in developing, maintaining, and testing infrastructures for data generation, processing and storage; work closely with data scientists and help architecting solutions with the objective of driving right KPIs for the business.

Core Responsibilities:
• Create and maintain optimal data pipeline architecture ETL/ ELT into Structured data
• Assemble large, complex data sets that meet functional / non-functional business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.
• Expert level experience creating Fact tables, Dimensional tables and ingest datasets into Cloud based tools. Job Scheduling, automation experience is must.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Setup and maintain data ingestion, streaming, scheduling, and job monitoring automation. Connectivity between Lambda, Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and “big data” technologies like AWS and Google
• Build analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics
• Work with cross-functional teams including external consultants and IT teams to assist with data-related technical issues and support their data infrastructure needs
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

Experience Required
• 4-8 years of in-depth hands-on experience in data warehousing (Redshift or any OLAP) to support business/data analytics, business intelligence (BI)
• Advanced knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouses
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience building and optimizing “big data” data pipelines, architectures and data sets
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores
• Strong analytical and problem-solving skills to be able to structure and solve open ended business problems (pharma experience is highly preferred)

Education
• Bachelor’s/ Master’s degree in Technology OR Computer Sciences

Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response.

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,True,False,False,True
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications

• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
GE,Senior Data Engineer,"Job Description Summary
GE HealthCare is on a transformational journey leveraging Data and Analytics to drive business growth. GE HealthCare is looking for Senior Data Engineer who will be responsible for building and implementing the data ETL pipelines for Finance function data (from data ingestion to consumption).The Data Engineering team helps solve our customers' toughest challenges leveraging data and analytics. The Senior Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE HealthCare to drive business analytics to a new level of predictive analytics while leveraging On-prem, Cloud Platform, Big data tools and technologies.

GE HealthCare is a leading global medical technology and digital solutions innovator. Our purpose is to create a world where healthcare has no limits. Unlock your ambition, turn ideas into world-changing realities, and join an organization where every voice makes a difference, and every difference builds a healthier world.

Job Description

In this role you will:
• Responsible for building data and analytical engineering solutions with standard end to end design & ETL patterns, implementing data pipelines, data modelling and overseeing overall data quality.
• Responsible to work with cross functional teams in GEHC to make the data usable for functional users, data scientists and application users to enable delivery of business values to customers.
• Responsible to enable access of data in AWS storage layers and transformations in AWS Datawarehouse and further transporting in respective databases, consumers, data marts etc.
• As a Senior Data Engineer, you will be part of a data engineering or cross-disciplinary team on Finance facing development projects, typically involving large, complex data sets. These teams typically include data engineers, data visualization engineers, architects, data scientists, product managers, and end users, working in cohorts with partners in GE business units.
• Implement Data warehouse entities with common re-usable data model designs with automation and data quality capabilities.
• Demonstrate proficiency at industry standard data modeling tools (e.g., Erwin, ER Studio, etc.).
• Integrate domain data knowledge into development of data requirements.
• Develop processing codebase using pySpark and implement medium to complex transformations, business logics.
• Look across multiple systems, understands the purpose of each system and defines data requirements by systems.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Lead other horizontal improvement initiatives to benefit technology and leap further on a problem area or Hackathon etc
• Establish and maintain as a trusted advisor relationship within GE Healthcare Data & Analytics (Finance Function)
• Establish and maintain close working relationships with teams responsible for delivering solutions to the businesses and functions
• Engage collaboratively with project teams to support project objectives through the application of sound data engineering principles
• Identify risks and assumptions for the in scope Data & Analytics solutions
• Work with the contract/vendor resources to deliver the solution and manage the technical resources work

Qualifications
• Bachelor's Degree in Computer Science, Information Technology or equivalent (STEM)
• A minimum of 6 year of similar experience working on Database(s), SQL, Python, Datawarehouse, Java, ETL and AWS cloud platform is required. AWS certifications would be added advantage
• Experienced in Deployment process on-prem and on-cloud using Kubernetes, Dockers, Jenkins
• Ability to drive projects in big data (structured/unstructured/machine/logs/streaming data types)
• 3+ Year of Data modelling & Data warehousing experience with MPP systems (Teradata, Netezza, Greenplum etc.)
• 3+ years in AWS Services Like Redshift, RDS, S3, Glue, Step Function, Lambda etc.
• Hands on experience in delivering analytics in modern data architecture (Massively Parallel Processing Database Platforms and Semantic Modelling)
• Demonstrable knowledge of ETL and ELT patterns and when to use either one; experience selecting among different tools that could be leveraged to accomplish this. (i.e. Informatica, HVR, Talend etc)
• Demonstrable knowledge of and experience with different scripting languages (python, shell)
• Understands data quality and solves for application-level needs
• Understanding of DaaS, Data management tools / solutions
• Strong verbal & written communication
• Experience working with solutions delivery teams using Agile/Scrum ore similar methodologies
• Added advantage if experienced in working on Finance data

Desired skills:
• Delivers results when working on shorter-term (weeks-months), outcome-focused service engagements
• Proactively learning new technology, predicts trends, and identifies new opportunities based on trends
• Leverages knowledge about technology trends, and changing business needs across the broad environment to bring new ideas to the team
• Articulates the value proposition of existing technology capabilities and maps them to customer requirements to minimize incremental cost of development
• Experienced in working with On-prem (Teradata) data warehouse – Dimensional and data modelling. Experienced in one of the ETL like Informatica.
• Identifies the customer’s business and strategic needs, concerns, and desires for the value delivery capabilities of the Product
• Functional understanding of finance - Close Book, Treasury, Cash, Controllership, Credit, Account Payables, Account Receivables, Cash Forecasting, Balance sheet exposure, Debt, Forex etc.

Inclusion and Diversity

GE Healthcare is an Equal Opportunity Employer where inclusion matters. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

We expect all employees to live and breathe our behaviors: to act with humility and build trust; lead with transparency; deliver with focus, and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.
#LI-Hybrid
#LI-GM2

Additional Information

Relocation Assistance Provided: Yes",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,False
Concinnity Media Technologies,Senior Data Engineer,"Preferred Experience:

• 8+ years’ experience building mobile, web and/or API-based applications

• Follow engineering standards and best practices

• Knowledge of databases: MySQL, PostgreSQL, SQL, etc...

• 4+ years of Python server development experience

• Django, Flask, Bottle, or similar framework experience

• 2+ years industry experience

• Knowledge of cloud deployment strategies using AWS, Azure, Rackspace, etc.

• Expertise working with and building RESTful APIs

• Ability to operate in Agile / Scrum development environments

• Understanding of OOP and Data Structures and know when to apply them in daily coding scenarios

• Knowledge in the following web-technologies:
• JavaScript
• HTML & HTML5
• CSS3
• JavaScript frameworks (React, Angular, Next.js, etc.)

• Understand the development of the following:
• Responsive Web Development
• Accessibility
• Secure web applications

• Message queue implementations (RabbitMQ, ZeroMQ, Kafka, etc.)

• Background task processing (Celery, etc.)

• Experience configuring container like systems (Vagrant, Docker, etc.)

• Container orchestration with Kubernetes

• Ability to self-organize with minimal guidance/competing priorities and work effectively within a team

• Ability to provide innovative, creative solutions to tasks/problems

• Ability to complete work following engineering standards and best practices

• Experience with GIT and Gitlab is a plus

• Experience with JSON is a plus",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Hewlett Packard Careers,Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.
We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

The Data Engineer will develop, test, and maintain Big Data solutions for a company. Gather large amounts of data from multiple sources and ensure that downstream users can access the data quickly and efficiently. Essentially, the company’s data pipelines are scalable, secure, and able to serve multiple users.

Job description
• Meeting with managers to determine the company’s Big Data needs.
• Developing Hadoop systems.
• Loading disparate data sets and conducting pre-processing services using Spark, Hive or Pig.
• Finalizing the scope of the system and delivering Big Data solutions.
• Managing the communications between the internal system and the vendor.
• Collaborating with the software research and development teams.
• Building cloud platforms for the development of company applications.
• Maintaining production systems.
• Training staff on data management.

Big Data Engineer Requirements:
• Bachelor’s degree in computer engineering or computer science.
• Previous experience as a big data engineer.
• In-depth knowledge of Hadoop, Spark, and similar frameworks.
• Knowledge of scripting languages is preferred .
• Knowledge of NoSQL and RDBMS databases including Redis and MongoDB.
• Familiarity with Mesos, AWS, and Docker tools.
• Excellent project management skills.
• Good communication skills.
• Ability to solve complex data, and software issues.

Education and Experience Required:
• Typically, 6+ years of progressive professional experience as a big data engineer.
• Bachelor’s degree in computer engineering or computer science.

We love our work environment. We think you will too:
• It’s a friendly atmosphere with supportive leaders to bring your creativity to the max.
• Work-life balance support including flex-time arrangements and work from home opportunities.
• Corporate Social Responsibility initiatives to help you make an impact to communities at large.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Omnivio,Data Engineer - Full Time,"Job Profile

Omnivio is a startup in the Supply Chain and Logistics domain. We help retailers deliver an 'Amazon like' shopping experience to their customers. We optimize and manage delivery times and cost, inventory, geo-distributed stores etc. One of the core pieces of our infrastructure is the data engineering required to get data from various upstream systems into a data model that we use for intelligence. If you've worked in data engineering before, you might imagine that this has a good number of challenging engineering problems. We are looking for junior to mid level data engineers to join our team.

Experience / Skills required

We are looking for previous experience in data engineering for this role. Here's a list of tools and technologies that you can expect to be working with in this job. The listed examples are not necessarily all a part of our stack, but they are solid indicators of your skills being a good fit for the job.
• Relational Databases, both OLTP and OLAP, such as MySQL, Postgres, Redshift, BigQuery, CLickhouse etc
• Solid software engineering fundamentals, and experience with one or more general purpose programming languages such as Python, Typescript
• Data engineering programming libraries such as Pandas, NumPy etc
• Building data engineering pipelines using orchestration tools such as Airflow, Airbyte, Temporal, or other commercial offerings
• Transformations using dbt, or a similar alternative
• Experience with AWS's data engineering stack is definitely a plus
• Deployment/operating experience with any of these tools would be really interesting too

Work culture
• No ego anywhere in the team, including higher management.
• Remote, asynchronous, flexible work timings.
• Collaboration and team-thinking. No single person owns the failure.
• Ample time and attention to help developers level up.

Work Ex - 3+ years

Omnivio focuses on Supply Chain Management, Logistics, Cloud Infrastructure, Logistics Software, and Logistics / Transportation / Shipping. Their company has offices in Noida. They have a small team that's between 11-50 employees. To date, Omnivio has raised $400k of funding; their latest round was closed on July 2022 at a valuation of $5M.

You can view their website at https://omnivio.io or find them on LinkedIn.",,True,False,True,False,False,False,False,False,False,False,False,False,True,True,True,False
Mercedes-Benz Research and Development India Private Limited,Big Data Engineer,"AufgabenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team playerQualifikationenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team player",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Confidential,Data Engineer - SQL/Data Pipeline,"Job Description : : - Hands on working knowledge on building and optimizing 'big data' data pipelines, architectures, and data sets- Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases- Strong know-how on data ingestion tools and APIs to prioritize data sources, validate them, and dispatch data to ensure an effective ingestion process. Knowledge on data ingestion tools such as Apache Kafka/ Apache Storm/Apache Flume/Apache Sqoop/Wavefront, and more.- Working knowledge on data mining tools such as Apache Mahout/KNIME/Rapid Miner/Weka,- Strong know-how on ETL tools such as Talend/Informatica PowerCenter/AWS Glue/Stitch,- Ability to handle various types of data in the form of text, speech, image, video, or live stream from IoT/ Sensors/ Web- Ability to manipulate, process and extract value from large, disconnected datasets and articulate the same in business contextPreferred : - Skilled in the use of business intelligence and visualization tools, such as PowerBI, Tableau - Experience with stream-processing systems such as Storm, Spark-Streaming, etc.- Ability to leverage MLOps Platforms such as Teraform, Ansible, Kubeflow, Google AI Platform- Know-how on software development languages such as Python, Java, C++, Scala, etc (ref:hirist.com) IT",,True,False,True,True,False,True,False,False,False,True,False,False,False,False,False,False
NIRA,Sr Data Engineer/Architect (3y-7y),"About the job

Starting with credit, NIRA aspires to be the pre-eminent financial brand for the mass market or ""Middle India"". We already have customers in over 5,000 towns and cities, and we're growing quickly (15% MoM for the last 20 months!). It's a very exciting time to join us. We have over 200+ employees.

Currently, only 10% of Indians can use banks when they need credit: banks typically require a high credit score or collateral, something most people don't have. It need not be this way. Using a combination of traditional data and the vast amount of digital data available, it is now possible to score the unscored.

Today, we receive 15000 new loan applications daily from across 4000 cities in India, and we are growing 15-20% MoM. People reach us at their time of need, and we offer them credit via our app. Money reaches their bank account within 24 hrs of application.

We are addressing head-on a big challenge. It's also a great opportunity from both a commercial and societal impact perspective. We can improve lives for millions. It is no exaggeration to say that our addressable market will be 400mm within 5 years. It's pretty exciting, we think.

If our mission resonates with you, and you are a talented and hardworking individual that wants to commit yourself to an incredible challenge, then we want to hear from you.

As one of the senior data engineers on the team, you’ll be working on our core data platform and infrastructure powering business decisions and data science workloads.

Job Overview

We are looking for an experienced Data Engineer to join our engineering team. The hire will be responsible for building our data and data pipeline architecture. You will optimise our data flow starting with the collection of data for cross functional teams and purposes. You will support our key data science initiatives while maintaining consistency of data delivery architecture throughout ongoing projects. Ideal candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities

• Create and maintain optimal data pipeline architecture

• Assemble large, complex data sets that meet performance and business requirements

• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies

• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, credit risk, operational efficiency and other business KPIs.

• Create data tools for analytics and data scientist team members that assist them in building and scaling our core products

• Work with cross domain data and analytics experts to strive for stronger data driven outcomes for the business.

Qualification

• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science. They should also have experience using the following software/tools:

• Experience with big data tools: Hadoop, Spark / PySpark, Kafka.

• Experience with relational SQL and NoSQL databases.

• Experience with object-oriented/object function scripting languages: Python.

• Expertise in data modelling and buiding data driven systems.

• Well versed with Shell Scripting.

• Hands on experience on various AWS services like EMR,EC2,Glue.

• Deploy existing data projects using CICD pieplines.

• Knowledge on Docker is a plus.

What we offer:

• Competitive salary

• Medical Insurance

You can learn more about NIRA here:

Press:

https://yourstory.com/2019/05/startup-fintech-nira-entrepreneur-loans/

https://www.livemint.com/companies/news/muthoot-finance-partners-with-nira-to-provide-personal-loans-11620309871295.html

https://www.financialexpress.com/money/personal-loan-collection-rates-return-to-pre-covid-levels-data-from-nira-reveals/2313170/

NIRA focuses on Consumer Lending and Fin Tech. Their company has offices in Bengaluru. They have a large team that's between 201-500 employees. To date, NIRA has raised $3.1M of funding; their latest round was closed on April 2020.

You can view their website at https://www.nirafinance.com or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Referrals Only,Consultant-Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.
Job responsibilities• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will collaborate with Data Scientists in order to design scalable implementations of their models
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Assure effective collaboration between Thoughtworks' and the client's teams, encouraging open communication and advocating for shared outcomes
Job qualificationsTechnical skills• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Professional skills• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more
Other things to knowL&DThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.
About ThoughtworksThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 28+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.

Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
ANI Calls India Private Limited,Senior Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Bengaluru, India
Candidate should have:
Worked collaboratively with cross-functional teams and stakeholders to achieve an organizational goal.
Worked in an agile environment and are comfortable running an agile process for the data and analytics team.
Strong experience in data pipelines, ETL design (both implementation and maintenance), data warehousing, and data modeling (preferably in dbt).
Implementation and tuning experience in the Big Data Ecosystem,
(such as Data Analytics (Dataproc, Airflow, Hadoop, Spark, Hive),
Google Cloud Platform AI and ML Services and Data Warehousing (such as BigQuery, schema design,
query tuning and optimization) and data migration and integration.
End to end hands-on to carry out complex POC, Pilot, Limited production rollout, assignments requiring the development of new or improved techniques and procedures.
Participated in deep architectural discussions to build confidence and ensure customer, success when building new, or migrating existing, applications, software, and services on the Google Cloud Platform.
advanced skills in SQL, data modeling, ETL/ELT development, and data warehousing.
Strong skills in Optimization - performance, pipeline, spark.
Experience on Pyspark.
5+ years of design & implementation experience with distributed applications.
5+ years of experience architecting/operating solutions built on Google Cloud Platform.
. Bachelor's degree.

Experience: 5.00-10.00 Years",,False,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
deloitte,Consulting- SAMA- A&C-Azure Data Engineer- AD,"JD:
• Location - Mumbai OR Pune
• Experience range:
• 12-15 yrs for AD
• Strong experience in Python programming and related skills like PySpark
• Strong SQL skills
• Strong experience with any of the data engineering platforms like Hadoop, Spark, Synapse, Databricks, Apache Airflow, etc.
• Preferred: Knowledge of any cloud platform like Azure/AWS/Google
• For AD level: Need technology leadership experience in terms of architecture/solutioning and team leading",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Comcast,"Data Engineer, Data Products Engineering","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary INTRODUCTION: At Comcast, we believe in the talent of our people. It's our passion and commitment to excellence that drives Comcast's vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It's what makes us uniquely Comcast. Here you can create the extraordinary. Join us. ABOUT THE ROLE: Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered Job Description Responsibilities Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc) 2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas. Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications. Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data. Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time. REQUIREMENTS: Bachelor's degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience. Strong Computer Science/Engineering/Information Systems background 3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies. Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas Strong problem-solving skills. Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences. PREFERRED QUALIFICATIONS: 1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills Ability to design concise and visually appealing reports, user interfaces, mockups and documentation Ability to read external API documentation and write pipelines to extract data from our partners systems Ability to write and stand up internal API endpoints to share data with other internal teams. Strong analytical focus, results-oriented and execution driven. Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures. Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed. Flexibility to adjust to changing requirements, schedules and priorities. Ability to work independently under minimum supervision and proactive in solving issues Energetic, committed and solution focused with the ability to perform under pressure and meeting targets Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Relevant Work Experience 2-5 Years Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,False,True
Versor Investments,Data Engineer,"India

Versor Investments (“Versor”) is a quantitative investment boutique headquartered in Midtown Manhattan. The Firm currently has an AUM of $1.8 billion*. Versor creates diversified sources of absolute returns across multiple asset classes. Within a scientific, hypothesis-driven framework, Versor leverages modern statistical methods and vast datasets to drive every step of the investment process. Alpha forecast models, portfolio construction, and the trading process rely on the ingenuity and mathematical expertise of 60+ investment professionals. Versor offers two categories of investment products – Hedge Funds and Alternative Risk Premia. Both are designed to provide superior risk-adjusted returns while exhibiting low correlation to traditional and alternative asset classes. Each invests in liquid, scalable markets. On average, Versor’s partners have spent over 20 years researching, investing and trading systematic alternative investment strategies.

Role Summary

The Data Engineer position will be based in Mumbai and be part of the Portfolio Analytics team. They will collaborate closely with senior researchers to design and develop a large-scale data lake. We are seeking candidates who have excelled in engineering (specifically computer science). Prior experience in investments and finance is beneficial but not mandatory.

This role is ideal for candidates who are passionate about technology and excited about building a data platform.

Responsibilities
• Design architecture for a data platform.
• Design data pipelines based on business and functional requirements.
• Extract, transform, and load logic to automate data collection and manage data processes/pipelines. This includes data quality and monitoring.
• Develop data access tools to allow researchers to access data seamlessly.
• Develop integration tools and analytical reports for the databases and data warehouse.
• Write and review technical documents. This includes requirements and design documents for existing and future data systems, as well as data standards and policies.
• Collaborate with analysts, support/system engineers, and business stakeholders to ensure data infrastructure meets constantly evolving requirements.

Requirements
• E., B.Tech., M.Tech., or M.Sc. in Computer Science, Computer Engineering or similar discipline from a top tier institute.
• 2+ years direct experience working as a data engineer.
• Experience in design, architecture and implementation of data lake, data pipelines and flows.
• Experience with developing software code and APIs in one or more languages such as Python and C#.
• Experience designing and deploying large scale distributed data processing systems with one or more technologies such as MS SQL Server, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, Hive, Teradata, or MicroStrategy.
• A high-level understanding of automation in a cloud environment (AWS experience preferred).
• Excellent communication, presentation, and problem-solving skills.
• Data as of December 31, 2022. AUM reflects regulatory AUM as per SEC definition for the purposes of Item 5.F on the Form ADV Part 1a.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tredence Inc.,Senior Data Engineer,"Associate Manager – Data Engineering (8-11 Years)

This position requires someone with good problem solving, business understanding and client presence.

Overall professional experience of the candidate should be above 8 years. A minimum of 4 years of experience in Data Engineering space. Should have good understanding of business operations, challenges faced, and business technology used across business functions.

The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.

Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.

Role Description:
• Engages with Leadership of Tredence' s clients to identify critical business problems, define the need for data engineering solutions and build strategy and roadmap
• S/he possesses a wide exposure to complete lifecycle of data starting from creation to consumption
• S/he has in the past built repeatable tools / data-models to solve specific business problems
• S/he should have hand-on experience of having worked on projects (either as a consultant or with in a company) that needed them to –
• Provide consultation to senior client personnel
• Implement and enhance data warehouses or data lakes.
• Worked with business teams or was a part of the team that implemented process re-engineering driven by data analytics / insights
• Should have deep appreciation of how data can be used in decision making
• Should have perspective on newer ways of solving business problems. E.g. external data, innovative techniques, newer technology
• S/he must have a solution creation mindset. Ability to design and enhance scalable data platforms to address the business need
• Working experience on data engineering tool for one or more cloud platforms -Snowflake, AWS/Azure/GCP
• Engage with technology teams from Tredence and Clients to create last mile connectivity of the solutions -
• Should have experience of working with technology teams
• Demonstrated ability in thought leadership – Articles/White Papers/Interviews

Mandatory Skills

Program Management, Data Warehouse, Data Lake, Analytics, Cloud Platform

Job Location - Bangalore , Chennai , Pune , Gurugram.

Experience Level - 8-11 yrs.

Expected Joining Time - Immediate to Max 30 days.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
Mercedes-Benz Research and Development India Private Limited,Big Data CoE - Engineering.IT Data Engineer,"TasksOverview
A highly motivated and technically proficient professional, capable of delivering solution on Data Engineering in Cloud platform. He or she must be skilled in developing all rituals for Data Engineering especially using pyspark.
Job Responsibilities
Data Engineer:

· Development, Enhancement, testing and release of existing application.
· Develop and enhance end to end data pipeline.
· Interpret data to analyze results to a specific business problem or bottleneck that needs to be solved using statistical techniques.
· Ensure data efficiency and reliability.
Qualification
Mandatory:
· Bachelor's /post graduate degree in Computer Science, Computer Engineering or Data Science, Data Engineering with strong knowledge in below technologies,
· Pyspark
· Databricks,
· ADLS
· SQL
Desired:
· Work experience on Agile/SDLC process.
· Experience in building Knowledge Base (KB)
· Experience in taking over Knowledge Transfer
· Open to work/explore new technology areas.
· Automotive and Aviation domain is plus.Qualifications",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
D2C Ecommerce India Pvt Ltd,D2C Ecommerce - Data Engineer,"What is D2cecommerce :D2C Ecommerce is India's first multi-D2C brand online platform that sells its own homegrown brands across multiple home and lifestyle categories, including - apparel, cosmetics, beauty, jewelry, accessories, fitness, sports, shoes, bags, books, kitchen, food, auto accessories, electronics, kids and travel packages. Along with selling these products on its own portal, D2CEcommerce also has these items listed on leading e-commerce sites.Job Summary : We are seeking a highly motivated and skilled Data Engineer to join our team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our databases and reports. You will work closely with our team of developers, data scientists, and analysts to ensure that our data systems are accurate, efficient, and scalable.What would be your responsibilities :- Design and develop databases that are scalable, efficient, and accurate- Develop and maintain ETL pipelines to move data from various sources into our databases- Create and manage database reports to provide insights to our team of data scientists and analysts- Collaborate with our team of developers to integrate our databases into our applications and services- Continuously monitor and optimize our databases for performance and security- Ensure that our data systems adhere to industry best practices and compliance regulations- Stay up-to-date with the latest database technologies and trendsWhat are we looking for in the candidate :- Ability to build things from scratch.- Self-starter and motivated individuals who can drive processes on their own- A bachelor's or associate degree in management information systems, computer science, or a related field- 0-1 years of experience in database management or a similar role- 0-1 years of experience designing, developing, and producing database reports- Proficiency in SQL and experience with relational databases such as MySQL, Postgre SQL, or Oracle- Experience with ETL tools and techniques- Understanding of data modelling concepts- Familiarity with database administration and management tools- Strong analytical and problem-solving skills- Excellent verbal and written communication skillsWhy you should join D2cecommerce :In addition to an attractive compensation package, you own a piece of the company through ESOPs. Also you will have the opportunity to take up a role in a rapidly growing, highly disruptive organization, and shape the face of ecommerce for the future.Interested? What to do next :If reading the details above excited you and made you feel you can help us take D2Cecommerece to the next level, all you have to do is let us know!",Gurugram,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Thoughtworks Inc.,Senior Consultant - Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Aryng,Sr. Data Engineer,"Welcome You made it to the job description page

Aryng is looking for a cloud data engineer with experience in developing
enterprise-class distributed data engineering solutions on the cloud. We are seeking
an entrepreneurial and technology-proficient Data Engineer who is an expert in the
implementation of a large-scale, highly efficient data platform, batch, and real-time
pipelines and tools for Aryng clients. This role is based out of India. You will work
closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength
is creative and effective problem-solving.

Key Responsibilities:

● Should have implemented asynchronous data ingestion, high volume stream data
processing, and real-time data analytics using various Data Engineering
Techniques.
● Implement application components using Cloud technologies
and infrastructure.
● Assist in defining the data pipelines and able to identify bottlenecks to enable
the adoption of data management methodologies.
● Implementing cutting edge cloud platform solutions using the latest tools and
platforms offered by GCP, AWS, and Azure.

Requirements
• Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
5+ years of data engineering experience is a must.
• 2+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers
• 2+ years' experience in Python.
• Must be strong in SQL and its concepts.
• Experience in Big Query, Snowflake, Redshift, DBT.
• Strong understanding of data warehousing, data lake, and cloud concepts.
• Excellent communication and presentation skills
• Excellent problem-solving skills, highly proactive and self-driven
• Consulting background is a big plus.
• Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study
Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
This role requires mandatory overlap hours with clients in the US from 8 am - 1
pm PST.

Benefits
• Direct Client Access
• Flexible work hours
• Rapidly Growing Company
• Awesome work culture
• Learn From Experts
• Work-life Balance
• Competitive Salary
• Executive Presence
• End to End Problem Solving
• 50%+ Tax Benefit
• 100% Remote company
• Flat Hierarchy
• Opportunity to become a thought leader

Why Join Aryng: Click on the Youtube link",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,True,True
Emerson,Data Engineer - Sustainability,"AS AN Data Engineer, YOU WILL:

· Architect and design platform solutions to meet and exceed expectations of Projects.

· Proactively evolve and apply DevSecOps methodologies, standards and leading practices

· Apply architectural standards/principles, security standards, usability design standards, as approprioate.

· Lead a project from delivery perspective, including giving periodic updates to all stakeholders.

Skills Requirements:

· Must be able to communicate fluently in English, both written and verbal

· Excellent interpersonal communication and organizational skills

· Able to distil complex technical challenges to actionable and explainable decisions

· Inspire DevSecOps teams by building consensus and mediating compromises when necessary

· Demonstrate excellent technical & architecture skills, service management and product lifecycle management

· Demonstrate ability to rapidly learn new and emerging technologies

· Operational abilities including early life support and driving root cause analysis and remediation

·Any Azure/Microsoft Big Data related certifications is highly preferred.

REQUIRED EXPERIENCE :

· Bachelor’s Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline)

· 10+ years overall IT industry experience

· 3+ years in a solution design role using service and hosting solutions such as private/public cloud IaaS, PaaS and SaaS platforms.

· Large scale design, implementation and operations of OLTP, OLAP, DW and NoSQL data storage technologies such as SQL Server, Azure SQL, Azure SQL DW, PostgreSQL, CosmosDB, RedisCache, Azure Data Lake Store, Hadoop, Hive, MongoDB, MySQL, Neo4j, Cassandra, HBase

· Creation of descriptive, predictive and prescriptive analytics solutions using Azure Stream Analytics, Azure Analysis Services, Data Lake Analytics, HDInsight, HDP, Spark, Databricks, MapReduce, Pig, Hive, Tez, SSAS, Watson Analytics, SPSS

· Design and configuration of data movement, streaming and transformation (ETL) technologies such as Azure Data Factory, HDF, Nifi, Kafka, Storm, Sqoop, SSIS, LogicApps, Signiant, Aspera, MoveIT, Alteryx, Pentaho, IDQ,

· Enablement of data reuse through Data Catalog/Marketplace, Metadata, Search and Governance technologies such as including Azure Data Catalog, Waterline, Apache Atlas, Apache Solr, Azure Search, Alteryx Connect, Datawatch Monarch Swarm, Collibra Catalog, Enigma Councourse, Adaptive, Cambridge Semantics, Data Advantage Group (DAG), Global IDs, Alation

· Experience with any of the following: Azure, O365, Azure Stack, Azure AD

· Delivery using modern methodologies especially SAFe Agile.

Requisition ID : 23000590

Emerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.",Pune,False,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Confidential,Fractal.ai - Azure Data Engineer - SQL/PySpark,"Mandatory Skills :- Azure Databricks (ADB)- Azure DataFactory (ADF)- Python or Pyspark- SQLResponsibilities :- Be an integral part of large scale client business development and delivery engagements- Develop the software and systems needed for end-to-end execution on large projects- Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions- Build the knowledge base required to deliver increasingly complex technology projectQualifications & Experience :- A bachelor's degree in Computer Science or related field with 3-12 years of technology experience- Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space- Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks- Database programming using any flavours of SQL- Expertise in relational and dimensional modelling, including big data technologies Exposure across all the SDLC process, including testing and deployment- Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.- Good knowledge of Python and Spark are required- Good understanding of how to enable analytics using cloud technology and ML Ops- Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus- Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)- Characteristics of a forward thinker and self-starter - Ability to work with a global team of consulting professionals across multiple projects- Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems- Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to success. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LatentView,Principal Data Engineer,"About LatentView:
• LatentView Analytics is a leading global analytics and decision sciences provider, delivering solutions that help companies drive digital transformation and use data to gain a competitive advantage. With analytics solutions that provide 360-degree view of the digital consumer, fuel machine learning capabilities and support artificial intelligence initiatives., LatentView Analytics enables leading global brands to predict new revenue streams, anticipate product trends and popularity, improve customer retention rates, optimize investment decisions and turn unstructured data into a valuable business asset.
• We specialize in Predictive Modelling, Marketing Analytics, Big Data Analytics, Advanced Analytics, Web Analytics, Data Science, Data Engineering, Artificial Intelligence and Machine Learning Applications.
• LatentView Analytics is a trusted partner to enterprises worldwide, including more than two dozen Fortune 500 companies in the retail, CPG, financial, technology and healthcare sectors.

Job Description:
As a Manager - data engineer, they should build and maintain scalable, rock solid self-serve data pipelines for data analysts and data Scientists and support them by understanding the content and context of data and collaborating with them to figure out the best way to Extract, Transform, Load, and access it.
• Be an SME in DE and should know how to set up the process, requirement gathering for any movement or data ingestion and data platform creation
• Has to be the end-to-end project manager, allocating work, monitoring progress, providing feedback, and taking necessary steps to ensure agreed timelines are met
• Scripting skills: SQL and Python or PySpark
• Excellent understanding of Data Warehouse and its architecture
• Excellent understanding of Issue Tracking System like JIRA/ Dev-Ops
• Excellent experience in Version control like Github or Gitlab
• Ideal to have knowledge of other DE platforms like Azure databricks, Snowflake etc.

Education: UGEmployment Type: CONTRACTOR",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
CIEL HR Services,Data engineer,CTC-6LPA EXP-Freshers Area of Expertise - A) Data Hygiene B) Application of ML DL Tools If interested please call 9000338173 or forward cv to [Confidential Information],Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer - ETL,"Design, build, and maintain the data infrastructure that supports our data-
driven applications and services.• Develop and maintain ETL (Extract, Transform, Load) processes to ensure
data accuracy and completeness.
• Optimize data pipeline performance and scalability.
• Collaborate with data scientists, analysts, and developers to ensure that our
data pipeline meets their needs.• Implement data governance policies and procedures to ensure data quality
and consistency.
• Design and develop data models that support data-driven applications and
services.
• Troubleshoot and resolve issues related to the data pipeline.
• Participate in the evaluation and selection of data management and analytics
tools and technologies.
• Keep up to date with emerging trends and technologies in data engineering
and big data.

experience

8",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Swift,Data Engineer,"About us:

Swift is building a next generation checkout stack for India - a platform rolling up payments and logistics solution for all fulfillment needs. We give businesses the opportunity to provide a customer experience at par with the likes of Amazon and Flipkart, all the while saving money and time.

Its basically Amazon without the website listing - we let our sellers design their own sales channel :-)

We believe there are many things a seller or small business has to worry about when selling online, logistics/payments/etc shouldn't be one of them. With our solution, SMBs and D2C brands get access to technologies and services like next day delivery, same day delivery, live package tracking, Card/Cash on delivery, scheduled delivery etc, making parcel delivery just as simple as collecting payment.

We also provide robust APIs which makes it easy for developers to add shipping capabilities to their multichannel online store.

We want to be the #1 checkout platform that’s reliable, easy to use and affordable.

About you:

You have experience in working with data pipelines and ETL sets (programmatically and using tools) – say MongoDB, Spark streaming, Python/Java, Apache Beam, PARQR, Delta Lake, Airflow, etc. You are looking for challenges in growing a data backed company to deal with from hundreds to millions of visitors data points per month.

You like working with streaming/reactive architectures and have experience/interest in setting up data pipelines on cloud infra from scratch. You generally prefer to use a minimal set of simple tools to a diverse range of complex ones. We are looking to build a back-end cloud infrastructure (Google Cloud Platform preferably) which will be a fault-tolerant real-time stream processing system on the cloud - Our system will need to meet liveliness guarantees from a big data/ETL perspective.

You like to work on a variety of projects - at this job, you’ll be developing a complex ETL infra, a reactive streaming architecture and a cloud-native, highly available API for our customers.

You are someone who is:
• Experienced in any JVM based language or Python.
• Have worked on NoSQL (MongoDB)/ SQL databases.
• Have worked on creating data pipelines (both programmatically, say using spark streaming) or using tooling like Airflow, dataflow)
• Strong verbal and written communication skills and the ability to work well cross-functionally.
• We offer: *
• You to be a part of a small, but a super capable team.
• The opportunity to work closely with founders to define, scope, estimate and plan various aspects of the product.
• Being one of the first hires at Swift, you will be involved in both high and low-level decision making. This means a lot of ownership, which we cultivate by having a flat structure.

Swift focuses on E-Commerce, B2B, Small and Medium Businesses, Logistics, and D2C. Their company has offices in Bengaluru. They have a mid-size team that's between 51-200 employees. To date, Swift has raised $2.34M of funding; their latest round was closed on July 2021.

You can view their website at https://goswift.in or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
CarbyneTech India Pvt Ltd,CarbyneTech - Azure Data Engineer - IoT,"- Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions - Azure Synapse/Azure SQL DWH, Azure Data Lake, Azure Blob Storage, Spark, HDInsights, Databricks, CosmosDB, EventHub/IOTHub.- Experience in migrating on-premise data warehouses to data platforms on AZURE cloud.- Designing and implementing data engineering, ingestion, and transformation functions- Azure Synapse or Azure SQL data warehouse- Spark on Azure is available in HD insights and data bricks- Good customer communication.- Good Analytical skill",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Brillio,GCP Data Engineer - R01523647,"About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

GCP Data Engineer
Primary Skills

• BigQuery, Cloud Logging, Cloud Storage, Cloud Trace, Composer, Data Catalog, Data Modelling Fundamentals, Data Warehousing, Dataflow, Datafusion, Dataproc, ETL Fundamentals, Modern Data Platform Fundamentals, PLSQL, T-SQL, Stored Procedures, Python, SQL, SQL (Basic + Advanced)

Specialization

• GCP Data Engineering Basic: Senior Data Engineer

Job requirements

• Job description below. • 6 years of experience in software design and development • 5 years of experience in the data engineering field is preferred • 3 years of Hands-on experience in GCP cloud data implementation suite such as Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage, • Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms. • Mandatory 1 year of software development skills using Python • Extensive hands-on experience working with data using SQL and Python • Cloud Functions. Comparable skills in AWS and other cloud Big Data Engineering space is considered. • Experience in DevOps(CI/CD) pipeline facilitating automated deployment and testing • Experience with agile development methodologies • Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions • Bachelor's Degree in Computer Science, Information Technology, or closely related discipline

Know what it’s like to work and grow at Brillio: Click here",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,True,False
Latent View Analytics Private Limited,Data Engineer,"Job Title :
Data Engineer Experience : 2.5-5 Location : INDIA
• Chennai Job Description: Experience working with Cloud Data Platforms, especially AWS and its services, must be strongly experienced in building data pipelines.
Experience with big data tools like Python, Pyspark, and Spark SQL. Focus on scalability, performance, service robustness, and cost trade-offs.

A continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs.
Attention to detail, coupled with the ability to think abstractly. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product. Keen to learn new technologies and apply the knowledge in production systems.

AWS skills:
AWS Glue AWS EMR (any two AWS services)

Data Engineer Skills:
Python Pyspark Spark SQL HIVE HQL Scala Good to have: Snowflake querying Databricks AWS API Gateway AWS Lambda",Chennai,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,True
Whiteforce,Data Engineer - Java,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20246

LocationIndia

Job Descriptions

Job Purpose and Primary Objectives : Develop and Deploy data and analytics-led solutions on GCP Key responsibilities (please specify if the position is an individual one or part of a team): Data engineering solution on GCP using Cloud Bigquery, Cloud Dataflow, Pu-Sub, Cloud BigTable and AI/Ml solutions Key Skills/Knowledge : - Good Experience in GCP. - Python/Java, PySpark/Spark Java. - GCP BigQuery. - GCP Pub-Sub. - Secondary Skill - DataFlow, Compute Engine, Cloud Fusion. (ref:hirist.com)

Skills",,True,False,False,True,False,False,False,False,False,False,False,False,False,True,False,False
Confidential,Big Data Engineer - Python/AWS,"JOB_DESCRIPTION :- Has experience in the following #Python, #AWS_Athena, #Glue #Pyspark, #EMR, #DynamoDB, #Redshift, #Kinesis, #Lambda, #Snowflake.- Proficient in #AWS_Redshift, #S3, #Glue, #Athena, #DynamoDB.- Design, build and operationalize large-scale enterprise data solutions and applications using one or more of #AWS_data and analytics services in combination with 3rd parties - #PySpark, #EMR, #DynamoDB, #RedShift, #Kinesis, #Lambda, #Glue, #Snowflake. - Analyze, re-architect, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services. - Design and build production data pipelines from ingestion to consumption within a big data architecture, using Python/PySpark. - Design and implement data engineering, ingestion, crawling, manipulation and curation functions on AWS cloud using AWS native or custom programming. - Must have strong knowledge of databases like Postgres, MySQL, MongoDB, Cassandra, etc. - Must have experience in using AWS SDKs and libraries for interacting with different AWS services. - Experience in building REST APIs. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Varite India Private Limited,Data Engineer,"snowflake- Data Engineer Data engineering, integration, and data modeling experience . Can write scalable/performant pipelines, queries, and summaries of data . Has worked with various data systems and tools . Understands analytics and data science workflows and common use cases that leverage their work . Python . SQL . Datawarehouse experience . AWS experience . Data QA / validation skills (to check their work) . Snowflake experience (MUST) . Matillion, DBT, or other Data tech experience (ideal) . Marketing technology experience (ideal)",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
deloitte,Consulting-SAMA- A&C-Data Engineer/Architect-Associate Director,"Location: No Preference

Years of Exp: 10-15 Years

Hybrid: Yes

Mandatory Skill: MS Azure, Analytics, Delivery Management & Operations, People Management

Responsibilities:
• 10 - 15 years of deep delivery experience in cloud data engagements, with proven experience to lead teams sizes of 10+ resources
• Experience with platforms such as Azure Cloud Services, Solution Design & Review, Data Management, Data Visualization Tools
• Deep experience handling large volumes of data across multiple data sources like csv, relational data, SAP, json, parquet, flat files, streaming data, etc.
• Strong conceptual understanding of data warehousing, data modeling principles
• Strong Experience with visualization / reporting solutions
• Good understanding of data quality, data management and data governance principles
• Depth in data transformation / data modernization / ETL and ELT based data pipeline development
• Hands on with Agile/Scrum Methodology based implementations and end to end software delivery lifecycle
• Exposure working with international clients / geographies
• Excellent client handling, team handling and interpersonal skills
• Excellent written and spoken communication skills

Ability to understand and appreciate business / domain context and develop data and analytics solutions",Hyderabad,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Wipro Technologies,Data Engineer,"Share resume to akshara.raju@wipro.com

Location - Bangalore , Chennai , Pune , Hyd

JD :
• Azure data factory
• Azure data bricks with PySpark coding experience
• Experience in Snowflake
• Good to have knowledge in data visualization tool Tableau or PowerBI
• Good to have knowledge in data warehousing & data analytics

Data Analyst / Data Engineer
• 5 yrs. experience in working with large, complex data sets
• Create reports for internal teams and/or external clients
• Hands on Experience on Azure Data Factory and Azure
• Need to be able to code the data pipelines from ADF standpoint / Data Ingestion.
• Collaborate with team members to collect and analyze data
• Knowledge of Snowflake will be a big plus
• Need to ensure they can work on Data Mapping / Data Enrichment and Data Transformations.
• Use graphs and other methods to visualize data
• Establish KPIs to measure the effectiveness of business decisions
• Structure large data sets to find usable information
• Reporting and Data Visualization skills
• Experience in Data Mapping, data cleansing.",Bengaluru,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
ANI Calls India Private Limited,Lead Snowflake Data Engineer,"Anicalls

Industry : IT

Total Positions : 3

Job Type : Full Time / Permanent

Gender : No Preference

Salary : 900000 INR - 1800000 INR (Annually)

Education : Bachelor s degree

Experience : 5-10 Years

Location : Noida, India

Candidate should have :

Knowledge and experience in Big data and Data Vault methodology

Experience in power shell, shell scripting, and python

Exposure to data modeling for Snowflake

Experience in working with agile / scrum methodologies.

Experience in building data pipelines for large volumes of data across disparate data sources

Experience in DBT for Snowflake

Good experience on Azure Databricks

experience in Confluent cloud platform

Experience in building pipelines through Confluent Kafka and Knowledge of Azure Kubernetes Service

Good communication and presentation skills

Expertise in building data pipelines for Snowflake using Snowpipe, Snowpark, SnowSQL's and stored procedures.

Azure experience must be focused on Azure Data Factory, Azure storage solutions (such as Blob and Azure Data lake Gen2), and Azure data pipelines

Good experience on Snowflake and Snowflake architecture

7+ years of total experience in data projects with a focus on data integration and ingestion

3+ years of experience working primarily on Snowflake",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Impetus Technologies India Pvt. Ltd,GCP Data Engineer,"Role : GCP Data Engineer

Job Description :

- The candidate should have extensive production experience in GCP, Other cloud experience would be a strong bonus.

- Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.

- Exposure to enterprise application development is a must.

Roles & Responsibilities :

- 6-10 years of IT experience range is preferred.

- Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.

- Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOps.

- Good hands on expertise on either Python or Java programming.

- Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.

- Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.

- Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.

- Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.

- Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.

- Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.

- Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.

,",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Stantec Technology International,Senior Data Engineer,"Description
Grow with the best. Join a smart, creative, and inspired team that works behind the scenes to support operational excellence. As part of the Innovation Office, the Digital Technology & Innovation team is composed of digital experts who conduct research and development to keep our teams and our client's projects ahead of the technological curve. They implement established technologies and find emerging solutions for all business lines (Buildings, Energy & Resources, Environmental Services, Infrastructure, and Water), bridging existing knowledge domains and facilitating the integration of powerful tools and methods. The team's goal is to make projects more efficient and help provide higher-quality results to our clients. The ideal candidate will be a self-starter, a critical thinker, and highly interested in the application of new technologies and methods. The candidate will become a member of the Innovation Office, however, he or she will also be accessible to Stantec's project teams to support project work as needed.

Your Opportunity
The Innovation Office's Digital Technology & Innovation (DTI) team has an opportunity for a Senior Data Engineer. This position requires a person who is technically savvy, experienced in data engineering, and enjoys working with data to solve business problems, shaping & creating solutions, and helping to champion implementation. As a member of the Innovation Office, the Digital Technology Development group, as part of the DTI team, also engages in research & development and provides guidance and oversight as a center of excellence for the business. This group also engages in new product research and testing and the incubation of new ideas. The candidate will be responsible for the delivery of professional services and will recommend solutions to achieve complex strategic objectives across our large global team spanning Stantec IT, Business Lines, and the Office of Innovation.

Your Key Responsibilities
Serve at the direction of the Digital Technology Development Leader to:
- Take ownership of the project, work independently in a team environment, and mentor others as needed.
- A passion for solving problems and providing workable solutions, flexible to learn new technologies to meet the business needs.
- Translate complex functional and technical requirements into detailed designs.
- Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining.
- Implement access governance of production data systems to ensure compliance with our privacy and security policies.
- Oversee, design, and develop algorithms for real-time data processing within the business units and to create the frameworks that enable quick and efficient data acquisition.
- Build and maintain best practices to support the Continuous Integration and Delivery (CI/CD) of data engineering solutions.
- Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets (batch and stream data) into a readable and accessible format for end-user facing reports, data science, and ad-hoc analyses.
- Build and maintain reliable and scalable ETL on big data platforms as well as work with varied forms of data infrastructure inclusive of relational databases and NoSQL databases.
- Work collaboratively with DTI's Data & Analytics group, Stantec's internal business units, and clients to define problem statements, collect data and define solution approaches.
- Deliver highly reliable software and data pipelines using Software Engineering best practices like automation, version control, continuous integration/continuous delivery, testing, security, etc.
- Possess excellent time-management skills, a thorough understanding of task assignments and schedules, and efficient use of time and available resources.
- Perform other miscellaneous tasks associated with being a member of the Digital Technology & Innovation team and those typical of a data engineer.",Pune,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
TensorGo Technologies,TensorGo Technologies - Senior Data Engineer - Python,"Skillset : Python, PySpark, Kafka, Airflow, Sql, NoSql, API Integration,Data pipeline, Big Data, AWS/ GCP/ OCI/ AzureRequirements :Understanding our data sets and how to bring them together.Working with our engineering team to support custom solutions offered to the product development.Filling the gap between development, engineering and data ops.Creating, maintaining and documenting scripts to support ongoing custom solutions.Excellent organizational skills, including attention to precise detailsStrong multitasking skills and ability to work in a fast-paced environment3+ years experience with Python to develop scripts.Know your way around RESTFUL APIs.[Able to integrate not necessary to publish]You are familiar with pulling and pushing files from SFTP and AWS S3.Experience with any Cloud solutions including GCP / AWS / OCI / Azure.Familiarity with SQL programming to query and transform data from relational Databases.Familiarity to work with Linux (and Linux work environment).Excellent written and verbal communication skillsExtracting, transforming, and loading data into internal databases and HadoopOptimizing our new and existing data pipelines for speed and reliabilityDeploying product build and product improvementsDocumenting and managing multiple repositories of codeExperience with SQL and NoSQL databases (Casendra, MySQL)Hands-on experience in data pipelining and ETL. (Any of these frameworks/tools: Hadoop, BigQuery, RedShift, Athena)Hands-on experience in AirFlowUnderstanding of best practices, common coding patterns and good practices aroundstoring, partitioning, warehousing and indexing of dataExperience in reading the data from Kafka topic (both live stream and offline)Experience in PySpark and Data framesResponsibilities :You'll :Collaborating across an agile team to continuously design, iterate, and develop big data systems.Extracting, transforming, and loading data into internal databases.Optimizing our new and existing data pipelines for speed and reliability.Deploying new products and product improvements.Documenting and managing multiple repositories of code.",,True,False,True,False,False,False,False,True,False,False,False,False,True,True,True,False
Confidential,Data Engineer (Data Bricks) Manager,"EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL's proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients' decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries. Please visit www.exlservice.com for more information about EXL Analytics. Location - Bangalore/ Gurgaon Note- Currently Remote & expecting candidates to relocate to either Bangalore/Gurgaon once office reopens.Requirements: 7+ years of Data engineering exp with 3+ years hands on Databricks (DB) experience. Should be able to create New Clusters, Cluster Pools and attach existing clusters to pool in DB. Should have some pool management experience. Should be good in Datalakehouse concepts. Should have good experience in Data Engineering in Databricks Batch process, Streaming is good to have. Should have good experience in creating Workflows & scheduling the pipelines. Should have good exposure on how to make packages or libraries available in DB. Should have good experience in Databricks default runtimes, Photon & Light is good to have. Some experience in Databricks SQL / DW in Databricks. Delta Live Tables experience is good to have. IT Services and IT Consulting",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Streamline Digital,Sr. Data Engineer- Azure/ Snowflake/ Databricks,"Sr. Data Engineer

Who We Are

At Streamline, we are experts in Enterprise Mobility, Product Engineering, and IT Transformation. We help organizations navigate the constantly evolving landscape of IT. Our sole focus is ensuring that our client’s organization is armed with the strategies, products and solutions that are transformative to their business. Streamline works closely with our clients, takes pride in developing genuine relationships and embraces open communication and collaboration with our clients. We become a part of our client’s team, working together to achieve short-term goals and enable long-term success. Our team is comprised of world-class strategists, architects, engineers, and developers.

In our new flagship product, iEnterprise, we are taking things to the next level, using our collective experience and customer input to create new enterprise mobility management products that reduce operational costs, prevent issues before they happen, and resolve issues faster than with traditional tools and approaches.
Role Summary

This position is full time remote position. The Data Engineer will work with a team of other software developers to define, design, develop, integrate, and re-engineer the Enterprise Data warehouse. Data Marts, Data Virtualization and Data Visualization components in different environments which meet customers’ analytical and business intelligence requirements, scales easily and supports deployment in highly available environments. The Senior engineer acts as the development and technical lead and individual contributor on complex projects, contributing to strategic vision and technical decisions, participating in vendor analysis and selection projects, introducing process improvements, completing proof-of-concept projects for the introduction of changes to our architecture, and providing oversight on the work of other technical staff.

Role Responsibilities
• Build data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective
• Create and maintain highly scalable data pipelines using Databricks, Azure, AWS, Kafka.
• Design and build ETL/ELT data pipelines to extract and process data from a variety of external/internal data sources.
• Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery
• Build data insights, data analytics, ML models, fraud and anomaly detection using Snowflake
• Build and deploy modern data solutions.
• Define, implement and build jobs to populate data models.
• Relational and NoSQL Database management
• DevOps building CI/CD pipelines

Technical System Expertise: Understands data warehouse development practices and processes including ETL design, Dimensional models, slowly changing dimensions, Data Security components, Data quality methods, how MPP databases operate, and data flows. Aware of current technology benefits. Expected to independently develop full stack ETL solution from source to staging to presentation layers. Understands the building blocks, interactions, dependencies, and tools required to complete BI Data warehousing software and automation work. An Independent study of current technology is expected. Interact with system engineers to define continuous delivery and/or DevOps solutions, data security, and/or necessary requirements for automation.

Technical Engineering Services: Supports BI Data Warehousing and Enterprise Data Solutions projects by analyzing, designing, and developing ETL solutions; conducting tests and inspections; creating BI reports and virtualization components. Create interface jobs/APIs for data sharing with internal and external stakeholders Ensure we use accurate and secure methods to extract data. Analyze Big Data and MPP Databases to discover trends and patterns. Participates in reviews (walkthroughs) of technical specifications and program code with other members of the DevOps team. Expected to supervise associate engineers on occasion.

Innovation: Presents new ideas which improve existing BI Data Warehousing systems/processes/services. Presents new ideas which utilize new frameworks and reusable components to improve existing ETL jobs/processes/services. Express new perspectives based on an independent study of the industry. Review current company processes to highlight questions that may drive process refinement and optimization.

Technical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the engineer level. Develop application support documentation as required by the application support teams for acceptance of systems changes into production.

Technical Leadership: Collaborates with other engineering, development teams and utilizes data engineering/analyst expertise to deliver technical solutions. Continuously learn and mentor on new technologies.
Qualifications & Skills
• Bachelor’s degree in CS, Statistics, Information systems or equivalent experience.
• Strong expertise working with relational databases. Exceptional ability to author and optimize complex SQL queries/workflows Strong analytical skills to work with unstructured data.
• Experience building and optimizing highly scalable data pipelines, architectures, and data sets.
• A successful history of manipulating, processing, and extracting value from large, disconnected datasets and provide valuable insights.
• Proficiency in workflow orchestration (Databricks, Azure data factory).
• Experience working with streaming data solutions such as Kafka, IoT hub and Spark Streaming.
• Working experience building insights, ML models, fraud and anomaly detection using Snowflake/Databricks.
• Experience working with NoSQL databases like MongoDB, Cassandra, Cosmos DB
• Proficiency in Java/Python/Scala, pandas, pySpark, NumPy
• 8+ years of ETL Development experience using Azure Databricks, Azure data factory, SSIS, Talend.
• 8+ years Professional experience in designing and building cost-effective large scale data marts, data warehousing, distributed big data processing MPP Systems (Databricks, Spark, Snowflake)
• 5+ years of Expertise in Logical and Physical Data Model design using various modelling Tools like Erwin 7.3 and Power Designer.
• 5+ years of experience with Azure and AWS cloud stack
• 3+ years BI Visualization experience developing reports using Snowflake, PowerBI, Grafana, Qlik and analytic cubes utilizing SQL Server Analysis Services (SSAS).

Preferred Skills
• Certifications
• DevOps experience
• Bash, Golang scripting experience
• Docker/Kubernetes experience
• CI/CD pipelines

Powered by JazzHR",Hyderabad,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,True
Whiteforce,Data Engineer - ETL/,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20453

LocationIndia

Job Descriptions

Key Responsibilities - ETL pipeline design and implementation - Streaming and batch data processingStorage optimization - Storage optimization - DataOps / MLOps - Preparing, cleaning, structuring data for statistical modeling / machine learning - Deployment, packaging, versioning of Machine Learning models that operate on data Educational Qualification & Experience - BE/BTech in Computer Science or Information Systems - (Preferred) ME/MTech in Computer Science or Information Systems - Minimum 5 years total experience working in the industry - Minimum 3-5 years experience in Data Engineering Knowledge and Skills Required - Experience with Big Data technologies: Presto / Trino / Hive / Hadop - Cloud data storage, query and analytics technologies, esp. on AWS - S3 / Athena / Glue or Elastic Stack (specifically for data/ML workflows) - Message passing / data broker systems: Kafka / RabbitMQ

Skills",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Apple,Cloud Data Engineer,"Summary

The people here at Apple don’t just build products — they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Imagine what you could do here. Are you passionate about handling large & complex data problems, want to make an impact and have the desire to work on groundbreaking big data technologies? Then we are looking for you. At Apple, phenomenal ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Business Intelligence team is looking for passionate, technical savvy, energetic leader who like to think creatively. Someone who is self motivated and ready to lead team of most hardworking engineers building high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse team deals with Petabytes of data catering to a wide variety of real- time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We leverage a diverse technology stacks such as Snowflake, AWS, Teradata, HANA, Vertica, Single Store, Dremio, Hadoop, Kafka, Spark, Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.

Key Qualifications

High expertise in modern cloud data lakes and implementation experience on any of the cloud platforms like AWS/GCP/Azure - preferably AWS.

Good Experience in cloud based data warehouse - Snowflake.

Hands on Experience in developing and building data pipelines on Cloud & Hybrid infrastructure for analytical needs- Preferably having Cloud certifications.

Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.

Database development experience with Relational or MPP/distributed systems such as Teradata/ SingleStore/ Hadoop

Experience working with data at scale (peta bytes) with big data tech stack and sophisticated programming languages is a plus e:g Python, Scala.

Description

As a Cloud Development Engineer you will design, develop and implement modern cloud data warehouse/ datalakes and influence overall data strategy for the organization

Translate sophisticated business requirements into scalable technical solutions meeting data warehousing/analytics design standards

Strong understanding of analytics needs and proactive-ness to build solutions to improve the efficiency along with that help execute leading data practices & standards

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business

Ability to communicate effectively, both written and verbal, with technical and non- technical multi-functional teams

You will engage with many other group’s & internal/external teams to deliver best-in-class products in an exciting constantly evolving environment

Education & Experience

Bachelors or Masters Degree in Computer Science or equivalent in Engineering.

Role Number: 200154652",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
SecureKloud,Data Engineer,"The candidate will work in a cloud development team using Informatics Data Architecture and Data Engineering concepts to deliver foundational data capabilities such as Data Lakes and Master Data Management. The candidate will be part of the DevOps Team and responsible for building data platforms, test cases, deployment documentation, and support documentation in accordance with internal and industry standards. This is a highly technical and hands-on role.

Responsibilities
• Defining database design, data flows, and data integration techniques
• Design and build data solutions ensuring data quality, reliability, and availability
• Create data processing routines for managing enterprise master data throughout the data lifecycle (capture, processing, and consumption)
• Maximize business outcomes using Mobile Device Management via improved data integrity, visibility, and accuracy
• Manage and evolve global data lakes platforms
• Develop data ingestion, data enrichment, data deidentification routines, and RESTful APIs for consumption of data lakes data",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Varite India Private Limited,Data Engineer II (India - Contract),"Description: Location: Gurgaon / Bangalore Contract Duration: 8 months (extension and conversion based on project and performance basis) Shift: Early US (over lapping with India) Please provide 2-3 values or traits that are important to this role: Strong data warehouse and modeling skills You are a self-starter who is highly organized and communicative Deals well with ambiguity Please list 3-4 functional activities the resource should be capable of: Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) What technical skills will successful candidates possess Bachelor's or Master's degree in Computer Science or Engineering with 6+ years of proven experience in related field Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Experience building low-latency data product APIs You value strong pull request reviews, understand when to stand your ground and when to let go You are a self-starter who is highly organized, communicative, quick learner, and team-oriented Successful background as a technical leader driving cross-organizational data initiatives to completion Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Deep expertise in data access patterns, data validation, data modeling, database performance, and cost optimization Hands-on knowledge with BI tools, modern OLAP engines such as Presto, Clickhouse, Druid, Pinot, and data processing frameworks such as Spark and Flink Experience establishing and applying standards for operational excellence, code quality, and software engineering best-practices Effective verbal and written communication skills with the ability to think strategically about technology decisions and manage relationships with stakeholders and senior leadership Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage Experience with BI tools like Tableau, DataDog Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) Experience working with SQL & NoSql databases along with programming experience in Python and/or JAVA Strong business acumen with an understanding of business drivers and of how to drive value by supporting data discoverability across the organization Experience building low-latency data product APIs Experience working in an agile environment Tell us about your team: Clientis seeking a data engineer to join the Business Enablement Technology team. As we challenge the status quo and take the CTO's agenda to the next level, your focus will be on building a data platform to enable analytics and reporting capabilities across Product & Technology. This is an exciting, high-impact, and cross-functional role. This role requires a highly inventive individual with strong technical and analytical skills, execution drive, and self-motivation. Is there any industry specific experience that would separate one candidate from another Experience migrating data/apps from on-prem to cloud, engineering degree, tech industry experience",Gurugram,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
ITC Infotech India Ltd,Azure Data Engineer,"• Azure Data Engineer
• Ability to understand the functional & technical specification to develop the Notebooks
• Perform Data Loads and Transformations
• Schedule the Jobs with the Azure Data Factory and monitoring the jobs
• Strong in write complex SQL queries",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
bp,Senior Data Engineer - dataWorx,"Job Profile Summary
Role Synopsis:
As part of bp “reinvent”, we have created a major new business line called “Innovation & Engineering” (I&E). One key remit of this group is to drive the transformation of the company through its use of digital and data. A major digital sub-team within I&E is Digital Production & Business Services (DP&BS). DP&BS are responsible for all digital and data initiatives and operations across the following areas of the bp business:
• Production & Projects including Health, Safety, Environment & Carbon
• Refining & Operations
• Wells & Subsurface
• Business Services including Finance, Procurement, People & Culture, Performance Management
• Strategy & Sustainability
• “DataWorx” is the name of the data team that is responsible for all data within these areas and we are developing deep data capabilities to transform the access, supply, control and quality to our vast and ever growing data reserves that are measured in Petabytes. The DataWorx team covers many data sub-disciplines, including data science, data analytics, data engineering and data management as well as specialist areas such as geospatial, remote sensing, knowledge management and digital twin. The DataWorx team works with a wide variety of data from structured data to unstructured data & we also work on Real-time streaming data processing along with Batch data processing. Key Responsibilities :
• Architects, designs, implements and maintains reliable and scalable data infrastructure
• Leads the team to write, deploy and maintain software to build, integrate, manage, maintain, and quality-assure data
• Architects, designs, develops, and delivers large-scale data ingestion, data processing, and data transformation projects on the Azure cloud
• Mentors and shares knowledge with the team to provide design reviews, discussions and prototypes
• Leads customer discussions from a technical standpoint to deploy, manage, and audit best practices for cloud products
• Leads the team to follow software & data engineering best practices (e.g. technical design and review, unit testing, monitoring, alerting, source control, code review & documentation)
• Leads the team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline
• Leads the team in following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Actively contributes to improve developer velocity
• Part of a cross-disciplinary team working closely with other data engineers, software engineers, data scientists, data managers and business partners in a Scrum/Agile setup
• responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Work closely with other data engineers, software engineers, data scientists, data managers and business partners

Job Advert
Job Requirements :
Education :
Bachelor or higher degree in computer science, Engineering, Information Systems or other quantitative fields

Experience :
• Years of experience: 8 to 12 years with minimum of 5 to 7 years relevant experience
• Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
• Hands on experience with:
• Databricks and using Spark for data processing (batch and/or real-time)
• Configuring Delta Lake on Azure Databricks
• Languages : Python, Scala, SQL
• Cloud platforms : Azure (ideally) or AWS
• Azure Data Factory
• Azure Data Lake, Azure SQL DB, Synapse, and Cosmos DB
• Data Management Gateway, Azure Storage Options, Stream Analytics and Event Hubs
• Designing data solutions in Azure incl. data distributions and partitions, scalability, disaster recovery and high availability
• Data modeling with relational or data-warehouse systems
• Advanced hand-on experience with different query languages
• Azure Devops (or similar tools) for source control & building CI/CD pipelines
• Understanding Data Structures & Algorithms & their performance
• Experience designing and implementing large-scale distributed systems
• Deep knowledge and hands-on experience in technologies across all data lifecycle stages
• Stakeholder management and ability to lead large organizations through influence

Desirable Criteria :
• Strong stakeholder management
• Continuous learning and improvement mindset
• Boy Scout mindset to leave the system better than you found it

Key Behaviours :
• Empathetic: Cares about our people, our community and our planet
• Curious: Seeks to explore and excel
• Creative: Imagines the extraordinary
• Inclusive: Brings out the best in each other

Entity
Innovation & Engineering

Job Family Group
IT&S Group

Relocation available
Yes - Domestic (In country) only

Travel Required
Yes - up to 10%

Time Type
Full time

Country
India

About BP
INNOVATION & ENGINEERING
Join us in creating, growing, and delivering innovation at pace, enabling us to thrive while transitioning to a net zero ‎world. All without compromising our operational risk management.

Working with us, you can do this by:
• deploying our integrated capability and standards in service of our net zero and ‎safety ambitions
• driving our digital transformation and pioneering new business models
• collaborating to deliver competitive customer-focused energy solutions
• originating, scaling and commercialising innovative ideas, and creating ground-breaking new ‎businesses from them
• protecting us by assuring management of our greatest physical and digital risks

Because together we are:
• Originators, builders, guardians and disruptors
• Engineers, technologists, scientists and entrepreneurs‎
• Empathetic, curious, creative and inclusive

Experience Level
Intermediate",Pune,True,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Domnic Lewis Private Limited,Big Data Engineer,"we are hiring for Big Data Engineer with the experience in spark , python , SQL , AWS glue Big Data Engineer: Spark, Python, SQL, AWS Cloud (Glue, Lambda, Athena) Hyderabad Location A big data engineer is an information technology (IT) professional who is responsible for designing, building, testing and maintaining complex data processing systems that work with large data sets.",Secunderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Invsto,Data Engineer Intern (2024/2025 graduates),"You are
• A self-starter who is fueled by a desire to improve customer experiences in the moments that matter most, approaching your work with a bias toward accountability, decision-making and action
• Inquisitive and creative, with an ability to listen to identify customer needs and dig deeper to understand the reasons behind those needs
• Able to transform conceptual thinking into deliverables that generate excitement, feedback and alignment among stakeholders
• Thrive in a collaborative environment, partnering across the company with business experts, software developers, data engineers, and marketers

You have
• Enthusiasm to take the initiative to tackle problems and work with others to expand on your experience and expertise
• Experience with Python, AWS and databases such as Postgres
• Know-how to build data engineering pipelines using services such as Airflow

You will

Work closely with and learn from a team of engineers to contribute to a build a variety of features and infra.

Must-Have skills
• Python, Database experience
• Django (in lieu of Django, an equivalent tech stack)

Qualification and Experience:
• 0-2 Years software engineering
• Education: BE/BTech or equivalent (in lieu of academics, equivalent software developer experience required)

Benefits
• Fully remote, forever
• Annual retreats

About Us

Invsto is building the future of financial engineering.

We believe in hiring the best and providing complete autonomy to our employees to build stuff that they think would make a difference to the world

How to Apply

Does this role sound like a good fit? Email us at [hello@invsto.com].
• Include the role's title in your subject line.
• Send along links that best showcase the relevant things you've built and done (Github, Behance, Dribbble etc)

Invsto focuses on Hedge Funds and Stock Exchanges. Their company has offices in Bangalore Urban. They have a small team that's between 11-50 employees.

You can view their website at https://invsto.com/ or find them on LinkedIn.",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False
Everyday Health Group,Data Engineer,"Description

Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.

Health eCareers, a property of Everyday Health Group, is looking for a Data Engineer to support our growing business.

Key Responsibilities
• Understand overall data collection strategies and implement them in data tables and connections.
• Program Python-based APIs and web services per implementation schema, such as creating applications to access Facebook, Twitter, Salesforce, and other data sources.
• Use SQL Server, MySQL, HDFS and AWS to design, develop and deploy data processing.
• Analyze and organize raw data from various ETL tools.
• Monitor/Forecast computing resources usage (data lakes, AWS, EC2, etc.).
• Collaborate with ETL developers located at various offices (US and India)
• Implement coding standards, procedures and techniques, concluding writing technical code base.
• Detect, repair, prevent data pipeline failures; identify systematic weaknesses and provide pre-emptive remedies with programming or processes.
• Build and optimize data storage systems.
• Prepare data for prescriptive and predictive modeling.
• Recommending and implement emerging database technologies.
• Create automation for repeating database tasks.

Job Qualifications
• 4+ years of programming experience with an emphasis on data processing.
• Coding skills: Python, SQL.
• Ability to create object-oriented programming and data architectures
• Knowledge of new, leading technology strategy in data engineering and management
• Understanding of industry technologies in scalability, performance, delivery pipeline and maintenance of these tools and systems.
• Hands-on experience with SQL database design, AWS or other cloud systems.
• 2+ year’s experience in Linux environments.
• Experience with SQL Server, MySQL or other popular database management tools.
• Good/Expert knowledge of API services
• Familiarity with Agile process
• Clear documentation requirements and specifications
• Bachelors or Advanced Degree in Information Management, Computer Science or related field.

Desirables:
• Experience in AWS, Tableau
• Experience in developing custom data pipelines in HDFS
• Experience working with social, Double Click, Adobe APIs
• Working with teams across multiple locations
• Skills: Pig; Hive; Spark; Impala; EMR

Our Culture and Values

We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.

Life at Everyday Health

At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work.

#HealtheCareers",,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Unusual Hire,Data Engineer,"Job type: Partial Onsite

Expert Level

Project detail

General:

– Ability to architect Data Science solutions

– Ability to lead a team

– Ability to gather requirements

Must have :

– 6-8 Years Data Science Experience

– At least 4 Data Science solutions

– At least 2 NLP / ML Solutions

– At least 1 solution with Deep learning framework Deep

– Python / PySpark

– Knowledge in Statistical Algorithms like classification, Regression , recommendation and Clustering , Neural Networks

– Azure ML , Databricks , Delta Lake , Data science workspace

Industry Categories

Data Scientist

Languages required

English

₹200 - ₹500

Cost

Location

India

Project ID: 00002514",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Autodesk,Data Engineer,"Job Requisition ID # 22WD66509 Job Description Position Overview In this role, you will be part of a highly energetic team of engineers working on Autodesk Enterprise Integration Platform to assemble, enrich and enhance the data as per the business needs. You would be part of the team working on providing advisory services and support to business in deriving sales strategies. you will be working to build robust and scalable self-service platform for Autodesk while using innovative technologies in big data space. Responsibilities Design and develop components of end-to-end Enterprise Integration Platform Work with the team to make the Enterprise Integration Platform an efficient, robust and scalable platform Enthusiastic and passionate member of a highly skilled and motivated agile development team Contribute to a team culture that values quality, robustness, and scalability while fostering initiatives and innovation Minimum Qualifications BS or MS in computer science or a related field with 5+ years of experience. 3+ years of experience with cloud ETL/ELTs Working experience in Agile methodologies Strong knowledge and experience in AWS technologies Experience in Matillion and Redshift Strong programming skills in either Java, Scala or Python Experience with Hive and one relational DB Strong problem-solving skills Strong communication skills Ability to learn new technical skills Preferred Qualifications Experience with Matillion Experience with Python, Scala Experience with SQL (Redshift) Experience with Kanban methodology At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. Are you an existing contractor or consultant with Autodesk Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact .",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Finxera,Data Engineer,"Company Description

Finxera, Powered by Priority Technology Holdings, Inc. (NASDAQ: PRTH), is headquartered in Alpharetta, Georgia USA. Our India office is located in Chandigarh, where our dynamic team builds state of the art, sophisticated Fintech products & solutions.

We are an emerging payments powerhouse that offer a single unified platform for Banking & Payments powering modern commerce.

Finxera offers a unique family of products which integrate into SMB Payments, B2B Payments and Enterprise Payments to help businesses thrive. We are on a mission to offer an industry agnostic platform that enables businesses to collect, store and send money using various new age payment methods.

Finxera is an employee-first organisation and we continually strive to ensure their professional and personal success supported by employee friendly policies and a positive work environment built on mutual respect and professionalism. We offer a dynamic work environment, with continuous growth & learning opportunities. We believe in growing together and our people are the driving force behind our success.

Summary Requirement

We are looking for a Data Engineer for a position in the Data and Analytics team.We need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively.

Responsibilities

1. Data Warehousing experience

2. Develop ETL pipelines against traditional databases and distributed systems to allow our team to remain agile in data requirements, and to flexibly produce data back to the business and analytics teams for analysis.

3.Dimensional Modelling experience

4.Database Skills - SQL / PLSQL

5.Python / Spark / Hive / Nifi Knowledge

6.Elasticsearch / Kafka knowledge would be plus

7.Data visualization tools knowledge

8.Participate in design reviews and code reviews

9.Trouble shoot and resolve production issues

10.Resolve performance related issues

11.Knowledge on Data Science / ML / Analytics would be plus

12.Build processes that analyze and monitor data to help maintain data accuracy and completeness.

13.Ability to work with colleagues across global locations remotely

14.Independent and able to work with little supervision

15.Able to mentor and supervise junior team members

16.Agile Methodology Experience

Required Skills & Qualifications

l **Should have scored 70% & above in 10th and 12th grade.

l SQL/PLSQL

l ETL/ETL

l Python

l Pyspark (Optional)

l Any BI Tool like Si Sense, Looker, Quick Sight, Tableau, Power BI, etc (Preferred LOOKER)

l Dimension Modelling (Kimbal) / Data warehousing

l Redshift (Good to have)

l OLAP Cubes",Chandigarh,True,False,True,False,False,False,False,False,True,True,False,False,True,False,False,False
Confidential,Associate - Data Engineer,"JOB DESCRIPTIONRole and responsibilitiesResponsible for overall data analysis (e.g wrangling, cleansing), tools, and technology implementationBuild data systems, pipelines, and workflowsAbility to organize structure/unstructured raw data sources of many types with the ability to combine into useable dataDevelop data wrangling and analytical applicationsCollaborate with data scientists and analyst to management data pipelinesExplore opportunities to enhance quality of data and processesManage a team of data analysts, MIS, and operations resourcesProactive communication with project manager, ensuring all client requirements are met and reports are submitted on timeOversee implementation of tools and technology to build efficiency and consistencyResolve and escalate issues with tools and applicationsDesired candidate profile3-5 years of experience as a data engineer and/or developerStrong background within the role of a data engineer for capabilities such as knowledge of various programming language with a strong emphasis on Python, SQL, Power Query and VBA/MacrosExcellent knowledge in Microsoft Excel and knowledge of advance functionsExperience with SQL set-up and advanced queries developmentStrong technical knowledge in data mining, wrangling, and structuringManaging large volumes of data with the ability to scale as neededAbility to develop and design end-to-end solutions with operations/analyst groups for deliverablesCreating custom scripts and applications to perform wrangling, cleansing, and storingTrouble shoot data issues at any level of project/structure pipeline(s)Presenting data/reports as neededExcellent people management skillsPassionate to drive business metrics - Productivity, Quality, and other key deliverablesAbility to prioritize between multiple complex projects/timelinesExcellent written and Verbal communicationHigh level of positive attitude with good listening skillsAbility to priorities between multiple complex projects/timelinesStrong attention to detail and the ability to conduct root cause analysisCandidates with demonstrated experience in Data Breach Response, or Incident Response will be preferredKnowledge and hands-on experience in breach notification and privacy laws around data breach scenarios is desirable but not must.UnitedLex is committed to preserving the confidentiality, integrity, and availability of all the physical and electronic information assets throughout the organization. Consistent with the UnitedLex ISMS policy and the ISO 27001 standard, every employee is responsible for complying with UnitedLex information security policies and reporting all security concerns, weaknesses, and breaches. Legal Services",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Oil Field Instrumentation (India),DATA ENGINEER,"Experience :

4+ years of Mud Logging experience as Data Engineer in India and abroad, both onshore and offshore rig operations. Deep Water experience will be an advantage. Proficient with real-time data monitoring and data management, WITSML transmission, gas detection and analysis, MLU maintenance.

Qualification :

Graduate or above in Geology, Applied Geology, Geoscience, Petroleum Technology or related, or B.E. in Petroleum Engineering.",Mumbai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
News Corp,Senior Data Engineer,"One of the most innovative and high-profile teams in News Corp is looking for a seasoned data engineer to help accelerate its vision. The role will involve combining all of News Corp’s data across categories, countries and organizations into a singular view of a customer enabling engagement, measurement and targeting. The Dow Jones Senior Data Engineer will partner closely with product, data & engineering teams to help make this data available and accessible to our businesses, teams and partners in order to drive revenue and improve the customer experience.

Responsibilities
• Design, implement and maintain high performance big data infrastructure/systems & big data processing pipelines scaling to billions of structured and unstructured events daily
• Design, implement and maintain deep integration with up-stream systems
• Monitor performance of the data platform and optimize as needed
• Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
• Support products with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
• Data analysis, understanding of business requirements and translation into logical pipelines & processes
• Conduct timely and effective research in response to specific requests (e.g. data collection, summarization, analysis, and synthesis of relevant data and information)
• Evaluate and prototype new technologies in the area of data processing
• Think quickly, communicate clearly and work collaboratively with product, data, engineering, QA and operations teams
• High energy level, strong team player and good work ethic

Technologies we use
• AWS (emr,ec2,glue,athena,redshift,lambda,s3,dynamodb,sns)
• Python, Spark (PySpark)
• Kubernetes, Docker
• Airflow
• Git for source code management
• Jira

Qualifications
• BS in Computer Science or other technical discipline
• 5+ years of experience designing and developing big data processing systems using distributed computing
• Fluency in Python and Spark
• Expert knowledge in optimizing complex SQL queries
• Experience with job orchestration tools like Airflow
• An affinity for automation
• Experience working with cloud platforms such as AWS & GCP
• Familiarity with networking and network application programming, including HTTP/HTTPS, JSON, and REST APIs
• Experience with at least one object oriented language (ex: Java)
• Strong OO design, data structure, and algorithm design skills
• Strong interest in emerging technologies: Hadoop, Hive

Nice to Have
• Experience in the digital advertising and marketing industry
• Contribution to Open Source projects",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,True,False
Saasvaap,Sr Data Engineer,"JOB DESCRIPTION
• Job Description (Data Engineer) ROLE AND RESPONSIBILITIES You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
• You will support our product, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
• You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
• You will be excited by the prospect of optimizing or even re-designing Peer Data architecture pipeline to support our next generation of products and data initiatives.
• Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implergent internal process improvements: automating manual processes, optimizing data delivery, re-designing hastructure for greater scalability, etc.
• Buld the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep our data separated and secure across national boundaries through multiple data centers and cloud regions.
• Create data tools for analytics assist in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.
• QUALIFICATIONS AND EDUCATION REQUIREMENTS • 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field PREFERRED SKILLS Experience with Big Data platforms such as Apache Hadoop and Apache Spark Deep understanding of REST, good API design, and OOP principles Experience with object-oriented/object function scripting languages: Python, C#, Scala, etc.
• Experience with relational SQL and NoSQL databases, including Postgres, Cosmos and Cassandra.
• Experience with data pipeline and workflow management tools: Keboola, Stitch, Azkaban, Luigi, Airflow, etc.

SKILLS REQUIRED

My SQL, AWS

EXPERIENCE

4-10

LOCATION

Kochi, Trivandrum

WORK TYPE

FullTime

TIME SHIFT

Day",,True,False,True,False,False,False,False,False,False,False,False,True,False,False,True,False
Confidential,Chistats - Senior Data Engineer - ETL/Data Pipeline,"Day-to-Day Responsibilities :- Create and manage ETL pipelines and job schedulers.- Handle unstructured data and work to automate data ingestion and validation.- Develop APIs with Flask and Python for data mining, transformation and ingestion to AWS.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Write clean, performant code to develop functional applications; build reusable code and libraries- Collaborate with team to evaluate technologies we can leverage, including open-source frameworks, libraries, and tools.- Support Business and application teams with respect to data-related requirements.- Build proactive data validation automation to catch data integrity issues.- Troubleshoot and resolve data issues using critical thinking.Required Qualifications :- Minimum of 3 years of relevant data platform experience (structured/non-structured database, data extraction, data ingestion)- A Degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines.- Experience in at least one of these databases: MS SQL, mySQL, PostGreSQL.- Experience in AWS would be ideal and basic cloud infrastructure knowledge.- Experience in Python is required for Web Scraping.- Strong fundamentals in data mining & data processing methodologies- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.- Build processes supporting data transformation, data structures, metadata, dependency and workload management.Good to have Critical Skills :- Ability to thrive in challenging situations and solve complex problems- Strong bias for action, and see yourself as an 'initiator' and 'problem solver'- Analytical and problem-solving skills- Customer centricity and Good communication (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Calix,Senior Data Engineer - Snowflake,"This position is based in Bangalore, India.

Calix is undergoing a growth transformation, and we are looking for the best and brightest engineers for our Data Engineering team. Our team is facilitating Calix’s transformation into a more data centric enterprise with our business operational leaders. We partner with our operational teams to identify the key points of decision. We create decision support tools that enable optimal data driven selection of business actions and we build out & maintain these decision support tools on a modern data technology stack using DataOps processes. We are building out the data foundations for the next phase of our growth journey. This is a great opportunity to join a rapidly scaling enterprise with a lot of opportunity for personal growth.

The Data Engineering team is seeking a Lead Data Engineer who will be an extraordinary addition to our growing team. You will build and maintain our cloud-based enterprise data platform. Key areas that you will own include data architecture, data modeling, data pipeline flow, data warehousing, security and governance protocols, data integrity processes, and data QA best practices. You will lead buildout of the end-to-end ETL/ELT data environment, integrating with new technologies, and the development of new processes to support the creation and deployment of trusted, accurate and secure decision support tools to the Calix operational business units.

The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

Responsibilities and Duties:
• Lead data modeling, data ingestion, ELT/ETL, and data integration development using our cloud-based tooling including Snowflake, AWS, Fivetran, dbt, Airflow and GitHub.
• Establish and maintain a DataOps approach for our data pipeline infrastructure and processes.
• Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
• Deploy production machine learning pipelines into business operations analytic tooling.
• Ensure data quality throughout all stages of acquisition and processing.
• Create and maintain secure and governed access to the enterprise data warehouse and reporting tools.
• Evaluate new technologies for continuous improvement in data engineering.
• Participate in project meetings, providing input to project plans and providing status updates.
• A desire to work in a collaborative, intellectually curious environment.
• Highly motivated self-starter with a bias to action and a passion for delivering high-quality data solutions.

Qualifications
• 5+ years of experience in related field; preferably experience building and delivering data pipelines, data lakes and ELT solutions at scale.
• Expert knowledge of data architecture, data engineering, data modeling, data warehousing, and data platforms.
• Experience with Snowflake, BigQuery, Redshift, AWS, and pipeline orchestration tools (Fivetran, Stitch, Airflow, etc.).
• Coding proficiency in at least one modern programming language (Python, Java, Ruby, Scala, etc.).
• Deep SQL expertise.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, operations, and technical documentation.
• Excellent verbal and written communication skills and technical writing skills.
• Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership to gain alignment, and drive progress.
• Bachelor’s degree or equivalent experience in Computer Science, Engineering, Management Information Systems (MIS), or related field.

Preferred Qualifications
• Experience with dbt SQL development environment.
• Experience developing and deploying machine learning models in a production environment.
• Experience with Power BI and/or SFDC Einstein/Tableau.
• Experience with Oracle ERP and Oracle Data Cloud tools.

Location:
• Bangalore, India",Bengaluru,True,False,True,True,False,False,False,True,True,True,False,False,True,True,True,True
Paradise Placement Consultancy,Azure Data Engineer (Mercedes-Benz)(BSL),"Job Description : Job Description: Job Description, Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, ETL,Big Data. Responsibilities: Touch base with customers to collect the requirements and analyze them Design and build end-to-end data pipelines to get the data for customers Unit testing of the pipelines and UAT support Deployment and post production support Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents. Adapt changes to the existing scripts, codes and pipelines. Reviewing design, code and other deliverables created by your team to guarantee high-quality results Capable enough to own the PoCs and deliver the results in reasonable time Accommodate and accomplish any ad-hoc assignments Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users. Job Qualifications Qualifications: Bachelor's or Master's degree in Computer Science, Information Technology or equivalent work experience 2+ years of full time data engineering experience 2+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks Skilled in ETL/ELT process. Good working knowledge with ETL tools. Experienced with Hadoop/Big data eco systems Data migration experience from on premise to cloud Expertise in structured query language and PL/SQL Exposure to log analytics and debugging Good to have DevOps, Continuous Deployment and testing techniques Agile development experience Fluent English in spoken and written Mercedes-Benz Research and Development India Private Limited. Preferred Qualifications: Microsoft Azure Certifications Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics Hands-on experience with Databricks notebooks Hands-on experience with Kafka/eventhubs Working experience in an international team or abroad.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Kaplan,Senior Data Engineer (Hybrid),"Job Title

Senior Data Engineer (Hybrid)

Job Description

For more than 80 years, Kaplan has been a trailblazer in education and professional advancement. We are a global company at the intersection of education and technology, focused on collaboration, innovation, and creativity to deliver a best-in-class educational experience and make Kaplan a great place to work.

Our offices in India opened in Bengaluru in 2018. Since then, our team has fueled growth and innovation across the organization, impacting students worldwide. We are eager to grow and expand with skilled professionals like you who use their talent to build solutions, enable effective learning, and improve students’ lives.

The future of education is here and we are eager to work alongside those who want to make a positive impact and inspire change in the world around them.

Job Impact and Scope Summary

The Senior Data Engineer at Kaplan North America (KNA) within the Analytics division will work with world class psychometricians, data scientists and business analysts to forever change the face of education. This role is a hands-on technical expert who will own the design and implementation of an Enterprise Data Warehouse powered by AWS RA3 as a key feature of our Lake House architecture.

The perfect candidate is an expert in data warehousing technical components (e.g. data modeling, ETL, reporting). You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be able to work with business customers in a fast-paced environment understanding the business requirements and implementing data & reporting solutions. Above all you should be passionate about working with big data and someone who loves to bring datasets together to answer business questions and drive change.

Responsibilities
• Hands-on technical leader. Continually raises the bar for the data engineering function.
• Leads the design, implementation, and successful delivery of large-scale, critical, or difficult data solutions. These efforts can be either a new data solution or a refactor of an existing solution and include writing a significant portion of the “critical-path” code.
• Sets an example through their code, designs and decisions. Provides insightful code reviews and take ownership of the outcome. (You ship it, you own it.)
• Proactively works to improve data quality and consistency by considering the architecture, not just the code for their solutions.
• Makes insightful contributions to team priorities and overall data approach, influencing the team’s technical and business strategy. Takes the lead in identifying and solving ambiguous problems, architecture deficiencies, or areas where their team bottlenecks the innovations of other teams. Makes data solutions simpler.
• Leads design reviews for their team and actively participates in design reviews of related development projects.
• Communicates ideas effectively to achieve the right outcome for their team and customer. Harmonizes discordant views and leads the resolution of contentious issues.
• Demonstrates technical influence over 1-2 teams, either via a collaborative development effort or by increasing their productivity and effectiveness by driving data engineering best practices (e.g. Code Quality, Data Quality, Logical and Physical Data Modelling, Operational Excellence, Security, etc.).
• Actively participates in the hiring process and is a mentor to others - improving their skills, their knowledge, and their ability to get things done.
• Hybrid Schedule: 3 days remote / 2 days in the office
• 30-day notification period preferred

Requirement:
• In-depth knowledge of the AWS stack (RA3, Redshift, Lambda, Glue, SnS).
• Expertise in data modeling, ETL development and data warehousing.
• 3+ years’ experience with Python, or Java, Scala
• Effective troubleshooting and problem-solving skills
• Strong customer focus, ownership, urgency and drive.
• Excellent verbal and written communication skills and the ability to work well in a team.

Preferred Qualification:
• 3+ years’ experience with AWS services including S3, RA3.
• Ability to distill ambiguous customer requirements into a technical design.
• Experience providing technical leadership and educating other engineers for best practices on data engineering.
• Familiarity with Airflow, Tableau & SSRS.

#LI-Remote

#LI-AK1

Location

Bangalore, KA, India

Additional Locations

Employee Type

Employee

Job Functional Area

Data Analytics/Business Intelligence

Business Unit

00092 Kaplan Health

Kaplan is an Equal Opportunity Employer. All positions with Kaplan are paid at least $15 per hour or$31,200 per year for full-time positions. Compensation for specific positions are based on job level, skills, years of experience, and education, among other factors. Additionally, certain positions are bonus or commission eligible. Information regarding benefits can be found here
.

Diversity & Inclusion Statement:

Diversity inspires innovation and growth in the Kaplan community. Kaplan strives to be a model employer for inclusiveness. Not only does Kaplan value its employees for their professionalism and skills, but also for the unique viewpoints they bring to the Organization. Kaplan's employees bring diverse perspectives, ideas, and backgrounds that give Kaplan a competitive edge in anticipating and exceeding our students' needs in today's global market. Learn more about our culture
.",Bengaluru,True,False,False,True,False,False,False,True,False,True,False,False,True,False,True,False
"Planview, Inc.",Data Engineer,"Overview

Planview is looking for a passionate data engineer to join our team innovating tools for connected work. You will work closely with other data engineers, data scientists and individual product teams to specify, validate, prototype, scale, and deploy features with a consistent customer experience across the Planview product suite.

Responsibilities
• Ability to work in a fast paced start up mindset. Should be able to manage all aspects of AI/ML activities
• Develop platforms that make data across applications/application deployments available for AI/ML-driven feature prototyping, proofs-of-concept, and general availability
• Refine data pipelines for analysis, while refining, automating, and scaling as needed for the use-case at hand
• Work on various aspects of the AI/ML ecosystem – data modelling, data pipelines, data observability, data documentation, scaling, deployment, monitoring and maintenance etc.
• Work closely with Data scientists and MLOps Engineers to come up with scalable system and model architectures for enabling real-time ML/AI services

Qualifications

Required qualifications
• Masters or equivalent experience in Informatics, CS, Data Science or a related field
• 2+ years of experience as a data engineer or data scientist with a focus on data engineering for ML applications
• Strong Python and SQL coding skills
• Familiar with AWS Data and ML Technologies (AWS Sagemaker, Data Pipeline, Glue, Athena, Redshift etc)

Preferred Qualifications
• Demonstrated experience building data and analytics pipelines/services that efficiently scale for cloud application usage, meeting a product team’s SLA for performance and resilience
• Exposure to database queries and strong in SQL
• Exposure to any of the libraries and frameworks in data science (Pandas, Numpy, Dask, PySpark etc)
• Exposure to data version control (DVC) and orchestration tools (Airflow, etc)
• AWS Certification is a plus
• Skilled at working as part of a global, diverse workforce of high-performing individuals
• AWS Certification is a plus",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Full Potential Solutions,Lead Data Engineering,"Overview:

About The Company

Full Potential Solutions (FPS Perch) is a global product company headquartered in the US. We help contact centers around the world better engage with their customers by combining our deep domain expertise with cutting-edge technology. Our AI enabled, cloud-scalable analytics products enhance the end-to-end customer journey via omnichannel engagement (voice, email, SMS, chat and conversational AI) and optimize agent performance. Our offices are spread across the US, India and Philippines.

Some exciting initiatives they are working on are:
• Designing and development amazing user experiences across data visualization and analytics in the customer engagement / contact center industry
• Salesforce based solutions for omni-channel customer engagement leveraging AI to enhance contact strategy and routing, guide agents on next best action (NBA) enable dynamic scripting, capture voice of customer (VOC)
• Developing standardized AI/ML models to optimize customer engagement and agent performance using various Python frameworks and AWS services. (Transcribe, Comprehend, Sagemaker, etc.)
• Developing a comprehensive Data, Analytics and Reporting platform to integrate, measure and analyze millions of daily customer interactions/metrics across all our clients.
• Building web/mobile apps to improve agent/manager performance across thousands of agents (metrics, gamification, collaboration).
• Deploying cutting-edge, tech-enablement services using the AWS ecosystem, including microservices and serverless architecture, CI/CD pipelines, event notification & search services, multi-tenant architecture ec.,

Our team has successfully built and scaled analytics-focused products at companies such as Salesforce, Demandware, and IBM. We have strong backgrounds in Computer Science, Math and Engineering from top universities such as IIT, Harvard and Yale. We strongly believe that empowered people are the key to building a great company, and our development process focuses on iteratively improving our products as well as ourselves as individuals and as a team. Our mission as a company is to create an environment where the people THRIVE.

Explore more at: FPS Website

Leadership Team: Explore here

About The Team

Our analytics team helps our clients leverage data to optimize their performance and results. Our Data Analysts possess a unique skill of understanding the data, making a clear sense of it, and telling a story to the business. We work closely with the Analytics Leaders and the rest of the Analytics product team to analyze and understand customer needs, explore granular data to identify key drivers that influence each KPI, measure the level of expression of attributes that move the KPI, and execute in a way that brings the best solutions to life. Most importantly our team exhibits our core values: Integrity, Excellence, Accountability and Grace.

Responsibilities:

Functional Requirements
• Data exploration skills to unravel data sources and identify the granularity, attributes & measure
• Discover data sources and determine the best EL approach and tools to bring data into the D
• Determine the data cleansing rules based on source data and ensure data replication accurac
• Coordinate & collaborate with data architects and BI engineers to ensure timely project deliver
• Understanding of data warehouse concepts and implementation methods of warehouse object
• Translate the data models from the ER diagram into physical models coded into data warehous
• Innovate reusable and configurable frameworks for data replication to build the bronze laye
• Well-versed with the data warehouse modeling concepts (star/snowflake/denormalized structures
• Define data quality rules, modeling standards, business metrics, standard processes and test case
• Understand the domain, business cases, objectives, and KPIs that need to be reported and analyse
• Participate in data discovery phase and capture the data sources, required KPIs with the formulae

Qualifications:

Technical Requirements
• Bachelor’s degree in a Technical/Quantitative subject such as Computer Science (B.E/B.Tech/MCA
• 7 - 9 years of relevant experience in the field of Data Engineering and Data Warehouse/BI solution
• Proficient in SQL, query optimization, coding stored procedures, and ad-hoc data analysis using SQ
• Develop data pipelines using some combination of ETL/ELT tools and data processing framework
• Develop DBT models as per the data model and implement the data transformations & test case
• Orchestrate data integration & transformation processing from sources to ssots in data warehous
• Design, develop, monitor and re-engineer database objects and processes/pipelines/schedules
• Use AWS services for data storage, access and retrieval and application setup and monitorin
• Design & document the data processing workflows and setup systems as per solution architecture
• Design the multi-dimensional data model in data warehouse in order to meet BI reporting needs
• Setup software engineering best practices for data pipelines with data-associated documentation
• CI/CD implementation with source code version control, QA checks and deployment automation
• Hands-on with languages like Spark, Python, Scala, R, Bash/Shell Scripting or stored procedures
• Must have worked with RDS or data lakes (MySQL, PostgreSQL, Oracle, Redshift, Snowflake, etc.)
• Hands-on experience in using ETL/ELT tools like Talend, Informatica, SSIS, Airbyte, Alteryx, Stitch etc.,
• Experience working with orchestration tools such as Airflow, Astronomer, Control-M, Prefect etc.,
• Experience working with DevOps tools such as Bitbucket, Github, Gitlab, Jenkins, CircleCI etc.,
• Experience working with AWS Cloud services such as S3, EC2, ECS, EFS, Lambda, Docker, Kubernetes
• Hands-on experience in using data-modeling tools like Erwin, DBSchema, MySQL Workbench, ER Studio

Leadership Responsibilities
• Lead a squad of data engineers to implement data warehouse solution across various projects
• Ensure adherence to the best practices of data engineering and testing across all projects
• Drive sprint planning/review and lead daily Kanban meetings in alignment with project plan
• Allocate work smartly and efficiently manage/ coach the team while monitoring the progress
• Develop systematic training plan for onboarding new joiners and upskilling existing resources",,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Kyndryl,Azure Data Engineer,"Why Kyndryl Kyndryl is a market leader that thinks and acts like a start-up. We design, build, manage, and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward - always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers, and our communities. We invest heavily in you - not only through learning, training, and career development, but also through the flexible working practices and stellar benefits that help you grow and progress long-term. And we give back - from planting 90,000 trees in our first 3 months as part of our One Tree Planted initiative to the Corporate Social Responsibility and Environment, Social and Governance practices embedded within everything we do, we are committed to powering human progress in an ethical, sustainable way. Your Role and Responsibilities Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Create and maintain clear and complete pipeline documentation, e.g., architecture diagrams and data transformations. Integrates data solutions with business processes. Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Provide actionable insights via compelling storytelling to drive business outcomes. Microsoft Certified: Azure Data Engineer Associate Required Technical and Professional Expertise Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Preferred Technical and Professional Experience Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Required Education Associate's Degree/College Diploma",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Role: AWS Data Engineer

Ex- 4 to 8 YRS

Locations- Delhi/NCR, Gurgaon, Bangalore, Ahmedabad, Pune, Indore, Mumbai, Kolkata

Must Have –
• Working on EMR, good knowledge of CDK and setting up ETL and Data pipeline
• Coding - Python
• AWS EMR, Athina, Supergule, Sagemaker, Sagemaker Studio
• Data security & encryption
• ML / AI
• Pipeline
• Redshift
• AWS Lambda

Expectations/Responsibility
• Industry experience in Data Engineering on AWS cloud with glue, redshift , Athena experience.
• Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python.
• 3+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
• Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.
• Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
• AWS engineer provides comprehensive systems administration functions on Amazon Web Services (AWS) infrastructure to include support of AWS products such as: AWS Console root user administration, Key Management, EC2 Compute, S3 Storage, Relational Database Service (RDS), AWS Networking & Content delivery (VPC, Route 53, ELB, etc.) Identity & Access Management, CloudWatch, CloudTrail, Cloud Formation, Auto Scaling, Cost and Usage Reports, and more.
• Train and guide the company’s HR engineering team on developing with aforementioned AWS tools, while also executing on specific deliverables (ingestion, Storage, integration, warehouse, visualization)
• Coach and mentor other technical resources on the team on AWS technologies
• Create ETL piplelines that are highly optimized with very large data sets
• Solve issues with data models and come up with solutions
• Developing and directing software system testing and validation procedures, programming and documentation
• Analyzing user needs and requirements to determine feasibility of design within time and cost constraints
• Provide technology expertise, direction, coordination, and consultation, in the development, integration, launch, scaling, and maintenance of new and existing products and solutions
• Establishes infrastructure technology architectures, standards, test plans, design templates and governance
• Works with the team to define standards and frameworks with regards to coding, programming, and the general development of applications for multiple platforms
• Work with business teams to understand customer issues and to investigate, prototype and deliver new and innovative solutions

FRESHERS DO NOT APPLY.",New Delhi,True,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False
Mercede,Azure Data Engineer-Karnataka-Bangalore,"Azure Data Engineer

Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, Big Data, Azure DevOps, Azure Boards, VSTS Git

Hey, do you want to change the world? Build #TheNextBigThing? Which means implementing ideas that are said to be unachievable? Like it was stated at first about smart chatbots or artificial intelligence. At MBRDI you are dealing with questions, for which there are no answers. Not yet.

About Us

The Corporate Center of Excellence (CoE) for AI, Advanced Analytics and Big Data is working on #TheNextBigThing. Besides conducting cool use cases in the field of Data Analytics and AI, we are inventing, building and running eXtollo a data analytics cloud platform based on Microsoft Azure for Daimler teams around the world. Our users trust in the cross-divisional platform to create secure and scalable cloud applications based on Machine Learning, Artificial Intelligence Big Data technologies. In addition to the provisioning and utilization of the technology we are also responsible for Daimler s data treasure the eXtollo Data Lake.

Role

We are searching you as a Data Engineer to continuously improve eXtollo within an agile working environment. You are expected to work directly with customers to understand their data demands and build end to end pipelines to get the data as they wish. Also be able to understand the current codes, pipelines and scripts which are already in production in a short time and support operations/changes.

Experience of data migration into Azure from various cloud and on-prem is a plus. You are expected to have good exposure to structured query language preferably MS Sql Server. Should be able to do PoC for the new adaptions and work independently with minimal guidance. Data warehouse/data engineering experience is appreciated. Most importantly the candidate should have real hands-on experience rather bookish/training experience.

Responsibilities
• Touch base with customers to collect the requirements and analyze them
• Design and build end-to-end data pipelines to get the data for customers
• Unit testing of the pipelines and UAT support
• Deployment and post production support
• Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents.
• Adapt changes to the existing scripts, codes and pipelines.
• Reviewing design, code and other deliverables created by your team to guarantee high-quality results
• Capable enough to own the PoCs and deliver the results in reasonable time
• Accommodate and accomplish any ad-hoc assignments
• Building CI/CD pipelines in Azure DevOps to deliver our services into various data centers worldwide
• Analyzing and solving incidents in productive environment while avoiding data loss and minimizing service outages
• Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users
Job Qualifications

Qualifications
• Bachelor s or Master s degree in Computer Science, Information Technology or equivalent work experience
• 3+ years of full time data engineering experience
• 3+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks
• Skilled in ETL/ELT process. Good working knowledge with ETL tools.
• Must be experienced with Hadoop/Big data eco systems
• Data migration experience from on premise to cloud
• Expertise in structured query language and PL/SQL
• Experienced in Powershell scripting for orchestration
• Exposure to log analytics and debugging
• Good knowledge in DevOps, Continuous Deployment and testing techniques
• Agile development experience
• Fluent English in spoken and written

Preferred Qualifications
• Microsoft Azure Certifications
• Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics
• Hands-on experience with Databricks notebooks
• Working experience in an international team or abroad
,

This job is provided by Shine.com",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
HARMAN International,Data Engineer,"What You Will Do :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis What You Need :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis",Bengaluru,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
GSPANN Technologies,Azure Data Engineer,"Should have experience in ADLS (Azure Data Lake storage) Experience implementing Azure Data Factory Pipelines using latest technologies and techniques Experience in working on Azure HDInsight Experience in working with Storage Strategy Azure developer should be able to ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements Expert in Azure Data Factory, Azure Data Lake Azure SQL Data Warehouse, Azure Functions, Databricks · Comfortable working with Spark, Python, and PowerShell Excellent problem solving, Critical and Analytical thinking skills Strong t-SQL skills with experience in Azure SQL DW DevOps, CI/CD, and Automation experience strongly preferred Able to interact with team members collaboratively Experience handling Structured and unstructured datasets Experience in Data Modelling and Advanced SQL techniques",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Uber,Data Engineer II,"What You'll Do
• Responsible for defining the Source of Truth (SOT), Dataset design for multiple Uber teams.
• Identify unified data models collaborating with Data Science teams
• Streamline data processing of the original event sources and consolidate them in source of truth event logs
• Build and maintain real-time/batch data pipelines that can consolidate and clean up usage analytics
• Build systems that monitor data losses from different sources and improve the data quality
• Owns the data quality and reliability of the Tier-1 & Tier-2 datasets including maintaining their SLAs, TTL and consumption
• Devise strategies to consolidate and compensate the data losses by correlating different sources
• Solve challenging data problems with cutting-edge design and algorithms.

What You'll Need
• 4+ years of extensive Data engineering experience working with large data volumes and different sources of data.
• Strong data modeling skills, domain knowledge, and domain mapping experience.
• Strong experience in using SQL language and writing complex queries.
• Experience with using other programming languages like Java, Scala, Python
• Good problem-solving and analytical skills
• Good communication, mentoring, and collaboration skills.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Cardinal Health,"Sr. Data Engineer, Data Engineering","Job function:

IT Quality Control is responsible for owning and implementing software testing and certification strategies for the enterprise. Debugs problems with software through standard tests and recommends solutions. Conducts defect trend analysis and continuous process improvement. Demonstrates knowledge of requirement and risk based testing principles, theories, concepts and techniques. Establishes internal IT service quality control standards, policies and procedures.

Job duties:

Create Test Strategy, define quality standards for Google Cloud Platform and define metrics to measure the efficiency and testing for applications on Google BigQuery, Dataflow and Airflow. Develop and maintain test automation frameworks, build regression test strategy and continuous testing process.

Skills:
• Ability to create test strategy balancing manual and automated testing
• 8+ years of experience with designing and implementing test frameworks in cloud
• Technical skills – SQL, Java/Python
• Good communication and collaboration skills across teams and business SMEs
• Should exhibit continuous testing mindset
• Knowledge on Devops and CI/CD process
• Develop automated test cases to be used in performance testing or as part of testing
• Identify and implement performance metrics to be measured
• Collaborate with functional and technical teams to identify test data or create through UI and database
• Generate automation or performance testing reports from execution
• Maintain record of test discrepancies, using designated QA tools
• Provide feedback of test results to development and infrastructure teams for resolution
• Review Business Requirements Documents and Functional and Technical Specifications towards determining test data scope
• Oversee defects from initial identification through post-deployment analysis
• Coordinate with other QA engineers, leadership, system administrators, architects and developers

What is expected of you and others at this level:
• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
• Participates in the development of policies and procedures to achieve specific goals
• Recommends new practices, processes, metrics, or models
• Works on or may lead complex projects of large scope
• Projects may have significant and long-term impact
• Provides solutions which may set precedent
• Apply design thinking mindset
• Independently determines method for completion of new projects
• Receives guidance on overall project objectives
• Acts as a mentor to less experienced colleagues

Cardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,True,True,False
Siemens Energy,Data Engineer (PMK Project),"A Snapshot ofYour Day

A Our team ofdata engineers supports several business teams to get needed data, prepare itproperly for the particular use cases and make it available in an efficientway. You are integrated in the business team and also work on business topics.

One strongexample is the Prescriptive Marketing team. They create forecasts for energymarkets about power prices, demand and other key factors. This is used to helpcustomers to improve their plants and thus their revenues. It is essential alsofor the data engineers to understand the big picture while fulfilling the datarequests.

You discussthe scope of the data and how to make it available, followed by implementingand maintaining data pipelines, normally Python based, with generic setups thatcan be re-used easily to scale up the market models. There is a wide range oftasks, from doing research about data sources up to presenting the data indashboards.

How You’ll Makean Impact
• As a data engineer you willsupport the team by using, building and maintaining ETL tools and pipelines forcollecting, transferring and preparing data for several use case like internalanalytics or consumer facing applications.
• You should be able to deliverrapidly in a reliable manner with the highest quality standards.
• You should be curious,passionate about problem solving, building and self-improving.

WhatYou Bring
• Master’s / bachelor’s degree in computer science or similar with a minimum 5 years of experience in the field of Software Development and Architecture.
• Deliver rapidly in a reliable manner with the highest quality standards
• Curious, passionate about problem solving, building and self-improving
• Experience in Agile development
• Fluent English Skills
• Experience working in and with international teams and stakeholders with different cultures

Technical Skills:
• Highly experienced in relational database setups and data warehouse environments
• Snowflake knowledge is a plus
• Solid experience with building ETL pipelines
• Strong SQL skills (aggregations, windows functions, pivoting etc.)
• Python knowledge
• Solid experience in software development, incl. design patterns is a plus
• Experience with OOP concepts (e. g. Java/C# background) is a plus

Working experience:
• version control systems (Gitlab) and tools like Confluence and Jira
• deployments with CI/CD pipelines
• AWS cloud environments
• Time series data
• integrating and managing structured and unstructured data in cloud based data management systems for example with PostgreSQL or Redshift, Snowflake is a plus
• Tableau dashboards is a plus

About The Team

""Let’s make tomorrowdifferent today"" is our genuine commitment at Siemens Energy to allcustomers and employees on the way to a sustainable future.

Ourteam belongs to the Software Engineering andProduct Development Function within Siemens Energy. Our missionis to grow the digital software business and develop solutions and products forour internal and external customers. This covers from edge data acquisition,data lake and processing up to development of digital twins and apps aroundcustomer assets.

Who is Siemens Energy?

At SiemensEnergy, we are more than just an energy technology company. We meet thegrowing energy demand across 90+ countries while ensuring our climate isprotected. With more than 92,000 dedicated employees, we not only generateelectricity for over 16% of the global community, but we’re also using ourtechnology to help protect people and the environment.

Ourglobal team is committed to making sustainable, reliable, and affordable energya reality by pushing the boundaries of what is possible. We uphold a 150-yearlegacy of innovation that encourages our search for people who will support ourfocus on decarbonization, new technologies, and energy transformation.

Our Commitment to Diversity

Lucky for us, we are not all the same.Through diversity we generate power. We run on inclusion and our combinedcreative energy is fueled by over 130 nationalities. Siemens Energy celebratescharacter – no matter what ethnic background, gender, age, religion, identity,or disability. We energize society, all of society, and we do not discriminatebased on our differences.

Rewards/Benefits
• The opportunity to engage inan exciting environment on challenging projects
• Strong professional supportand working with colleagues around the world
• Professional developmentopportunities within the company
• To be part of a growingfunction with a dynamic, informal, and inspiring working environment in aposition that entails a large responsibility
• Medical benefits
• Remote/Flexible work
• Time off/Paid holidays
• Parental leave
• Continual learning throughthe Learn@Siemens-Energy platform

https://jobs.siemens-energy.com/jobs",Gurugram,True,False,True,True,False,False,False,False,False,True,False,False,True,False,False,True
Snowflake,Data Engineer,"Build the future of data. Join the Snowflake team.

Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Because traditional on-premises and cloud solutions struggle with this, Snowflake developed an innovative product with a new built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.

In addition, Snowflake’s culture was built on the following values that are even more important to us today:
• Put Customers First. We only succeed when our customers succeed
• Integrity Always. Be open, honest, and respectful
• Think Big. Be ambitious and have big goals
• Be Excellent. Quality and excellence count in everything we do
• Get It Done. Results matter!
• Own It
• Make Each Other the Best
• Embrace each others’ Differences

Job Description
• Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data infrastructure that scales across the company
• Drive the design, building, and launching of new data models and data pipelines in production
• Build data expertise and own data quality for allocated areas of ownership
• Align to Product roadmap in building tools for data platform users
• Mature requirement and follow design develop and communicate model using Agile methodology for data ingestion and data tooling.

MINIMUM QUALIFICATION
• Expertise in SQL statements and modeling concepts.
• Must be aware of the cloud environment from data ingestion and modeling perspective.
• Must be strong in python
• Experience with Apache Airflow is highly desirable.
• schema design and dimensional data modeling.
• custom ETL design, implementation and maintenance.
• object-oriented programming languages.
• Understanding of API and connectors is highly desirable
• analyzing data to identify deliverables, gaps and inconsistencies.
• Experience in data warehouse space.

PREFERRED QUALIFICATION :
• BE/BTECH in Computer Science, IT or other technical field.
• Experience with data ingestions and data analytics.
• 4 years experience using Python and SQL, .",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,True
PepsiCo,Data Engineer,"Overview

This role is with the Global business services arm of Media Center of Excellence (CoE) at PepsiCo.

In this role, you will play a key role in shaping the future of media measurement strategy for PepsiCo, esp. with focus on building an understanding role of media as key Growth Driver thru ROI and related analytics.

You will be laying down strong data foundation through implementation of process & technology.

This role is the backbone of media measurement agenda at Pepsico and is responsible for building data systems and pipelines to feed into prescriptive and predictive modeling by establishing and enhancing process around data capture, storage, quality and reliability.

You will work with Media, Data engineering, Data Science and IT teams globally and within AMESA sector to identify best practices in the industry and across all PepsiCo’s brands, providing support to codify and scale best in-class methods that inspire continuous improvement in marketing effectiveness and ROIs.

Responsibilities
• Collect, structure, analyze, organize and maintain RAW data from various data sources needed for creating predictive models in structured databases in order to ensure faster model building
• Partner with PepsiCo functional teams, agencies and third parties to build seamless process for acquiring, tagging, cataloging and managing all media, Nielsen and internal data periodically in structured format as needed for measurement statistical models
• Design, build and codify data structures in efficient way to periodically feed in raw data from various internal and external sources and also manage and house model outputs for quick input to businesses;
• Build data systems and pipelines as per business needs and objectives, in this case prepare data to feed specifically to MMM and media measurement models or any descriptive or prescriptive analysis
• Promote data consistency globally to support common standards and analytics
• Establish periodic data verification processes to ensure data accuracy
• Build new technologies and algorithms to optimize any business process around creation and maintenance of databases/data lakes running of batch processes for data updation

Qualifications
• 3-6 years of experience in the field of data structures, building and managing data lakes
• Hands on experience in building database/Data Management Solutions
• Mandatory experience Python, Data modelling, data pipeline set up and meta data management
• Experience in relational databases as well as unstructured data streams
• Experience with schema design and dimensional data modeling (for ex: using data vault/snowflake/star schema)
• Hands on experience, Airflow (or any other Orchestration tools)
• Hands on experience in AWS (or any other cloud operator)
• Good to have experience on technologies like, DBT
• Good to have experience in data engineering teams in consulting or ecom/telecom sector
• Desirable - Experience optimizing larger applications to increase speed, scalability, and extensibility
• Educational Background- BE/B T ECH/ MS in computer science or related technical field",Peeramcheru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,True,True
HTC Global Services,Data Engineer-GCP,"Greetings from HTC Global Services

We are hiring Data Engineer- GCP

Skills Required:

Data Engineer- GCP

Experience with Big query, Terraform ,Hive

Experience:

3+ Years

Location:

Chennai

Notice:

Looking for candidates who can join within 15 days.

Interested candidates please drop your resume to sunitha.manohar@htcinc.com

Regards

Sunitha",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
HP,Senior Data Engineer,"The Company

HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations and dreams. We apply new thinking and ideas to create simpler, more valuable and trusted experiences with technology, continuously improving the way our customers live and work.

Position background

In the GTM analytics COE our mission is to deliver impact by building machine learning products to optimize pricing and marketing investments and provide guidance to our sales organization.

As a Big Data Engineer, you will be in a unique position to support the development one of our internal assets. You will work together with the project and asset team to understand the end state in which the data must be delivered and you will model the data using Big Data technologies like Spark.

We offer an international experience, collaborative culture, top rate experience in AI and ML and opportunity to create significant real-world impact.

What You Will Do

Create / Maintain ETL pipelines.

Ensure that processes are optimized.

Use Spark to model big volumes of data.

Contribute to the database architecture, design and implementation.

What You Will Need

Bachelor’s in computer engineering, Computer Science, Electrical Engineering, Robotics or a related field

2+ years on a similar role.Ability to work independently under a fast-paced environment, comfortable to deliver results under pressure.

You Have Strong Problem-solving Skills.Agile Experience.

Experience with modern application lifecycle management tools (Git, Visual Studio, Intellij, Code Reviews).

Proficient in at least one of the following languages: Python, PySpark, Scala, Spark, SparkR.

Experience with SQL & NoSQL databases is preferred (PostgreSQL, MongoDB, Elastic Search).

Experience in working with DataIku DSS software.

Strong analytical skills with demonstrated problem solving ability.

Who We Are

At HP, we believe in the power of ideas. We use ideas to put technology to work for everyone. And we believe that ideas thrive best in a culture of teamwork. That is why everyone – at every level in every function, is encouraged to think big, have original ideas and express and share them. We trust anything can be achieved if you really believe in it, and we will invest in your ideas to change lives and the way people work. This vision is what sets us apart as a company. At HP, we work across borders and without limits. Global virtual teams share resources, pool their big ideas to solve our biggest business opportunities. Everyone is valued for the unique skills, experiences and perspective they bring. That’s how we work at HP. And this is how ideas and people grow.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
DataTheta,Azure Data Engineer,"Experience Required: 5-10 Years

Location: Noida/Chennai

Azure Data Engineer | Job Description
• Dev / Architect
• 2+ years of experience in Azure cloud data stack such as Synapse/DW, Azure SQL DB, Azure Blob Storage
• 1+ years of experience in Logic Apps
• 3+ years of experience in Python
• 5+ years of experience in SQL
• 3+ years of experience in Databricks
• 2+ years of experience in Azure/AWS Lambda Functions
• 2+ years of experience in Microservices (REST) architecture
• Ability to project manage and work within an agile, flexible environment.
• Performs peer reviews for other data engineers’ work.
• Ensuring adherence to programming and documentation policies, software development, testing and release.
• Develop modeling, design, and coding practices.
• Experience with Lean / Agile development methodologies
• Positive attitude with great collaboration and communication skills",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Tech Mahindra,GCP Data Engineer,"Job Role - GCP Data Engineer
• Looking only GCP Data Engineers. (GCP Certification is not mandatory)
• Experience should be 4-8 Years Max.
• Candidates should be aware about BigQuery, Data Flow, Composures.
• GCP Services, SQL, Migration Process
• Migration Tools ( Plate spin, Cloud Physics, Stratozone)
• Work Location Pune/Bangalore/Noida/Chennai",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False,False
MOURI Tech,Sr. Data Engineer,"Hi Folks,

Greetings from Mouritech!!

We are hiring for Sr. Data Engineer

Mandatory Skill: Data Engineer, Python, SQL, DWH & GCP

Location: Gurgaon (Hybrid)

Exp: 4+ Yrs to 12 Yrs

Notice Period: Immediate to 30 Days Serving.

If interested pls share your profile on below mail id

surabhim.in@mouritech.com.",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
NAB,Senior Data Engineer [T500-7047],"Essential Skills:
• Advanced SQL skills (or equivalent database querying language), Database SQL skills (MySQL preferably), Able to write complex queries (subquery, window function, CTE, removing duplication), Able to analyse query bottleneck by “Explain” command.
• Construct RMDB database-Database modelling skills, Able to operate DDL (stored procedure, indexing, trigger, table, materialised view, view), Understand database modelling (normalisation)
• Develop, maintain and Implement Power BI dashboards,
• Understand batch job process, able to modify it, and solve batch job issue- Python (ODBC DB connection, Pandas, XML handling), PowerShell (General PS command), ETL experience.
• Extract meaningful insights out of the data.
• AWS experience -AWS EC2, RDS monitoring, parameter store
• Aware of GitHub skills- Merge/pull request/resolving conflicts, Pull/push.
• Translate business information requirements into a meaningful set of SQL queries.
• Develop and maintain key reporting metrics that drive business performance.
• Creating Views, Stored Procedures and Materialised Views in MySQL.
• Ability to parse XML tags into SQL human readable tables.

Job Requirements:
• Advanced SQL skills (or equivalent database querying language)
• 6+ years’ experience working in a similar role.
• 6+ years of experience working with BI tools such as Power BI
• Proficient in Python language
• Data Science enthusiast
• Understand Jenkin pipeline, Jira & Confluence
• Advanced MS Office skills, including Excel, PowerPoint, Access etc.
• Ability to deal with ambiguity, solve complex problems, and navigate large, global organisations.
• Self-motivated, assertive, analytical, and comfortable working in a fast-paced environment
• Stakeholder management

Department : Group Security

Sub Department : Cyber Security

Job code : AAZA01",Gurugram,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
SID Global Solutions,Senior Data Engineer(8+yrs),"Skillset: SQL, AWS Stack, Python, Redshift/MYSQL

Roles & Responsibilities:

Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift

Hands on Python programming.

Working knowledge on S3

AWS certification is a nice to have

Must be punctual and follow deadlines and deliver on time.

Must be able to clearly communicate with stakeholders and team",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
InVisions Ltd.,Data Engineer,"Hello people,

We are happy to assist the product and service company “Saras Analytics” in welcoming new Data Engineers to the team.

Now, a little bit about the company and the product:

Saras Analytics is a rapidly growing data management and analytics advisory firm with offices in Austin, USA and Hyderabad, India. We are a group of engineers and analysts focused on accelerating growth for e-commerce and digital businesses by setting up or transforming their data (analytics & BI) ecosystems and providing further analytics services. We are laser focused on providing the best ROI for our clients and leave no stone unturned in our quest to provide the best results for our customers.

We are an employee-centric organization and, to meet the ever-growing demand for our services, are looking for individuals who share our passion to make a difference and would be great additions to our analytics and growth consulting practice.

How Saras Analytics describes your role:

As a Data Engineer at Saras Analytics, you will be responsible for building and maintaining large-scale data pipelines as well as create and data pipelines that deal with large volumes of data.

You will deal with:
• Database programming using multiple flavors of SQL and Python.
• Understand and translate data, analytic requirements and functional needs into technical requirements.
• Build and maintain data pipelines to support large scale data management projects.
• Ensure alignment with data strategy and standards of data processing.
• Deploy scalable data pipelines for analytical needs.
• Big Data ecosystem - on-prem (Hortonworks/MapR) or Cloud (Dataproc/EMR/HDInsight).
• Work with Hadoop, Pig, SQL, Hive, Sqoop and SparkSQL.
• Experience in any orchestration/workflow tool such as Airflow/Oozie for scheduling pipelines.
• Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow.
• Understand and execute IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, writing optimized queries in Spark.
• Hands-on experience in using Spark Streaming, Kafka and Hbase.

What you bring with you:
• 4 to 6 years of experience in building data processing applications using Hadoop, Spark and NoSQL DB and Hadoop streaming. Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow is a plus.
• Expertise in data structures, distributed computing, manipulating and analyzing complex high-volume data from variety of internal and external sources.
• Experience in building structured and unstructured data pipelines.
• Proficient in programming language such as Python/Scala.
• Good understanding of data analysis techniques.
• Solid hands-on working knowledge of SQL and scripting.
• Good understanding of in relational/dimensional modelling and ETL concepts.
• Understanding of any reporting tools such as Looker, Tableau, Qlikview or PowerBI.
• Degree: Bachelor of Engineering - BE, Bachelor of Science - BS, Master of Engineering - MEng, Master of Science – MS or equivalent work experience.

Eligibility:
• Significant technical academic course work or equivalent work experience.
• Excellent communication and interpersonal skills.
• Willingness to work under labor contract, B2B contract is an option too.
• Dedicate 40 hours/weekly to Saras Analytics.

Let’s connect and check if we match!

You can state your interest by sending your CV and we will get in touch with the short-listed candidates.

We treat your personal information with respect and confidentiality, guaranteed and protected by the professional ethics, the Bulgarian and European law.

“InVisions” agency license № 2420 from 19.12.2017.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,False
LTIMindtree,Specialist Data Engineer- AZURE-ADF/ADB,"• Job Title- Specialist Data Engineer
• Primary skill- Azure+Databricks (ADF+ADB+Pyspark)
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 12 Years
• Notice Period- 0 to 30 Days
• Job Description-

Primary Skills

• 5+ years of experience in Python and Databricks.

• Deep understanding of data modelling techniques for analytical data (i.e. facts, dimensions, measures)

• Experience developing and managing reporting solutions, dashboards, etc. Design and architecture experience in data transformation.

• Should have experience with data platforms and in data transformation and extraction: some combination of ETL/ELT, table and database design, query design, performance analysis and optimization

• Worked as a data engineer or related specialty (Software Engineer/Developer, BI Engineer/Developer, DBA)

Secondary Skills

• Experience in Azure Data Factory and Azure Storage

• Hands on experience with handling of large amount of data using SQL, Azure Data Factory, Spark, Azure Cloud architecture

• Knowledge of cloud architecture and data solutions

• Proficiency in Snowflake would be added advantage.

• Excellent written and verbal communication skills",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
Rently,Data Engineer,"Must have:
• Overall experience of 4+ years
• Experience in AWS Cloud services & solutions
• Experience working with enterprise data warehouse
• Experience as an ETL/ELT Developer using various ETL/ELT tools
• Experience in SQL/NoSQL/DWH databases across SQL DB, Managed instance & Data warehouse
• Experience in AWS platform services such as S3, EMR, RedShift, Glue, Kinesis, OpenSearch, Athena, QuickSight
• Working on SnowFlake and pipeline tools like Fivetran or Matillion.
• Experience in Apache Spark, Databricks
• Experience in creating data structures optimized for storage and various query patterns like Parquet
• Experience in building secured visualization reports and dashboards with access controls
• Experience in working in an Agile SDLC methodology
• Experience in DevOps Services using Git Repos, deployment artifacts and release packages for Test & production environment
• Experience in building end-end scalable data solutions, from sourcing raw data, and transforming data to producing analytics reports
• Should have experience in developing a complete DWH ETL lifecycle
• Experience in Data Analysis, Data Modelling and Data Mart design
• Should have experience in developing ETL processes - ETL control tables, error logging, auditing, data quality, etc. - using ETL tools.
• Experience in Data Integrator Scripts, workflows, Dataflow, Data stores, Transforms, and Functions.
• Should have worked on at least 2 end-to-end implementations
• Worked on Change Data Capture on both SOURCE and TARGET levels and a good understanding of Slowly changing Dimension (SCD)
• Should be able to implement reusability, parameterization, workflow design
• Should have experience in interacting with customers in understanding business requirement documents and translating them into ETL specifications and Low/High-level design documents
• Strong database development skills like complex SQL queries, complex stored procedures
• Able to work in Agile Framework Should have exposure to Scrum meetings.

Good to have:
• Exposure to other ETL/ELT, DWT technologies
• Hands-on with Data visualization tools like Power BI, Tableau, Qlik, QuickSight etc.
• Exposure to Python on ETL and Data Visualization libraries

Additional Skills:
• Good Communication Skills.
• Able to deliver independently.
• Team player.

Professional Commitment:

Being a product based company we heavily invest in developing functional/ technology/ leadership skill sets in our team members. So candidates who are willing to commit to a minimum of 2 years need to apply.",Coimbatore,True,False,True,False,False,False,False,False,True,True,False,True,True,False,False,True
Embibe,Data Engineer,"Requirements
• Should have knowledge in Coding: Preferred Java.
• Good to have - ( Python / Scala).
• Should have Knowledge in Technologies: Spark, spark streaming, scala spark/py spark.
• Good to have Knowledge of Messaging buses like Apache Kafka/ Rabbit MQ.
• Good to have Knowledge of NoSQL databases like - MongoDB, ElasticSearch, Cassandra, Hive, Impala, ADX, Synapse, Redshift, Athena, etc.
• Should have Knowledge in building Microservices with Spring boot/ Fast-Api.",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Concentrix,Data Engineer Big Data,"Job Title:

Data Engineer Big Data

Job Description

Data Engineer Big Data

Keywords: RDBMS SQL & Spark/Hive SQL, Performance tuning, Modeling Design

Job Description

Develops and maintains scalable data pipelines

Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.

Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.

Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.

Designs data integrations and data quality framework.

Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.

Qualification:

Bachelor's Degree in Computer Science or related field

3+ years of work experience

Strong experience in SQL ( include complex SQL query , SQL performance tuning , Index , Lock )

Experience with schema design and dimensional data modeling

Experience with Hive SQL , Spark(Spark SQL, DataFrame)

Experience with near-realtime data warehouse (10-30 mins level)

Experience with data quality check

Experience in Java or Python or Shell Script

#CSS

Location:

India Bangalore - Divyashree

Language Requirements:

Time Type:

Full time

If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents

R1357932",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
CirrusLabs,AWS Data Engineer / Data Engineer / Lead AWS Data Engineer,"Job Role: Aws Data Engineer

Location: Bangalore / Hyderabad

Type: Fulltime

JOB DESCRIPTION

Data Engineer/Operational Support with Snowflake

Must-Have:
• 7+ years of experience in an Oracle/Informatica environment with knowledge of views, packages, stored procedures, functions, constraints, cursors, indexes, and table partitions.
• 7+ years of experience with an ETL tool such as SSIS, Azure Data Factory, or AWS Glue
• Strong background in a data warehouse, data management, and data analytics
• Monitor ETL production batch schedules to meet predefined SLAs.
• Resolve functional and system errors as identified by Business Partners
• Coordinate activity between Business Units and EIS to drive open action items to closure.
• Work with other technical teams to resolve infrastructure-related problems.
• Maintain a good relationship with other technology teams within the client enterprise.
• Generate, Control, and Resolve incident tickets relating to Production batch and Data availability issues.
• Serve as senior contact for production support issues and escalations.
• Enterprise L3 support to resolve production support issues in a timely manner.
• Candidate is expected to exude a take-charge attitude toward problems and thrive for excellence. This is a hands-on, delivery-focused role.
• Attempt to isolate, reproduce, and resolve problems using available systems and tools, and investigate potential workarounds for verified defects.
• Participate in the creation of Knowledge Base articles, solutions, and other related support collateral.
• To interface with Subject Matter Experts, where the problem cannot be resolved at a frontline support level.
• Good to have:
• Excellent written and verbal communication skills
• Detail-oriented; Analytical with problem-solving abilities",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Tata Technologies,Data Engineer,"Job Title : Data Engineer

Job Location : Thane(Mumbai)

Domain Knowledge:

Should be capable of carrying out the following operations on the data with any application.

• Familiarity with data loading tools like Flume, Sqoop.

• Analytical and problem-solving skills applicable to Big Data domain

• Proven understanding with Hadoop, PySpark, Hive, Hadoop

• Good aptitude in multi-threading and concurrency concepts",Thane,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Trademo,Data Engineer,"Position : Data engineer - Full Time and Intern

Role: Python/ Data scraping with Algo and Data structures with Automation

Experience: 0-1 years

Location: Gurgaon (Work from office)

About Trademo

Trademo is a Global Supply Chain Intelligence SaaS Company, headquartered in Palo-Alto, CA. Trademo collects public and private data on global trade transactions, sanctioned parties, trade tariffs, ESG and other events using its proprietary algorithms. Trademo analyzes and performs advanced data processing on billions of data points using technologies like Graph databases, NLP and Machine Learning to build end-to-end visibility on Global Supply Chains. Trademo's vision is to build a single truth on global supply chains to help large and small businesses - discover new commerce opportunities, ensure compliance with trade regulations and build operational resilience. Trademo last closed its $12.5 mn Seed Round from marquee Silicon Valley VCs.

Trademo has been founded by Shalabh Singhal who is a third-time tech entrepreneur. Shalabh last co-founded ZipLoan. ZipLoan is a leading fintech lending startup in India. He earlier founded Credence, a Data-driven Digital Marketing, CRM Product and Sales Solutions company. Shalabh is an Alumni of Goldman Sachs, IIT BHU, CFA Institute USA and Stanford GSB SEED. Trademo has recently closed a $12.5 mn Seed round from some of the marquee investors in Silicon Valley.

Website

https://www.trademo.com

Location

Gurgaon (Work from office)

Technical Skills Required
• Python 3.6+ version, Pandas
• Scraping → Selenium, Beautiful Soap
• Knowledge NOSQL/MYSQL Database
• Knowledge how to tackle the problems with optimal Solution
• Individual contributor role with eagerness to learn new technologies - Elasticsearch , BigData etc.
• Knowledge of Basics DS and algorithms",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Pracemo Global Solutions,Data Engineer,"We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Responsibilities
• Analyze and organize raw data
• Build data systems and pipelines
• Evaluate business needs and objectives
• Interpret trends and patterns
• Conduct complex data analysis and report on results
• Prepare data for prescriptive and predictive modeling
• Build algorithms and prototypes
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Requirements And Skills
• Previous experience as a data engineer or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills
• Degree in Computer Science, IT, or similar field; a Master’s is a plus
• Data engineering certification (e.g IBM Certified Data Engineer) is a plus
• Self-motivated with a results-driven approach
• Aptitude in delivering attractive presentations
• High school degree
Skills: data warehousing,etl,sql,python,java,hadoop,hive,spark,nosql databases,cloud computing,aws,azure,gcp,data modeling,data mining,data visualization,communication,project management,data lake,data quality,data architecture",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Thompsons HR Consulting LLP,Lead Data Engineer,"We are looking for Lead Data Engineer

with Strong experience in Python, Development, Business Intelligence (BI tools), AWS, Mysql

Experience: 10+ years

It is a Remote opportunity.

If interested, please share your resume at deepika.ashok@thompsonshr.com",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Anonymous,Data Engineer - Partime / Freelance,"Required skills: Pyspark, AWS-cloud, Hive
Good to have: streamsets, CICD
Experience: 2 - 5yr
Timing : 8pm to 12am on weekdays",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
New Era India,Data Engineer/Sr. Data Engineer/Lead Data Engineer - Data Axle,"Data Engineer / Sr. Data Engineer / Lead Data Engineer (Pune)

About Data Axle

Data Axle Inc. has been an industry leader in data, marketing solutions, sales and research for over 45 years in the USA. Data Axle has set up a strategic global center of excellence in Pune. This center delivers mission critical data services to its global customers powered by its proprietary cloud-based technology platform and by leveraging proprietary business & consumer databases. Data Axle is headquartered in Dallas, TX, USA.

Roles And Responsibilities
• Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
• Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
• Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency.
• Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
• Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning.
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

Basic Qualifications
• 3 to 12 years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
• Demonstrated strength in data modeling, ETL development, and data warehousing.
• Experience using big data processing technology using Spark.
• Knowledge of data management fundamentals and data storage principles
• Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, Power BI etc.)

Preferred Qualifications
• Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
• Experience working with AWS big data technologies (Redshift, S3, EMR, Spark)
• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience working with distributed systems as it pertains to data storage and computing.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.",Pune,False,False,True,False,False,False,False,True,True,True,False,False,True,False,False,False
Quadratyx,Lead Data Engineer,"About Quadratyx

We are a product-centric insight & automation services company globally. We help the world’s organizations make better & faster decisions using the power of insight & intelligent automation. We build and operationalize their next-gen strategy, through Big Data, Artificial Intelligence, Machine Learning, Unstructured Data Processing and Advanced Analytics. Quadratyx can boast of more extensive experience in data sciences & analytics than most other companies in India. We firmly believe in Excellence Everywhere.

Purpose of the Job/ Role:

As a Lead Data Engineer, your work is a combination of hands-on contribution, customer engagement and technical team management. Overall, you’ll design, architect, deploy and maintain big data solutions.

Key Requisites:

• Expertise in Data structures and algorithms.

• Technical management across the full life cycle of big data (Hadoop) projects from requirement gathering and analysis to platform selection, design of the architecture and deployment.

• Scaling of cloud-based infrastructure.

• Collaborating with business consultants, data scientists, engineers and developers to develop data solutions.

• Leading and mentoring a team of data engineers.

• Hands-on experience on test-driven development (TDD).

• Expertise in No SQL like Mongo, Cassandra etc., preferred is Mongo and strong knowledge of relational database.

• Good knowledge of Kafka and Spark Streaming internal architecture.

• Good knowledge of any Application Servers.

• Extensive knowledge on big data platforms like Hadoop; Hortonworks etc.

• Knowledge of data ingestion and integration on cloud services such as AWS; Google Cloud; Azure etc.

Skills/ Competencies Required

Technical Skills

• Strong expertise (9 or more out of 10) in at least one modern programming language, like Python, Java.

• Clear end-to-end experience in designing, programming, implementing large software systems.

• Passion and analytical abilities to solve complex problems.

Soft Skills

• Always speaking your mind freely.

• Communicating ideas clearly in talking and writing, integrity to never copy or plagiarize intellectual property of others.

• Exercising discretion and independent judgment where needed in performing duties; not needing micro-management, maintaining high professional standards.

Academic Qualifications & Experience Required

Required Educational Qualification & Relevant Experience

• Bachelor’s or Master’s in Computer Science, Computer Engineering, or related discipline from a well-known institute.

• Minimum 7 - 10 years of work experience as a developer in an IT organization (preferably Analytics /

Big Data/ Data Science / AI background.

Quadratyx is an equal opportunity employer - we will never differentiate candidates based on religion, caste, gender, language, disabilities or ethnic group.",Hyderabad,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Persistent Systems,Data Engineer (Immediate joiner),"About Persistent

We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above.

We are experiencing tremendous growth, with $701.1 million in trailing 12-month revenue, representing 29.8% year-over-year growth. Along with that growth, we onboarded over 4,500 new employees in the past year, bringing our total employee count to over 16,500 people located in 18 countries across the globe.

At Persistent, our values are more than a list of ideals to improve our corporate image. We’re dedicated to building an inclusive culture that reflects what’s important to our employees and is based on what they value. As a result, 95% of our employees approve of the CEO and 83% recommend working at Persistent to a friend.

For more details please login to www.persistent.com

About Position
• 4+ years of strong technology experience in the field of transactional data and analytics systems
• Lead client conversations and data discovery sessions
• Should understand and be able to command architecture design for transactional and analytics systems.
• Strong SQL skills
• Hands on experience in building end to end data / orchestration pipelines using Python.
• Cloud Experience- Should have experience with any cloud data products (AWS, GCP, Azure)
• Experience in Agile Methodologies
• Familiarity with source repositories (Git, BitBucket etc.)
• Excellent communication skills",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Niftel Resources,Senior Data Engineer,"Responsibilities:

 Design and build reusable components, frameworks and libraries at scale to support analytics

products

 Design and implement product features in collaboration with business and Technology

stakeholders

 Anticipate, identify and solve issues concerning data management to improve data quality

 Clean, prepare and optimize data at scale for ingestion and consumption

 Drive the implementation of new data management projects and re-structure of the current data

architecture

 Implement complex automated workflows and routines using workflow scheduling tools

 Build continuous integration, test-driven development and production deployment frameworks

 Drive collaborative reviews of design, code, test plans and dataset implementation performed by

other data engineers in support of maintaining data engineering standards

 Analyze and profile data for the purpose of designing scalable solutions

 Troubleshoot complex data issues and perform root cause analysis to proactively resolve product

and operational issues

 Mentor and develop other data engineers in adopting best practices

Qualifications:

Primary skillset:

 Experience working with distributed technology tools for developing Batch and

Streaming pipelines using SQL, Spark, Python [3+ years], Airflow [2+ years], Scala [1+

years].

 Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc.

 Able to quickly pick up new programming languages, technologies, and frameworks.

 Strong skills building positive relationships across Product and Engineering.

 Able to influence and communicate effectively, both verbally and written, with team members and

business stakeholders

 Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed

Spark jobs, build Docker images, etc.

 Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture

Good to have:

 Experience working with Data platforms, including EMR, Airflow, Databricks (Data Engineering &

Delta Lake components, and Lakehouse Medallion architecture), etc.

 Experience working in Agile and Scrum development process

 Experience in EMR/ EC2, Databricks etc.

 Experience working with Data warehousing tools, including SQL database, Presto, and

Snowflake

 Experience architecting data product in Streaming, Server less and Microservices Architecture

and platform.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,True,True
"Giant Eagle, Inc.",Senior Data Engineer,"Job Summary

As a Sr Data Engineer on the Marketing Data Platforms team, you will be working on a team to bring customer-centric personalization to life. In this role, you will be empowered to develop data solutions in support of analytics, data science, and business partners to understand capability requirements and develop data solutions based on priorities. This leading technical and architecture role will collaborate with product managers, architects, technology teams, analysts, marketing operations specialists, and monetization business partners to understand capabilities and that will be brought to life for Giant Eagle’s 4M+ customers. The ideal candidate will have experience within multiple technology platforms (e. g. GCP, Engagement Platforms, Customer Data Platform, Ad-Tech, etc.) while providing the vision and design for integrating customer data. Additional key skills and qualifications below

Job Description
• Primary Job Responsibilities:
• 5+ years of relevant technical experience working with various data engineering methodologies such as data integration and data pipelines (ETL/ELT) to activate against data at scale.
• 3+ years of experience of data modeling for analytic projects activities that include design, curation, and management of large datasets
• 3+ years of experience adeveloping on big data technologies with Spark and Hive, preferably leveraging such as DataBricks, Juypter notebooks, or GCP, AWS, and Azure equivalent technology.
• 2+ years of experience data solution design for data engineering pipelines
• Strong Experience building event driven systems using cloud technology: storage, Pub/Sub, cloud functions, API’s, and DataProc
• Expertise with databases experience such as BigQuery, Snowflake, and Synapse designing schema and dimensional data modeling
• Experience leveraging RESTful web services to collect and publish data.
• Experience in software engineering development and testing life cycles using but not limited to Python, R, Linux, Java, JavaScript, Lambda, and SQL programming
• Bachelor's degree in Computer Science, Mathematics, or other technical field or equivalent work experience. Advanced degree a plus
• Experience with Retail Media Networks and Ad Tech preferred

Role Requirements:
• Architect, develop and implement end-to-end complex data projects and technical solutions through translating business requirements into technical solutions and data-flow architectures.
• Architect, build and automate data pipelines that clean, transform, and aggregate unorganized data into data sources that are ready for analysis.
• Use expertise to apply various analytic methods to discover and interpret information about customer behavior from multiple data sources to implement analytics solutions
• Use expertise in database design to implement, operate stable and scalable dataflows from multiple marketing platforms into a cloud data lake for Ad Tech
• Experience building data visualization tools Tableau, PowerBI, and Looker with data modeling and Looker ML preferred
• Design, implement and deploy data applications and mechanisms using big data technology
• Provides subject matter expertise for multiple projects concurrently through all phases of the development lifecycle.
• Develop, enhance, govern, and administer for data platform to: collect data, transform, enrich, unify, segment, and integrate data
• Strong adherence data ethics rules around PII data sets
• Work collaboratively with IT teams, Performance Marketing team, and business leaders to ensure actionable is provided key stakeholders
• Research and analyze customer behavior data to improve customer experience
• Experience with agile or other rapid application development methods a plus
• Retail industry experience a plus

About The Company

Since our founding in 1931, Giant Eagle, Inc. has evolved into one of the top 40 largest private corporations in the U. S. and one of the country’s largest food retailers and distributors. With more than 37,000 Team Members and $9.7 billion in revenue, we are committed to investing in people, technology, and data to elevate our customer’s experience across multiple touchpoints. It helps us follow on our commitment to serving others and improving our communities.

About Giant Eagle Bangalore

The Giant Eagle GCC in Bangalore is our global capability center. Our team of more than 370 members at the GCC enables us to expand internal capabilities in the areas such as data analytics, merchandising and eCommerce, quality engineering, and automation to generate insights for faster decision-making and helping us accelerate our business strategy. Our team in India plays a pivotal role in helping the company transition to new ways of working by redefining the food and grocery shopping experience for over 4.6 million customers.

About Us

At Giant Eagle Inc., we’re more than just food, fuel and convenience. We’re one giant family of diverse and talented Team Members. Our people are the heart and soul of our company. It’s why we strive to create a nurturing environment that offers countless career opportunities to grow. Deep caring and solid family values are what makes us one of the top work places for jobs in the Greater Pittsburgh, Cleveland, Columbus and Indianapolis Areas. From our Warehouses to our GetGo’s, our grocery Stores through our Corporate home office, we are working together to put food on shoppers' tables and smiles on their faces. We’re always searching for the best Team Members to welcome to our family. We invite you to join our Giant Eagle family. Come start a lasting career with us.",,True,False,True,True,False,False,True,False,False,True,False,False,False,True,False,True
TMRW House of Brands,Data Engineer-III,"Responsibilties:

Create, implement, and operate the strategy for robust and scalable data pipelines for business intelligence and machine learning.

Develop and maintain core data framework and key infrastructures

Create and support the ETL pipeline to get the data flowing correctly from the existing and new sources to our data warehouse.

Data Warehouse design and data modeling for efficient and cost-effective reporting

Collaborate with data analysts, data scientists, and other data consumers within the business to manage the data warehouse table structure and optimize it for reporting.

Constantly striving to improve software development process and team productivity

Define and implement Data Governance processes related to data discovery, lineage, access control, and quality assurance

Perform code reviews and QA data imported by various processes

Qualifications

6-10 years of experience.

At least 3+ years of experience in data engineering and data infrastructure space on any of the big data technologies: Hive, Spark, Pyspark(Batch and Streaming), Airflow, and Delta Lake.

Experience in product-based companies or startups.

Strong understanding of data warehousing concepts and the data ecosystem.

Strong Design/Architecture experience architecting, developing, and maintaining solutions in AWS.

Experience building data pipelines and managing the pipelines after they’re deployed.

Experience with building data pipelines from business applications using APIs.

Previous experience in Databricks is a big plus.

Understanding of Dev Ops would be preferable though not a must

Working knowledge of BI Tools like Metabase, and Power BI is plus

Experience in architecting systems for data access is a major plus.",Bengaluru,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False
Impetus,GCP Data Engineer,"Qualification
• The candidate should have extensive production experience (3-5 Years ) in GCP, Other cloud experience would be a strong bonus.
• Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
• Exposure to enterprise application development is a must

Role
• 6-10 years of IT experience range is preferred.
• Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.
• Strong experience in Big Data technologies – Hadoop, Sqoop, Hive and Spark including DevOPs.
• Good hands on expertise on either Python or Java programming.
• Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
• Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
• Ability to drive the deployment of the customers’ workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
• Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
• Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
• Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
• Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.",इन्दौर,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Newell Brands,Cloud Data Engineer,"Job Title: Cloud Data Engineer

Report To: Sr. Manager, Data Engineering

Job Location: Guindy, Chennai, India

Job Duties
• Participates in the full lifecycle of cloud data architecture (Preferably Azure cloud) from gathering, understanding end-user analytics and reporting needs.
• Migrate On-Prem applications and build CI/CD pipeline in Cloud platform.
• Design, develop, test, and implement on Cloud platform (Ingestion, Transformation and export pipelines that are reliable and performant) .
• Ensures best practices are followed and business objectives are achieved by focusing on process improvements.
• Quickly adapt by learning and recommending new technologies and trends.
• Develop and Test Data engineering related activities on Cloud Data Platform.
• Work with dynamic tools within a BI/reporting environment.

Job Requirements
• B.E/B.Tech, M.Sc/MCA.
• 5+ years experience in Rapid development environment, preferably within an analytics environment.
• 3+ years experience with Cloud experience (Preferably Azure cloud but not mandatory).
• DB : T-SQL, SQL Scripts, Queries, Stored Procedures, Functions and Triggers
• Language : Python / C# or Scala
• Cloud: Azure / AWS / Google cloud
• Frameworks: Cloud ETL/ELT framework

Preferred
• Azure Data Factory, Azure Synapse Analytics, Azure SQL, Azure Data lakes, Azure Data bricks, Airflow and Power BI
• Data warehousing principles and frameworks.
• Knowledge in Cloud DevOps and CI/CD pipelines would be an added advantage.

Newell Brands (NASDAQ: NWL) is a leading global consumer goods company with a strong portfolio of well-known brands, including Rubbermaid, FoodSaver, Calphalon, Sistema, Sharpie, Paper Mate, Dymo, EXPO, Elmer's, Yankee Candle, Graco, NUK, Rubbermaid Commercial Products, Spontex, Coleman, Campingaz, Oster, Sunbeam and Mr. Coffee. Newell Brands' beloved, planet friendly brands enhance and brighten consumers lives at home and outside by creating moments of joy, building confidence and providing peace of mind.",Chennai,True,False,True,False,False,False,False,False,True,False,False,False,False,False,True,False
Zupee,Lead Data Engineer,"About Zupee

Zupee is India’s fastest growing Technology backed Behavioral Science company. We are innovating Skill-Based Gaming with a mission to become the most trusted and responsible entertainment company in the world. We have been constantly focusing on innovation of indigenous games to entertain the mass.

Our strategy is to invest in our people & user experience to drive profitable growth and become the market leader in our space. We have been experiencing phenomenal growth since inception and running profitable at EBT level since Q3, 2020. We have closed Series B funding at $102 million, at a valuation $600 million.

The company also announced a partnership with Reliance Jio Platforms, post which Zupee games will distribute its content across all customers using Jio phones. The partnership now gives Zupee the biggest reach of all gaming companies in India, transforming it from a fast-growing startup to a firm contender for the biggest gaming studio in India.

About The Job

Lead Data Engineer

We are looking for someone to develop the next generation of our Data platform

collaborating across functions like product, marketing design, growth, strategy, customer

experience and technology.

Core Responsibilities

●Understand, implement and automate ETL and data pipelines with up-to-date

industry standards

●Hands-on involvement in the design, development and implementation of optimal and

scalable AWS services

What are we looking for?

●S/he must have experience in Python

●S/he must have experience in Big Data – Spark, Hadoop, Hive, HBase and Presto

●S/he must have experience in Data Warehousing

●S/he must have experience in building reliable and scalable ETL pipelines

Qualifications and Skills

●6-12 years of professional experience in data engineering profile

●BS or MS in Computer Science or similar Engineering stream

●Hands-on experience in data warehousing tools

●Knowledge of distributed systems such as Hadoop, Hive, Spark and Kafka etc.

●Experience with AWS services (EC2, RDS, S3, Athena, data pipeline/glue, lambda, dynamodb etc.
•",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
UST Product Engineering,Data Engineer,"Job Description :

- 4-8 Years experience in data warehousing , ETL processes, and data analytics.

- Good experience in developing, maintaining, and testing infrastructures for data generation, verification and transformation.

- Good understanding database concepts (SQL, Cloud DBs)

- Strong SQL query, profiling and troubleshooting skills

- Good understanding AWS Data related concepts like big data, big query etc.

- Basic understanding AWS (or supported) ETL tools - Glue, Airflow etc etc. would be an added advantage

- Good Understanding of python programming

- Basic understanding of programming language like C# or similar

- Basic knowledge of working in scrum/agile teams and tools like JIRA, confluence etc.",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.

Apply for this job",Mumbai,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
ThousandEyes,Cloud Application and Data Engineer,"Cloud Application and Data Engineer

Who We Are

The name ThousandEyes was born from two big ideas: the power to see what’s not ordinarily possible, and the ability to collect intelligence from vantage points as diverse and global as the Internet. As organizations depend on cloud services, the Internet has become their defacto network connecting cloud applications to users. Our Internet and cloud intelligence platform is like a ‘Google maps of the Internet’, providing the only collectively powered view of digital experiences end-to-end. We enable our customers made up of the world’s largest and fastest-growing brands, to identify problems before they impact revenue, brand reputation, or employee productivity.

In August 2020, Cisco Systems completed the acquisition of ThousandEyes, which now forms the ThousandEyes Business Unit within Cisco’s Network Services Business Group,and is a foundational component of Cisco’s growing Observability business.About The Team

Digital experiences rely on a vast ecosystem of ISPs, cloud providers, SaaS applications, individual configurations, unique devices, and many other external services that are critically dependent on the Internet. Trying to identify the root cause of a problem or a deviation from normal is like finding a needle in a haystack. This leads to long downtimes and poor customer or employee experience.

The AI Analytics team at ThousandEyes is leveraging machine learning at scale, while working across several different business units, to our help customers answer tough questions like:
• What is normal in my network and how do I catch deviations from this normal?
• How do I understand the root cause of a problem in my network or application stack?
• How do I ensure that devices joining my network are who they say they are?
• How do I know when my networking gear is about to break?

The goal of the AI Analytics Team is to leverage different machine learning techniques to deliver actionable insights for our customers to solve real world problems in their complex environments.

What You’ll Do

You will be part of our data and platform team. A worldwide distributed team responsible for data collection, ingestion, processing, and quality. You will play an important role in helping to deliver new ML powered features to our customers as well as monitoring and improving the existing features for performance and quality. You will work in the AI Cloud hosted on AWS with Python, Go, Spark, Hive, Open Search, and other cutting-edge technologies.

Responsibilities
• Collaborating closely with ML engineer to bring new features to production.
• Create and maintain an optimal data pipeline architecture.
• Contribute and operate data quality tooling.
• Monitor and optimize compute and query performances.
• Troubleshoot and debug issues across our applications and services.

Who you are

Agile, pragmatic and hardworking. You also love to interact with data scientists, machine learning engineers and software engineers to develop pipelines that scale seamlessly at huge volumes of data. You love technology, innovation and building products at scale.

You hold a degree in computer science, or a related field and you can demonstrate a consistent track record in the following areas:
• At least 4 years of software development experience.
• 2 years of experience building and developing data-intensive systems at industrial scale.
• Dimensional data modeling and schema design for both SQL and NoSQL databases.
• Prior exposure to data science, machine learning or statistics is a plus.
• Previous experience developing applications running on a public cloud infrastructure is a plus.
• Strong Communication and documentation skills in English.
• Strong sense of ownership, drive, attention to detail and ability to work in a distributed team.

We are looking for candidates based in Bangalore to work hybrid

Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.

Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do –you can’t put us in a box! But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. So, you have colourful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us.

We recognize that diverse teams make the strongest teams, and we encourage people from all backgrounds to apply.

Cisco COVID-19 Vaccination Requirements

The health and safety of Cisco's employees, customers, and partners is a top priority. Our goal is to protect and mitigate the spread of COVID-19 infection for strong business resiliency during the pandemic. Therefore, Cisco may require new hires to be fully vaccinated against COVID-19 if the role requires business-related travel, meeting with customers/partners (including visiting third-party sites on behalf of Cisco), attending trade events, and Cisco office entry, unless otherwise prohibited by applicable law, and in countries where COVID-19 vaccination is legally required. The company will consider legally required accommodations/exceptions for medical, religious, and other reasons as per the requirements of the role and in accordance with applicable law. Additional information will be provided to candidates about the requirements and accommodation process at the offer time based on region.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Verizon,Principal Engineer - Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

You will be expected to architect solutions for business projects, work with enterprise architects to align application & system architecture to enterprise strategy and deliver individually and/or with the help of a team. You need to have passion to learn and educate fellow associates/subordinates and guide them to follow best practices. Principal consultant to the team that develops, maintains and enhances the service delivery and management for NS applications
• Architecting/Developing solutions for the application which deals with big data platforms.
• Engaging with Enterprise Architects on HLAs and defining new solutions that adhere to big data volume processing.
• Driving a Culture of Innovation: Champion a culture of innovation and drive as an example.
• Supporting customers with major platform issues and coordinating triage efforts to solve them.
• Identifying and aligning project requirements and conducting impact analysis.
• Working closely with the business team, and other internal IT teams to deliver projects on time.
• Preparing presentations and reports to internal and external customers, as well as internal Executives.
• Evaluating various new technical products based on changing business needs and making product recommendations to management keeping in mind the architecture of the entire list of applications supported.
• Providing technical leadership and business-related subject matter expertise on large, highly complex projects.
• Guiding the team on best practices for efficient and streamlined delivery of software to production. Guiding teams on maintaining security posture and code quality of applications keeping the tech debt in check.
• Identifying chronic production issues, pain points of customers by evaluating feedback and monitoring the NPS to maintain it above the required threshold.
• Working with Quality Assurance, UAT & Production Support teams to support releases, troubleshoot progression/regression issues, integration & E2E testing and implement deliverables as per the targeted timelines.
• Working with infrastructure teams to implement DevOps capabilities that help streamline the CICD process. Leverage innovative technologies to build proof-of-concepts that help build customer experiences, reduce pain points in the current experience, and provide a delight factor to customers.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You view technology through a lens of making things better and more effective. Understanding and building continual improvements to the digital value chain is something you flourish with. You enjoy the process of solving complex issues while empowering the team around you to do the same.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Experience in Hadoop, Hive, Pyspark , Spark Scala, PIG, Java, GCP, AWS, CICD (Jenkins/ Gitlab).
• Experience in Big query, Composer, Cloud Functions & Java script.
• Experience with any of RDBMS, Druid and MongoDB.
• Experience in Devops & automation.
• Experience in Docker/K8s & SRE Practice.
• Experience in Agile & SAFe methodologies.

Even better if you have one or more of the following:
• A Master's degree.
• Ability to design products which can scale up for large volumes of data.
• Knowledge in Wireless Domain.
• Knowledge of Security Vulnerabilities.
• Strong written and verbal communication skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False
lululemon India Tech Hub,Data Engineer - SQL & Python,"We are looking to hire dynamic Data Engineers for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.

Responsibilities
• Uses structured tools for analysis and presentation of concepts and models to enhance the BRD
• Develop, maintain and deliver training materials to the supply chain end-users
• Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs
• Support day to day reporting needs where required
• Support production issues as relate to application functionality and integrations
• Excellent spoken and written communication skills (verbal and non-verbal)
• Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)
• Solid scripting capability for analysis and reporting (ANSI SQL)
• Solid experience in RDBMS and NoSql technologies
• Strong analytical skills to support BAs.
• Strong problem-solving skills (Math skills required for data modeling)
• Ability to work as an integration / data engineer.
• Ability to manage and complete multiple tasks within tight deadlines
• Possess expert level understanding of software development practices and project life cycles.
• Working experience with Java batch spring boot/ python.
• Working Experience with cloud-native technologies
• Must have: Working experience in dealing with big data and data manipulation.
• Desired: Familiarity with Retail planning / merchandising systems/ supply chain.
• Desired: Familiarity with DevOps practices like CICD pipeline
• Desired: Retail experience is a plus. (fashion retail experience would be ideal)
• Must Have: Working experience with cloud platforms namely AWS
• Must Have: Working experience with large data sets (at least 80 – 100 GB data)

Requirements
• name : lululemon India Tech Hub
• location : Bengaluru, IN
• experience : 5 - 8 years
• Primary Skills: SQL or RDBMS or NoSQL,Python,AWS,Springboot,ETL",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Splunk,Senior Data Engineer - 27505,"The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will work as part of an evolving Enterprise Data Management (EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.

The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible for building and scaling the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineers and Infrastructure teams plus other data roles, including Data Governance, Quality, and Architecture Stewards). Strong technical experience within enterprise software is essential.

Responsibilities:
• Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
• Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
• Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
• Identify and document standard methodologies, standards, and architecture guidelines
• Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
• Identify ways to improve Data Reliability, Data Efficiency and Data Quality

Required Qualifications, Skills & Experience:
• 7+ years of data engineering related experience such as data analysis, data modeling, and data integration.
• Experience with Sales Operations, Partner Operations and customer success business processes and applications
• Experience in custom ETL design, implementation, and maintenance
• Strong knowledge of programming languages (e.g. Python and Object Oriented Programming)
• Hands-on experience with SQL database design
• Experience working on CI/CD processes and source control tools such as GitHub and GitLab
• Experience working in Snowflake and relational databases
• Extensive hands-on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake
• Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
• Experience with or exposure to data governance & quality principles and practices
• Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
• Passionate about data solutions, technologies, and frameworks
• Experience in Data Visualization tools such as Tableau

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Knowledge of enterprise systems such as Salesforce, Workday, SAP etc.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
Reverate,Senior Data Engineer - Remote,"Reverate Tech is a product and service-based start-up, working with International Clients. Our services include Data Engineering, Web Development, BI/Data Warehousing, Enterprise Application Implementation (ERP/CRM), and NetSuite. Our product portfolio has business apps in the domain of ERP, Auto Service, and Personal Safety.

This is an exciting opportunity to work as Senior Data Engineer for our client SellerX.

SellerX is the 3rd fastest growing company in the EU evaluated at more than 1 Billion Euros. It has an ambitious goal: to become a leading global acquirer and operator of a new generation of eCommerce businesses.

Your Job:
• You are responsible for all types of data management processes (collection, storage, cleansing, preparation, maintenance, accumulation, transfer to business reporting).
• You optimize and develop existing and new data warehouse applications using tools for data ingestion and data modeling
• You design and document new data models and best-practice solutions.
• You are responsible for prototyping and implementing new ETL jobs and modeling approaches.
• As a data engineer, you will deal with python programs and their configurations in order to create or improve automated data engineering tasks.
• In addition to technical project management, you advise our other tech teams in Data Management aspects.
• Ensuring data security (e.g. encryption) and improving the backup strategy.
• You work hand in hand with data architects, data analysts, and data scientists.
• You ensure that quality, stability, and robustness along the entire process chain meet our high standards.

Your Background:
• You have a bachelor's degree with a focus on software engineering.
• 5+ years of experience in data engineering.
• Strong with Algorithms and have worked on scaling pipelines/solutions
• Hands-on experience with data ingestion tools like Fivetran, Daton, Stitch, or Data Virtuality.
• Hands-on experience with ETL and orchestration tools like Apache Airflow or similar.
• Object-oriented Python programming is more than just a plus.
• Expert knowledge in the areas of data modeling and ETL processes on the SQL level (e.g. using DBT), as well as experience working with REST APIs, is beneficial.
• DevOps experience
• In addition to your ""hands-on"" mentality, you score points with a high technical affinity and a strong analytical mindset.
• Your working standards do not suffer in terms of quality, even in hectic times.

Benefits:
• Compensation: up to 40 LPA
• 100% remote-working;
• Flexible working hours;
• Development of your personal strengths in a dynamic environment;
• An attractive and varied job with a high level of personal responsibility;
• A collegial togetherness and a modern management style/startup;

Interested? Join us and start your learning and growth journey.

Reverate focuses on Software Engineering. Their company has offices in India. They have a small team that's between 11-50 employees.

You can view their website at https://reverate.tech/",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
"6221, Roche",Senior Data Engineer,"The Position

Roche sequencing solution is developing the next generation sequencing based on nanopore technology. This has the potential to make sequencing based diagnostics cheaper, faster and more accurate enabling precision medicine and early diagnosis of many diseases improving the health outcome.

As part of Data Science Automation group, you will get to work on key software technologies enabling research and development of sequencer. You will solve complex problems related to processing terabytes of data coming out sequencer and deriving useful insights from the data. This requires massively parallel computation locally on GPU as well as in the cloud. You will gain exposure to latest and greatest in data engineering and data pipeline tools and technologies. You will also work with advanced data visualization problems involving millions of data points.

You also will get to collaborate with multidisciplinary team of scientists and engineers working in fields ranging from protein engineering, bio chemistry, biophysics, stats modeling, bioinformatics and deep learning.

If you are excited to become part of the next generation sequencing research and development and revolutionize the healthcare, we have a rare opportunity for you to come and work with us.

We need an experienced Data/Workflow Engineer with a strong background in designing and developing highly scalable data management solutions and workflow pipelines. You will work across a variety of problems and application spaces involved in high availability systems, for data management and compute systems, at a very large scale. You will be working with a hardworking team of engineers and data-scientists who are passionate about building creative and novel solutions at the forefront of Sequencing research.

Required Qualifications:
• BS in CS or similar and 10+ years professional experience, or MS with 7+ years of experience in building highly scalable, performant software systems, in a Linux environment.
• Strong, hands on experience building and supporting Enterprise level Workflow management systems such as airflow, nexflow, kubeflow etc. Experience with building performant airflow pipelines with a large number of DAGs and dynamic DAGs is desirable.
• Working experience in deploying and managing airflow platforms, knowledge of Terraform, Kind, Helm etc. is a huge plus.
• At least 3+ years’ experience of developing solutions using container and cloud technologies. Preferably Kubernetes, Docker.
• Experience building with cloud native technologies (e.g. GCP, AWS), blob stores and knowledge of various data compression formats.
• Demonstrated skill with software development following current software engineering best practices using languages such as: Python, Java and BASH scripting.
• Have a strong understanding of modern software development practices and tools, including: version control systems (e.g., Git), issue trackers, and test frameworks.
• Experience building and using automation tools, CI/CD, unit testing.

You 'll go above and beyond our required requirements if you...
• Possess a PhD/MS in Computer Science, Computer Engineering, or another related, technical discipline.
• Have at least ten years of relevant experience in the development of software systems ideally in a Linux environment.
• Have experience using modern frontend and backend software frameworks for software applications.
• Knowledge of challenges involved in large-scale, high-availability data platforms. Experience with designing and implementing platforms providing secured access to large datasets.
• Have experience applying software expertise to full project lifecycles, including requirements analysis, design, implementation, and testing.

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,True,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False
Koch,Senior Data Engineer,"Description

Position Description/ Requirements

The Data Engineer will be a part of an international team that designs, develops and delivers Data Pipelines and Data Analytics Solutions for Koch Industries. Koch Industries is a privately held global organization with over 120,000 employees around the world, with subsidiaries involved in manufacturing, trading, and investments. Koch Global Solution India (KGSI) is being developed in India to extend its IT operations, as well as act as a hub for innovation in the IT function. As KSGI rapidly scales up its operations in India, it’s employees will get opportunities to carve out a career path for themselves within the organization. This role will have the opportunity to join on the ground floor and will play a critical part in helping build out the Koch Global Solution (KGS) over the next several years. Working closely with global colleagues would provide significant international exposure to the employees.

The Enterprise data and analytics team at Georgia Pacific is focused on creating an enterprise capability around Data Engineering Solutions for operational and commercial data as well as helping businesses develop, deploy, manage monitor Data Pipelines and Analytics solutions of manufacturing, operations, supply chain and other key areas.

A Day In The Life Could Include:

(job responsibilities)
• Partner/collaborate with Business stakeholders and build high-quality end-to-end data solutions.
• Build a data architecture for ingestion, processing, and surfacing of data for large-scale applications in the cloud (AWS/ Azure)
• Create and maintain optimal data pipeline architecture.
• Follow best practices of Agile and DevOps focusing on the delivering of high-quality products and providing the ongoing support to meet the customers' needs
• Implement processes for Continuous integration, Test automation and Deployment (CI/CD Pipelines)
• Provide quality documentation of your design (process and workflows) and implementation including experiment tracking / logs.
• Provide on-call support on an as-needed basis
• Handle support cases to ensure issues are recorded, tracked, resolved, and follow-ups finished in a timely manner.

What You Will Need To Bring With You:

(experience & education required)
• Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science). Master’s degrees preferred.
• 6+ years of IT experience.
• In depth knowledge of Data Engineering concepts and platforms - SQL based systems, Hadoop, Spark, Distributed computing, In-memory computing, real time processing, pub-sub, orchestration, etc.
• Expertise of building data pipelines using (Pyspark based) and Databricks utilising techniques in Azure or AWS.
• 4-5 years of experience in DevOps and CI/CD using tools like Git, Terraform, Jenkins, Ansible.
• 5+ year of experience in Data modeling, SQL, Data Warehouse skills are a MUST.
• A passion and fearlessness for learning new technologies and methods in the areas of Administration
• Ability to thrive in a team environment and juggle multiple priorities.
• Excellent written and verbal communication skills.

What Will Put You Ahead:

(experience & education preferred)
• In depth knowledge of entire suite of services in AWS/Azure Cloud Platform.
• Strong coding experience using Pyspark.
• Experience of designing and implementing ETL process using SSIS.
• Cloud Data Anaytics/Engineering certification.

Other Considerations:

(physical demands/ unusual working conditions)
• Some work may involve hours outside of normal KGS works hours.

Koch is proud to be an equal opportunity workplace",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Greetings from TCS !!!

TCS India presents excellent opportunities for IT professionals.

Role :- Data Engineer

Experience:- 7 to 10 years

Location- Bangalore / Mumbai / Chennai / Bhubaneswar

Required Technical Skill Set- Data Engineer – Big Data, Hadoop, Hive, Spark, Yarn

Must-Have:-

1. 4-8 Yrs of hands-on development experience

2. Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is mandatory.

3. Experience working with various data exchange formats (JSON, CSV, XML etc.).

4. Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred.

5. Excellent knowledge of SQL and Linux shell scripting.

6. Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)

Good-to-Have:-

1. Experience building real-time data pipelines using Kafka or spark streaming is preferred.

2. Exposure to Microsoft Azure (or other cloud) platforms is preferred.

3. Experience with Agile methodologies for project development.

4. Excellent diagnostic, analytical and problem-solving skills are preferred.

5. Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred

TCS Eligibility Criteria:
• BE/B.Tech/MCA/M.Sc/MS with minimum 3 years of relevant IT-experience post Qualification.
• B.Sc Graduates with minimum 4+ years of relevant IT-experience post qualification.
• Only Full Time courses would be considered.
• Consistent academic records class X onwards (Minimum 50%)
• Candidates who have attended TCS interview in the last 3 months need not apply.

Interested candidate can share their resumes with the mandatory details mentioned below.

Please update the details:

1. Total years of Exp:

2 Email ID :

3. Present Company:

4. Current & Preferred Location:

5. Mobile No.:

6. Current CTC:

7. Expected CTC:

8.Notice Period:

9: Working With TCS /CMC ( Direct Payroll) earlier (Yes/ NO):

10. No Of job change-

Interested candidate can share their resumes with hiba.fathima@tcs.com",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LTIMindtree,GCP Data Engineering POD Lead,"Primary Skill – GCP Data Engineering POD Lead

Total Exp – 3 to 14 Years

Notice Period – 0 to 30 Days

Job Location – Kolkata, Bangalore, Mumbai, Pune, Chennai, Hyderabad

Job Description:

Job Description:

TPrimary Skill – GCP

Secondary Skill – Python, Big query

Overall, more than 8+ Yrs of experience in Data Science Statistical Modeling and Projects to Develop and Deliver Data Science work Strong understanding of Machine Learning Statistics fundamentals Technology Skill Set Python R Pandas Scikit Learn R s

Desired Candidate Profile Technology & Engineering Expertise

• 5+ years of experience in implementing data solutions using GCP/SQL programming

• Proficient in dealing data access layer, RDBMS | NO-SQL.

• Experience in implementing and deploying Big data applications with GCP Big Data Services.

• Good to have SQL skills.

• Experience with different development methodologies (RUP | Scrum | XP) Soft skills

• Able to deal with diverse set of stakeholders

• Proficient in articulation, communication, and presentation

• High integrity

• Problem solving skills & learning attitude

• Team player Key Responsibilities

• Implement data solutions using GCP and need to be familiar in programming with SQL/python.

• Ensure clarity on NFR and implement these requirements.

• Work with Client Technical Manager by understanding customer’s landscape & their IT priorities

• Lead performance engineering and capacity planning exercises for databases",,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Arcadis,Azure Data Engineer,"ARCADIS is looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in “Everything Digital, Digital Everything”. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.

Technology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.

Role accountabilities:
• Possess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment
• You are excited about working with Azure Data Platform
• challenges while building the next wave of software engineering solutions
• Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform
• Leading the craftsmanship, security, availability, resilience, and scalability of your solutions
• Very strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.
• Strong experience in
• T-SQL, SSIS, SSAS, SSRS
• Azure Data Factory
• Azure Data Lake Store
• Azure Data Lake Analytics (Good to have, not mandatory)
• Azure SQL DB
• Azure SQL DW
• Azure Analysis Services, DAX
• Azure Data Bricks with Python/Scala
• Experience in building end to end solution using Azure data analytics platform.
• Experience in building generic framework solution which can be reused for upcoming similar use cases.
• Experience in building Azure data analytics solutions with DevOps (CI/CD) approach.
• Experience in using TFS, Azure Repos.
• Mentor peers to gain expertise on Azure data platform solutions skills.
• Experience in developing, maintaining, publishing, and supporting dashboards using Power BI.
• Strong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded

Qualifications & Experience:

Basic Qualifications:
• Bachelor in Engineering/Math/Statistics/Econometrics or related discipline
• Should have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.
• Preferred Qualifications:
• Master’s or Minor in Computer Science
• 3+ years of experience developing Data Engineering solutions
• Architecture, design experience with good knowledge of data model design & their implementation.

Why Become an Arcadian?

Our work with clients has a direct impact on people’s lives and on the planet. We make moving, living and belonging in cities safer, more resilient and more sustainable. By partnering with our clients as responsible custodians of our earth's resources, we can create a sustainable planet.

We continue to think of new ways to make positive impacts and create better experiences for people; data driven and digital solutions have become part of the Arcadis DNA. Working together with clients and using techniques like design thinking, we can get to the heart of our clients’ most pressing challenges and work together to solve them.

As a global business, we have committed to support five of the UN’s Sustainable Development Goals to ensure that our projects contribute to a better and more sustainable future for all. But it’s not just the work that we do on client projects that benefits communities and our planet. As a global business, we are committed to making a positive impact to society by supporting local communities where we operate.

To help protect our planet, we monitor and measure non-financial information to inform business decisions and reduce our own environmental impact as part of our commitment to be net zero carbon as a global company by 2030.

Our Commitment to Equality, Diversity, Inclusion & Belonging:

We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.

In accordance with the Colorado Equal Pay Transparency Rules:

Arcadis offers benefits for full time positions. These benefits include medical, dental, and vision coverage along with a 401K plan, STD and LTD, and Life Insurance as well as some additional optional benefits. Full time positions also come with annual PTO days and at certain levels a bonus program may apply. The Salary range for this role is $61,360 - $95,000 for Colorado based positions only. Other locations will vary in salary range

Transform Your World",Hyderabad,True,False,True,False,False,False,False,True,True,False,True,False,False,False,False,False
Dolby Laboratories,Data Engineer,"Join the leader in entertainment innovation and help us design the future. At Dolby, science meets art, and high tech means more than computer code. As a member of the Dolby team, you’ll see and hear the results of your work everywhere, from movie theaters to smartphones. We continue to revolutionize how people create, deliver, and enjoy entertainment worldwide. To do that, we need the absolute best talent. We’re big enough to give you all the resources you need, and small enough so you can make a real difference and earn recognition for your work. We offer a collegial culture, challenging projects, and excellent compensation and benefits, not to mention a Flex Work approach that is truly flexible to support where, when, and how you do your best work.

Play a key role as part of Dolby's new R+D Center in Bangalore as a Data Engineer in our Advanced Technology Group ""ATG"". ATG is the research and technology arm of Dolby Labs. It has multiple competencies that innovate technologies in audio, video, AR/VR, gaming, music, and movies. Many areas of expertise related to computer science and electrical engineering, such as AI/ML, computer vision, image processing, algorithms, digital signal processing, audio engineering, data science & analytics, distributed systems, cloud, edge & mobile computing, natural language processing, knowledge engineering and management, social network analysis, computer graphics, image & signal compression, computer networking, IoT are highly relevant to our research.

Responsibilities:

As a Data Engineer, you’ll be a part of a growing engineering team building and designing our core data infrastructure for our internal technology research and development efforts. You’ll have the chance to partner closely with our research and data science teams to understand data and functional requirements. We are looking for an experienced data professional who is a problem solver, logical thinker and passionate about everything relating to data and analytics. Your responsibilities include:
• Create and maintain optimal data pipeline architecture for data coming from different sources, in various formats and of different content type (text, audio, video etc.) allowing to standardize, clean and ingest data.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Design and develop solutions which are scalable, generic and reusable. Be responsible for collecting, storing, processing, and analyzing huge sets including, but not limited audio, video, and metadata.
• Develop techniques to analyze and enhance both structured/unstructured data and work with big data tools and frameworks.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Databricks, and AWS ‘big data’ technologies.
• Create data tools for research and data scientist teams.

What You Bring To The Role
• BsC/Msc degree in CS or EE. Work experienced desired, but not required.
• Experience building and optimizing streaming big data pipelines, architectures, and data sets.
• Deep understanding data pipeline frameworks including Databricks and Fivetran.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Experience or solid theoretical understanding of data workflows including:
• Ingestion
• Batch and stream processing
• Storage and archiving
• Visualization/Reporting and Dashboards
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Understanding of the current state of infrastructure automation, continuous integration/deployment - CI/CD, SQL/NoSQL, security, networking, and cloud-based delivery models.
• In-depth understanding of:
• NoSql databases (Kafka, HBase, Spark, Hadoop ,Cassandra, MongoDb etc). SQL development and any procedural extension language (T-SQL, PL/SQL, Pg/PLSQL etc.)
• Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Distributed data processing frameworks like Apache Spark, Apache Flink
• Scalable ML pipelines for image, video and audio modalities with tools such as Flyte, MLflow, Prefect, or AirFlow
• Data collection, labeling, cleaning, and generation tools such as LabelBox, SuperAnnontate, Scale Ai, or V7
• Scripting abilities with two or more general purpose programming languages including but not limited to Java, C/C++, C#, Objective C, Python, JavaScript.
• Data modeling and extraction of data from different sources
• Strong documentation skills, communication and client facing Experience
• Experience supporting and working with cross-functional teams in a dynamic environment.

Build your career profile, also within the Careers tab in Employee Central to open the possibility of new opportunities finding you. Express your interest. If you want to express your interest in a specific opportunity and be contacted by a recruiter, click the apply button associated with the relevant job description. The Recruiter is the only one who will see your application.

Please refer to the recruiting website for more information: https://jobs.dolby.com/careers

]]>",Bengaluru,True,False,True,True,False,True,True,True,False,False,False,True,False,False,True,False
Mercede,Positions for Data Engineer,"Technical Skills Competencies
• Deep hands-on expertise in Databricks (Scala or Python).
• Experience in Design and implementation of Big Data technologies (Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases) and familiarity with data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture).
• Experience in working as a Big Data Engineer: query tuning, performance tuning, troubleshooting, and debugging Spark and other big data solutions.
• Familiarity with a full range of data engineering approaches, covering theoretical best practices and the technical applications of these methods.
• Experience building and deploying a range of data engineering pipelines into production, including using automation best practices for CI/CD.
• Very good experience in writing SQL queries.
• Hands-on experience with any of the cloud providers such as AWS or Azure.
• Familiarity with databases and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Ability to juggle and prioritize multiple tasks within a collaborative team environment
• Desire to learn and grow both technical and functional skill sets, and drive team s potential
• Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience And Skills

Microsoft Azure and AWS Certifications
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Trained in Data Factory, Delta lake, Data bricks Notebooks
• Working experience in SAFe - Scaled agile framework
• Working experience in an international team environment
,

This job is provided by Shine.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
HuQuo,Interesting Job Opportunity: Azure Data Engineer - ETL/MDM,"Job Description
• To collaborate with various teams/regions in driving facilitating data design, identifying architectural risks and key areas of improvement in data landscape, and developing and refining data models and architecture frameworks
• Technical experience and knowledge in Cloud Data Warehousing, data migration and data transformation
• Develop and test ETL components to high standards of data quality and performance as a hands-on development lead
• Familiarity with Data Lakes, Data Warehouses, MDM, BI, Dashboards, AI, ML
• Design data architecture patterns and ecosystems including data stores (operational systems, data lakes, data warehouses, data marts), ingress patterns (API, streaming, ETL/ELT), and egress patterns (analytics/decision tools, BI tools). Lead, consult or oversee multiple architectural engagements
• Oversee and contribute to the creation and maintenance of relevant data artifacts (data lineages, source to target mappings, high level designs, interface agreements, etc.) in compliance with enterprise level architecture standards
• Experience in leading and delivering data centric projects with concentration on Data Quality and adherence to data standards and best practices.
• Experience in data modeling, metadata support, development and testing for enterprise wide data solutions
• Azure cloud experience is a must have with familiarity of the services: Azure Databricks, Azure Datafactory, Azure Datalake, Spark SQL, PySpark, Airflow, SQL server and Informatica MDM.
• Additional exposure to GCP and AWS is good to have.

Key Skill: Azure Databricks, ADF, ETL, Pipeline Dev, SQL, DWH, ADLS.

(ref:hirist.com)",Gurugram,False,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
AXA XL,Data Engineer,"Gurgaon, Haryana, India

The Application Developer plays a critical role within the Data and Analytics SDC as this person is responsible for designing and implementing data structures to support current and future analytical projects. We are looking for candidates that have experience working with data from a raw, unprocessed state and organizing it intuitively. Building this data pipeline enables our partners to analyze data better and faster – ultimately leading the organization in optimizing the decision-making process.

DISCOVER your opportunity

What will your essential responsibilities include?
• Candidates for this role should have experience developing data processes with source data in a variety of formats (structured / unstructured, databases, APIs) into a target state. This will involve building proper data pipelines to support initial exploration and real-time integration.
• Data development using appropriate tools and techniques to process data required for advanced analytics. A candidate would be expected to interact with Data Engineering Leads and Data Scientists to understand requirements and would be responsible for the development of the solution.
• Providing the right context of data required for a given analysis. This would require the candidate to work with data modelers/analysts to understand the business problems they are trying to solve and create data structures to feed into their analysis.
• Build upon learnings of internal and external data to become more proactive. This includes thinking ahead of what modelers will anticipate with their data needs and designing structures that are intuitive to use.
• Making sure quality and understanding of analytical data. This would require hands-on data experience to look into data issues and seek resolution or acceptance. Create the appropriate amount of documentation, leverage standards, and build upon them. Data should be reconciled and documented at various stages for integrity.
• Take part in developing governance and rigor of data management practice within the Data and Analytics SDC. This will also include partnering with enterprise IT groups and involvement in enterprise data-related functions.
• You will report to Data Manager/Principal Data Engineer.

SHARE your talent

We’re looking for someone who has these abilities and skills:
• Demonstrated ability to work through data complexities which include a variety of sources, formats, and structures. Robust preference for experience in the Insurance domain.
• Ability to see through ambiguous concepts and break down complex problems into manageable components.
• Detail-orientated, proven ability to recognize patterns in data.
• Demonstrated ability to incorporate data quality standards into data development.
• Possesses natural curiosity. Seek to understand the world around you, and question when appropriate.
• Robust SQL Skills required.
• 2-4 years of development experience using data development (visual ETL or coded) / analysis tools (ex. SAS, SPSS, R, Microsoft SSIS/SSAS, Informatica, DataStage, AbInitio).
• Experience in .NET, Python, or Java development is a plus.
• Experience in web extraction, unstructured data, advanced text parsing, machine learning, and NLP a plus.
• Familiarity with developer support tools (TFS/GIT, Jenkins) is a plus.
• College Degree in MIS, Information Technology, Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, or equivalent.

FIND your future

AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals, and even some inspirational individuals we don’t just provide re/insurance, we reinvent it.

How? By combining an effective and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business − property, casualty, professional, financial lines, and specialty.

With an innovative and flexible approach to risk answers, we partner with those who move the world forward.

Learn more at axaxl.com

Inclusion & Diversity

AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic.

At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That’s why we have made a strategic commitment to attract, develop, advance, and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It’s about helping one another — and our business — to move forward and succeed.
• Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability, and inclusion with 20 Chapters around the globe
• Robust support for Flexible Working Arrangements
• Enhanced family-friendly leave benefits
• Named to the Diversity Best Practices Index
• Signatory to the UK Women in Finance Charter

Learn more at axaxl.com/about-us/inclusion-and-diversity. AXA XL is an Equal Opportunity Employer.

Sustainability

At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called “Roots of resilience”, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations.

Our Pillars
• Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society – are essential to our future. We’re committed to protecting and restoring nature – from mangrove forests to the bees in our backyard – by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans.
• Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions.
• Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We’re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting.
• AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL’s “Hearts in Action” programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day – the Global Day of Giving.

For more information, please see axaxl.com/sustainability

Flexible Work Eligible

None

AXA XL is an Equal Opportunity Employer.

Location

IN-HR-Silokhera Gurgaon

Job Field

IT

Schedule

Full-time

Job Type

Standard",Gurugram,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,False
Inference Labs,Data Engineer,"Responsibilities for the job Key Responsibilities: - Data Model Designing, Developing and maintaining Data pipelines on cloud (AWS Platform) Translate business needs to technical specifications and framework Maintain and support data mart, data analytics platforms & application. Perform quality assurance to make sure the data correctness Develop sub-marts using SQL and OLAP function to fulfil immediate/ad-hoc need of the business users basis the comprehensive marts Monitoring of the performance of ETL and Mart Refresh processes, understand the problem areas and open a project to fix the performance bottlenecks. Other Responsibilities (If Any):- Availability during month-end Deck generation, may be sometime during week-end/holidays. Eligibility Criteria for the Job Education B.E/B.Tech in any specialization, BCA, M.Tech in any specialization, MCA Work Experience Data Engineer: 4+ years of experience in data engineering on cloud platforms like AWS, Azure, GCP Exposure with working on BFSI domain / big data warehouse project Exposure to manage multiple source of the information, both structured / unstructured data Manage data lake environment for point in time analysis (SCD Type 2), multiple refresh during the day, event based refresh Should have exposure on Managing environment having real time dashboard, data mart requirement. Primary Skill Must have orchestrated using any of the cloud platforms Expert in writing complex SQL Command using OLAP Working experience on BFSI Domain Technical Skills Must have orchestrated at least 3 projects using any of the cloud platforms (GCP, Azure, AWS etc.) is a must. Must have worked on any cloud PaaS/SaaS database/DWH such as AWS redshift/ Big Query/ Snowflake Python/Java Hands - on Exp from data engineering perspective is a must Experience with any of the object-oriented/object function scripting languages: Python, Java, Scala, Shell, .NET scripting, etc. is a must Experience in at least one of the major ETL tools (Talend + TAC, SSIS, Informatica) will be added advantage Management Skills Ability to handle given tasks and projects simultaneously in an organized and timely manner. Soft Skills Good communication skills, verbal and written. Attention to details. Positive attitude and confident.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
PwC,Data Engineer-Manager-P&T Labs,"Line of Service
Internal Firm Services

Industry/Sector
Not Applicable

Specialism
IFS - Internal Firm Services - Other

Management Level
Manager

Job Description & Summary
A career in National Special Functions, within Internal Firm Services, will provide you with the opportunity to support service, sector, and market leaders deliver the unique PwC client experience to our clients. You’ll play an important part in continuously innovating and improving Firm operations so that we can continue to provide the highest quality of services to our current and prospective clients.

Our team focuses on representing data as a strategic business asset to help serve our clients. You’ll focus on using data and information across PwC to drive change and improvements in data related operations to help enable the business as well as provide insights related to attendant risks.

Preferred Knowledge/Skills:

Demonstrates intimate knowledge and/or a proven record of success in the following areas:
• Understanding architectural design and data platform delivery in technologies that include, but are not limited to cloud, ETL, data streaming, data storage, data modeling, APIs/microservices, automation, continuous integration/continuous deployment;
• Showcasing work experience as a Data Engineer, Data Architect or similar role;
• Showcasing data engineering knowledge around complex efforts within established Software Development Lifecycles and methodologies including agile, scrum, iterative and waterfall;
• Showcasing technical knowledge that spans multiple platforms and portfolio of applications with demonstrated knowledge of the business strategic priorities in order to resolve complex problems;
• Utilizing IT processes and frameworks including, but not limited to, Identity Access Management (IdAM), Enterprise Application Integration, Data Warehousing, Business Intelligence, Reporting, Mobility, Master Data Management, and Search;
• Understanding of database structure principles;
• Showcasing advanced experience building and maintaining optimal data pipeline architecture and data streaming and integrations using tools such as ADF, SSIS, Informatica, API Management, Enterprise Service Bus (preferably Kafka);
• Showcasing advanced SQL knowledge and experience working with relational databases and performance optimization;
• Demonstrating data mining and segmentation techniques;
• Exhibiting knowledge in relational SQL, NoSQL and Big Data technologies;
• Understanding Data Federation/Virtualization technologies, such as PowerBI, Tableau, D3.js, and implementing Cloud based solutions;
• Assessing and analyzing system requirements;
• Showcasing analytical skills and a problem-solving attitude;
• Demonstrating virtual leadership and motivational skills;
• Recommending and participating in activities related to the design, development and maintenance of the Enterprise Data Architecture;
• developing internal relationships and PwC brand;
• Demonstrating time management skills with the ability to handle multiple projects simultaneously;
• Leveraging business knowledge and interpersonal skills to build, maintain, and influence relationships with leaders throughout the business and IT.

Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements
Not Specified

Available for Work Visa Sponsorship?
No

Government Clearance Required?
Yes

Job Posting End Date
May 10, 2023",Hyderabad,False,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Mindera,Data Engineer,"We are looking for an experienced Data Engineer to join our team.

Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.

As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.

This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.

National and international expected travelling time varies according to project/client and organisational needs: 0%-15% estimated

Requirements

You’re great at
• Python
• AWS like (Glue, S3, EMR, Athena and ECS/Fargate)
• SQL
• Airflow
• Data Modelling
• Pyspark

It also would be cool if you have
• Exposure to DBT would be preferable
• Experience working with modern data platforms such as redshift or snowflake would be preferable
• Experience working with Airflow, Docker, Terraform and CI/CD would be preferable
• Experience working with docker, Scala, and Kafka would be an added advantage

What You Will Be Doing
• Implement/support new data solutions in the data lake/warehouse built on the snowflake
• Develop and design data pipelines using python.
• Design and Implement Continuous Integration/Continuous Deployments pipelines.
• Perform Data Modelling using downstream requirements.
• Develop transformation scripts using advanced SQL and DBT.
• Write test cases/scenarios to ensure incident-free production release.
• Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
• Debug production and development issues and provide support to colleagues where necessary.
• Perform data quality checks to ensure the quality of the data exposed to the end users.
• Build strong relationships with team, peers and stakeholders.
• Contributes to overall data platform implementation.

Benefits

We offer
• Flexible working hours (self-managed)
• Competitive salary
• Annual bonus, subject to company performance
• Access to Udemy online training and opportunities to learn and grow within the role

At Mindera we use technology to build products we are proud of, with people we love.

Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.

We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world.

You get to work with a bunch of great people, and the whole team owns the project together.

Our culture reflects our lean and self-organisation attitude.

We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.

Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook

Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Rishabh Software,Big Data Engineer,"Job Description:

Roles and Responsibilities:

1) Work with BigData Practice Tech lead to Execute BigData Projects

2) Work with Techlead , helping him in Solutioning, Architecture and Technical Design

3) Analyze requirements and prepare low level design

4) Hands on implementation of Data ingestion, data processing and Data storage code and algorithms

5) Team management under Techlead guidance - including work distribution and delivery

6) Participate in potential client meetings and demos

Required Skills:

Any one programming Language - Java or Scala

Good Experience with Apache Spark

Any one Data integration platform - Kafka or similar

Any one No Sql data storage - S3 or No Sql Database

One live BigData project - Data ingestion , processing and Storage

Basic Cloud Exposure - AWS preferred

Excellent Analytical and problem solving skills.

Excellent Communication Skills",Vadodara,False,False,True,True,False,False,False,True,False,False,False,True,False,False,False,False
Cloud Software Group,Senior Data Engineer,"About Cloud Software Group

Cloud Software Group combines the capabilities of both Citrix and TIBCO, creating one of the world’s largest cloud software providers, serving more than 100 million users around the globe. When you join Cloud Software Group, you are making a difference for real people, each of whom count on our suite of cloud solutions to get work done – from anywhere. Members of our team will tell you that we value diverse lived experiences, varied perspectives, and having the courage to take risks. Our teams are encouraged to learn, dream, and build the future of work. We are on the brink of another Cambrian leap - a moment of immense evolution and growth. And we need your expertise and experience to do it. Now is the perfect time to move your skills to the cloud.

Position Summary

This is an individual contributor role with responsibility for supporting all data warehouse processes including technical analysis, design, development, implementation, and support of ETL solutions. The ideal candidate needs to have at least 5 years of experience developing with Microsoft SQL applications in an implementation and support role of a business intelligence organization.

Primary Duties / Responsibilities

Responsibilities will include, but are not limited to:

• Supporting the designs, tasks, and continuous improvements to maintain a scalable data warehouse

• Analyzing and validating data to ensure that business requirements are satisfied

• Creating data flow diagrams to depict business logic relating to data transformations

• Creating conceptual, logical, and physical data models for relational and dimensional solutions

• Breaking down, estimating, and executing increments of work

• Developing ETL packages of high complexity to fulfill all the business requirements

• Supporting deployment and delivery of defined technical solutions

• Communicating accurate and timely project status, issues, risks, and scope changes

• Performing root cause analysis of data discrepancies

• Creating data dictionaries and business glossaries to document data lineages, data definitions and metadata for all business-critical data domains

• Documenting all work (both technical and procedural) and ensuring that co-workers understand how to support system from an operational perspective

• Working in a highly collaborative team environment following the Agile Framework for planning and executing deliverables

Qualifications (include knowledge, skills, abilities, and related work experience)

• Bachelor’s degree in computer science or related field, or equivalent combination of education and recent, relevant work experience

• Minimum 5 years of experience in developing T-SQL Queries, Stored Procedures, and ETL packages with Microsoft SQL databases

• Strong understanding of data warehouse design and report development principles

• Experience in creating data flow diagrams and data models pertaining to business intelligence

• Experience in analyzing and developing reporting output such as Power BI, Tableau, or SSAS

• Strong interpersonal and problem resolution skills

• Strong teamwork and customer support focus

• Strong written (technical documentation) and verbal communication skills

• Ability to handle numerous conflicting priorities in a professional manner

Cloud Software Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all federal, state and local laws that prohibit employment discrimination on the basis of age, race, color, gender, sexual orientation, gender identity, ethnicity, national origin, citizenship, religion, genetic carrier status, disability, pregnancy, childbirth or related medical conditions, marital status, protected veteran status, and other protected classifications.",Bengaluru,False,False,True,False,False,False,False,False,True,True,True,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"Roles and responsibilities:
• Mandatory: Strong in Azure, ADF, Data Lake, Databricks, Pyspark
• Hands-on-experience in developing data lake solutions using Azure (Azure data factory for ingestion, Data Lake gen 2 and Azure SQL server for storage, Azure analysis service for transformations, Azure data bricks)
• Implement a robust data pipeline using Microsoft Stack.
• Create reusable and scalable data pipelines.
• Development and deployment of new data platforms.
• Leverage Azure BI services for development of Big Data Platforms.
• Work closely with the Product Owners and Architects to develop Azure Data Platforms.
• Work with the leadership to set the standards for software engineering practices within the team and support across other disciplines.
• Produce high-quality code that allows us to put solutions into production.
• Refactor code into reusable libraries, APIs, and tools.",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Affine,Data Engineer,"Company Description

About Company

http://www.affine.ai

""AFFINE"" cited by GARTNER as a SPECIALIST MIDSIZE CONSULTANCY in ANALYTICS and MACHINE LEARNING solutions and services. Click to Read More ""

Affine is a provider of high-end analytics services to solve complex business problems with offices in NJ, USA & Bangalore, India. We combine data driven algorithmic analysis with heuristic domain expertise to provide actionable solutions that empower organizations make better and informed decisions. Affine's value proposition is enabling clients to implement and realize ROI of the recommendations.

Affine has a group of people with significant experience in Analytics industry along with solid pedigree, deep business understanding and strong problem solving acumen. Our group primarily consists of Statisticians, Operations Researchers, Econometricians, MBAs and Engineers. Our employees have experience of working for many Fortune 500 companies.

Job Description

What the candidate will do:
• Contribute to adoption of cloud & cloud-based technologies and good design practices, while finding opportunities to simplify and scale
• Resolve problems and roadblocks as they occur with peers and help unblock junior members of the team. Follow through on details and drive issues to closure
• Define, develop, and maintain artifacts like technical design or partner documentation
• Drive for continuous improvement in Data engineering process within an agile development team
• Own and deliver assigned sprint tasks and help drive the team forward.
• Communicate and work effectively with geographically distributed cross functional teams

Experience

4 to 6 Years in Deploying models, Sage Maker or TensorFlow

Required skillset.
• Big Data: Spark, Kafka, Hadoop, Hive, SQL and NoSQL
• Cloud: AWS, EMR, Qubole/Databricks, VPC
• Devops: Docker containers and Jenkins. Spinnaker is preferred but not required.
• Programming languages: Scala and Pyspark is mandatory
• Agile and scrum experience and working with a remote team (nice to have, not required)

Must Have Skills
• Spark, AWS, Scala/Python, SQL, Java
• ML ops tools:Tensorflow or Sagemaker

Additional Information

Others
• Quick learner
• Excellent written and oral communication skills
• Excellent interpersonal & organizational skills
• Good listening and comprehension skills",Bengaluru,True,False,True,True,True,False,False,True,False,False,False,False,False,False,False,False
DISH Careers,Data Engineer,"About DISH:

DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world’s best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH’s largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market

Job Duties and Responsibilities:
• Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production
• Creation and optimization of data engineering pipelines for analytic projects
• Strong analytic capability and the ability to create innovative solutions
• Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues
• Provide technical guidance to L1 team members and help to resolve ETL related issues
• Need to work as on call-support

Skills, Experience and Requirements:
• Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment
• Experience in developing/debugging and fixing data ingestion pipelines both real time and batch
• Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb
• Experience in transforming/integrating data in Redshift/Snowflake
• Strong in writing complex SQLs to ingest data into cloud data warehouses
• Good hands on experience in shell scripting or python
• Experience with scheduling tools - ControlM, Airflow , StepFunction
• Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements
• Perform root cause analysis (RCA) for failures
• Good Communication skills – written and verbal with the ability to understand and interact with the diverse range of stakeholders
• Capable of working without much supervision",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
MPOWER Financing,"Data Engineer - Data and Analytics - Bangalore, India","THE COMPANY

MPOWER’s borderless loans and scholarships enable students from around the world to realize their full academic and career potential by attending top universities in the U.S and Canada.

As a mission-oriented fintech/edtech company, we move extremely quickly and leverage the latest technologies, global best practices, and heavy analytics to tackle one of the biggest challenges in financial inclusion. We’re backed by over $150 million in equity capital from top global investors, which enables fast growth and provides our company with financial stability and a clear path to an IPO over the coming years.

Our global team is composed of former management consultants, financial service and technology professionals, and other experts in their respective fields. We work hard, have fun, and believe strongly in our cause. For us, MPOWER’s mission is personal.

As a member of our team, you’ll be challenged to think quickly, act autonomously, and constantly grow creatively in an environment where fast change and exponential growth are the norm. Ideation and implementation happen very quickly. We value feedback and emphasize personal and professional development by providing the resources you need to further your skills and grow with the company. MPOWER is committed to cultivating your strengths and curiosity and helping you make an immediate impact.

MPOWER has been named one of the best fintechs to work for by American Banker for 2018, 2019, 2020, and 2021. We pride ourselves on being a “growth company for grown-ups,” where there are no pool tables but rather great health, education, and maternity/paternity benefits instead. Our team diversity has been recognized as well; we’re one of the most diverse workforces in the world in terms of nationality, gender, religion, age, sexual orientation, and educational background.

THIS IS A FULL-TIME POSITION, BASED IN OUR BANGALORE OFFICE

THE ROLE

You will be tasked with building and maintaining MPOWER’s data infrastructure. You’ll also play a key role in acquiring, organising and analysing data to provide insights that enable the company in making sound business decisions. This includes, but is not limited to:
• Maintaining MPOWER’s database and building on the existing database infrastructure
• Establishing the needs of different users and monitoring user access and security
• Capacity planning and refining the physical design of the database to meet system storage requirements
• Creating efficient queries and tools to obtain data for different business needs
• Building data models to identify, analyze and interpret trends or patterns in data sets that inform business decisions and strategy
• Working with various internal and external stakeholders to maintain and develop enhanced data collection systems
• Performing periodic data analyses, creating and presenting findings and insights
• Performing scheduled data audits in order to locate and correct code errors and maintain data integrity
• Collaborating with MPOWER’s global tech team to build data collection and data analysis tools

THE QUALIFICATIONS
• Undergraduate degree in computer science; advanced degree preferred
• 5+ years of experience in database programming, database administration and data analysis
• Must have prior experience in building high quality databases in accordance with end users information needs and views
• Proficiency in Big Data and Hadoop ecosystems.
• Deep familiarity with database design and documentation
• Hands-on expertise and exposure to at least one database technology (MySQL, PostgreSQL)
• Advanced knowledge of R/Python, PySpark, or Scala is a plus
• Prior experience building data pipelines and data orchestration is a plus.
• Superior analytical and problem solving skills
• Proven ability to create and present comprehensive reports
• Ability to multitask and own several key responsibilities at a given time
• Passion for excellence: constantly striving to improve professional skills and business operations

A passion for financial inclusion and access to higher education is a must, as well as comfortable working with a global team across multiple-time zones and sites!

In addition, you should be comfortable working in a fast growth environment, meaning a small agile team, fast-evolving roles and responsibilities, variable workload and tight deadlines, a high degree of autonomy, and 80-20 everything.

MPOWER Financing focuses on Financial Services, Finance, Finance Technology, Higher Education, and Education Technology. Their company has offices in New York City, Washington DC, and Washington. They have a small team that's between 11-50 employees. To date, MPOWER Financing has raised $7.291M of funding; their latest round was closed on October 2016.

You can view their website at http://www.mpowerfinancing.com/ or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,False,False,False
Cortex Consultants LLC,Data Engineer,"Hi,

Welcome to Cortex

Job Title: Data Engineer

Job Description

2+ years of Data Engineer experience in Snowflake (on Azure Cloud Preferred).

Strong knowledge of SQL to build queries and Optimization techniques.

Strong Knowledge of the ETL process using SSIS / ADF (Azure Data Factory) / Matillion

Experience of Python programming is an added advantage.

Location: Chennai

Work type-Hybrid

Immediate joiners

Interested candidates share your resume to

Deepak.g@cortexconsultants.com

Contact No: 9080100600",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Roche,Data Engineering Manager,"The Position
Engineering Manager is a critical leadership role in our Data Engineering team. This is a people management role that needs the ability to hire and grow top engineering talent and to manage multiple teams. It includes responsibility to deliver and operate high quality, scalable, and extensible products & solutions, including making appropriate design and technology choices. The role requires strong strategic thinking and making build/buy/partner decisions for technical capabilities. Effective Communication is critical, as you will be working closely with a variety of stakeholders to understand and address their needs. A healthcare background with experience in integrating healthcare IT systems would be good to have.

KEY RESPONSIBILITIES
• Manage team of Data Engineers working on multiple data analytics products.
• Work with different agile product teams, understand and fulfill their staffing needs.
• Work with business stakeholders to develop high level project plans and roles and responsibilities.
• Prepare training and development plans for the team.
• Understand and create a career path for the team members.
• Evolve and develop a long-term roadmap for team and projects.
• Apply data engineering best practices in terms of quality, security, scalability and maintainability.
• Participate in how the budget and staff is allocated for the projects.
• Maintain project time frames, budget estimates and status reports.
• Create management, communication plans and processes. Analyze and develop process for management and technical duties.
• Foster team bonding and trust within the team. Responsible for hiring, growing and motivating engineers on your team, ensuring you recruit and retain top talent.

REQUIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• BS degree in Computer Science, Computer Engineer or a related technical discipline with 10+ years of IT industry experience.
• At least 4-6 years of proven managerial experience developing a high-performing team.
• Experience in Agile Solution Delivery and Operations Management and people management.
• Quick learner with the ability to understand complex workflows and develop and validate innovative solutions to solve difficult problems.
• Strong communication, with the ability to explain complex technical problems to non-technical audiences and the ability to translate customer requirements to technical designs.
• Strong interpersonal skills, with proven ability to navigate complex corporate environments and influence stakeholders and partners.

DESIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• Proven work experience in AWS or other cloud related technologies.
• Experience of working in product based organization
• Proven work experience as an Engineering Manager or similar role
• Communication skills for overseeing staff and working with other management personnel
• Organizational skills for keeping track of various budgets, employees, and schedules simultaneously
• Leadership, team-building, and mentoring skills
• Personnel and project management skills
• Ability to work on multiple projects in various stages simultaneously
• Experience in the Healthcare Laboratory domain is a plus.

EDUCATION

Bachelor’s degree in Engineering

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Data.Ai,DNA Team - Data Engineer,"data.ai is the mobile standard and the trusted source for the digital economy. Our vision is to be the first Unified Data AI company that combines consumer and market data to provide insights powered by artificial intelligence. We passionately serve enterprise clients to create winning digital experiences for their customers.

We care deeply about our high-performance culture and operate as a global team. We put our customers at the center of every decision [Customer First], follow through with what we say we are going to do [Own It & Deliver] and propose solutions, not just issues [Challenge, Them Commit] to Win As A Team.

We are a remote-first company and we trust our people to get it done from the location that works for them.

What can you tell your friends when they ask you what you do?

As a DNA Team Data Engineer, I’ll be a key contributor to DNA team data services. I’ll help the DNA team to build and enhance internal processes of data production and transaction/transformation, as well as internal tools. And help colleagues from other teams and/or external clients to better experience the DNA team services.

You will be responsible for and take pride in…
• Exciting Projects using technical expertise across Python, SQL, Spark, DataBricks
• Build data pipeline across different data sources/databases such as AWS S3, PG database, and Snowflake
• Produce and maintain relevant documentation
• Support internal and external customers
• Becoming better at what you do every day

You should recognize yourself in the following…
• Bachelor’s degree in Computer Science, Engineering, or equivalent experience
• At least 5 years of related work experience in building data pipelines
• Strong skills in Python and PL/SQL
• Deep understanding and experience in building data pipelines across different data sources/databases such as AWS S3, PG database, and Snowflake
• Experience in data processing such as ETL
• Knowledge of machine learning and AI is preferred
• Familiarity with specific app markets (e.g.: Gaming, Entertainment, Finance, etc.) is a big plus
• Strong problem-solving, analytical, and troubleshooting skills
• A self-starter who identifies and solves problems before anyone has noticed
• Fluent in English, both written and oral

data.ai are in the process of establishing an entity in India, in the interim the employees will be on the rolls of Leap 29 our Global Employer of Record",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Danske Bank,Senior Data Engineer-ETL Datastage,"Experience 5-8Years

The ideal applicant should have the following skills:

- Strong technical experience in Data Warehousing and Experience in working with ETL tools (Datastage, Informatica etc) for the purpose of creating data marts for analytical purposes

- Strong understanding of relational database concepts & technology. Exposure to Big Data technologies is an added advantage.

- Strong analytical and problem solving skills with the ability to collect, organize, analyse and process large volumes of data in a complex environment

- Good written and verbal communication skills with the ability to communicate and articulate one's thought process clearly.

- Be self driven and work closely with business stakeholders, in a global environment, to gather enough context to translate the business
objective into an analytical solution.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Splunk,Data Engineer - 27516,"As a Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have a deep understanding of the architecture for enterprise-level data lake solutions using multiple platforms (RDBMS, AWS, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.

What you'll do: Yeah, I want to and can do that.
• As a Data Engineer, you will be responsible for engineering data pipelines for Splunk’s enterprise data platform, democratizing datasets, enabling advanced analytics capabilities, integrating data from various systems, and applications. You will work as part of an evolving Enterprise Data Management(EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. The role will collaborate closely with other specialists, Product Managers & key stakeholders across the company.
• Build large-scale batch and real-time data pipelines using the cloud data technologies, such as Snowflake, Matillion, Kubernetes, Python, Apache Airflow and Apache Kafka
• Serve as a resource for data management implementations on other technology teams and collaborate with data owners, business owners, and leaders.
• Supports the design and development of framework based data integration and interoperability across multiple Splunk Business applications.
• Advanced level skills in Python, SQL, data integration, data modeling and data architecture.

Requirements: I’ve already done that or have that!
• A minimum of 5 years of related experience
• 3+ years of experience as a Data Warehouse Architect or Data Engineer.
• 2+ years of experience driving adoption and building automation of data management services and tools.
• 2+ years of experience with API based ELT automation framework, data management, or interface design, development and maintenance.
• Large scale design, implementation and operations of Cloud data storage technologies such as AWS Redshift, Snowflake, Kubernetes, etc.
• 3+ years of experience with programming scripting and data science languages such as Python, SQL, etc.
• Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.
• Advanced knowledge of Big Data concepts in organising both structured and unstructured data

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Experience with Sales Operations, Partner Operations and customer success business processes and applications

Education: Got it!
• Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Verizon,Manager-Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

As a Manager for Data Engineering team, you will be managing data platforms and implementing new technologies and tools to further enhance and enable data science/analytics, focus to drive scalable data management and governance practices. Leading the team of data engineers & solutions architects to deliver solutions to business teams.
• Driving the vision with leadership team for data platforms enrichment covering the areas like Data Warehousing/Data Lake/BI across the portfolio.
• Defining and executing on a plan to achieve that vision.
• Building a high-quality Data engineering team and continue to drive to scale up.
• Ensuring the team adheres to the standard methodologies on data engineering practices.
• Building cross-functional relationships with Data Scientists, Data Analysts and Business teams to understand data needs and deliver data for insight solution.
• Driving the design, building, and launching of new data models and data pipelines.
• Driving data quality across all data pipelines and related business areas.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You are curious and passionate about Data and highly scalable data platforms. People count on you for your expertise in data management in all phases of the software development cycle. You create environments where teams thrive and feel valued, respected and supported. You enjoy the challenge of managing resources and competing priorities in a dynamic, complex and deadline-oriented environment. Building effective working relationships with other managers across the organization comes naturally to you.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Two or more years of experience in leading the team and tracking the end-to-end deliverables.
• Experience in end-to-end delivery of Data Platform Solutions and working on large scale data transformation.
• Knowledge of Spark, Hive, Scala, Pig, Kafka, Pulsar, Nifi, Python, Shell scripting.
• Knowledge of Google Cloud Platform/BigQuery.
• Knowledge of Teradata.
• Experience in working with DevOps tools like Bitbucket, Artifactory, Jenkins.
• Knowledge of Data Governance and Data Quality.
• Experience in building / mentoring the team.

Even better if you have one or more of the following:
• Master’s degree.
• Experience in data engineering, big data, hadoop and DevOps technologies.
• Certifications in any Data Warehousing/Analytical solutioning.
• Certification in program/project management.
• Experience in technical leadership in architecture, design, implementation and support of large-scale data and analytics solutions that are highly reliable, flexible, and scalable.
• Ability to meet tight deadlines, multi-task, and prioritize workload.
• Experience in collaborating with cross-functional teams and managing stakeholder expectations.
• Experience in working with globally distributed teams.
• Good Communication and Presentation skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
FairMoney,Senior Data Engineer,"About FairMoney

FairMoney is a credit-led mobile bank for emerging markets. The company was launched in 2017, operates in Nigeria & India, and raised close to €50m from global investors like Tiger Global, DST & Flourish Ventures. The company has offices in France, Nigeria, and India.

Role and responsibilities

At FairMoney, we are making a lot of data driven decisions in real time: risk scoring, fraud detection as examples.

Our data is mainly produced by our backend services, and is being used by data science team, BI team, and management team. We are building more and more real time data driven decision making processes, as well as a self serve data analytics layer.

As a senior data engineer at FairMoney, you will help building our Data Platform:

• Ensure data quality and availability for all data consumers, mainly data science and BI teams.
• Ingest raw data into our DataWarehouse (BigQuery / Snowflake)
• Make sure data is processed and stored efficiently:
• Work with backend teams to offload data from backend storage
• Work with data scientists to build a machine learning feature store
• Spread best practices in terms of data architecture across all tech teams
• Effectively form relationships with the business in order to help with the adoption of data-driven decision-making.

You will be part of the Datatech team, sitting right between data producers and data consumers. You will help building the central nervous system of our real time data processing layer by building an ecosystem around data contracts between producers and consumers.

Our current stack is made of

• Batch processing jobs (Apache Spark in Python or Scala)
• Streaming jobs (Apache Flink deployed on Kinesis Data Analytics - Apache Beam deployed on Google Dataflow)
• REST apis (Python FastApi)

Our tool stack

• Programming language: Python, SQL
• Streaming Applications: Flink, Kafka
• Databases: MySQL, DynamoDB
• DWH: BigQuery, Snowflake
• BI: Tableau, Metabase, dbt
• ETL: Hevo, Airflow
• Production Environment: Python API deployed on Amazon EKS (Docker, Kubernetes, Flask)
• ML: Scikit-Learn, LightGBM, XGBoost, shap
• Cloud: AWS, GCP

Requirements

You will work on a daily basis with the below tools, so you need working experience on

• Languages: Python and Scala.
• Big data processing frameworks: all or one of Apache Spark (batch/streaming) - Apache Flink (streaming) - Apache Beam.
• Streaming services: Apache Kafka / AWS Kinesis.
• Managed cloud services: one of AWS EMR / AWS Kinesis Data Analytics / Google Dataflow.
• Docker.
• Building REST APIs.

Ideally, you have experience with:

• deployment/management of stateful streaming jobs.
• the Kafka ecosystem: Kafka connects mainly.
• infrastructure as code frameworks (Terraform).
• architecture around data contracts: Avro Schemas management, schema registries (Confluent Kafka / AWS Glue).
• Kubernetes.

Overall experience required for this role: 6+ Years.

Benefits

• Training & Development
• Family Leave (Maternity, Paternity)
• Paid Time Off (Vacation, Sick & Public Holidays)
• Remote Work

Recruitment Process • A screening interview with one of the members of the Talent Acquisition team for 30 minutes.
• Takeaway assignment to be done at home.
• Technical design interview for 60-90 minutes.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,True,False,True,True,True
Boston Consulting Group,IT Senior Data Engineer,"WHAT YOU'LL DO
Under the general supervision of senior management and the Data Engineering Chapter Lead in the Enterprise Data Tribe, you will be working with key customers to deliver timely and accurate data engineering pipelines in a secure manner. You are expected to provide guidance on proper engineering design ensuring that our architectural guidelines are met, and the appropriate support model is in place for production deployments. This role will work in a multi-functional agile squad and support the product owner. You will also be supporting the Chapter Lead and other team members of the Data Engineering chapter in proof-of-concept activities and other Data Engineering chapter related work.
YOU'RE GOOD AT
You have experience in data warehousing, data modelling, and the building of data engineering pipelines. You are well versed in data engineering methods, such as ETL and ELT techniques through scripting and/or tooling. You are good in analysing performance bottlenecks and providing enhancement recommendations; you have a passion for customer service and a desire to learn and grow as a professional and a technologist.
• Viewed as subject matter expert for stakeholders, possessing in-depth knowledge and specialized technical skill set
• Able to work independently with minimal supervision
• Proactively identify and independently solve non-routine problems by applying expertise
• Perform research of viable technical and/or non-technical solutions
• Develop internal network with senior leaders within the chapter and key stakeholders in the tribe.
• Develop strategies for data engineering in Snowflake using DBT and Talend.
• Architect, design, and implement data pipelines to feed data models for subsequent consumption
• Actively monitor and resolve user support issues, working closely with your assigned squad and other squads as part of the chapter.
• Develop and maintain architectural standards, best practices, and measure compliance

YOU BRING (EXPERIENCE & QUALIFICATIONS)
You bring to us experience in data engineering technologies, database development, and data model design; both in IaaS and PaaS Cloud (AWS and/or Azure) environments.
• Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
• 5-7+ years of project experience, preferably as a Data Engineer/Developer and minimum of 3 years of agile project experience is a must (preferred tool - JIRA)
• Essential: Must have exposure to technologies such as DBT, Talend and Apache airflow
• Essential: SQL is heavily focused. An ideal candidate must have hands-on experience with SQL database design
• Essential: Extremely talented in applying SCD, CDC and DQ/DV framework
• Essential: Experience in data platforms: Snowflake, Oracle, SQL Server, PostgreSQL, and MySQL
• Essential: Lead R&D efforts to find solutions for data engineering requirements not addressed by existing technology standards
• Essential: Demonstrate ability to write new code i.e., well-documented and stored in a version control system (we use GitHub & Bitbucket)
• Essential: Develop metrics that illuminate the flow of data across the organization
• Essential: Experience in data modelling and relational database design
• Preferred: Experience in AWS and Azure data platforms.
• Preferred: Experience in Qlik Compose, Fivetran and HVR
• Preferred: Strong programming/ scripting skills (Python, Powershell, etc.)

YOU'LL WORK WITH
As part of the Enterprise Data Tribe, you don t have to fit into a mould at BCG. We seek people with strong drive, relentless curiosity, desire to create their own path, ability to work collaboratively, and the passion and leadership to make an impact. You ll collaborate on challenging projects with team members from many backgrounds and disciplines, increasing your understanding of complex business problems from diverse perspectives and developing new skills and experience to help you at every stage of your career. You ll be able to experience business on a genuinely global scale and learn how to bring together people from different cultures to uncover insights that challenge the status quo. As a member of the Product Engineering Group, you will work closely with a cross functional team that is collaborative, passionate and that holds themselves to a high standard.",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
General Mills,Data Engineer,":

India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.

Job Description:

Job Overview

The Enterprise Data Development team is responsible for designing & architecting solutions to integrate & transform business data into Data Lake to deliver data layer for the Enterprise using cutting edge technologies like Big Data - Hadoop. We design solutions to meet the expanding need for more and more internal/external information to be integrated with existing sources; research, implement and leverage new technologies to deliver more actionable insights to the enterprise. We integrate solutions that combine process, technology landscapes and business information from the core enterprise data sources that form our corporate information factory to provide end to end solutions for the business.

This position will develop solutions for the Enterprise Data Lake & Data Warehouse. You will be responsible for developing data lake solutions for business intelligence and data mining.

Job Responsibilities

70% of time Create, code, and support a variety of Hadoop, ETL & SQL solutions

Experience with agile techniques or methods

Work effectively in a distributed global team environment.

Works on pipelines of moderate scope & complexity

Effective technical & business communication with good influencing skills

Analyze existing processes and user development requirements to ensure maximum efficiency

Participates in the implementation and deployment of emerging tools and processes in the big data space

Turn information into insight by consulting with architects, solution managers, and analysts to understand the business needs & deliver solutions

20% of time Support existing Data warehouses & related jobs.

Job Scheduling experience (Tidal, Airflow, Linux)

10% of time Proactive research into up to date technology or techniques for development

Should have automation mindset to embrace a Continuous Improvement mentality to streamline & eliminate waste in all processes.

Desired Profile

Education:

Minimum Degree Requirements: Bachelors

Preferred Degree Requirements: Bachelors

Preferred Major Area of Study: Engineering

Experience:

Minimum years of Hadoop experience required: 2 years

Preferred years of Data Lake/Data warehouse experience: 2-4+ years

Total Experience required : 4-5 years

Specific Job Experience or Skills Needed

Skills Level: Beginner  Intermediate Expert  Advance

HDFS, Map reduce

Beginner

Hive, Impala & Kudu

Beginner

Python

Beginner

SQL, PLSQL

Proficient

Data Warehousing Concepts

Beginner

Other Competencies:
• Demonstrate learning agility & inquisitiveness towards latest technology
• Seeks to learn new skills via experienced team members, documented processes, and formal training
• Ability to deliver projects with minimal supervision
• Delivers assigned work within given parameter of time and quality
• Self-motivated team player and should have ability to overcome challenges and achieve desired results",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Fidelity India Careers,Lead - Software Engineering - Data Engineering,"Job Description:

Job Title – Lead Data Engineer [Data CoE]

The Purpose of This Role

At Fidelity, we use data and analytics to personalize incredible customer experiences and develop solutions that help our customers live the lives they want. As part of our digital transformation, we have significant investments to create innovative big data capabilities and platforms. One of them is to build various enterprise data lakes by gathering data across Business Units. We are looking for a hands-on data engineer who can help us design and develop our next generation, cloud enabled data capabilities.

The Value You Deliver
• You will be participating in end to end development which includes design, development, testing and deployment.
• You will be working closely with Technical Lead/Architects to ensure that solutions are consistent with IT Roadmap.
• You will be participating in technical life cycle processes, which include impact analysis, design review, code review, and peer testing.
• You will be participating in hands on development of application framework code in Oracle PL-SQL, pySpark, Python, NiFi, Informatica Power Center, along with Control-M and UNIX shell scripts.
• You will be troubleshooting and fixing any issues reported on data issues and performance.
• You will be presenting the findings and outcome to Senior Leadership teams and provide insights from the data to the business.
• You will be helping business teams optimize their current tasks and increase their productivity.

The Skills that are Key to this role

Technical / Behavioral
• You must be an expert in using SQL and PLSQL on Oracle or Netezza with UNIX shell scripting skills.
• You should be having working knowledge in Hadoop, HDFS, Hive, Spark, NoSQL DBs,
• Good knowledge on Python, JavaScript, Java and Scala
• You should have experience of using AWS services like RDS, EC2, S3, EMR and IAM to move data onto cloud platform
• Experience/Knowledge on Kubernetes, Containerization and building applications in Containers
• Knowledge of Logging, Telemetry and Data Security on AWS / Azure
• Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
• Experience of query tuning and optimization in one of the RBMS (oracle or DB2)
• You should be having experience in Control-M or similar scheduling tools.
• You should have proven analytical and problem-solving skills
• You should be strong in Database and Data Warehousing concepts.
• You must be able to work independently in a globally distributed environment
• You should have clear understanding of the business needs and incorporate these into technical solutions.

The Skills that is good to have for this role
• Experience in performance tuning and optimization techniques on SQL (Oracle and Netezza) and Informatica Power Center.
• Having strong inter-personal and communication skills including written, verbal, and technology illustrations.
• Having adequate knowledge on DevOps, JIRA and Agile practices.

How Your Work Impacts the Organization

Cloud Enablement and Data Model ready for Analytics.

The Expertise we’re looking for
• 3+[SE] / 7+ [Lead] years of experience in Data Warehousing, Big data, Analytics and Machine Learning
• Graduate / Post Graduate

Location: Bangalore , Chennai

Shift timings: 11:00 am - 8:00pm

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following:

For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2
For roles based in Ireland: Contact AccommodationsIreland@fmr.com
For roles based in Germany: Contact accommodationsgermany@fmr.com

Fidelity Privacy policy

Certifications:

Company Overview

At Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. Fidelity India has been the Global Inhouse Center of Fidelity Investments since 2003 with offices in Bangalore and Chennai. For information about working at Fidelity, visit India.Fidelity.com.

Fidelity Investments is an equal opportunity employer.",Bengaluru,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
EMERSON,Data Engineer - Sustainability,"JOB DESCRIPTION AS A PROFFESSIONALYOU WILL: Work closely with key stakeholders to understand business needs and translate them into technical requirements that would feed into developing effective data analytics solutions Design and implement end-to-end data solutions in collaboration with other technical and functional teams. Review and revise existing software development lifecycle andcode standards. Work closely with the data Architect onproduct roadmaps. Work on SharePoint and Power BI tools to manage, analyse and deduce data insights. Act as a point of escalation for complex operational issues to ensure optimal performance of analytics systems. WHO YOU ARE: You anticipate customer needs and provide services that are beyond customer expectations. You understand interpersonal and group dynamics and react in an effective manner. You encourage others to learn and adopt new technologies. You show a tremendous amount of initiative in tough situations and are exceptional at spotting and seizing opportunities. You promote high visibility of shared contributions to goals. REQUIRED EDUCATION, EXPERIENCE, & SKILLS: Bachelor's degree in Computer Science/Information Technology or equivalent Must have a minimum of 6+ years of experience in a Engineering role with experiences with: SharePoint Online and Power BI Experience in Visualization and Interpreting Data in various forms Technical expertise in data modelling, data mining, and segmentation techniques Experience with building new and troubleshooting existing data pipelines using Experience with batch and real-time data ingestion and processing frameworks Experience with languages such asPython andJava Knowledge of additional cloud-based analytics solutions Hands-on experience working on Linux and Windows systems Using Agile development methods Ability to work in a large, global corporate structure Ability to lead, manage and deliver large scale projects Advanced English level Demonstrated ability to clearly isolate and define problems, effectively evaluate alternative solutions, and make decisions in a timely manner Good decision-making ability, ability to operate in ambiguous situations, and high analytical ability to judge pros/cons of approaches against objectives PREFERRED EDUCATION, EXPERIENCE, & SKILLS: Expert level knowledge of data analytics and warehousing frameworks, including Snowflake and Cloud-based data integration solutions Experience with DevOps andCI/CD development practices Advanced level of software development knowledge",Chandigarh,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 24 2023

Come join us as we supercharge GSK’s data capability!

At GSK we are building a best-in-class data and prediction powered team that is ambitious for patients.

Scientific Digital and Tech’s goal is to power the discovery, development and supply of medicines and vaccines to patients. This means new tools to discover new medicines and vaccines, predictive capability for pre-clinical research, accelerated CMC and supply chain and an improved day-to-day laboratory experience for our scientists. Our Digital & Tech solutions will automate workflows and speed up decisions; freeing hands and releasing minds to focus on science.

As R&D enters a new era of data driven science, we are building a data engineering capability to ensure we have high quality data captured with context and aligned data models, so that the data is useable and reusable for a variety of use cases.

GSK R&D and Digital and Tech’s collective goal is to deliver business impact, including the acceleration of the discovery and development of medicines and vaccines to patients. The R&D Digital and Tech remit has expanded over the past 2 years, and to position GSK for the future, The change will strengthen R&D Tech, to provide more strategic impact, focus, accountability, and improved decision making in the use of Digital, Data and Analytics (DDA) to strengthen the pipeline.

Job Purpose

This role contributes to the construction of the development data fabric and data strategy. This role will interact with architects, engineers, data modelers, product owners as well as other team members in Clinical Solutions and R&D. This role will actively participate in creating technical solutions, designs, implementations & participate in the relentless improvement of R&D Tech systems in alignment with agile and DevOps principles.

The Data Engineer demonstrates both depth and breadth across key data engineering competencies e.g. Software Development, Testing, DevOps, Data Science/Analytics, and cloud. Can collaborate with experts from other subject domains. Primary responsibilities include using Azure cloud services and GSK data platform tools to ingest, egress, and transform data from multiple sources.

In addition, the role will demonstrate core engineering knowledge/experience of industry technologies, practices, and frameworks such as data fabric and scaling data platforms, containerization, cloud-based platforms, data analytics, machine learning, and data streaming. Examples of technologies include Java/C#/Python, Denodo, GIT, Azure Devops, Data Bricks, Presto, Spark, Azure Data Factory, ADLS V2, Kafka, Selenium, JUnit/NUnit, SAFe, Kanban, Docker, AI/ML, Azure/GCP Cloud Architecture including networking principles and scaling applications.

The Data Engineer, Clinical Solutions role is a senior technical role and will provide you the opportunity to lead key activities to progress your career. These responsibilities include the following:
• Working with other teams that are defining devops and data platform practices to meet the requirements of clinical solutions.
• Supporting engineering teams in the adoption and creation of data fabric best practices.
• Conducting PoCs of new technologies and helping to embed them in product teams
• Being part of a cutting-edge team creating the Development Data Fabric
• Ensures that technical delivery is fully compliant with GSK Security, Quality and Regulatory standards
• Ensures use of relevant R&D Tech / central services and collaborating with service partners in identification and delivery of service improvements
• Maintains best practices for engineering and architecture on our Confluence site. This requires hands on experience with cutting edge technology.
• Pro-actively engages in experimentation and innovation to drive relentless improvement
• Provides leadership, technical direction and GSK expertise to architecture and engineering teams composed of GSK FTEs, strategic partners and software vendors.

Why you?

Basic Qualifications:

Are you ready to work in an environment where you are continuously expected to work on projects with new technology and expected to use this technology to deliver real business value?

We are looking for professionals with these required skills to achieve our goals:
• Total 15+ years of experience and proficient with at least 3 of the below skills and can demonstrate knowledge and value with relevant experience in all the following competencies:
• Must have experience in Spark, Python and Databricks
• Software development, architecture design & technology platforms/frameworks
• Data Platforms and Domain-driven design
• Agile, DevOps & Automation [of testing, build, deployment, CI/CD, etc.]
• Data science (e.g. AI/ML), data analytics & data quality/integrity
• Testing strategies & frameworks
• Role requires:
• Demonstrated skill in delivering high-quality engineered data products
• Knowledge of industry standards and technology platforms aligned to GSK and R&D roadmaps
• Excellent communication, negotiation, influencing and stakeholder management skills
• Customer focus and excellent problem-solving skills
• Computer Science or related bachelor’s degree – MS in Computer Science is preferred
• Familiarity and use of various open-source ecosystems including JavaScript, Bigdata, java, python etc.
• Good understanding of various software paradigms: domain-driven, procedural, data-driven, object-oriented, functional
• Familiar with .Net Core (C#), Java, Python
• Demonstrable knowledge depth in more than one area of software engineering and technology

Preferred Qualifications:

If you have the following characteristics, it would be a plus:
• Experience in agile software development and DevOps, relevant technology platforms [e.g., Kubernetes] and frameworks [e.g. Docker] including cloud technologies & data structures (i.e. information management), data models or relational database design
• Subject matter expertise in clinical development
• R&D Tech requires Engineers with understanding of the relevant technical and scientific domains. Able to deliver continuous change to meet rapidly evolving R&D strategy and ambition.
• Experience with agile development methods, with security strategies and best practices, data integration mechanisms, architectural design tools, delivering and integrating COTS applications, areas of Service Oriented Architecture (SOA), Application Integration, Business Process Management and Data Quality.
• Experience in applying AI/ML, data curation, virtualization, predictive modelling, workflow, and advanced visualization techniques to enable decision support across multiple products and assets to drive results across R&D business operations.

At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.
#LI-GSK

GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in “gsk.com”, you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,True,False,False,True,False,False,True,True,False,False,False,False,False,False,False,False
Bloom Consulting Services,Data Engineer,"Data Engineer ( Job ID : 815310498 )

data engineer

NA

Contract

Experience

06.0 - 08.0 years

Offered Salary

10.00 - 14.00

Notice Period

Not Disclosed

Job Description

Total Experience6 to 8 years

Min Relevant Experience: 3 to 5 years

Location :Bangalore

JD: Data Engineer

Role Description:

In this role, you will be part of a growing, global team of data engineers, who collaborate in DevOps mode, in order to enable business with state-of-the-art technology to leverage data as an asset and to take better informed decisions.

The Life Science Data Engineering Team is responsible for designing, developing, testing, and supporting automated end-to-end data pipelines and applications on Life Science’s data management and analytics platform (Palantir Foundry, Hadoop and other components).

The Foundry platform comprises multiple different technology stacks, which are hosted on Amazon Web Services (AWS) infrastructure or own data centers. Developing pipelines and applications on Foundry requires:
• Proficiency in SQL / Java / Python (Python required; all 3 not necessary)
• Proficiency in PySpark for distributed computation
• Familiarity with Postgres and ElasticSearch
• Familiarity with HTML, CSS, and JavaScript and basic design/visual competency
• Familiarity with common databases (e.g. JDBC, mySQL, Microsoft SQL). Not all types required

This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.

Roles & Responsibilities:
• Develop data pipelines by ingesting various data sources – structured and un-structured – into Palantir Foundry
• Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application
• Acts as business analyst for developing requirements for Foundry pipelines
• Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline
• Document technical work in a professional and transparent way. Create high quality technical documentation
• Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)
• Deploy applications on Foundry platform infrastructure with clearly defined checks
• Implementation of changes and bug fixes via change management framework and according to system engineering practices (additional training will be provided)
• DevOps project setup following Agile principles (e.g. Scrum)
• Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java
• Work closely with business users, data scientists/analysts to design physical data models

Education
• Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields

Professional Experience
• 5+ years of experience in system engineering or software development
• 3+ years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.

Required Knowledge, Skills, and Abilities

Data engineer",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"• Experience with Azure Data Bricks, Data Factory
• Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics
• Experience in Python/Pyspark/Scala/Hive Programming.
• Experience with Azure Databricks/ADB
• Good understanding of SQL queries, joins, stored procedures, relational schemas
• Experience with NoSQL databases, such as HBase, Cassandra, MongoDB",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Genpact,Data Engineer,"With a startup spirit and 90,000+ curious and courageous minds, we have the expertise to go deep with the world's biggest brands--and we have fun doing it! We dream in digital, dare in reality, and reinvent the ways companies work to make an impact far bigger than just our bottom line. We're harnessing the power of technology and humanity to create meaningful transformation that moves us forward in our pursuit of a world that works better for people.

Now, we're calling upon the thinkers and doers, those with a natural curiosity and a hunger to keep learning, keep growing. People who thrive on fearlessly experimenting, seizing opportunities, and pushing boundaries to turn our vision into reality. And as you help us create a better world, we will help you build your own intellectual firepower.

Welcome to the relentless pursuit of better.

In this role, resource will be expert in designing, building and maintaining data infrastructure. Work will help people with unmet medical needs, including those who wish to quit smoking, those with major depression disorder, and those with schizophrenia--ultimately improving lives through engineering. Help design and build a data infrastructure using state-of-the-art technologies with data security at utmost importance and employ elegant solutions to help ensure Client's data products meet compliance needs (e.g., GDPR and HIPAA) in different regions of the world.

Responsibilities!
• Design, build and maintain analytical data infrastructure which includes both data processing and data reporting.
• Onboarding data from both internal and external systems.
• Collaborate with Product, Engineering, Science, Data analysts and Data scientists to implement rich and re-usable datasets/metrics.
• To make data infrastructure and applications scalable, reliable, and secure.
• Strong attitude towards automating routine tasks via coding/scripting.
• Research on security and privacy requirements and provide solutions.

Qualifications we seek in you!
• B Tech/M Tech/BCA/MCA
• Experience in building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience writing complex, highly optimized SQL queries.
• Experience with reporting to enable explanatory and exploratory analytics.
• Python development experience.
• Have experience with dbt, Airflow, Snowflake and AWS infrastructure.
• Have experience implementing APIs to share data with internal / external vendors.
• Experience implementing streams.
• Understanding of privacy and security regulations (e.g., GDPR, HiTrust, HIPA)",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
Vanderlande Careers,Lead Data Engineer,"Lead Data Engineer at DSF

Vanderlande provides baggage handling systems for 600 airports around the globe, capable of moving over 4 billion pieces of baggage around the world per year. For the parcel market our systems handle 52 million parcels per day. All these systems generate data. Do you see a challenge in building data-driven services for our customers using that data? Do you want to contribute to the fast growing Vanderlande Technology Department on its journey to become more data driven? If so, then join our Digital Service Factory team!

Your Position

As a lead data engineer you will be leading the data engineering efforts in a product team. You will work together with product/solution architecture to provide technical necessities to design and develop end-to-end data ingestion pipelines and well tested and monitored data services. You will assess the technical dependency between different functional components and define a resolution. You will also provide technical guidance and coaching to the junior/medior data engineers in the team, set technical standards and best practices.

Your responsibilities:
• You will be designing, developing, testing, and documenting the data collection framework. The data collection consists of (complex) data pipelines with data from (IoT) sensors and low/high level control components to our Digital Service and Data Science platform.
• You will build monitoring solutions for data pipelines which enable data quality improvement.
• You will develop scalable data pipelines to transform and aggregate data for business use, following software engineering and Data Mesh best practices. For these data pipelines you will make use of the best and most applicable frameworks available for data processing.
• You develop our data services and data products for customer sites towards a product, using (test & deployment) automation, componentization, templates, and standardization to reduce delivery time of our projects for customers. The product provides insights in the performance of our material handling systems at customers all around the globe.
• You design and build a CI/CD pipeline, including (integration) test automation for data pipelines. In this process you strive for an ever-increasing degree of automation and high levels of security.
• You will work with infrastructure engineers to extend storage capabilities and types of data collection (e.g. streaming)
• You have experience in developing APIs.
• You will coach and train the junior data engineer with the state of art big data technologies.
• You will lead the Data Engineering Guild where passionate members discuss current trends, short term development, and solutions for ongoing issues that span multiple teams.

Your Profile
• Total experience of 10+ years (with at least 7+ years of programming exp)
• Experience programming in Python and/or Scala (Java programming exp is a plus)
• You are familiar with DevOps practices and have relevant experience in automation (CI/CD), measurement, applying lean practices and what DevOps culture entails
• You know how to achieve high performing secure pipelines, maintain and test them
• You are familiar with different storage formats (e.g. Azure Blob, SQL, noSQL)​
• Experience with scalable data processing frameworks (e.g. Spark)​
• Experience with event processing tools like Splunk or the ELK stack​
• Deploying services as containers (e.g. Docker and Kubernetes)​
• You have experience with streaming data platforms (e.g. Kafka )​ and messaging formats (e.g. Apache AVRO)
• Strong experience with cloud services (preferably with Azure)

Diversity & Inclusion

Vanderlande is an equal opportunity employer. Qualified applicants will be considered without regards to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Pune,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Visa,Sr. Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.
Job Description

This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. You will be an integral part of the Payment Products Development team focusing on design and development of software solutions that leverage data to solve business problems. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and testing of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Responsible for the design, development, and implementation
• Work on development of new products iteratively by building quick POCs and converting ideas into real products
• Design and develop mission-critical systems, delivering high-availability and performance
• Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology
• Develop code to ensure deliverables are on time, within budget, and with good code quality
• Have a passion for delivering zero defect code and be responsible for ensuring the team's deliverables meet or exceed the prescribed defect SLA
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
• Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.
Qualifications

We are seeking team members that are passionate, visionary and insatiably inquisitive. Successful candidates frequently have a mix of the following qualifications:

• Bachelor’s Degree or an Advanced Degree (e.g. Masters) in Computer Science/ Engineering, Information Science or a related discipline
• Minimum of 3 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies
• Extensive experience with SQL and Big Data technologies (Hadoop, Java, Spark, Kafka, Hive, Python) for large scale data processing and data transformation
• Deep knowledge of Unix/Linux
• Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
• Familiar with software design patterns
• Experience working in an Agile and Test-Driven Development environment
• Strong knowledge of API development is highly desired
• Strategic thinker and good business acumen to orient data engineering to the business needs of internal and external clients
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
• Previous exposure to financial services is a plus, but not required
Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
Shell,"Senior Data Engineer- Azure (ADF, Data lake)","Join the number One Global Lubricants supplier in the world and be part of the team that helps in shaping up the digital and the IDT strategy which delights our customers in over 100 countries across every sector.

If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

Where you fit

Shell's Projects and Technology (P&T) business exists to make the delivery of our strategies and the growth of our company possible. Our team develops the advanced products and technologies Shell needs to meet customer demand. Our solutions help our partners grow the LNG, Gas and Power businesses, deepen the integration of Manufacturing, Chemicals and Trading, and maximise the competitiveness of our Upstream business.

What's the role?

As a Data Engineer in Shell, you will create and maintain optimal data pipeline architecture and also will a ssemble large, complex data sets that meet functional / non-functional business requirements.

You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

More specifically, your role will include:
• Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.
• Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

What we need from you

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree and at least have a Seniority level in their previous workplace.

They should also have experience using the following software/tools:
• Experience with Azure: ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.
• Experience with relational SQL/NoSQL databases, file handlings and API integrations
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODS

Skills - Nice to Have
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing data pipelines using ADF
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of transforming, processing and extracting value from large disconnected datasets
• Strong team player with organizational and communication skills
• Experience supporting and working with cross-functional teams in a dynamic environment",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fibe India,Data Engineer - SQL,"Responsibilities:
• The candidate is expected to lead one of the key analytics areas end-to-end. This is a pure hands-on role.
• Ensure the solutions are built to meet the required best practices and coding standards.
• Ability to adapt to any new technology if the situation demands.
• Requirement gathering with business and getting this prioritized in the sprint cycle.
• Should be able to take end-to-end responsibility for the assigned task
• Ensure quality and timely delivery.

Requirements:
• Experience: 3- 6 years.
• Strong at PySpark, Python, and Java fundamentals
• Good understanding of Data Structure
• Good at SQL query/optimization
• Strong fundamental of OOPs programming
• Good understanding of AWS Cloud, Big Data.
• Nice to have Data Lake, AWS Glue, Athena, S3 Kinesis, SQL/NoSQL DB",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer,"Role: Data Engineer Job Description
• Design, build, and maintain distributed batch and real-time data pipelines and data models.
• Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
• Be curious and eager to work across a variety of engineering specialties (i.e., Data Science, and Machine Learning to name a few).
• Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
• Enforce privacy and security standards by design.
• Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Qualifications
• +3 years experience building complex data pipelines and working with both technical and business stakeholders.
• Experience in at least one primary language (e.g., Java, Scala, Python) and SQL (any variant).
• Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
• Experience creating and maintaining ETL processes.
• Experience designing, building, and operating a DataLake or Data Warehouse.
• Experience with DBMS and SQL tuning.
• Strong fundamentals in big data and machine learning.

Preferred Qualifications
• Experience with RESTful APIs, Pub/Sub Systems, or Database Clients.
• Experience with analytics and defining metrics.
• Experience with measuring data quality.
• Experience productionalizing a machine learning workflow; MLOps
• Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
• Language ability in Japanese and English is a plus (We have a professional translator but it is nice to have language skills).
• Experience with AWS services.
• Experience with microservices.
• Knowledge of Data Security and Privacy.

experience

6",Hyderabad,True,False,True,True,False,False,False,False,False,False,False,False,True,True,False,False
deloitte,Consulting - BO - Cloud Engineering - Manger - Azure Data Engineer,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organisations, Deloitte is where you will find numerous opportunities to succeed and realise your full potential.

The team

Deloitte is working with global customers on cloud technologies to help unlock growth, stability, and sustainability by enabling them to spot unseen business trends through curation, transformation, and blending of data. In our endeavors for continued expansion, we’re searching for like-minded individuals to help us ‘take it to the next level’.

In this exciting opportunity for an experienced developer, you will join a team delivering a transformative cloud hosted data platform for some of the world’s biggest organizations. The candidate we seek, needs to have a proven track record in implementing data ingestion and transformation pipelines on Microsoft Azure. Deep technical skills and experience with working on Azure Databricks. Familiarity with data modelling concepts and exposure to Synapse.

You will also be required to participate in stakeholder management, highlight risks, propose deliver plans and estimate for time and team size based on requirements. Hence, adequate levels of communication skills and relevant experience in handling such situations is desired.

Scope of work

Your main responsibilities will be:
• Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
• Delivering and presenting proofs of concept of key technology components to project stakeholders.
• Developing scalable and re-usable frameworks for ingesting and enriching datasets
• Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
• Working with event based / streaming technologies to ingest and process data
• Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
• Evaluating the performance and applicability of multiple tools against customer requirements
• Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.

Qualifications
• Strong knowledge of Data Management principles
• 9+ years of total years of experience
• Experience in building ETL / data warehouse transformation processes
• Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably Databricks).
• Experience using Apache Spark and associated design and development patterns
• Microsoft Azure Big Data Architecture certification is an advantage.
• Hands-on experience designing and delivering solutions using Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
• Experience with Apache Kafka / Nifi for use with streaming data / event-based data (Nice to have but not mandatory)
• Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)
• Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
• Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Terraform etc.

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters.

In addition to living our purpose, managers across our organisation:
• Develop self by actively seeking opportunities for growth, share knowledge and experiences with others, and act as a strong brand ambassadors
• Understand objectives for clients and Deloitte, align own work to objectives and set personal priorities
• Seek opportunities to challenge self
• Collaborate with others across businesses and borders to deliver and take accountability for own and team results
• Identify and embrace our purpose and values and put these into practice in their professional life
• Build relationships and communicate effectively in order to positively influence peers and other stakeholders

Professional growth

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn.From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Our Purpose

Deloitte is led by a purpose: To make an impact that matters.

Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and work—always striving to be an organisation that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description

insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.

Job Description
• Develops and maintains scalable data pipelines for bulk data movement between systems of record and systems of reference
• Develops and maintains scalable application to application integrations
• Aligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
• Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes
• Writes appropriate unit or integration tests to implement test-driven development
• Continually contributes to and enhances data team documentation
• Performs data analysis required to troubleshoot and resolve data related issues
• Works closely with a team of frontend and backend engineers, product managers, and analysts
• Defines company data assets, artifacts and data models

Qualifications

Required qualifications:
• 5 years of Data Engineering and Data Integration
• 5 Years of Data Warehousing
• 3 Years of Data Architecture and Modeling
• 2 years of Cloud Data Engineering
• Agile Methodologies

Preferred skills:
• AWS or Azure Data Certifications
• Experience with databricks, spark, python
• Experience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)
• Experience with Salesforce

Additional Information

All your information will be kept confidential according to EEO guidelines.
• * At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. **

insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Northern Tool + Equipment, India",Senior Data Engineer,"Are you an individual who wants to play a game changing role and make an impact in a fast-growing organization? We at Northern are waiting for you. Join us and unleash your potential!!

We are hiring <>!!

Join the core group of founding members at the NTE India to build an organization from the ground up.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Role Objective

PRIMARY OBJECTIVE OF POSITION:

We are looking for an Experienced Data Engineer who will partner with a specific business function and understand the requirements, builds data model, creates data pipelines and stored procedures. Also work with Data Analysts/Modelers, Data Visualization Engineers to deliver high performing analytics.

MAJOR AREAS OF ACCOUNTABILITY:
• SME for data structures and data models for specific line of business.
• Analyze and understand various source systems and related data structures.
• Build and automate creation of ETL pipelines and stored procedures to move data from source system to consumption layer using variety of ETL methods.
• Collaborate with Data Analysts/Data Architect/Data Visualization Engineers to provide them with Data mapping documents and ensure adherence to a common data model.
• Responsible for administration and security of data and analytics assets in Azure.
• Works collaboratively and effectively communicates with others across departments in order to perform and complete necessary tasks and projects.
• Follows established Software Development Life Cycle (SDLC) to enable CI/CD in relevant areas.
• Follow established change control, release management and incident management processes.
• Responsible for performance and tuning, scaling of Azure resources to optimize costs
• Builds and maintains relationships cross-functionally in order to stay current with the needs and operations of the business functional areas supported.
• Supports the day-to-day operation of the reporting and analytic solutions by troubleshooting ETL and other errors encountered during data processing.
• Keeps manager informed of important developments, potential problems, and related information necessary for effective management. Coordinates and communicates plans and activities with others, as appropriate to ensure a coordinated work effort and team approach.

Job Description

Performs related work as apparent or assigned.

QUALIFICATIONS:
• To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
• Bachelor’s Degree in Computer Science, Statistics, Mathematics, Business or related field.
• At least 6 years relevant work experience in Data and Analytics field.
• In-depth understanding of Data warehousing concepts.
• Hands-on experience in writing complex, highly optimized SQL queries across large data sets
• Hands-on experience in building performance optimized data pipelines (ETL/ELT)
• Experience in configuring, deploying, and provisioning of IaaS, PaaS with Terraform and PowerShell using Azure DevOps and GIT.
• Specific familiarity with the Microsoft Azure Data Stack - ADF, Azure SQL DB, Azure Synapse (SQL Data Warehouse), Azure Data Lake, Azure Storage and Analysis Services.
• Azure Security & Identity: Azure Active Directory App Permissions, Key Vaults.
• Hands-on experience in creating user groups, creating security policy and implementation of Row-Level Security(RLS) to restrict the data access to the users.
• Hands-on experience with data cataloging and data profiling concepts.
• Diversity of perspective for various tools and technologies like Azure Stream Analytics, Azure Databricks, NoSQL databases, read or write optimized databases to advocate for their appropriate adoption at Northern Tool.
• Basic programming experience using .NET, Python, or any scripting language.
• Must be willing to work as a team and possess the skills to work independently.
• Demonstrated ability to take initiative and utilize creativity on assigned projects.
• Must possess strong analytical, problem-solving, and technical design skills.
• Demonstrates Northern Tool + Equipment’s 12 Core Competencies.
• Sounds interesting? Here’s your chance to join our family at Northern.

About the Company

Northern Tool + Equipment is a retailer and manufacturer that specializes in offering superior quality tools at great prices, along with the knowledge and support needed to help customers get the job done right.

They’ve been in business for over 40 years, recently reaching revenues over $1.5 billion. The company not only supplies over 100,000 tools from the top brands in the industry but also designs, manufactures, and tests an extensive lineup of premium private label products that customers can’t get anywhere else.

Northern Tool’s far-reaching customer base includes handy men and women, weekend hobbyists, serious do-it-yourselfers, full-fledged contractors, trade professionals, and more. The company’s products can be found in over 120 retail stores in the USA, on its comprehensive international website, and via numerous catalogs throughout the year. Recently Northern Tool has expanded operations to offices in India to serve its global distribution better.

We are recently named as one of the Top Workplaces for MidSize Employers by Forbes in the US.

We have also been recognized as the “Top GCC to work for in AI and analytics” and our India HR team as the “Top HR Professionals in AI and Analytics” by 3AI which is a professional firm associated with analytics within India.

About NTE India

Northern Tool is making a significant investment in business transformation. We are committed to providing our customers with an exceptional experience. The team in India will enable Northern Tool to expand its internal capabilities in Finance, Merchandising, Product Engineers, Manufacturing Ops, Marketing, Contact Center, and Information Technology.

Why Northern?

True Northern: We know that our strength is our people. The distinct abilities they bring into the system are the key to our success. We seek talented people who wish to share their initiative, ideas, and expertise; we develop and support our teams, and we put them in a position to succeed. We know our customer; we provide value, and we act with integrity. We are True Northern.

Build Lasting Relationships: At Northern Tool + Equipment, we’re far more interested in building relationships than we are in simply making transactions. Our purpose is building a long-lasting relation with our customers and employees.

We care for our customers, employees and society. Our customer base is exceptionally loyal because customers know that we will give them the right solution.

Accelerate Decision Making: by collaborating with the brightest minds, bring ideas to life across our value chain of business operations across our vast network of over 120 stores across the US.

Lead with Innovation: Join us to elevate our customer experience?with cutting-edge products, technology, and business processes and?drive our business forward.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Does this sound interesting?? Be an early applicant!!

Northern Tool is an Equal Opportunity Employer. We encourage and empower everyone and support diversity in experience, and point of view. We are pledged to a fair and a transparent hiring process with no discrimination of race, color, ancestry, religion, gender, national origin, age, citizenship, marital status, disability, or veteran status.

Requirements
• name : Northern Tool + Equipment, India
• location : Hyderabad, IN
• experience : 6 - 9 years
• employmentType : Full-Time
• Primary Skills: ETL or ELT,Python,Data Warehousing,Azure Data Lakes or Data Factory,SQL",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Comcast India Engineering Center I, LLP",Data Engineer 3,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary About Sky We’re Sky, Europe’s biggest entertainment brand. Think top-quality shows. Breaking news. Innovative tech. Must-have products. Careers here mean the freedom and support you need to make an impact – pushing boundaries, creating solutions, hitting targets. And as part of our close-knit team, you’ll enjoy plenty of benefits. Plus, experiences you’ll only find at Sky. We love telling the world we work at Comcast . We’re fans too. We move fast and embrace pace. We have the freedom to be brilliant. And we work collaboratively because together we can. This is how we work at Comcast and why we love it. Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Documents all development activity. Assists with training non-technical personnel. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as a resource for colleagues with less experience. Job Description Core Responsibilities Create and maintain an optimal data pipeline architecture focussed upon network data, including real-time and batch data sources. Assemble large, complex data sets that meet functional and non-functional business requirements. Build batch/streaming ELT/ETL solutions from a wide variety of data sources in varying formats (SQL, JSON, AVRO, HTTP, API, etc.) using the right blend of tools. Keep our data compliant, relevant and secured across multiple data centres and regions. Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery and evolving current solutions whilst ensuring continuity of service. Create data tools for data scientist team members that assist them in building and optimizing into an innovative industry leader. Guide and collaborate with data consumers on analytics, tooling and platform related queries. Employees at all levels are expected to: Graduate degree BSc in Computer Science, Electrical Engineering or similar. Strong communications skills. SQL knowledge and experience working with relational databases. Strong analytic skills related to working with structured and unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Hands-on experience in building scalable data platforms. Awareness of security practices and privacy concerns when working with data across both in-house and cloud platforms. Ideally an awareness of network technologies and concepts or an insatiable desire to learn. Key technologies Apache Airflow / NiFi / Kafka / ZooKeeper Confluent ecosystem (Connect / Schema Registry / ksqlDB) GCP (BigQuery, Dataflow, Pub/Sub, IAM) Linux / Terraform / Ansible Python / Docker (Nice to have). Experience: 5 Years to 7.5 Years Location: Chennai Disclaimer: This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. Comcast is an EOE/Veterans/Disabled/LGBT employer. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools that are personalized to meet the needs of your reality—to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the benefits summary on our careers site for more details. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Certifications (if applicable) Relative Work Experience 5-7 Years Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. At Comcast , you have the power to connect the world. Your career options are endless as you grow in your career. Explore your future with access to a variety of teams, locations, and resources in an expanding network. You can also explore additional opportunities at our company, NBCUniversal.",,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
Revolo Infotech,Data Engineer - SQL/Python,"Job Description :

- Design, develop, and maintain data pipelines and architecture for data storage, processing, and analysis

- Work with cross-functional teams to understand and implement data requirements

- Build and optimize data pipelines using various cloud-based technologies such as AWS, Azure, or Google Cloud Implement data visualization solutions using cloud-based tools such as Tableau, Power BI, or Looker Monitor and troubleshoot data pipeline issues, and implement solutions to improve performance and scalability

- Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis

- Stay up-to-date with the latest technologies and industry trends in data engineering and data visualization

Requirements :

- 2+ years of experience as a data engineer with a focus on cloud-based data pipelines and visualization

- Strong experience with cloud-based technologies such as AWS, Azure, or Google Cloud

- Experience with data visualization tools such as Tableau, Power BI, or Looker

- Strong knowledge of SQL and programming languages such as Python or Java

- Familiarity with big data technologies such as Hadoop, Spark, or Hive

- Strong problem-solving and analytical skills

- Experience working in an Agile development environment Bachelor's degree in Computer Science or related field.

Preferred Qualifications :

- Experience with data warehousing concepts and technologies

- Experience with data governance and data management best practices.

- Experience with machine learning and AI technologies Strong communication and teamwork skills.

Job Types : Full-time, Regular / Permanent, Contractual / Temporary

Salary : 1,000,000.00 - 1,200,000.00 per year

Benefits :

- Health insurance

- Internet reimbursement

- Paid sick time

- Paid time off

Schedule :

- Day shift

- Monday to Friday

Power BI: 2 years (Preferred)

Tableau: 2 years (Preferred)

AWS: 2 years (Preferred)
(ref:hirist.com)",Navi Mumbai,True,False,True,True,False,False,False,False,True,True,False,False,False,False,False,False
MediaMath,Data Engineer,"About Us

MediaMath is the leading technology pioneer on a mission to make advertising better. We deliver outstanding results through powerful ad tech, partnership and a curiosity for what’s next. We help more than 3,500 advertisers solve complex marketing problems so they can deepen their customer relationships across screens and around the world.

Key Responsibilities

MediaMath’s Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by Analytics to discover and deliver insights that drive value for our clients. The Analytics team fulfils customers’ advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. As the Data Engineer on the Analytics Engineering team within the Analytics team, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:
• Become an expert in MediaMath data flows and the Analytics data infrastructure.
• Build, maintain, and own scalable data pipelines to support client data integration.
• Become a team SME in data munging and automated ETL processes.
• Work with Analysts to understand and leverage big data to solve client problems and needs.
• Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
• Be a team player, and bring the team and company forward by solving team and company priorities.

You are:
• Experienced in writing readable, re-usable code SQL and Python (our entire team uses Jupyter Notebook and Pandas!)
• Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
• Experienced in implementing data pipeline health monitoring, alerting
• Experienced with data infrastructure troubleshooting and working with system logs
• Experienced developing data flow schematics/blueprints
• Advocate for automation and building efficient, scalable solutions
• Self-driven, with a hunger to learn and spread knowledge by teaching others
• Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:
• Bachelor’s Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
• 3 - 5 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
• Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
• Experience partnering with client-facing teams to understand client needs and translate them to technical requirements

Nice-to-have’s:
• Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)
• Experience working with REST APIs, web services, object-oriented technologies like Java, C++
• Public GitHub repos or notebooks that illustrate the way you think about data
• Exposure to ad-tech, digital marketing, or e-commerce industries

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are three values that provide a framework for how we approach our work and the world: Win Together, Obsess Over Growth, and Do Good, Better. These values inform how we energize one another and engage with our clients. They get us amped to come to work.

Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath is recognized as a Leader in the Gartner 2020 Magic Quadrant for Ad Tech and has won Best Account Support by a Technology Company for two years in a row in the AdExchanger Awards.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristic as established by law.

MediaMath focuses on Digital Media, Internet, Advertising, Software, and Marketing. Their company has offices in New York City, San Francisco, Chicago, Durham, and Singapore. They have a large team that's between 501-1000 employees. To date, MediaMath has raised $617.877M of funding; their latest round was closed on July 2018.

You can view their website at http://www.mediamath.com or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,False
"Atlassian, Inc.",Senior Data Engineer,"Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.",,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
Motilal oswal,Data Engineer,"Job Description : Strong AWS Data Engineering skills. Exposure to SSIS, SSRS, SSAS will be an advantage,Handson experience working with S3, Redshift, Glue, EMR, RDS, Athena, Aurora,Strong development skills and experience coding with SQL, Pyspark, Python,High on ownership and accountability,Comfortable with change, initial hiccups and small failures,Experience with understanding designs, creating low level designs, unit test cases, unit testing and assisting with Integration and User acceptance testing,Experience of 2-6yrs with AWS Data technologies.",,True,False,True,False,False,False,False,False,False,False,True,False,True,False,False,False
Fisker Inc.,Data Engineer,"Responsibilities
• Work with leaders, engineering and data scientists to understand data needs.
• Design, build and launch efficient and reliable data pipelines to best utilize connected vehicle data for real-time systems and within data warehouses.
• Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
• Help insure that best practices are followed when storing, retrieving and accessing data.

Qualifications
• 3+ years of Python development experience.
• 3+ years of SQL experience.
• 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
• 3+ years experience with Data Modeling.
• Experience in organizing queries, tables and pipelines with proper indexing, partition and sharding.
• 3+ years experience in custom ETL design, implementation and maintenance.
• Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Clickhouse, Spark, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience with more than one coding language, ideally Go or C++ and java.
• Experience with designing and implementing real-time pipelines.
• Experience with data quality and validation.
• Experience with SQL performance tuning and E2E process optimization.
• Experience with notebook-based Data Science workflow.
• Experience with Airflow.
• Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,True,True,False
Poshmark,"Software Developer, Data Engineering","The Big Data team is a central player in the Poshmark organization. Our mission is to build a world-class big data platform to bring value out of data for us and for our customers. Our goal is to democratize data, support exploding business, provide reporting and analytics self-service tools, and fuel existing and new business critical initiatives.

The Data Engineering team at Poshmark is looking for an experienced software engineer to take care of Poshmak’s growth data, ensuring real-time access to quality data for all the stakeholders. The role requires strong understanding of software engineering best practices and excellent software development skills to build and maintain real-time and batch data pipelines with a focus on scalability and optimizations. In addition, the role also requires collaborating with Data Science, Analytics and other Engineering teams to build newer ETLs analyzing terabytes of data.

The role also requires being able to write clean and scalable code to pull datasets from disparate sources involving External APIs, S3 transfers, Web Scraping. You will work with cutting edge technologies and frameworks like Scala, Ruby, Apache Spark, Airflow, Redshift, Databricks, Docker. You will also manage the growth data infrastructure comprising ETL pipelines, Hive tables, Redshift tables, BI tools. We are looking for a software engineer who can help us define the next phase of growth data systems in terms of scalability and stability.

Responsibilities
• Design, Develop & Maintain growth data pipelines and integrate paid media sources like Facebook and Google to drive insights for business.
• Build highly scalable, available, fault-tolerant data processing systems using AWS technologies, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 100s of terabytes of data ingested every day and a petabyte-sized data warehouse.
• Responsible for architecting/designing/developing critical data pipelines at Poshmark.
• Productionizing ML models in collaboration with the Data Science and Engineering teams.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.
• Participate and contribute to constantly improving best practices in development.

Desired Skills & Experience
• Excellent technical problem solving using data structures and algorithms, with emphasis on optimization and code quality.
• 1-3 years of relevant software engineering experience using object oriented programming languages like Scala / Java / Ruby / Python / C++ etc.
• Expertise in architecting and building large-scale data processing systems using Big Data technologies like Spark, Hadoop, EMR, Kafka/ Kinesis, Flink, Druid.
• Expertise in SQL with knowledge on any existing data warehouse technology like Redshift
• Expertise in Google Apps Script, Databricks or API Integrations is a plus.
• Be self-driven, take complete ownership of initiatives, make pragmatic technical decisions and collaborate with cross-functional teams.",Chennai,True,False,True,True,True,True,False,True,False,False,False,True,True,False,True,False
Confidential,Data Engineer - AWS/ETL,"Role and responsibilities :- The Data Engineer will be responsible for leading design, development, transformation, deployment, and maintenance of Data Warehousing stack on AWS- Work with BI and dev team to build data pipelines using AWS Glue and similar tools.- Develop custom data ingestion jobs and ETL scripts using Python/Spark scripts- Perform data modelling and schema design activities in Data Lake and Data Warehouse environments as per the standard practices- Advanced SQL knowledge and experience working with relational databases, able to write/debug complex SQL queriesYour profile must have :- 4+ years of experience in building data pipelines and data warehouse architectures on cloud platforms such as AWS- Exposure to agile methodology- Experience in developing Python or Spark jobs- Strong understanding of data modelling principles- Good communication and collaborative skillsExtra points if you have :- Built processes supporting data transformations, data structures and workload management in Database/Data Warehouse- Experience in performance tuning of Redshift databases and implement recommendations- Experience with sourcing data using APIs from external systems- Experience working with teams across the globe, in a fast-paced, high-tech and customer-obsessed environment- Exposure to Shopify, Amazon and Syndicated data (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
ANI Calls India Private Limited,Azure Data Engineer with Big Data,"Anicalls Industry:

IT
Total Positions: 3

Job Type:
Full Time/

Permanent Gender:
No Preference Salary: 900000 INR - 1400000 INR ( Annually )

Education:
Bachelor′s degree Experience: 8 -12

Years Location:
Hyderabad, India . Azure Data Factory . Azure Databricks . Python, Scala, PySpark, Spark . HIVE / HIVE LLAP / HBASE / CosmoDb . Azure Active Directory Domain Services . Apache Ranger / Apache Ambari . Azure Key Vault . Expertise in HDInsight ( Minimum 2 -3 years ' experience with multiple implementations ) . Expertise in Cloud Native and Open Cloud Architecture",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
DAZN,Senior Data Engineer,"Are you an engineer who loves to make things that just work better? Do you love to work with cutting edge technologies and think about how can this run faster, be deployed quicker or fail less and deliver killer streaming applications that add business value and stick with customers?

DAZN is a tech-first sport streaming platform that reaches millions of users every week. We are challenging a traditional industry and giving power back to the fans. Our new Hyderabad tech hub will be the engine that drives us forward to the future. We’re pushing boundaries and doing things no-one has done before. Here, you have the opportunity to make your mark and the power to make change happen - to make a difference for our customers. When you join DAZN you will work on projects that impact millions of lives thanks to your critical contributions to our global products

This is the perfect place to work if you are passionate about technology and want an opportunity to use your creativity to help grow and scale a global range of IT systems, Infrastructure and IT Services. Our cutting-edge technology allows us to stream sports content to millions of concurrent viewers globally across multiple platforms and devices. DAZN’s Cloud based architecture unifies a range of technologies in order to deliver a seamless user experience and support a global user base and company infrastructure.

Join us in India’s beautiful “City of Pearls” and bring your ambition to life.

Benefits will include access to DAZN, an annual performance related bonus, family friendly community, free access for you and one other to our workplace mental health platform app (Unmind), learning and development resources, opportunity for flexible working, and access to our internal speaker series and events.
As our new Data Engineer, you'll have the opportunity to:

• Support building real-time user-facing analytics and data driven operations applications
• Be responsible with the rest of the team for the availability, performance, monitoring, emergency response, and capacity planning
• Use your love of big data systems, thinking about how to make them run as smoothly and securely as possible, support operational endpoints
• Have a strong sense of teamwork and put team’s / company’s interests first

You'll be set up for success if you have

• 5+ years’ experience writing clean, robust and testable code, preferably in Typescript
• Experience building high performant, low latency and high velocity data pipelines
• Working knowledge in AWS services, such as Kinesis, EventBridge, SQS, SNS Topic, S3, Lambda, Kinesis, EKS, Firehose
• Experience with infrastructure-as-code (preferably Terraform) and CI/CD processes
• Comfortable building & maintaining production level data pipelines; streaming or event driven.
• Strong analytical and communication skills.

Even better if you have:

• Exposure to streaming technologies such as Apache Kafka / Google PubSub, Apache Beam, Google Dataflow.
• Having worked in an agile environment with scrum / kanban delivery methodologies

At DAZN, we bring ambition to life. We are innovators, game-changers and pioneers. So if you want to push boundaries and make an impact, DAZN is the place to be.

As part of our team you'll have the opportunity to make your mark and the power to make change happen. We're doing things no-one has done before, giving fans and customers access to sport anytime, anywhere. We're using world-class technology to transform sports and revolutionise the industry and we're not going to stop.

If you're ambitious, inventive, brave and supportive, then you're the kind of person who's going to enjoy life at DAZN.

We are committed to fostering an inclusive environment, both inside and outside of our walls, that values equality and diversity and where everyone can contribute at the highest level and have their voices heard. For us, this means hiring and developing talent across all races, ethnicities, religions, age groups, sexual orientations, gender identities and abilities. We are supported by our talented Employee Resource Group communities: proud@DAZN, women@DAZN, disability@DAZN and ParentZONE.

If you’d like to include a cover letter with your application, please feel free to. Please do not feel you need to apply with a photo or disclose any other information that is not related to your professional experience.

Our aim is to make our hiring processes as accessible for everyone as possible, including providing adjustments for interviews where we can.

We look forward to hearing from you.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer: Enterprise Content Management,"Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As Enterprise Content Management, you will be working as an application developer on projects in OpenText Process suite BPM. Your role would also involve in playing a critical role in design of a new system

Responsibilities:
• As a Business Process Management (BPM) Developer, you will manage asset services and application development while collaborating with global team in harmonizing the development of asset management applications.
• You will focus on improving corporate performance by managing business processes.
• Identification and driving of related service quality improvements and engineering deliverables.
• Management and progression of Action items on time with prompt response
• Automation and process improvement, if applicable
• Client communication

Required Technical and Professional Expertise
• Minimum 4 years of core development experience as OpenText Process Suite Developer
• Proficient in OpenText Process Suite BPM and having knowledge to design and develop the workflow
• Experience in Xform, HTML5, Angular JS. Javascript, & SQL
• Working knowledge of Core Java, Web Services & Ws APP integration is an added advantage
• Knowledge on Rest API's and SOAP' API's

Preferred Technical and Professional Expertise
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications

About Business UnitIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients' businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBMIn a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you'll be able to learn and develop yourself and your career, you'll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBMIBM's greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we're also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it's time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location StatementWhen applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBMIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Bengaluru,False,False,True,True,False,False,True,False,False,False,True,False,False,False,False,False
Digital Mapout Solutions India Private Limited,Azure Data Engineer,"Role : Azure Data Engineer

Location : Bangalore / Hyderabad

Experience : 4+

M.O.H : Full Time

M.O.W : Work From Office

NP Immediate / 15 days

Education : ÂBE / BTech, ME / MTech / MCA.

Skillsets : Azure Data Factory+Azure Data Lake + Azure SQL+ Azure Synapse+PowerBI

JD : -

Azure data / Lead Engineer :

Mandatory Skill sets : T SQL, Data Warehousing (DW) , ADF, Synapse Analytics

Optional : Power BI-DAX,Data Bricks, Python,PySpark

experience : 3 to 15 years

Senior developer to leads
• Candidate must have a strong experience background in database, Data warehousing & ETL / ELT design and development
• Exposure to complex & large scale enterprise development environment
• Excellent communication & collaboration skills required. Ability to work with internal and external stakeholders is must.
• Good business acumen
• Good problem solving and analytical ability
• Ability & willingness to learn new skills

Core technical skills
• Azure data lake Gen 2
• Synapse Analytics
• Python / scala programming
• Synapse pipeline / Azure data factory
• Azure SQL
• T-SQL programming
• Power BI-DAX",Bengaluru,True,False,True,False,True,False,False,True,True,False,False,False,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.Job DescriptionDevelops and maintains scalable data pipelines for bulk data movement between systems of record and systems of referenceDevelops and maintains scalable application to application integrationsAligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organizationImplements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processesWrites appropriate unit or integration tests to implement test-driven developmentContinually contributes to and enhances data team documentationPerforms data analysis required to troubleshoot and resolve data related issuesWorks closely with a team of frontend and backend engineers, product managers, and analystsDefines company data assets, artifacts and data modelsQualificationsRequired qualifications:5 years of Data Engineering and Data Integration5 Years of Data Warehousing3 Years of Data Architecture and Modeling2 years of Cloud Data EngineeringAgile MethodologiesPreferred skills:AWS or Azure Data CertificationsExperience with databricks, spark, pythonExperience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)Experience with SalesforceAdditional InformationAll your information will be kept confidential according to EEO guidelines.** At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. ** insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Factspan,Factspan Analytics - Azure Data Engineer - Big Data/Hadoop,"Job Description :- 7+ Years of deep experience with complex data systems and good instincts around data modelling and usage.- Knowledge of data engineering technologies, architecture, and processes. Specifically, Azure Data Lake, Hadoop ecosystem, Kafka, and common third-party integration and orchestration tools.- Good knowledge of multi-cloud data ecosystem and build scalable solutions on cloud (Azure)- Good knowledge of Big Data Ecosystem-Spark, Hadoop, Databricks- Work across 3-4 teams to develop practices which lead to the highest quality products and contribute transformation change within the cloud- Experience building large scale data processing ecosystems with real time and batch style data as input using big data technologies- Experience in any programming language like Scala or Python.- Exposure to agile methodology and proven ability to technically lead a team of engineers across geographies.- Implement Data Quality, Data Governance on Azure Cloud ecosystem- Good instincts around technical architecture, including metadata, Rest API Integrations, Data API and Solution design of NoSQL and File systems.- Willingness and ability to invest in engineering growth.- Strong communication skills and ability to coordinate across a diverse group of technical and non-technical stakeholders.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Big Data Engineer,"DATA ENGINEER - JD

The role will be part of the Data and Analytics Team responsible for expanding and optimizing AECOM’s data and data pipeline architecture, data flow, and collection for cross functional teams. The role will support software developers, database architects, data analysts, and data scientists on data initiatives and will ensure consistent optimal data delivery architecture throughout ongoing projects.

Responsibilities & Duties
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep AECOM’s data separated and secure across national boundaries through multiple data centres and regions.
• Create data tools for analytics and data scientist team members -to assist them in building and optimizing our product into an innovative industry leader.
• Collaborate with data and analytics experts to strive for greater functionality in our data systems.
• Escalate issues and recommend resolutions to the Team Lead for timely. May support junior members of the team in addressing routine issues within the assigned processes.
• Maintain the SOP/DTP of current processes and incorporate documentation updates as required.
• Perform moderately complex tasks in compliance with service level agreement, process, policies, and procedures.
• Propose alternatives in identified issues and assist in investigating and in resolving common and unusual issues.
• Contribute in various and simultaneous process improvement initiatives to streamline processes, improve customer experience, and increase productivity. This includes automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Contribute specialized expertise to different assigned projects and may provide key updates to Team Lead and Manager.

Qualifications & Requirements

Minimum Requirements:
• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or relevant discipline in the quantitative field
• 6-10years
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Advanced working SQL/nosql, ADLS, Databricks, ADF, Azure DevOps
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency,and workload management.
• Demonstrated ability to manipulate, process, and extract value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.

Preferred Qualifications
• Experience with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Attributes
• Demonstrated ability to champion and drive ideas/programs/solutions
• Excellent organizational and time management skills, able to work under pressure and prioritize effectively
• Able to demonstrate passion, energy and drive, especially in the face of resistance
• Ability to effectively communicate and collaborate within a varied audience and internal and external customers. (Communication)
• Ability to maintain good customer relationship with the ability to suggest ways to improve customer support customer experience (Customer Service)
• Ability to be thorough and meticulous in completing assigned tasks and identifying errors, duplicates, & discrepancies through defined methods. (Attention to Detail)
• Ability to identify and resolve simple to moderate with the ability to provide resolution alternatives by following defined policies and procedures. (Problem Solving)

experience

10",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,True,False,True,False
Tiger Analytics India Consulting Private Limited,Senior Data Engineer - Denodo,"Job Title: Senior Data Engineer – Denodo

Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 2800+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Curious about the role? What your typical day would look like?
· Engage with clients to understand their business context.
· Translate business needs to technical specifications.
· Define Data Virtualization architecture, deployments, and standards.
· Support the development of data architecture principles, standards, and processes and applies these to deliverables
· Involving in data exploitation and the development of (advanced) analytical data models with multiple data sources using Denodo/Tibco or AtScale semantic layer.
· Developing integrated data solutions, modernizing, consolidating, and coordinating business needs across several applications.
· Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Senior Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Senior Data Engineer

Senior Data Engineer, Delivery Engineering Platform
Delivery Engineering Platforms is part of MasterCard Data & Services group and one of the most rapidly growing organization in the space. Platform Teams provides cloud-based analytic software tools that enable large, consumer-focused businesses to seize the Big Data analytics opportunity by triangulating between business strategy, algorithmic math, and large databases to improve decisions.

100 of the largest corporations in the world uses these products. Test & Learn™ for Sites, Test & Learn™ for Customers, Test & Learn™ for Ads, and other similar products employ patented algorithms and workflow to design and interpret business experiments that evaluate, target, and refine proposed business programs

The Delivery Engineering Platform team is a core component to consulting services, managing the data acquisition, integration and transformation of client provided data within the Test & Learn platform for global engagements.

Role

The Senior Data Engineer will lead and participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace. A Senior Data Engineer will:
• Independently lead projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base
• Act as an expert technical resource within the team and region
• Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors
• Build trust-based working relationships with peers and clients across local and global teams
• Implement best practices and collaborate in the design of effective streamlined processes for a complex global solutions group
• Leverage industry best practices including proper use of source control, participation in code reviews, data validation and testing
• Plays a lead role where he/she oversees the activities of the data engineers and ensures the efficient execution of their duties
• Act as an advisor/mentor and helps in managing careers for junior team members
• Comply and uphold all MasterCard internal policies and external regulations

All about you:
• BE/BTech in a quantitative field (e.g., Computer Science, Statistics, Econometrics, Engineering, Mathematics, Operations Research). ME/MTech preferred
• Excellent English quantitative, technical, and communication (oral/written) skills; is an excellent listener
• Expertise with hands-on experience with RDMS technologies, preferably with Microsoft SQL Server, the SSIS Stack and .Net; Proficiency with at least one scripting language (VB Script, Perl, Python)
• Proven self-motivated leader with experience working in teams
• Demonstrate excellent skills in the ability to innovate, think critically and disaggregate problems. Able to provide oversight, validation and quality control to own and team work product
• Ability to easily move between business, data management, and technical teams; ability to quickly intuit the business use case and identify technical solutions to enable it
• Able to balance multiple projects and differing project priorities
• Flexible to work with global offices across several time zones

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LodgIQ,Data Engineer,"About LodgIQ

Headquartered in New York, LodgIQ delivers a revolutionary SaaS platform for Algorithmic Pricing and Revenue Management for the hospitality industry by incorporating machine learning and artificial intelligence. For more information, visit http://www.lodgiq.com.

Backed by Highgate Ventures and Trilantic Capital Partners, LodgIQ is a well-funded company, seeking for a motivated and entrepreneurial Developer to join its Product/ Engineering team. Qualified candidates will be offered an excellent compensation and benefit package.

Title: Data Engineer

Location: India

Requirements:
• In-depth knowledge of Python.
• Understanding of Django/Flask, Pandas.
• Familiarity with AWS Environment (EC2, S3, IAM, Athena).
• Working knowledge of NoSQL databases such as MongoDB.
• Proficiency in consuming and developing REST APIs with JSON data.
• Ability to perform data mining and data exploration with intuitive sense for problem solving and strong desire for craftsmanship.

Specific Job Knowledge, Skills & Abilities:
• Real world experience with large-scale data on AWS or similar platform.
• Must be a self-starter and an effective data wrangler.
• Intellectual curiosity and strong desire to learn new Big Data and Machine Learning technologies.
• Deadline driven, and capable of delivering projects on time under a fast paced, high growth environment.
• Willingness to work with unstructured and messy data.
• Bachelor’s degree or Masters degree in relevant quantitative fields (e.g. Computer Science, Statistics, Electrical Engineering, Applied Mathematics, etc).",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Edu Angels India Private Limited,Data Engineer (PySpark),"Responsibilities
• Develop process workflows for data preparations, modeling, and mining Manage configurations to build reliable datasets for analysis Troubleshooting services, system bottlenecks, and application integration.
• Designing, integrating, and documenting technical components, and dependencies of big data platform Ensuring best practices that can be adopted in the Big Data stack and shared across teams.
• Design and Development of Data pipeline on AWS Cloud
• Data Pipeline development using Pyspark, AWS, and Python.
• Developing Pyspark streaming applications

Eligibility
• Hands-on experience in Spark, Python, and Cloud
• Highly analytical and data-oriented
• Good to have - Databricks",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Zepto,Data Engineer III (Lead Data Engineer),"Responsibilities:
• Collaborate with Tech and Analytics team to build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
• Oversee and govern the expansion of the current data architecture as the business grows and ensure best practices are followed.
• Design and build best-in-class architecture for data tables to ensure optimal querying performance in relational databases.
• Create and maintain connectors that expose the data securely for consumption by downstream systems and services in near real-time.
• Create and maintain data architecture docs to communicate data requirements that are important to business stakeholders and work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that.
• Build data governance and security protocols and ensure adherence from analytics, tech, and business teams.
• Build and mentor the data engineering team, recognize their strengths, and lead them to take ownership of end-to-end data architecture.
• Stay on top of the latest developments in the tech stack and propose potential upgrades to existing systems.

Requirements:
• 6 to 10 years of experience in Data Engineering - Designing databases, building data pipelines, and maintaining data governance protocols in cloud platforms.
• A visionary in technical architecture, with experience building and maintaining Data.
• Engineering Products, along with the demonstrated ability to take accountability for achieving results.
• Hands-on working experience with Python, ETL pipelines, and advanced SQL.
• Strong understanding of AWS Services - Redshift, Lambda, Glue, Athena, and security protocols.
• Experience in any Cloud DW Redshift/Snowflake/BigQuery/Synapse.
• Strong data Modelling and database design experience with Redshift or other relational databases.
• Experience working with Agile methodologies, Test Driven Development, and implementing CI/CD pipelines using Gitlab and Docker.
• Good understanding of ETL/ELT technology and processes.
• Experience in gathering and processing raw data at scale including writing scripts, web scraping, and calling APIs.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,True,False,True
Confidential,Data Engineer 3 - AWS & Python (Contractual),"IntroductionThe Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed. At our heart is a 50 year forward look, a global forecast of the majority of the world's economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions. We're changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products. What will you experience At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to. Accountabilities How you will contribute: Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment. Work in an agile environment: within a collaborative agile product team using Kanban Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements. Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives. Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals. Experience, skills and professional attributesTo succeed in this role it would be an advantage if you possess: Experience with programing in Python, and Lambda functions Knowledge of building bespoke ETL solutions, and extracting data using Data APIs MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting Prior experience in design and developing microservice architecture Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. A combination of IT skills, data governance skills, analytics skills and economics knowledge An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience. Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. This employer is a corporate member of myGwork - LGBTQ+ professionals, the business community for LGBTQ+ professionals, students, inclusive employers & anyone who believes in workplace equality. PRB",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Axtria - Ingenious Insights,Data Engineer,"• 5-8 years of experience in data engineering, consulting, and/or technology implementation roles
• Expertise in the design, data modeling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation and delivery
• Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena
• Strong proficiency in SQL and database design, development and maintenance
• Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence
• Good understanding of modern architecture patterns like serverless and microservices
• Expertise with analytics and business intelligence solutions (e.g. 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation and Agile Methodology
• Experience working with some of the following marketing data sources
• Traditional > TV / Print / Email
• Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data
• Experience leading project teams with members with different roles and skills
• Experience working in hybrid onshore/offshore team models
• Strong communication skills",New Delhi,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Valiance Solutions,Big Data Engineer,"About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc. Our team comprises 150+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire a Senior Data Engineer to help our customers create scalable data engineering pipelines and infrastructure for downstream analytics workloads. You should be good at understanding client data needs, the landscape of various heterogeneous data sources, identifying a set of services for data ingestion & transformation workloads, and timely execution of projects.

Roles & Responsibilities:
• As a data engineer with Pyspark & SQL skills you will be required to highly scalable, robust, and resilient data engineering pipelines .
• You will be working closely with business stakeholders & the data science team to understand their data requirements and underlying business logic.
• Deploy and monitor pyspark jobs on cloud infrastructure.
• Troubleshoot job failures and ensure system recovery at earliest.
• Attending regular client calls, communicating work status and pro-actively highlighting any delays to the product release.

Technical Skills :
• Hands on experience on pyspark for at least 3 years
• Solid programming experience in Python & SQL is required.
• Working experience of any one cloud platform; AWS, GCP or Azure
• Intermediate plus proficiency in shell scripting
• Experience deploying ML algorithms in production is preferred

Personal Skills :
• Excellent communication skills, both written & oral.
• Ability to learn new skills quickly, adjust to the changing needs of the project.
• You are highly enthusiastic about your work
• Ability to multi-task, manage high-pressure release scenarios occasionally.

Valiance Solutions focuses on Financial Services, Cloud Computing, Artificial Intelligence, Internet of Things, and Big Data Analytics. Their company has offices in Noida and Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://valiancesolutions.com or find them on Twitter and LinkedIn.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
DarioHealth,Data Engineer - Hybrid,"About The Position

At Dario, Every Day is a New Opportunity to Make a Difference.

﻿We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career.

DarioHealth is looking for an experienced Data Engineer who will join our team and create new data solutions, maintain existing solutions and be a focal point of all technical aspects of our data activity. As part of this position, you will develop advanced data and analytics solutions to support our analysts and production units with validated and reliable data.

Responsibilities
• Develop and maintain DarioHealth data infrastructure.
• Develop in-house applications for providing self-service tools.
• Develop real-time data applications for production.
• Provide analysts and data scientists technical support related to data infrastructure.
• Design, build and launch new data models and visualizations in production, leveraging common development toolkits.

Requirements
• At least 4 years of proven experience with Python - a must.
• Very high level of SQL and data warehouse modeling.
• Experience with 24/7 systems and real-time analytics.
• Experience developing data pipelines with Airflow or similar - a must
• Experience with big data solutions like Kinesis/Sparks - an advantage
• Experience with NoSQL databases like MongoDB/Redis.
• Experience with web development using Django/javascript/react - an advantage.
• Experience in the online industry.
• B.A./B.Sc. in industrial/information systems engineering, computer science, statistics, or equivalent.
• **DarioHealth promotes diversity of thought, culture and background, which connects the entire Dario team. We believe that every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions. We are passionate about building and sustaining an inclusive and equitable working and learning environments for all people, and do not discriminate against any employee or job candidate.***",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False
AlphaGrep Securities,Data Engineer,"About the Company

AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes (equities, commodities, currencies, fixed income) that trade on global exchanges..

We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.

Roles & Responsibilities
• Build infrastructure tools and applications to support trading teams across the firm.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Coordinate with global teams to understand their requirements and work alongside them.
• Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
• Set up practices on data reporting and continuous monitoring
• Write a highly efficient and optimized code that is easily scalable.
• Adherence to coding and quality standards.

Required Skills
• Strong working knowledge in Python.
• Strong working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have
• Experience with web crawling and scraping, text parsing
• Experience working in Linux Environment
• Experience with Stock Market Data

Why You Should Join Us
• Great People. We’re curious engineers, mathematicians, statisticians and like to have fun while achieving our goals
• Transparent Structure. Our employees know that we value their ideas and contributions
• Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.
• Health & Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Robert Bosch,Azure Data Engineer,"Job Description

Location : Bengaluru
Experience : 6 to 8 years
Requirements
• Overall 6+ IT experience out of which 3+ years of working experience in Azure, architecting data soultions with good experience on Azure SQL, Azure Data Lake, ADF, Azure DataBricks/Synapse etc.
• 5+ years experience with data modelling,implementing backends and data optimization.
• Good experience working with with Automotive domain usecases.
• Experience implementing compliances like GDPR,HIPAA etc.
• Enforcing data security at rest and in transit.
• Good experience implemeting data security in Azure storage systems.
• Ability to thrive in a fast-paced, dynamic, client-facing role where delivering solid work products to exceed high expectations is a measure of success
• Excellent leadership and interpersonal skills
• Eager to contribute in a team-oriented environment
• Ability to be creative and analytical in a problem-solving environment
• Effective verbal and written communication skills

Skills
• Must have skills : SQL,ADF,other Azure storage services.
• Good to have:Synapse,spark or any big data framework or data warehouse.
• Key Responsibilities : conceptualizaing ,modelling Optimizing databases.Designing data flows
• Knowledge or basic experience with Nosql,parquet,Predictive modelling etc
• Working knowledge, creating ETL packages and deploying them

Qualifications

BE,MCA,MSC,MS,MTech
Experience : 6 to 8 years

Additional Information

Additional information
• Nice to have : Power BI experience, Azure DevOps, Cost Monitoring, Azure AD, Azure Synapse.
• Ability to quickly ramp up on new Azure Components which comes in Azure Roadmap.
• Excellent communication skills (English)

Experience: 6.00-8.00 Years",,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
CX Customer Experience,Salesforce - Data Engineer,"Come create the technology that helps the world act together

Nokia is committed to innovation and technology leadership across mobile, fixed and cloud networks. Your career here will have a positive impact on people’s lives and will help us build the capabilities needed for a more productive, sustainable, and inclusive world.

We challenge ourselves to create an inclusive way of working where we are open to new ideas, empowered to take risks and fearless to bring our authentic selves to work.

The team you'll be part of

You will work as part of the CX Global Sales Operation team. You will work making our Data strategy come alive across CX. You will be involved with integrating data across multiple platforms and especially the integration of the Advanced Analytic platform and the use cases being developed on it. The Advance Analytic projects will enable the CX organization to increase speed and efficiency, and assure the right things are done in the right way to maximize Nokia business.

What you will learn and contribute to

Be responsible for the data engineering of the Advanced Analytics Platform and CX AI use cases

Data integration from different source data platform to the advanced analytics platform

Data integration from the advanced analytic platform to different platform with UI

Ongoing support for the integration

Potentially doing data cleaning and wrangling on the advanced analytics platform

Potential involvement in dashboard and UI development

The type of use case you will work on will center around one or more of the following:
• Machine learning prediction models,
• The use of machine learning to automate the currently manual business planning process
• Knowledge mining and recommendation systems to improve the likelihood to win new business.

Your skills and experience
• Deep experience end data engineering including but not limited to experience using SQL and other types of databases and database languages
• Proficiency developing software probably in python using jupyter notebooks
• Proven competency in agile and lean software development
• Competency in SCM (Git), Automation tools, infrastructure automation,
• Good knowledge about Azure cloud infrastructure, security and application development.
• Experience with Python / Spark and Delta Lake. Familiar with big data patterns like lake house.
• Experience with Azure DevOps, CI/CD pipelines, version control tools like GIT / VSTS. Familiar with IDE’s like Visual Studio
• Nokia Business and process understanding
• Bachelor’s degree or higher in information technology, data science or related disciplines
• Effective communication in English (written and verbal)

Nice to have:
• Experience in app development

What we offer

Nokia offers flexible and hybrid working schemes, continuous learning opportunities, well-being programs to support you mentally and physically, opportunities to join and get supported by employee resource groups, mentoring programs and highly diverse teams with an inclusive culture where people thrive and are empowered.

Nokia is committed to inclusion and is an equal opportunity employer

Nokia has received the following recognitions for its commitment to inclusion & equality:
• One of the World’s Most Ethical Companies by Ethisphere
• Gender-Equality Index by Bloomberg
• Workplace Pride Global Benchmark
• LGBT+ equality & best place to work by HRC Foundation

At Nokia, we act inclusively and respect the uniqueness of people.

Nokia’s employment decisions are made regardless of race, color, national or ethnic origin, religion, gender, sexual orientation, gender identity or expression, age, marital status, disability, protected veteran status or other characteristics protected by law.

We are committed to a culture of inclusion built upon our core value of respect.

Join us and be part of a company where you will feel included and empowered to succeed.

Additional Information",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 20 2023

Ready to help shape the future of healthcare?

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organization where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to impact the health of 2.5 billion people around the world in the next 10 years.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves - feeling welcome, valued and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.

The Senior Principal Data Engineer is a vital technical role in the successful design and delivery of Data and Analytics (D&A) initiatives for the GSK's Pharmaceutical and Vaccines Supply Chains. The primary purpose of this role is to ensure that D&A Products have an optimal solution design and that the technical development work to then deliver them into production and support is smooth and successful. This requires deep expertise in data and analytics platforms and technologies as well as domain understanding of Pharmaceutical & Vaccines manufacturing and quality processes. This also requires close collaboration with D&A Product Managers, D&A Development Squads and the D&A Platform & Architecture team as well as with business stakeholders and other Digital and Tech teams.

The MSAT & Quality D&A team currently has a portfolio of around 15 D&A products across 4 product groups with over 100 people (GSK employees plus contractors) working in agile squads to deliver these. The Sr Principal Engineer will oversee and be accountable for the technical success of all of these products.

Key Responsibilities:
• Accountable for optimal solution designs for D&A Products that facilitates an agile, product management approach, can be rapidly and cost-effectively delivered to meet the true business requirements and are robust, sustainable and supportable throughout their lifecycle
• Work closely with D&A Product Mangers, using deep technical expertise and domain understanding to effectively influence (and when needed challenge) business and architectural stakeholders to arrive at the right design
• Steer solution design through D&A Architecture Review process, aligning with enterprise platforms and architectural patterns by first intent
• Oversee technical work of development teams ensuring it is remains aligned with agreed design, is of high quality, complies with relevant standards and policies and will meet agreed business objectives
• Provide hands-on technical problem-solving expertise to address technical challenges during development and, where needed, during lifecycle support
• Act as mentor for more junior technical roles, supporting their development and promoting adoption of best practice across development teams
• Lead discovery / proof-of-concept activities to establish early technical feasibility of new Products or Product Features
• Input to, review and approve key technical documents (e.g. design spec, validation plan)
• Drive adoption by development teams of existing and future best-practice approaches from D&A Platform team (e.g. implementation of DevOps CI/CD pipelines, automated testing)
Why you?

Basic Qualifications:
• Computer Science or related Bachelor's degree
• 16+ years of experience and track record of engineering and delivery of flexible, scalable, and supportable data and analytics applications for large complex, global organizations
• End-to-end / 'full stack' D&A experience from data ingestion through transformation to user interaction (visualisation, analytics, etc.)
• Track record of designing and delivering solutions in a cloud environment using modern data architectures and engineering technologies
• Experience designing with DataOps and FinOps in mind to ensure solutions are flexible/future-proof and can scale to handle growing demand, while remaining cost effective
• Experience in Agile development
• Track record of designing and delivering solutions compliant with industry regulations and legislation
• Ability to oversee and matrix manage GSK and 3rd party technical resources
• Excellent communication, negotiation, influencing and stakeholder management skills.
• Customer focus and excellent problem-solving skills.
Preferred Qualifications:
• Computer Science or related Master's degree
• Microsoft Azure accreditation and experience
• SAFe (Scaled Agile) accreditation experience
• Experience developing and delivering GxP-validated solutions for the Pharma/Vaccines industry
• Experience with specific technologies in GSK stack: Talend, Databricks/DeltaLake, Azure Synapse, Snowflake, PowerBI, Azure Functions, Azure App Services,
At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.

#LI-GSK

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We're committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in ""gsk.com"", you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Confidential,Data Engineer,"Job purpose We are hiring a Data Engineer to join our Enterprise Analytics team. As a Data Engineer, you will be responsible for building, maintaining, and optimizing the data infrastructure needed to support our company's data-driven initiatives. You will work closely with our data analysts, data scientists, and business intelligence developers to ensure that our data is accurate, complete, and secure.Job Responsibilities:Design, build, and maintain the data infrastructure needed to support our company's data-driven initiatives.Develop and maintain data pipelines and ETL processes that move data from source systems to our data warehouse.Implement data quality checks to ensure that our data is accurate, complete, and consistent.Work closely with our data analysts, data scientists, and business intelligence developers to understand their data needs and ensure that our data infrastructure meets those needs.Optimize our data infrastructure to ensure that it can handle large amounts of data and support complex queries.Develop and maintain documentation for our data infrastructure and processes.Stay up to date with the latest technologies and trends in data engineering and recommend new tools and techniques as appropriate.Collaborate with other members of the BI and Data team to ensure that our data infrastructure is aligned with our company's strategic goals.Background and experience:Bachelor's degree in Computer Science, Engineering, or a related field.3+ years of experience in data engineering or a related field.Competencies and skills:· Strong knowledge of SQL and experience working with relational databases.· Experience with data modeling and schema design.· Experience building and maintaining data pipelines and ETL processes.· Experience with cloud-based data warehousing technologies, such as Azure Data Lake, Data Factory, Synapse Analytics.· Strong problem-solving skills and attention to detail.Excellent communication and collaboration skills. Transportation, Logistics and Storage,IT Services and IT Consulting,Truck Transportation",,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Loop Health,Data Engineer - Remote,"About Loop

Looking for a great mission? Help build a customer focused healthcare company.

Loop wants to create an inspirational healthcare and insurance company. We believe in the transformative nature of empathetic primary care, proactive financial coverage and want to bring that to our members. We want to fundamentally change how healthcare assurance is designed and delivered. We believe in the power of incentives. We are successful when we deliver health outcomes — when our members and their families get healthier.

“Why exactly are we building a new revolutionary healthcare system? The obvious answer is India deserves better care for its people. Not enough of it around, and what exists can be tough to navigate.

Imagine if doctors were paid to actually make you better. What a concept! What if they didn't have to worry about finishing consults in 10 minutes to meet their daily quota. What if they could take their time, really understand the symptoms, the family history & the lifestyle to come up with a plan, rather than just a prescription.

Imagine if hospital admissions, treatments, billing and insurance were as easy as ordering food home and your care doesn't end when they send you home from the hospital. It goes till you are back on your feet. And further, now imagine if your family had access to this great care anytime they wanted. From serious conditions to the smallest questions. So that they live longer. Wouldn't you worry less?

At the end of it. It's not why you would build this system... Why wouldn't you?”

Here’s how we are going about it:

- We built a high quality concierge and primary care program that allows members and their families to access unlimited care when they need it.
- We use technology to deliver this through highly engaging care.
- We work with insurers to bring financial protection to our members so that they do not worry about their families’ well being.
- As a healthcare insurance broker, we provide companies with the best coverage and claims service for their employees and dependents.

Doing this will mean that we create great products and services that work on changing behaviors and mindsets. This will need a deep understanding of design of products and services through an empathetic lens of what members need for their health and technology will play a very pivotal role in enabling our members to use our programs . We are looking for folks in our ‘EngineeringTeams’ to work with us to take Loop to this future.

If you'd like to learn more about what we are building at Loop, there are tons of resources. Here are some of our favorites:
https://yourstory.com/2021/06/loop-health-insurance-plans-improve-
healthcare/amphttps://yourstory.com/2022/04/loop-health-raises-25m-elevation-capital-general-catalyst/amp

Join us in making healthcare simple, reliable, and human.

Roles and Responsibilities

• Work in collaboration with engineers and stakeholders to build a platform for enabling data-driven decisions.
• Build reliable, scalable, CI/CD driven streaming and batch data engineering pipelines.
• Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.
• Create a conceptual data model to identify key business entities and visualize their relationships.
• Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and relationships
• Storage (cloud data warehouse, S3 data lake), orchestration (Airflow), processing (Spark, Flink), streaming services (Kafka), BI tools, graph database, and real-time large scale event aggregation store are all examples of data architecture to design and maintain.
• Work on cloud data warehouses, data as a service, business intelligence, and machine learning solutions.
• Data wrangling in a diverse environment.
• Ability to provide data and analytics solutions that are cutting-edge.
• Identify strategic and Operational KPIs for the team and drive the team to deliver the committed targets.

Qualifications

• SQL knowledge, as well as programming skills in Scala or Python.5+ years of applicable data warehousing, data engineering, or data architecture experience
• Experience with the GCP stack (BigQuery, GCP Databricks) is a plus
• Ability to design data analytics solutions to meet performance and scaling requirements.
• Demonstrated analytical and problem-solving abilities, particularly in the context of large data.
• Data warehousing concepts and modern data warehouse/Lambda architecture are well-understood.
• Good understanding of the Machine Learning and Artificial Intelligence (AI) solution space.
• Communication and interpersonal skills at all levels of management
• You are a detail-oriented person with excellent communication skills and a strong sense of teamwork.

What you can expect from us

  ‍  ‍   Loop Family Healthcare Health insurance for you and your family for all medical emergencies.

   High agency You'll always have the agency to shape projects, processes and outcomes independently.

  Learning Budget If there's a workshop, book or event you think will help you learn, we'll cover your bill.

   Work from home setup We'll help you set up your office the way you want to with the best equipment around.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,True,True,False
Visa,Sr. Data Engineer - Big Data Testing,"This position is ideal for an engineer who is passionate about solving challenging business problems. You will be an integral part of the Payment Products Development team focusing on test automation. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, and testing of new and existing functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Develop systems and processes to refine efficiency of automated testing solutions
• Design and execute tests for applications and services
• Develop and maintain tools for automation tracking and reporting
• Review product requirements and specifications and recommend improvements to ensure product testability
• Recommend areas of applications and services where automation would be beneficial
• Present technical solutions, capabilities, considerations, and features in business terms
• Effectively communicate status, issues, and risks in a precise and timely manner
• Perform other tasks on data governance, system infrastructure, and other cross team functions on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
PayPal,"MTS 1, Data Engineer","Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
The MTS 1 ? Data Engineer will directly report to and support Sr. Manger of Finance Technologies in the development and execution of strategic transformation programs & initiatives, strategic engineering architecture design, resource allocation, and platform performance monitoring. Ideal candidate is a technologist who believes that use of technology is in its infancy and the best is yet to come. The Regulatory Reporting Hadoop product owner (Business System Analyst) will be part of the Global Regulatory Reporting, and Merger & Acquisition Integration support. The nature of role is strategic, analytical and highly collaborative, working with team members across World and also as a liaison for Global projects.
• Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
• Undergraduate/ Master degree in Computer Engineering or equivalent from a leading university.
• 11+ years of post-college working experience as a Business System Analyst and leading large scale projects end to end.?
• Minimum 4+ years? experience working with large data sets, experience working with distributed computing a plus (Map/Reduce, Hadoop, Hive, Spark, etc.)
• Experience in Data Analysis, Data Validation.
• Strong knowledge in writing complex queries for validation of ETL process.
• Preferred/Basic understanding of Payments/Finance/Accounting Industry Background.
• Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations.
• Experience with databases, systems integration, application development and reporting.?
• Works independently and able to make decisions quickly when necessary.
• Quick Learner with an ability to ramp up in technologies and modules to meet business needs.
• Works in an Agile environment and continuously reviews the business needs, refines priorities, outlines milestones and deliverables, and identifies opportunities and risks.
• Maintain, track and collaborate with dev teams to ensure project estimation for delivery.
• Experience using JIRA and Confluence, or similar User Story workflow and management tool is a must.
• Highlight the bugs and blockers and coordinate with the development and operations team to come up with the best solutions/fixes and document them.
• Work across internal team in various geo-locations across the world
• ?Drive For Results? - Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.
• ?Priority Setting? - Spends his/her time and the time of others on what’s important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.
• Weekly and Monthly status reporting to leadership.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Software Engineer II | Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Software Engineer II | Data Engineer

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Overview
The Enterprise Data Solutions team is looking for a Big Data Engineer to drive our mission to unlock potential of data assets by consistently innovating, eliminating friction in how users access data from its Big Data repositories and enforce standards and principles in the Big Data space. The candidate will be part of an exciting, fast paced environment developing Data Engineering solutions in the data and analytics domain.

Role
• Develop high quality, secure and scalable data pipelines using spark, Scala/ python on Hadoop or object storage.
• Leverage new technologies and approaches to innovate with increasingly large data sets.
• Drive automation and efficiency in Data ingestion, data movement and data access workflows by innovation and collaboration.
• Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency.
• Perform assigned tasks and support production incidents.

All About You
• 4+ years of experience in Data Warehouse related projects in product or service-based organization
• Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
• Experience of building data pipelines through Spark with Scala/Python/Java on Hadoop or Object storage
• Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
• Experience of working on Nifi will be an added advantage
• Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
• Strong communication skills - both verbal and written
• Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
• Flexibility to work as a member of a matrix based diverse and geographically distributed project teams

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Plume Design,Senior Data Engineer,"Plume’s Cloud Platform team is looking for engineers to build and operate data pipelines that power the gamut of Plume products and analytics. Due to the massive scale and performance requirements of many of our use cases, you will be solving challenging problems on a daily basis using a variety of cutting edge technologies.

What you will do:
• Interact with stakeholders to gather and understand data requirements
• Design and implement data pipelines with high data quality goals
• Maintain up-to-date documentation of data warehouse schemas
• Write clean, maintainable code, and perform peer code-reviews
• Refactor code as needed to improve performance and simplify operations
• Provide production support in triaging and fixing issues relating to data quality and availability
• Mentor and assist junior team members and new hires to become successful and productive
• Adhere to data protection requirements including data access, retention, residency and de-identification
• Play an integral role in driving the technology roadmap and enhancing best practices

What You’ll Bring
• Education Requirements: BS/MS/PhD in Computer Science, Electrical Engineering or related technical field
• 5+ years of software development experience with a proven track record of building, scaling, and supporting production data pipelines
• High proficiency in writing idiomatic code, preferably in Java or Scala
• High proficiency in writing SQL in data warehousing technologies
• Strong understanding of large-scale data processing technologies, e.g. Apache Spark (preferred) or Apache Flink
• Strong understanding of data warehousing concepts
• Strong analytical and problem-solving skills
• Strong oral and written communication skills

Plume Design focuses on Internet Service Providers and Cloud Data Services. Their company has offices in Palo Alto. They have a mid-size team that's between 51-200 employees. To date, Plume Design has raised $37.5M of funding; their latest round was closed on June 2017.

You can view their website at https://platform.plume.com or find them on Twitter, Facebook, and LinkedIn.",Hyderabad,False,False,True,True,False,False,False,False,False,False,False,True,False,False,False,False
Lilly,Data Engineer - (DT) Business Insights & Analytics,"At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 35,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world.

Business Insights and Analytics: Data Engineer

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease. We’re looking for people who are determined to make life better for people around the world.

The LCCI (Lilly Capability Center India), BI&A (Business Insights & Analytics) team was started in 2017 with the objective of supporting business decisions for the commercial and marketing functions in the US and ex-US affiliates. This team is part of the LCCI - Commercial Services organization and works very closely with business analytics team based in Indianapolis (HQ). The team currently comprises of more than 100 staff members, with varied backgrounds and skills across data management, analytics and data sciences, business and commercial operations etc.

To better meet the evolving analytics needs, the LCCI BI&A team is ramping up the data engineering pillar. We are looking for data engineers who can be play integral role in developing, maintaining, and testing infrastructures for data generation, processing and storage; work closely with data scientists and help architecting solutions with the objective of driving right KPIs for the business.

Core Responsibilities:
• Create and maintain optimal data pipeline architecture ETL/ ELT into Structured data
• Assemble large, complex data sets that meet functional / non-functional business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.
• Expert level experience creating Fact tables, Dimensional tables and ingest datasets into Cloud based tools. Job Scheduling, automation experience is must.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Setup and maintain data ingestion, streaming, scheduling, and job monitoring automation. Connectivity between Lambda, Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and “big data” technologies like AWS and Google
• Build analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics
• Work with cross-functional teams including external consultants and IT teams to assist with data-related technical issues and support their data infrastructure needs
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

Experience Required
• 4-8 years of in-depth hands-on experience in data warehousing (Redshift or any OLAP) to support business/data analytics, business intelligence (BI)
• Advanced knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouses
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience building and optimizing “big data” data pipelines, architectures and data sets
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores
• Strong analytical and problem-solving skills to be able to structure and solve open ended business problems (pharma experience is highly preferred)

Education
• Bachelor’s/ Master’s degree in Technology OR Computer Sciences

Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response.

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,True,False,False,True
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications

• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
GE,Senior Data Engineer,"Job Description Summary
GE HealthCare is on a transformational journey leveraging Data and Analytics to drive business growth. GE HealthCare is looking for Senior Data Engineer who will be responsible for building and implementing the data ETL pipelines for Finance function data (from data ingestion to consumption).The Data Engineering team helps solve our customers' toughest challenges leveraging data and analytics. The Senior Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE HealthCare to drive business analytics to a new level of predictive analytics while leveraging On-prem, Cloud Platform, Big data tools and technologies.

GE HealthCare is a leading global medical technology and digital solutions innovator. Our purpose is to create a world where healthcare has no limits. Unlock your ambition, turn ideas into world-changing realities, and join an organization where every voice makes a difference, and every difference builds a healthier world.

Job Description

In this role you will:
• Responsible for building data and analytical engineering solutions with standard end to end design & ETL patterns, implementing data pipelines, data modelling and overseeing overall data quality.
• Responsible to work with cross functional teams in GEHC to make the data usable for functional users, data scientists and application users to enable delivery of business values to customers.
• Responsible to enable access of data in AWS storage layers and transformations in AWS Datawarehouse and further transporting in respective databases, consumers, data marts etc.
• As a Senior Data Engineer, you will be part of a data engineering or cross-disciplinary team on Finance facing development projects, typically involving large, complex data sets. These teams typically include data engineers, data visualization engineers, architects, data scientists, product managers, and end users, working in cohorts with partners in GE business units.
• Implement Data warehouse entities with common re-usable data model designs with automation and data quality capabilities.
• Demonstrate proficiency at industry standard data modeling tools (e.g., Erwin, ER Studio, etc.).
• Integrate domain data knowledge into development of data requirements.
• Develop processing codebase using pySpark and implement medium to complex transformations, business logics.
• Look across multiple systems, understands the purpose of each system and defines data requirements by systems.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Lead other horizontal improvement initiatives to benefit technology and leap further on a problem area or Hackathon etc
• Establish and maintain as a trusted advisor relationship within GE Healthcare Data & Analytics (Finance Function)
• Establish and maintain close working relationships with teams responsible for delivering solutions to the businesses and functions
• Engage collaboratively with project teams to support project objectives through the application of sound data engineering principles
• Identify risks and assumptions for the in scope Data & Analytics solutions
• Work with the contract/vendor resources to deliver the solution and manage the technical resources work

Qualifications
• Bachelor's Degree in Computer Science, Information Technology or equivalent (STEM)
• A minimum of 6 year of similar experience working on Database(s), SQL, Python, Datawarehouse, Java, ETL and AWS cloud platform is required. AWS certifications would be added advantage
• Experienced in Deployment process on-prem and on-cloud using Kubernetes, Dockers, Jenkins
• Ability to drive projects in big data (structured/unstructured/machine/logs/streaming data types)
• 3+ Year of Data modelling & Data warehousing experience with MPP systems (Teradata, Netezza, Greenplum etc.)
• 3+ years in AWS Services Like Redshift, RDS, S3, Glue, Step Function, Lambda etc.
• Hands on experience in delivering analytics in modern data architecture (Massively Parallel Processing Database Platforms and Semantic Modelling)
• Demonstrable knowledge of ETL and ELT patterns and when to use either one; experience selecting among different tools that could be leveraged to accomplish this. (i.e. Informatica, HVR, Talend etc)
• Demonstrable knowledge of and experience with different scripting languages (python, shell)
• Understands data quality and solves for application-level needs
• Understanding of DaaS, Data management tools / solutions
• Strong verbal & written communication
• Experience working with solutions delivery teams using Agile/Scrum ore similar methodologies
• Added advantage if experienced in working on Finance data

Desired skills:
• Delivers results when working on shorter-term (weeks-months), outcome-focused service engagements
• Proactively learning new technology, predicts trends, and identifies new opportunities based on trends
• Leverages knowledge about technology trends, and changing business needs across the broad environment to bring new ideas to the team
• Articulates the value proposition of existing technology capabilities and maps them to customer requirements to minimize incremental cost of development
• Experienced in working with On-prem (Teradata) data warehouse – Dimensional and data modelling. Experienced in one of the ETL like Informatica.
• Identifies the customer’s business and strategic needs, concerns, and desires for the value delivery capabilities of the Product
• Functional understanding of finance - Close Book, Treasury, Cash, Controllership, Credit, Account Payables, Account Receivables, Cash Forecasting, Balance sheet exposure, Debt, Forex etc.

Inclusion and Diversity

GE Healthcare is an Equal Opportunity Employer where inclusion matters. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

We expect all employees to live and breathe our behaviors: to act with humility and build trust; lead with transparency; deliver with focus, and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.
#LI-Hybrid
#LI-GM2

Additional Information

Relocation Assistance Provided: Yes",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,False
Concinnity Media Technologies,Senior Data Engineer,"Preferred Experience:

• 8+ years’ experience building mobile, web and/or API-based applications

• Follow engineering standards and best practices

• Knowledge of databases: MySQL, PostgreSQL, SQL, etc...

• 4+ years of Python server development experience

• Django, Flask, Bottle, or similar framework experience

• 2+ years industry experience

• Knowledge of cloud deployment strategies using AWS, Azure, Rackspace, etc.

• Expertise working with and building RESTful APIs

• Ability to operate in Agile / Scrum development environments

• Understanding of OOP and Data Structures and know when to apply them in daily coding scenarios

• Knowledge in the following web-technologies:
• JavaScript
• HTML & HTML5
• CSS3
• JavaScript frameworks (React, Angular, Next.js, etc.)

• Understand the development of the following:
• Responsive Web Development
• Accessibility
• Secure web applications

• Message queue implementations (RabbitMQ, ZeroMQ, Kafka, etc.)

• Background task processing (Celery, etc.)

• Experience configuring container like systems (Vagrant, Docker, etc.)

• Container orchestration with Kubernetes

• Ability to self-organize with minimal guidance/competing priorities and work effectively within a team

• Ability to provide innovative, creative solutions to tasks/problems

• Ability to complete work following engineering standards and best practices

• Experience with GIT and Gitlab is a plus

• Experience with JSON is a plus",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Hewlett Packard Careers,Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.
We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

The Data Engineer will develop, test, and maintain Big Data solutions for a company. Gather large amounts of data from multiple sources and ensure that downstream users can access the data quickly and efficiently. Essentially, the company’s data pipelines are scalable, secure, and able to serve multiple users.

Job description
• Meeting with managers to determine the company’s Big Data needs.
• Developing Hadoop systems.
• Loading disparate data sets and conducting pre-processing services using Spark, Hive or Pig.
• Finalizing the scope of the system and delivering Big Data solutions.
• Managing the communications between the internal system and the vendor.
• Collaborating with the software research and development teams.
• Building cloud platforms for the development of company applications.
• Maintaining production systems.
• Training staff on data management.

Big Data Engineer Requirements:
• Bachelor’s degree in computer engineering or computer science.
• Previous experience as a big data engineer.
• In-depth knowledge of Hadoop, Spark, and similar frameworks.
• Knowledge of scripting languages is preferred .
• Knowledge of NoSQL and RDBMS databases including Redis and MongoDB.
• Familiarity with Mesos, AWS, and Docker tools.
• Excellent project management skills.
• Good communication skills.
• Ability to solve complex data, and software issues.

Education and Experience Required:
• Typically, 6+ years of progressive professional experience as a big data engineer.
• Bachelor’s degree in computer engineering or computer science.

We love our work environment. We think you will too:
• It’s a friendly atmosphere with supportive leaders to bring your creativity to the max.
• Work-life balance support including flex-time arrangements and work from home opportunities.
• Corporate Social Responsibility initiatives to help you make an impact to communities at large.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Omnivio,Data Engineer - Full Time,"Job Profile

Omnivio is a startup in the Supply Chain and Logistics domain. We help retailers deliver an 'Amazon like' shopping experience to their customers. We optimize and manage delivery times and cost, inventory, geo-distributed stores etc. One of the core pieces of our infrastructure is the data engineering required to get data from various upstream systems into a data model that we use for intelligence. If you've worked in data engineering before, you might imagine that this has a good number of challenging engineering problems. We are looking for junior to mid level data engineers to join our team.

Experience / Skills required

We are looking for previous experience in data engineering for this role. Here's a list of tools and technologies that you can expect to be working with in this job. The listed examples are not necessarily all a part of our stack, but they are solid indicators of your skills being a good fit for the job.
• Relational Databases, both OLTP and OLAP, such as MySQL, Postgres, Redshift, BigQuery, CLickhouse etc
• Solid software engineering fundamentals, and experience with one or more general purpose programming languages such as Python, Typescript
• Data engineering programming libraries such as Pandas, NumPy etc
• Building data engineering pipelines using orchestration tools such as Airflow, Airbyte, Temporal, or other commercial offerings
• Transformations using dbt, or a similar alternative
• Experience with AWS's data engineering stack is definitely a plus
• Deployment/operating experience with any of these tools would be really interesting too

Work culture
• No ego anywhere in the team, including higher management.
• Remote, asynchronous, flexible work timings.
• Collaboration and team-thinking. No single person owns the failure.
• Ample time and attention to help developers level up.

Work Ex - 3+ years

Omnivio focuses on Supply Chain Management, Logistics, Cloud Infrastructure, Logistics Software, and Logistics / Transportation / Shipping. Their company has offices in Noida. They have a small team that's between 11-50 employees. To date, Omnivio has raised $400k of funding; their latest round was closed on July 2022 at a valuation of $5M.

You can view their website at https://omnivio.io or find them on LinkedIn.",,True,False,True,False,False,False,False,False,False,False,False,False,True,True,True,False
Mercedes-Benz Research and Development India Private Limited,Big Data Engineer,"AufgabenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team playerQualifikationenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team player",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Confidential,Data Engineer - SQL/Data Pipeline,"Job Description : : - Hands on working knowledge on building and optimizing 'big data' data pipelines, architectures, and data sets- Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases- Strong know-how on data ingestion tools and APIs to prioritize data sources, validate them, and dispatch data to ensure an effective ingestion process. Knowledge on data ingestion tools such as Apache Kafka/ Apache Storm/Apache Flume/Apache Sqoop/Wavefront, and more.- Working knowledge on data mining tools such as Apache Mahout/KNIME/Rapid Miner/Weka,- Strong know-how on ETL tools such as Talend/Informatica PowerCenter/AWS Glue/Stitch,- Ability to handle various types of data in the form of text, speech, image, video, or live stream from IoT/ Sensors/ Web- Ability to manipulate, process and extract value from large, disconnected datasets and articulate the same in business contextPreferred : - Skilled in the use of business intelligence and visualization tools, such as PowerBI, Tableau - Experience with stream-processing systems such as Storm, Spark-Streaming, etc.- Ability to leverage MLOps Platforms such as Teraform, Ansible, Kubeflow, Google AI Platform- Know-how on software development languages such as Python, Java, C++, Scala, etc (ref:hirist.com) IT",,True,False,True,True,False,True,False,False,False,True,False,False,False,False,False,False
NIRA,Sr Data Engineer/Architect (3y-7y),"About the job

Starting with credit, NIRA aspires to be the pre-eminent financial brand for the mass market or ""Middle India"". We already have customers in over 5,000 towns and cities, and we're growing quickly (15% MoM for the last 20 months!). It's a very exciting time to join us. We have over 200+ employees.

Currently, only 10% of Indians can use banks when they need credit: banks typically require a high credit score or collateral, something most people don't have. It need not be this way. Using a combination of traditional data and the vast amount of digital data available, it is now possible to score the unscored.

Today, we receive 15000 new loan applications daily from across 4000 cities in India, and we are growing 15-20% MoM. People reach us at their time of need, and we offer them credit via our app. Money reaches their bank account within 24 hrs of application.

We are addressing head-on a big challenge. It's also a great opportunity from both a commercial and societal impact perspective. We can improve lives for millions. It is no exaggeration to say that our addressable market will be 400mm within 5 years. It's pretty exciting, we think.

If our mission resonates with you, and you are a talented and hardworking individual that wants to commit yourself to an incredible challenge, then we want to hear from you.

As one of the senior data engineers on the team, you’ll be working on our core data platform and infrastructure powering business decisions and data science workloads.

Job Overview

We are looking for an experienced Data Engineer to join our engineering team. The hire will be responsible for building our data and data pipeline architecture. You will optimise our data flow starting with the collection of data for cross functional teams and purposes. You will support our key data science initiatives while maintaining consistency of data delivery architecture throughout ongoing projects. Ideal candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities

• Create and maintain optimal data pipeline architecture

• Assemble large, complex data sets that meet performance and business requirements

• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies

• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, credit risk, operational efficiency and other business KPIs.

• Create data tools for analytics and data scientist team members that assist them in building and scaling our core products

• Work with cross domain data and analytics experts to strive for stronger data driven outcomes for the business.

Qualification

• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science. They should also have experience using the following software/tools:

• Experience with big data tools: Hadoop, Spark / PySpark, Kafka.

• Experience with relational SQL and NoSQL databases.

• Experience with object-oriented/object function scripting languages: Python.

• Expertise in data modelling and buiding data driven systems.

• Well versed with Shell Scripting.

• Hands on experience on various AWS services like EMR,EC2,Glue.

• Deploy existing data projects using CICD pieplines.

• Knowledge on Docker is a plus.

What we offer:

• Competitive salary

• Medical Insurance

You can learn more about NIRA here:

Press:

https://yourstory.com/2019/05/startup-fintech-nira-entrepreneur-loans/

https://www.livemint.com/companies/news/muthoot-finance-partners-with-nira-to-provide-personal-loans-11620309871295.html

https://www.financialexpress.com/money/personal-loan-collection-rates-return-to-pre-covid-levels-data-from-nira-reveals/2313170/

NIRA focuses on Consumer Lending and Fin Tech. Their company has offices in Bengaluru. They have a large team that's between 201-500 employees. To date, NIRA has raised $3.1M of funding; their latest round was closed on April 2020.

You can view their website at https://www.nirafinance.com or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Referrals Only,Consultant-Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.
Job responsibilities• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will collaborate with Data Scientists in order to design scalable implementations of their models
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Assure effective collaboration between Thoughtworks' and the client's teams, encouraging open communication and advocating for shared outcomes
Job qualificationsTechnical skills• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Professional skills• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more
Other things to knowL&DThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.
About ThoughtworksThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 28+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.

Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
ANI Calls India Private Limited,Senior Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Bengaluru, India
Candidate should have:
Worked collaboratively with cross-functional teams and stakeholders to achieve an organizational goal.
Worked in an agile environment and are comfortable running an agile process for the data and analytics team.
Strong experience in data pipelines, ETL design (both implementation and maintenance), data warehousing, and data modeling (preferably in dbt).
Implementation and tuning experience in the Big Data Ecosystem,
(such as Data Analytics (Dataproc, Airflow, Hadoop, Spark, Hive),
Google Cloud Platform AI and ML Services and Data Warehousing (such as BigQuery, schema design,
query tuning and optimization) and data migration and integration.
End to end hands-on to carry out complex POC, Pilot, Limited production rollout, assignments requiring the development of new or improved techniques and procedures.
Participated in deep architectural discussions to build confidence and ensure customer, success when building new, or migrating existing, applications, software, and services on the Google Cloud Platform.
advanced skills in SQL, data modeling, ETL/ELT development, and data warehousing.
Strong skills in Optimization - performance, pipeline, spark.
Experience on Pyspark.
5+ years of design & implementation experience with distributed applications.
5+ years of experience architecting/operating solutions built on Google Cloud Platform.
. Bachelor's degree.

Experience: 5.00-10.00 Years",,False,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
deloitte,Consulting- SAMA- A&C-Azure Data Engineer- AD,"JD:
• Location - Mumbai OR Pune
• Experience range:
• 12-15 yrs for AD
• Strong experience in Python programming and related skills like PySpark
• Strong SQL skills
• Strong experience with any of the data engineering platforms like Hadoop, Spark, Synapse, Databricks, Apache Airflow, etc.
• Preferred: Knowledge of any cloud platform like Azure/AWS/Google
• For AD level: Need technology leadership experience in terms of architecture/solutioning and team leading",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Comcast,"Data Engineer, Data Products Engineering","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary INTRODUCTION: At Comcast, we believe in the talent of our people. It's our passion and commitment to excellence that drives Comcast's vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It's what makes us uniquely Comcast. Here you can create the extraordinary. Join us. ABOUT THE ROLE: Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered Job Description Responsibilities Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc) 2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas. Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications. Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data. Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time. REQUIREMENTS: Bachelor's degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience. Strong Computer Science/Engineering/Information Systems background 3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies. Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas Strong problem-solving skills. Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences. PREFERRED QUALIFICATIONS: 1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills Ability to design concise and visually appealing reports, user interfaces, mockups and documentation Ability to read external API documentation and write pipelines to extract data from our partners systems Ability to write and stand up internal API endpoints to share data with other internal teams. Strong analytical focus, results-oriented and execution driven. Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures. Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed. Flexibility to adjust to changing requirements, schedules and priorities. Ability to work independently under minimum supervision and proactive in solving issues Energetic, committed and solution focused with the ability to perform under pressure and meeting targets Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Relevant Work Experience 2-5 Years Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,False,True
Versor Investments,Data Engineer,"India

Versor Investments (“Versor”) is a quantitative investment boutique headquartered in Midtown Manhattan. The Firm currently has an AUM of $1.8 billion*. Versor creates diversified sources of absolute returns across multiple asset classes. Within a scientific, hypothesis-driven framework, Versor leverages modern statistical methods and vast datasets to drive every step of the investment process. Alpha forecast models, portfolio construction, and the trading process rely on the ingenuity and mathematical expertise of 60+ investment professionals. Versor offers two categories of investment products – Hedge Funds and Alternative Risk Premia. Both are designed to provide superior risk-adjusted returns while exhibiting low correlation to traditional and alternative asset classes. Each invests in liquid, scalable markets. On average, Versor’s partners have spent over 20 years researching, investing and trading systematic alternative investment strategies.

Role Summary

The Data Engineer position will be based in Mumbai and be part of the Portfolio Analytics team. They will collaborate closely with senior researchers to design and develop a large-scale data lake. We are seeking candidates who have excelled in engineering (specifically computer science). Prior experience in investments and finance is beneficial but not mandatory.

This role is ideal for candidates who are passionate about technology and excited about building a data platform.

Responsibilities
• Design architecture for a data platform.
• Design data pipelines based on business and functional requirements.
• Extract, transform, and load logic to automate data collection and manage data processes/pipelines. This includes data quality and monitoring.
• Develop data access tools to allow researchers to access data seamlessly.
• Develop integration tools and analytical reports for the databases and data warehouse.
• Write and review technical documents. This includes requirements and design documents for existing and future data systems, as well as data standards and policies.
• Collaborate with analysts, support/system engineers, and business stakeholders to ensure data infrastructure meets constantly evolving requirements.

Requirements
• E., B.Tech., M.Tech., or M.Sc. in Computer Science, Computer Engineering or similar discipline from a top tier institute.
• 2+ years direct experience working as a data engineer.
• Experience in design, architecture and implementation of data lake, data pipelines and flows.
• Experience with developing software code and APIs in one or more languages such as Python and C#.
• Experience designing and deploying large scale distributed data processing systems with one or more technologies such as MS SQL Server, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, Hive, Teradata, or MicroStrategy.
• A high-level understanding of automation in a cloud environment (AWS experience preferred).
• Excellent communication, presentation, and problem-solving skills.
• Data as of December 31, 2022. AUM reflects regulatory AUM as per SEC definition for the purposes of Item 5.F on the Form ADV Part 1a.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tredence Inc.,Senior Data Engineer,"Associate Manager – Data Engineering (8-11 Years)

This position requires someone with good problem solving, business understanding and client presence.

Overall professional experience of the candidate should be above 8 years. A minimum of 4 years of experience in Data Engineering space. Should have good understanding of business operations, challenges faced, and business technology used across business functions.

The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.

Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.

Role Description:
• Engages with Leadership of Tredence' s clients to identify critical business problems, define the need for data engineering solutions and build strategy and roadmap
• S/he possesses a wide exposure to complete lifecycle of data starting from creation to consumption
• S/he has in the past built repeatable tools / data-models to solve specific business problems
• S/he should have hand-on experience of having worked on projects (either as a consultant or with in a company) that needed them to –
• Provide consultation to senior client personnel
• Implement and enhance data warehouses or data lakes.
• Worked with business teams or was a part of the team that implemented process re-engineering driven by data analytics / insights
• Should have deep appreciation of how data can be used in decision making
• Should have perspective on newer ways of solving business problems. E.g. external data, innovative techniques, newer technology
• S/he must have a solution creation mindset. Ability to design and enhance scalable data platforms to address the business need
• Working experience on data engineering tool for one or more cloud platforms -Snowflake, AWS/Azure/GCP
• Engage with technology teams from Tredence and Clients to create last mile connectivity of the solutions -
• Should have experience of working with technology teams
• Demonstrated ability in thought leadership – Articles/White Papers/Interviews

Mandatory Skills

Program Management, Data Warehouse, Data Lake, Analytics, Cloud Platform

Job Location - Bangalore , Chennai , Pune , Gurugram.

Experience Level - 8-11 yrs.

Expected Joining Time - Immediate to Max 30 days.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
Mercedes-Benz Research and Development India Private Limited,Big Data CoE - Engineering.IT Data Engineer,"TasksOverview
A highly motivated and technically proficient professional, capable of delivering solution on Data Engineering in Cloud platform. He or she must be skilled in developing all rituals for Data Engineering especially using pyspark.
Job Responsibilities
Data Engineer:

· Development, Enhancement, testing and release of existing application.
· Develop and enhance end to end data pipeline.
· Interpret data to analyze results to a specific business problem or bottleneck that needs to be solved using statistical techniques.
· Ensure data efficiency and reliability.
Qualification
Mandatory:
· Bachelor's /post graduate degree in Computer Science, Computer Engineering or Data Science, Data Engineering with strong knowledge in below technologies,
· Pyspark
· Databricks,
· ADLS
· SQL
Desired:
· Work experience on Agile/SDLC process.
· Experience in building Knowledge Base (KB)
· Experience in taking over Knowledge Transfer
· Open to work/explore new technology areas.
· Automotive and Aviation domain is plus.Qualifications",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
D2C Ecommerce India Pvt Ltd,D2C Ecommerce - Data Engineer,"What is D2cecommerce :D2C Ecommerce is India's first multi-D2C brand online platform that sells its own homegrown brands across multiple home and lifestyle categories, including - apparel, cosmetics, beauty, jewelry, accessories, fitness, sports, shoes, bags, books, kitchen, food, auto accessories, electronics, kids and travel packages. Along with selling these products on its own portal, D2CEcommerce also has these items listed on leading e-commerce sites.Job Summary : We are seeking a highly motivated and skilled Data Engineer to join our team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our databases and reports. You will work closely with our team of developers, data scientists, and analysts to ensure that our data systems are accurate, efficient, and scalable.What would be your responsibilities :- Design and develop databases that are scalable, efficient, and accurate- Develop and maintain ETL pipelines to move data from various sources into our databases- Create and manage database reports to provide insights to our team of data scientists and analysts- Collaborate with our team of developers to integrate our databases into our applications and services- Continuously monitor and optimize our databases for performance and security- Ensure that our data systems adhere to industry best practices and compliance regulations- Stay up-to-date with the latest database technologies and trendsWhat are we looking for in the candidate :- Ability to build things from scratch.- Self-starter and motivated individuals who can drive processes on their own- A bachelor's or associate degree in management information systems, computer science, or a related field- 0-1 years of experience in database management or a similar role- 0-1 years of experience designing, developing, and producing database reports- Proficiency in SQL and experience with relational databases such as MySQL, Postgre SQL, or Oracle- Experience with ETL tools and techniques- Understanding of data modelling concepts- Familiarity with database administration and management tools- Strong analytical and problem-solving skills- Excellent verbal and written communication skillsWhy you should join D2cecommerce :In addition to an attractive compensation package, you own a piece of the company through ESOPs. Also you will have the opportunity to take up a role in a rapidly growing, highly disruptive organization, and shape the face of ecommerce for the future.Interested? What to do next :If reading the details above excited you and made you feel you can help us take D2Cecommerece to the next level, all you have to do is let us know!",Gurugram,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Thoughtworks Inc.,Senior Consultant - Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Aryng,Sr. Data Engineer,"Welcome You made it to the job description page

Aryng is looking for a cloud data engineer with experience in developing
enterprise-class distributed data engineering solutions on the cloud. We are seeking
an entrepreneurial and technology-proficient Data Engineer who is an expert in the
implementation of a large-scale, highly efficient data platform, batch, and real-time
pipelines and tools for Aryng clients. This role is based out of India. You will work
closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength
is creative and effective problem-solving.

Key Responsibilities:

● Should have implemented asynchronous data ingestion, high volume stream data
processing, and real-time data analytics using various Data Engineering
Techniques.
● Implement application components using Cloud technologies
and infrastructure.
● Assist in defining the data pipelines and able to identify bottlenecks to enable
the adoption of data management methodologies.
● Implementing cutting edge cloud platform solutions using the latest tools and
platforms offered by GCP, AWS, and Azure.

Requirements
• Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
5+ years of data engineering experience is a must.
• 2+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers
• 2+ years' experience in Python.
• Must be strong in SQL and its concepts.
• Experience in Big Query, Snowflake, Redshift, DBT.
• Strong understanding of data warehousing, data lake, and cloud concepts.
• Excellent communication and presentation skills
• Excellent problem-solving skills, highly proactive and self-driven
• Consulting background is a big plus.
• Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study
Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
This role requires mandatory overlap hours with clients in the US from 8 am - 1
pm PST.

Benefits
• Direct Client Access
• Flexible work hours
• Rapidly Growing Company
• Awesome work culture
• Learn From Experts
• Work-life Balance
• Competitive Salary
• Executive Presence
• End to End Problem Solving
• 50%+ Tax Benefit
• 100% Remote company
• Flat Hierarchy
• Opportunity to become a thought leader

Why Join Aryng: Click on the Youtube link",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,True,True
Emerson,Data Engineer - Sustainability,"AS AN Data Engineer, YOU WILL:

· Architect and design platform solutions to meet and exceed expectations of Projects.

· Proactively evolve and apply DevSecOps methodologies, standards and leading practices

· Apply architectural standards/principles, security standards, usability design standards, as approprioate.

· Lead a project from delivery perspective, including giving periodic updates to all stakeholders.

Skills Requirements:

· Must be able to communicate fluently in English, both written and verbal

· Excellent interpersonal communication and organizational skills

· Able to distil complex technical challenges to actionable and explainable decisions

· Inspire DevSecOps teams by building consensus and mediating compromises when necessary

· Demonstrate excellent technical & architecture skills, service management and product lifecycle management

· Demonstrate ability to rapidly learn new and emerging technologies

· Operational abilities including early life support and driving root cause analysis and remediation

·Any Azure/Microsoft Big Data related certifications is highly preferred.

REQUIRED EXPERIENCE :

· Bachelor’s Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline)

· 10+ years overall IT industry experience

· 3+ years in a solution design role using service and hosting solutions such as private/public cloud IaaS, PaaS and SaaS platforms.

· Large scale design, implementation and operations of OLTP, OLAP, DW and NoSQL data storage technologies such as SQL Server, Azure SQL, Azure SQL DW, PostgreSQL, CosmosDB, RedisCache, Azure Data Lake Store, Hadoop, Hive, MongoDB, MySQL, Neo4j, Cassandra, HBase

· Creation of descriptive, predictive and prescriptive analytics solutions using Azure Stream Analytics, Azure Analysis Services, Data Lake Analytics, HDInsight, HDP, Spark, Databricks, MapReduce, Pig, Hive, Tez, SSAS, Watson Analytics, SPSS

· Design and configuration of data movement, streaming and transformation (ETL) technologies such as Azure Data Factory, HDF, Nifi, Kafka, Storm, Sqoop, SSIS, LogicApps, Signiant, Aspera, MoveIT, Alteryx, Pentaho, IDQ,

· Enablement of data reuse through Data Catalog/Marketplace, Metadata, Search and Governance technologies such as including Azure Data Catalog, Waterline, Apache Atlas, Apache Solr, Azure Search, Alteryx Connect, Datawatch Monarch Swarm, Collibra Catalog, Enigma Councourse, Adaptive, Cambridge Semantics, Data Advantage Group (DAG), Global IDs, Alation

· Experience with any of the following: Azure, O365, Azure Stack, Azure AD

· Delivery using modern methodologies especially SAFe Agile.

Requisition ID : 23000590

Emerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.",Pune,False,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Confidential,Fractal.ai - Azure Data Engineer - SQL/PySpark,"Mandatory Skills :- Azure Databricks (ADB)- Azure DataFactory (ADF)- Python or Pyspark- SQLResponsibilities :- Be an integral part of large scale client business development and delivery engagements- Develop the software and systems needed for end-to-end execution on large projects- Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions- Build the knowledge base required to deliver increasingly complex technology projectQualifications & Experience :- A bachelor's degree in Computer Science or related field with 3-12 years of technology experience- Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space- Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks- Database programming using any flavours of SQL- Expertise in relational and dimensional modelling, including big data technologies Exposure across all the SDLC process, including testing and deployment- Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.- Good knowledge of Python and Spark are required- Good understanding of how to enable analytics using cloud technology and ML Ops- Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus- Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)- Characteristics of a forward thinker and self-starter - Ability to work with a global team of consulting professionals across multiple projects- Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems- Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to success. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LatentView,Principal Data Engineer,"About LatentView:
• LatentView Analytics is a leading global analytics and decision sciences provider, delivering solutions that help companies drive digital transformation and use data to gain a competitive advantage. With analytics solutions that provide 360-degree view of the digital consumer, fuel machine learning capabilities and support artificial intelligence initiatives., LatentView Analytics enables leading global brands to predict new revenue streams, anticipate product trends and popularity, improve customer retention rates, optimize investment decisions and turn unstructured data into a valuable business asset.
• We specialize in Predictive Modelling, Marketing Analytics, Big Data Analytics, Advanced Analytics, Web Analytics, Data Science, Data Engineering, Artificial Intelligence and Machine Learning Applications.
• LatentView Analytics is a trusted partner to enterprises worldwide, including more than two dozen Fortune 500 companies in the retail, CPG, financial, technology and healthcare sectors.

Job Description:
As a Manager - data engineer, they should build and maintain scalable, rock solid self-serve data pipelines for data analysts and data Scientists and support them by understanding the content and context of data and collaborating with them to figure out the best way to Extract, Transform, Load, and access it.
• Be an SME in DE and should know how to set up the process, requirement gathering for any movement or data ingestion and data platform creation
• Has to be the end-to-end project manager, allocating work, monitoring progress, providing feedback, and taking necessary steps to ensure agreed timelines are met
• Scripting skills: SQL and Python or PySpark
• Excellent understanding of Data Warehouse and its architecture
• Excellent understanding of Issue Tracking System like JIRA/ Dev-Ops
• Excellent experience in Version control like Github or Gitlab
• Ideal to have knowledge of other DE platforms like Azure databricks, Snowflake etc.

Education: UGEmployment Type: CONTRACTOR",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
CIEL HR Services,Data engineer,CTC-6LPA EXP-Freshers Area of Expertise - A) Data Hygiene B) Application of ML DL Tools If interested please call 9000338173 or forward cv to [Confidential Information],Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer - ETL,"Design, build, and maintain the data infrastructure that supports our data-
driven applications and services.• Develop and maintain ETL (Extract, Transform, Load) processes to ensure
data accuracy and completeness.
• Optimize data pipeline performance and scalability.
• Collaborate with data scientists, analysts, and developers to ensure that our
data pipeline meets their needs.• Implement data governance policies and procedures to ensure data quality
and consistency.
• Design and develop data models that support data-driven applications and
services.
• Troubleshoot and resolve issues related to the data pipeline.
• Participate in the evaluation and selection of data management and analytics
tools and technologies.
• Keep up to date with emerging trends and technologies in data engineering
and big data.

experience

8",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Swift,Data Engineer,"About us:

Swift is building a next generation checkout stack for India - a platform rolling up payments and logistics solution for all fulfillment needs. We give businesses the opportunity to provide a customer experience at par with the likes of Amazon and Flipkart, all the while saving money and time.

Its basically Amazon without the website listing - we let our sellers design their own sales channel :-)

We believe there are many things a seller or small business has to worry about when selling online, logistics/payments/etc shouldn't be one of them. With our solution, SMBs and D2C brands get access to technologies and services like next day delivery, same day delivery, live package tracking, Card/Cash on delivery, scheduled delivery etc, making parcel delivery just as simple as collecting payment.

We also provide robust APIs which makes it easy for developers to add shipping capabilities to their multichannel online store.

We want to be the #1 checkout platform that’s reliable, easy to use and affordable.

About you:

You have experience in working with data pipelines and ETL sets (programmatically and using tools) – say MongoDB, Spark streaming, Python/Java, Apache Beam, PARQR, Delta Lake, Airflow, etc. You are looking for challenges in growing a data backed company to deal with from hundreds to millions of visitors data points per month.

You like working with streaming/reactive architectures and have experience/interest in setting up data pipelines on cloud infra from scratch. You generally prefer to use a minimal set of simple tools to a diverse range of complex ones. We are looking to build a back-end cloud infrastructure (Google Cloud Platform preferably) which will be a fault-tolerant real-time stream processing system on the cloud - Our system will need to meet liveliness guarantees from a big data/ETL perspective.

You like to work on a variety of projects - at this job, you’ll be developing a complex ETL infra, a reactive streaming architecture and a cloud-native, highly available API for our customers.

You are someone who is:
• Experienced in any JVM based language or Python.
• Have worked on NoSQL (MongoDB)/ SQL databases.
• Have worked on creating data pipelines (both programmatically, say using spark streaming) or using tooling like Airflow, dataflow)
• Strong verbal and written communication skills and the ability to work well cross-functionally.
• We offer: *
• You to be a part of a small, but a super capable team.
• The opportunity to work closely with founders to define, scope, estimate and plan various aspects of the product.
• Being one of the first hires at Swift, you will be involved in both high and low-level decision making. This means a lot of ownership, which we cultivate by having a flat structure.

Swift focuses on E-Commerce, B2B, Small and Medium Businesses, Logistics, and D2C. Their company has offices in Bengaluru. They have a mid-size team that's between 51-200 employees. To date, Swift has raised $2.34M of funding; their latest round was closed on July 2021.

You can view their website at https://goswift.in or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
CarbyneTech India Pvt Ltd,CarbyneTech - Azure Data Engineer - IoT,"- Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions - Azure Synapse/Azure SQL DWH, Azure Data Lake, Azure Blob Storage, Spark, HDInsights, Databricks, CosmosDB, EventHub/IOTHub.- Experience in migrating on-premise data warehouses to data platforms on AZURE cloud.- Designing and implementing data engineering, ingestion, and transformation functions- Azure Synapse or Azure SQL data warehouse- Spark on Azure is available in HD insights and data bricks- Good customer communication.- Good Analytical skill",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Brillio,GCP Data Engineer - R01523647,"About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

GCP Data Engineer
Primary Skills

• BigQuery, Cloud Logging, Cloud Storage, Cloud Trace, Composer, Data Catalog, Data Modelling Fundamentals, Data Warehousing, Dataflow, Datafusion, Dataproc, ETL Fundamentals, Modern Data Platform Fundamentals, PLSQL, T-SQL, Stored Procedures, Python, SQL, SQL (Basic + Advanced)

Specialization

• GCP Data Engineering Basic: Senior Data Engineer

Job requirements

• Job description below. • 6 years of experience in software design and development • 5 years of experience in the data engineering field is preferred • 3 years of Hands-on experience in GCP cloud data implementation suite such as Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage, • Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms. • Mandatory 1 year of software development skills using Python • Extensive hands-on experience working with data using SQL and Python • Cloud Functions. Comparable skills in AWS and other cloud Big Data Engineering space is considered. • Experience in DevOps(CI/CD) pipeline facilitating automated deployment and testing • Experience with agile development methodologies • Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions • Bachelor's Degree in Computer Science, Information Technology, or closely related discipline

Know what it’s like to work and grow at Brillio: Click here",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,True,False
Latent View Analytics Private Limited,Data Engineer,"Job Title :
Data Engineer Experience : 2.5-5 Location : INDIA
• Chennai Job Description: Experience working with Cloud Data Platforms, especially AWS and its services, must be strongly experienced in building data pipelines.
Experience with big data tools like Python, Pyspark, and Spark SQL. Focus on scalability, performance, service robustness, and cost trade-offs.

A continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs.
Attention to detail, coupled with the ability to think abstractly. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product. Keen to learn new technologies and apply the knowledge in production systems.

AWS skills:
AWS Glue AWS EMR (any two AWS services)

Data Engineer Skills:
Python Pyspark Spark SQL HIVE HQL Scala Good to have: Snowflake querying Databricks AWS API Gateway AWS Lambda",Chennai,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,True
Whiteforce,Data Engineer - Java,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20246

LocationIndia

Job Descriptions

Job Purpose and Primary Objectives : Develop and Deploy data and analytics-led solutions on GCP Key responsibilities (please specify if the position is an individual one or part of a team): Data engineering solution on GCP using Cloud Bigquery, Cloud Dataflow, Pu-Sub, Cloud BigTable and AI/Ml solutions Key Skills/Knowledge : - Good Experience in GCP. - Python/Java, PySpark/Spark Java. - GCP BigQuery. - GCP Pub-Sub. - Secondary Skill - DataFlow, Compute Engine, Cloud Fusion. (ref:hirist.com)

Skills",,True,False,False,True,False,False,False,False,False,False,False,False,False,True,False,False
Confidential,Big Data Engineer - Python/AWS,"JOB_DESCRIPTION :- Has experience in the following #Python, #AWS_Athena, #Glue #Pyspark, #EMR, #DynamoDB, #Redshift, #Kinesis, #Lambda, #Snowflake.- Proficient in #AWS_Redshift, #S3, #Glue, #Athena, #DynamoDB.- Design, build and operationalize large-scale enterprise data solutions and applications using one or more of #AWS_data and analytics services in combination with 3rd parties - #PySpark, #EMR, #DynamoDB, #RedShift, #Kinesis, #Lambda, #Glue, #Snowflake. - Analyze, re-architect, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services. - Design and build production data pipelines from ingestion to consumption within a big data architecture, using Python/PySpark. - Design and implement data engineering, ingestion, crawling, manipulation and curation functions on AWS cloud using AWS native or custom programming. - Must have strong knowledge of databases like Postgres, MySQL, MongoDB, Cassandra, etc. - Must have experience in using AWS SDKs and libraries for interacting with different AWS services. - Experience in building REST APIs. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Varite India Private Limited,Data Engineer,"snowflake- Data Engineer Data engineering, integration, and data modeling experience . Can write scalable/performant pipelines, queries, and summaries of data . Has worked with various data systems and tools . Understands analytics and data science workflows and common use cases that leverage their work . Python . SQL . Datawarehouse experience . AWS experience . Data QA / validation skills (to check their work) . Snowflake experience (MUST) . Matillion, DBT, or other Data tech experience (ideal) . Marketing technology experience (ideal)",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
deloitte,Consulting-SAMA- A&C-Data Engineer/Architect-Associate Director,"Location: No Preference

Years of Exp: 10-15 Years

Hybrid: Yes

Mandatory Skill: MS Azure, Analytics, Delivery Management & Operations, People Management

Responsibilities:
• 10 - 15 years of deep delivery experience in cloud data engagements, with proven experience to lead teams sizes of 10+ resources
• Experience with platforms such as Azure Cloud Services, Solution Design & Review, Data Management, Data Visualization Tools
• Deep experience handling large volumes of data across multiple data sources like csv, relational data, SAP, json, parquet, flat files, streaming data, etc.
• Strong conceptual understanding of data warehousing, data modeling principles
• Strong Experience with visualization / reporting solutions
• Good understanding of data quality, data management and data governance principles
• Depth in data transformation / data modernization / ETL and ELT based data pipeline development
• Hands on with Agile/Scrum Methodology based implementations and end to end software delivery lifecycle
• Exposure working with international clients / geographies
• Excellent client handling, team handling and interpersonal skills
• Excellent written and spoken communication skills

Ability to understand and appreciate business / domain context and develop data and analytics solutions",Hyderabad,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Wipro Technologies,Data Engineer,"Share resume to akshara.raju@wipro.com

Location - Bangalore , Chennai , Pune , Hyd

JD :
• Azure data factory
• Azure data bricks with PySpark coding experience
• Experience in Snowflake
• Good to have knowledge in data visualization tool Tableau or PowerBI
• Good to have knowledge in data warehousing & data analytics

Data Analyst / Data Engineer
• 5 yrs. experience in working with large, complex data sets
• Create reports for internal teams and/or external clients
• Hands on Experience on Azure Data Factory and Azure
• Need to be able to code the data pipelines from ADF standpoint / Data Ingestion.
• Collaborate with team members to collect and analyze data
• Knowledge of Snowflake will be a big plus
• Need to ensure they can work on Data Mapping / Data Enrichment and Data Transformations.
• Use graphs and other methods to visualize data
• Establish KPIs to measure the effectiveness of business decisions
• Structure large data sets to find usable information
• Reporting and Data Visualization skills
• Experience in Data Mapping, data cleansing.",Bengaluru,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
ANI Calls India Private Limited,Lead Snowflake Data Engineer,"Anicalls

Industry : IT

Total Positions : 3

Job Type : Full Time / Permanent

Gender : No Preference

Salary : 900000 INR - 1800000 INR (Annually)

Education : Bachelor s degree

Experience : 5-10 Years

Location : Noida, India

Candidate should have :

Knowledge and experience in Big data and Data Vault methodology

Experience in power shell, shell scripting, and python

Exposure to data modeling for Snowflake

Experience in working with agile / scrum methodologies.

Experience in building data pipelines for large volumes of data across disparate data sources

Experience in DBT for Snowflake

Good experience on Azure Databricks

experience in Confluent cloud platform

Experience in building pipelines through Confluent Kafka and Knowledge of Azure Kubernetes Service

Good communication and presentation skills

Expertise in building data pipelines for Snowflake using Snowpipe, Snowpark, SnowSQL's and stored procedures.

Azure experience must be focused on Azure Data Factory, Azure storage solutions (such as Blob and Azure Data lake Gen2), and Azure data pipelines

Good experience on Snowflake and Snowflake architecture

7+ years of total experience in data projects with a focus on data integration and ingestion

3+ years of experience working primarily on Snowflake",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Impetus Technologies India Pvt. Ltd,GCP Data Engineer,"Role : GCP Data Engineer

Job Description :

- The candidate should have extensive production experience in GCP, Other cloud experience would be a strong bonus.

- Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.

- Exposure to enterprise application development is a must.

Roles & Responsibilities :

- 6-10 years of IT experience range is preferred.

- Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.

- Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOps.

- Good hands on expertise on either Python or Java programming.

- Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.

- Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.

- Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.

- Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.

- Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.

- Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.

- Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.

,",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Stantec Technology International,Senior Data Engineer,"Description
Grow with the best. Join a smart, creative, and inspired team that works behind the scenes to support operational excellence. As part of the Innovation Office, the Digital Technology & Innovation team is composed of digital experts who conduct research and development to keep our teams and our client's projects ahead of the technological curve. They implement established technologies and find emerging solutions for all business lines (Buildings, Energy & Resources, Environmental Services, Infrastructure, and Water), bridging existing knowledge domains and facilitating the integration of powerful tools and methods. The team's goal is to make projects more efficient and help provide higher-quality results to our clients. The ideal candidate will be a self-starter, a critical thinker, and highly interested in the application of new technologies and methods. The candidate will become a member of the Innovation Office, however, he or she will also be accessible to Stantec's project teams to support project work as needed.

Your Opportunity
The Innovation Office's Digital Technology & Innovation (DTI) team has an opportunity for a Senior Data Engineer. This position requires a person who is technically savvy, experienced in data engineering, and enjoys working with data to solve business problems, shaping & creating solutions, and helping to champion implementation. As a member of the Innovation Office, the Digital Technology Development group, as part of the DTI team, also engages in research & development and provides guidance and oversight as a center of excellence for the business. This group also engages in new product research and testing and the incubation of new ideas. The candidate will be responsible for the delivery of professional services and will recommend solutions to achieve complex strategic objectives across our large global team spanning Stantec IT, Business Lines, and the Office of Innovation.

Your Key Responsibilities
Serve at the direction of the Digital Technology Development Leader to:
- Take ownership of the project, work independently in a team environment, and mentor others as needed.
- A passion for solving problems and providing workable solutions, flexible to learn new technologies to meet the business needs.
- Translate complex functional and technical requirements into detailed designs.
- Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining.
- Implement access governance of production data systems to ensure compliance with our privacy and security policies.
- Oversee, design, and develop algorithms for real-time data processing within the business units and to create the frameworks that enable quick and efficient data acquisition.
- Build and maintain best practices to support the Continuous Integration and Delivery (CI/CD) of data engineering solutions.
- Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets (batch and stream data) into a readable and accessible format for end-user facing reports, data science, and ad-hoc analyses.
- Build and maintain reliable and scalable ETL on big data platforms as well as work with varied forms of data infrastructure inclusive of relational databases and NoSQL databases.
- Work collaboratively with DTI's Data & Analytics group, Stantec's internal business units, and clients to define problem statements, collect data and define solution approaches.
- Deliver highly reliable software and data pipelines using Software Engineering best practices like automation, version control, continuous integration/continuous delivery, testing, security, etc.
- Possess excellent time-management skills, a thorough understanding of task assignments and schedules, and efficient use of time and available resources.
- Perform other miscellaneous tasks associated with being a member of the Digital Technology & Innovation team and those typical of a data engineer.",Pune,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
TensorGo Technologies,TensorGo Technologies - Senior Data Engineer - Python,"Skillset : Python, PySpark, Kafka, Airflow, Sql, NoSql, API Integration,Data pipeline, Big Data, AWS/ GCP/ OCI/ AzureRequirements :Understanding our data sets and how to bring them together.Working with our engineering team to support custom solutions offered to the product development.Filling the gap between development, engineering and data ops.Creating, maintaining and documenting scripts to support ongoing custom solutions.Excellent organizational skills, including attention to precise detailsStrong multitasking skills and ability to work in a fast-paced environment3+ years experience with Python to develop scripts.Know your way around RESTFUL APIs.[Able to integrate not necessary to publish]You are familiar with pulling and pushing files from SFTP and AWS S3.Experience with any Cloud solutions including GCP / AWS / OCI / Azure.Familiarity with SQL programming to query and transform data from relational Databases.Familiarity to work with Linux (and Linux work environment).Excellent written and verbal communication skillsExtracting, transforming, and loading data into internal databases and HadoopOptimizing our new and existing data pipelines for speed and reliabilityDeploying product build and product improvementsDocumenting and managing multiple repositories of codeExperience with SQL and NoSQL databases (Casendra, MySQL)Hands-on experience in data pipelining and ETL. (Any of these frameworks/tools: Hadoop, BigQuery, RedShift, Athena)Hands-on experience in AirFlowUnderstanding of best practices, common coding patterns and good practices aroundstoring, partitioning, warehousing and indexing of dataExperience in reading the data from Kafka topic (both live stream and offline)Experience in PySpark and Data framesResponsibilities :You'll :Collaborating across an agile team to continuously design, iterate, and develop big data systems.Extracting, transforming, and loading data into internal databases.Optimizing our new and existing data pipelines for speed and reliability.Deploying new products and product improvements.Documenting and managing multiple repositories of code.",,True,False,True,False,False,False,False,True,False,False,False,False,True,True,True,False
Confidential,Data Engineer (Data Bricks) Manager,"EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL's proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients' decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries. Please visit www.exlservice.com for more information about EXL Analytics. Location - Bangalore/ Gurgaon Note- Currently Remote & expecting candidates to relocate to either Bangalore/Gurgaon once office reopens.Requirements: 7+ years of Data engineering exp with 3+ years hands on Databricks (DB) experience. Should be able to create New Clusters, Cluster Pools and attach existing clusters to pool in DB. Should have some pool management experience. Should be good in Datalakehouse concepts. Should have good experience in Data Engineering in Databricks Batch process, Streaming is good to have. Should have good experience in creating Workflows & scheduling the pipelines. Should have good exposure on how to make packages or libraries available in DB. Should have good experience in Databricks default runtimes, Photon & Light is good to have. Some experience in Databricks SQL / DW in Databricks. Delta Live Tables experience is good to have. IT Services and IT Consulting",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Streamline Digital,Sr. Data Engineer- Azure/ Snowflake/ Databricks,"Sr. Data Engineer

Who We Are

At Streamline, we are experts in Enterprise Mobility, Product Engineering, and IT Transformation. We help organizations navigate the constantly evolving landscape of IT. Our sole focus is ensuring that our client’s organization is armed with the strategies, products and solutions that are transformative to their business. Streamline works closely with our clients, takes pride in developing genuine relationships and embraces open communication and collaboration with our clients. We become a part of our client’s team, working together to achieve short-term goals and enable long-term success. Our team is comprised of world-class strategists, architects, engineers, and developers.

In our new flagship product, iEnterprise, we are taking things to the next level, using our collective experience and customer input to create new enterprise mobility management products that reduce operational costs, prevent issues before they happen, and resolve issues faster than with traditional tools and approaches.
Role Summary

This position is full time remote position. The Data Engineer will work with a team of other software developers to define, design, develop, integrate, and re-engineer the Enterprise Data warehouse. Data Marts, Data Virtualization and Data Visualization components in different environments which meet customers’ analytical and business intelligence requirements, scales easily and supports deployment in highly available environments. The Senior engineer acts as the development and technical lead and individual contributor on complex projects, contributing to strategic vision and technical decisions, participating in vendor analysis and selection projects, introducing process improvements, completing proof-of-concept projects for the introduction of changes to our architecture, and providing oversight on the work of other technical staff.

Role Responsibilities
• Build data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective
• Create and maintain highly scalable data pipelines using Databricks, Azure, AWS, Kafka.
• Design and build ETL/ELT data pipelines to extract and process data from a variety of external/internal data sources.
• Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery
• Build data insights, data analytics, ML models, fraud and anomaly detection using Snowflake
• Build and deploy modern data solutions.
• Define, implement and build jobs to populate data models.
• Relational and NoSQL Database management
• DevOps building CI/CD pipelines

Technical System Expertise: Understands data warehouse development practices and processes including ETL design, Dimensional models, slowly changing dimensions, Data Security components, Data quality methods, how MPP databases operate, and data flows. Aware of current technology benefits. Expected to independently develop full stack ETL solution from source to staging to presentation layers. Understands the building blocks, interactions, dependencies, and tools required to complete BI Data warehousing software and automation work. An Independent study of current technology is expected. Interact with system engineers to define continuous delivery and/or DevOps solutions, data security, and/or necessary requirements for automation.

Technical Engineering Services: Supports BI Data Warehousing and Enterprise Data Solutions projects by analyzing, designing, and developing ETL solutions; conducting tests and inspections; creating BI reports and virtualization components. Create interface jobs/APIs for data sharing with internal and external stakeholders Ensure we use accurate and secure methods to extract data. Analyze Big Data and MPP Databases to discover trends and patterns. Participates in reviews (walkthroughs) of technical specifications and program code with other members of the DevOps team. Expected to supervise associate engineers on occasion.

Innovation: Presents new ideas which improve existing BI Data Warehousing systems/processes/services. Presents new ideas which utilize new frameworks and reusable components to improve existing ETL jobs/processes/services. Express new perspectives based on an independent study of the industry. Review current company processes to highlight questions that may drive process refinement and optimization.

Technical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the engineer level. Develop application support documentation as required by the application support teams for acceptance of systems changes into production.

Technical Leadership: Collaborates with other engineering, development teams and utilizes data engineering/analyst expertise to deliver technical solutions. Continuously learn and mentor on new technologies.
Qualifications & Skills
• Bachelor’s degree in CS, Statistics, Information systems or equivalent experience.
• Strong expertise working with relational databases. Exceptional ability to author and optimize complex SQL queries/workflows Strong analytical skills to work with unstructured data.
• Experience building and optimizing highly scalable data pipelines, architectures, and data sets.
• A successful history of manipulating, processing, and extracting value from large, disconnected datasets and provide valuable insights.
• Proficiency in workflow orchestration (Databricks, Azure data factory).
• Experience working with streaming data solutions such as Kafka, IoT hub and Spark Streaming.
• Working experience building insights, ML models, fraud and anomaly detection using Snowflake/Databricks.
• Experience working with NoSQL databases like MongoDB, Cassandra, Cosmos DB
• Proficiency in Java/Python/Scala, pandas, pySpark, NumPy
• 8+ years of ETL Development experience using Azure Databricks, Azure data factory, SSIS, Talend.
• 8+ years Professional experience in designing and building cost-effective large scale data marts, data warehousing, distributed big data processing MPP Systems (Databricks, Spark, Snowflake)
• 5+ years of Expertise in Logical and Physical Data Model design using various modelling Tools like Erwin 7.3 and Power Designer.
• 5+ years of experience with Azure and AWS cloud stack
• 3+ years BI Visualization experience developing reports using Snowflake, PowerBI, Grafana, Qlik and analytic cubes utilizing SQL Server Analysis Services (SSAS).

Preferred Skills
• Certifications
• DevOps experience
• Bash, Golang scripting experience
• Docker/Kubernetes experience
• CI/CD pipelines

Powered by JazzHR",Hyderabad,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,True
Whiteforce,Data Engineer - ETL/,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20453

LocationIndia

Job Descriptions

Key Responsibilities - ETL pipeline design and implementation - Streaming and batch data processingStorage optimization - Storage optimization - DataOps / MLOps - Preparing, cleaning, structuring data for statistical modeling / machine learning - Deployment, packaging, versioning of Machine Learning models that operate on data Educational Qualification & Experience - BE/BTech in Computer Science or Information Systems - (Preferred) ME/MTech in Computer Science or Information Systems - Minimum 5 years total experience working in the industry - Minimum 3-5 years experience in Data Engineering Knowledge and Skills Required - Experience with Big Data technologies: Presto / Trino / Hive / Hadop - Cloud data storage, query and analytics technologies, esp. on AWS - S3 / Athena / Glue or Elastic Stack (specifically for data/ML workflows) - Message passing / data broker systems: Kafka / RabbitMQ

Skills",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Apple,Cloud Data Engineer,"Summary

The people here at Apple don’t just build products — they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Imagine what you could do here. Are you passionate about handling large & complex data problems, want to make an impact and have the desire to work on groundbreaking big data technologies? Then we are looking for you. At Apple, phenomenal ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Business Intelligence team is looking for passionate, technical savvy, energetic leader who like to think creatively. Someone who is self motivated and ready to lead team of most hardworking engineers building high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse team deals with Petabytes of data catering to a wide variety of real- time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We leverage a diverse technology stacks such as Snowflake, AWS, Teradata, HANA, Vertica, Single Store, Dremio, Hadoop, Kafka, Spark, Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.

Key Qualifications

High expertise in modern cloud data lakes and implementation experience on any of the cloud platforms like AWS/GCP/Azure - preferably AWS.

Good Experience in cloud based data warehouse - Snowflake.

Hands on Experience in developing and building data pipelines on Cloud & Hybrid infrastructure for analytical needs- Preferably having Cloud certifications.

Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.

Database development experience with Relational or MPP/distributed systems such as Teradata/ SingleStore/ Hadoop

Experience working with data at scale (peta bytes) with big data tech stack and sophisticated programming languages is a plus e:g Python, Scala.

Description

As a Cloud Development Engineer you will design, develop and implement modern cloud data warehouse/ datalakes and influence overall data strategy for the organization

Translate sophisticated business requirements into scalable technical solutions meeting data warehousing/analytics design standards

Strong understanding of analytics needs and proactive-ness to build solutions to improve the efficiency along with that help execute leading data practices & standards

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business

Ability to communicate effectively, both written and verbal, with technical and non- technical multi-functional teams

You will engage with many other group’s & internal/external teams to deliver best-in-class products in an exciting constantly evolving environment

Education & Experience

Bachelors or Masters Degree in Computer Science or equivalent in Engineering.

Role Number: 200154652",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
SecureKloud,Data Engineer,"The candidate will work in a cloud development team using Informatics Data Architecture and Data Engineering concepts to deliver foundational data capabilities such as Data Lakes and Master Data Management. The candidate will be part of the DevOps Team and responsible for building data platforms, test cases, deployment documentation, and support documentation in accordance with internal and industry standards. This is a highly technical and hands-on role.

Responsibilities
• Defining database design, data flows, and data integration techniques
• Design and build data solutions ensuring data quality, reliability, and availability
• Create data processing routines for managing enterprise master data throughout the data lifecycle (capture, processing, and consumption)
• Maximize business outcomes using Mobile Device Management via improved data integrity, visibility, and accuracy
• Manage and evolve global data lakes platforms
• Develop data ingestion, data enrichment, data deidentification routines, and RESTful APIs for consumption of data lakes data",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Varite India Private Limited,Data Engineer II (India - Contract),"Description: Location: Gurgaon / Bangalore Contract Duration: 8 months (extension and conversion based on project and performance basis) Shift: Early US (over lapping with India) Please provide 2-3 values or traits that are important to this role: Strong data warehouse and modeling skills You are a self-starter who is highly organized and communicative Deals well with ambiguity Please list 3-4 functional activities the resource should be capable of: Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) What technical skills will successful candidates possess Bachelor's or Master's degree in Computer Science or Engineering with 6+ years of proven experience in related field Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Experience building low-latency data product APIs You value strong pull request reviews, understand when to stand your ground and when to let go You are a self-starter who is highly organized, communicative, quick learner, and team-oriented Successful background as a technical leader driving cross-organizational data initiatives to completion Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Deep expertise in data access patterns, data validation, data modeling, database performance, and cost optimization Hands-on knowledge with BI tools, modern OLAP engines such as Presto, Clickhouse, Druid, Pinot, and data processing frameworks such as Spark and Flink Experience establishing and applying standards for operational excellence, code quality, and software engineering best-practices Effective verbal and written communication skills with the ability to think strategically about technology decisions and manage relationships with stakeholders and senior leadership Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage Experience with BI tools like Tableau, DataDog Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) Experience working with SQL & NoSql databases along with programming experience in Python and/or JAVA Strong business acumen with an understanding of business drivers and of how to drive value by supporting data discoverability across the organization Experience building low-latency data product APIs Experience working in an agile environment Tell us about your team: Clientis seeking a data engineer to join the Business Enablement Technology team. As we challenge the status quo and take the CTO's agenda to the next level, your focus will be on building a data platform to enable analytics and reporting capabilities across Product & Technology. This is an exciting, high-impact, and cross-functional role. This role requires a highly inventive individual with strong technical and analytical skills, execution drive, and self-motivation. Is there any industry specific experience that would separate one candidate from another Experience migrating data/apps from on-prem to cloud, engineering degree, tech industry experience",Gurugram,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
ITC Infotech India Ltd,Azure Data Engineer,"• Azure Data Engineer
• Ability to understand the functional & technical specification to develop the Notebooks
• Perform Data Loads and Transformations
• Schedule the Jobs with the Azure Data Factory and monitoring the jobs
• Strong in write complex SQL queries",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
bp,Senior Data Engineer - dataWorx,"Job Profile Summary
Role Synopsis:
As part of bp “reinvent”, we have created a major new business line called “Innovation & Engineering” (I&E). One key remit of this group is to drive the transformation of the company through its use of digital and data. A major digital sub-team within I&E is Digital Production & Business Services (DP&BS). DP&BS are responsible for all digital and data initiatives and operations across the following areas of the bp business:
• Production & Projects including Health, Safety, Environment & Carbon
• Refining & Operations
• Wells & Subsurface
• Business Services including Finance, Procurement, People & Culture, Performance Management
• Strategy & Sustainability
• “DataWorx” is the name of the data team that is responsible for all data within these areas and we are developing deep data capabilities to transform the access, supply, control and quality to our vast and ever growing data reserves that are measured in Petabytes. The DataWorx team covers many data sub-disciplines, including data science, data analytics, data engineering and data management as well as specialist areas such as geospatial, remote sensing, knowledge management and digital twin. The DataWorx team works with a wide variety of data from structured data to unstructured data & we also work on Real-time streaming data processing along with Batch data processing. Key Responsibilities :
• Architects, designs, implements and maintains reliable and scalable data infrastructure
• Leads the team to write, deploy and maintain software to build, integrate, manage, maintain, and quality-assure data
• Architects, designs, develops, and delivers large-scale data ingestion, data processing, and data transformation projects on the Azure cloud
• Mentors and shares knowledge with the team to provide design reviews, discussions and prototypes
• Leads customer discussions from a technical standpoint to deploy, manage, and audit best practices for cloud products
• Leads the team to follow software & data engineering best practices (e.g. technical design and review, unit testing, monitoring, alerting, source control, code review & documentation)
• Leads the team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline
• Leads the team in following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Actively contributes to improve developer velocity
• Part of a cross-disciplinary team working closely with other data engineers, software engineers, data scientists, data managers and business partners in a Scrum/Agile setup
• responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Work closely with other data engineers, software engineers, data scientists, data managers and business partners

Job Advert
Job Requirements :
Education :
Bachelor or higher degree in computer science, Engineering, Information Systems or other quantitative fields

Experience :
• Years of experience: 8 to 12 years with minimum of 5 to 7 years relevant experience
• Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
• Hands on experience with:
• Databricks and using Spark for data processing (batch and/or real-time)
• Configuring Delta Lake on Azure Databricks
• Languages : Python, Scala, SQL
• Cloud platforms : Azure (ideally) or AWS
• Azure Data Factory
• Azure Data Lake, Azure SQL DB, Synapse, and Cosmos DB
• Data Management Gateway, Azure Storage Options, Stream Analytics and Event Hubs
• Designing data solutions in Azure incl. data distributions and partitions, scalability, disaster recovery and high availability
• Data modeling with relational or data-warehouse systems
• Advanced hand-on experience with different query languages
• Azure Devops (or similar tools) for source control & building CI/CD pipelines
• Understanding Data Structures & Algorithms & their performance
• Experience designing and implementing large-scale distributed systems
• Deep knowledge and hands-on experience in technologies across all data lifecycle stages
• Stakeholder management and ability to lead large organizations through influence

Desirable Criteria :
• Strong stakeholder management
• Continuous learning and improvement mindset
• Boy Scout mindset to leave the system better than you found it

Key Behaviours :
• Empathetic: Cares about our people, our community and our planet
• Curious: Seeks to explore and excel
• Creative: Imagines the extraordinary
• Inclusive: Brings out the best in each other

Entity
Innovation & Engineering

Job Family Group
IT&S Group

Relocation available
Yes - Domestic (In country) only

Travel Required
Yes - up to 10%

Time Type
Full time

Country
India

About BP
INNOVATION & ENGINEERING
Join us in creating, growing, and delivering innovation at pace, enabling us to thrive while transitioning to a net zero ‎world. All without compromising our operational risk management.

Working with us, you can do this by:
• deploying our integrated capability and standards in service of our net zero and ‎safety ambitions
• driving our digital transformation and pioneering new business models
• collaborating to deliver competitive customer-focused energy solutions
• originating, scaling and commercialising innovative ideas, and creating ground-breaking new ‎businesses from them
• protecting us by assuring management of our greatest physical and digital risks

Because together we are:
• Originators, builders, guardians and disruptors
• Engineers, technologists, scientists and entrepreneurs‎
• Empathetic, curious, creative and inclusive

Experience Level
Intermediate",Pune,True,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Domnic Lewis Private Limited,Big Data Engineer,"we are hiring for Big Data Engineer with the experience in spark , python , SQL , AWS glue Big Data Engineer: Spark, Python, SQL, AWS Cloud (Glue, Lambda, Athena) Hyderabad Location A big data engineer is an information technology (IT) professional who is responsible for designing, building, testing and maintaining complex data processing systems that work with large data sets.",Secunderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Invsto,Data Engineer Intern (2024/2025 graduates),"You are
• A self-starter who is fueled by a desire to improve customer experiences in the moments that matter most, approaching your work with a bias toward accountability, decision-making and action
• Inquisitive and creative, with an ability to listen to identify customer needs and dig deeper to understand the reasons behind those needs
• Able to transform conceptual thinking into deliverables that generate excitement, feedback and alignment among stakeholders
• Thrive in a collaborative environment, partnering across the company with business experts, software developers, data engineers, and marketers

You have
• Enthusiasm to take the initiative to tackle problems and work with others to expand on your experience and expertise
• Experience with Python, AWS and databases such as Postgres
• Know-how to build data engineering pipelines using services such as Airflow

You will

Work closely with and learn from a team of engineers to contribute to a build a variety of features and infra.

Must-Have skills
• Python, Database experience
• Django (in lieu of Django, an equivalent tech stack)

Qualification and Experience:
• 0-2 Years software engineering
• Education: BE/BTech or equivalent (in lieu of academics, equivalent software developer experience required)

Benefits
• Fully remote, forever
• Annual retreats

About Us

Invsto is building the future of financial engineering.

We believe in hiring the best and providing complete autonomy to our employees to build stuff that they think would make a difference to the world

How to Apply

Does this role sound like a good fit? Email us at [hello@invsto.com].
• Include the role's title in your subject line.
• Send along links that best showcase the relevant things you've built and done (Github, Behance, Dribbble etc)

Invsto focuses on Hedge Funds and Stock Exchanges. Their company has offices in Bangalore Urban. They have a small team that's between 11-50 employees.

You can view their website at https://invsto.com/ or find them on LinkedIn.",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False
Everyday Health Group,Data Engineer,"Description

Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.

Health eCareers, a property of Everyday Health Group, is looking for a Data Engineer to support our growing business.

Key Responsibilities
• Understand overall data collection strategies and implement them in data tables and connections.
• Program Python-based APIs and web services per implementation schema, such as creating applications to access Facebook, Twitter, Salesforce, and other data sources.
• Use SQL Server, MySQL, HDFS and AWS to design, develop and deploy data processing.
• Analyze and organize raw data from various ETL tools.
• Monitor/Forecast computing resources usage (data lakes, AWS, EC2, etc.).
• Collaborate with ETL developers located at various offices (US and India)
• Implement coding standards, procedures and techniques, concluding writing technical code base.
• Detect, repair, prevent data pipeline failures; identify systematic weaknesses and provide pre-emptive remedies with programming or processes.
• Build and optimize data storage systems.
• Prepare data for prescriptive and predictive modeling.
• Recommending and implement emerging database technologies.
• Create automation for repeating database tasks.

Job Qualifications
• 4+ years of programming experience with an emphasis on data processing.
• Coding skills: Python, SQL.
• Ability to create object-oriented programming and data architectures
• Knowledge of new, leading technology strategy in data engineering and management
• Understanding of industry technologies in scalability, performance, delivery pipeline and maintenance of these tools and systems.
• Hands-on experience with SQL database design, AWS or other cloud systems.
• 2+ year’s experience in Linux environments.
• Experience with SQL Server, MySQL or other popular database management tools.
• Good/Expert knowledge of API services
• Familiarity with Agile process
• Clear documentation requirements and specifications
• Bachelors or Advanced Degree in Information Management, Computer Science or related field.

Desirables:
• Experience in AWS, Tableau
• Experience in developing custom data pipelines in HDFS
• Experience working with social, Double Click, Adobe APIs
• Working with teams across multiple locations
• Skills: Pig; Hive; Spark; Impala; EMR

Our Culture and Values

We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.

Life at Everyday Health

At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work.

#HealtheCareers",,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Unusual Hire,Data Engineer,"Job type: Partial Onsite

Expert Level

Project detail

General:

– Ability to architect Data Science solutions

– Ability to lead a team

– Ability to gather requirements

Must have :

– 6-8 Years Data Science Experience

– At least 4 Data Science solutions

– At least 2 NLP / ML Solutions

– At least 1 solution with Deep learning framework Deep

– Python / PySpark

– Knowledge in Statistical Algorithms like classification, Regression , recommendation and Clustering , Neural Networks

– Azure ML , Databricks , Delta Lake , Data science workspace

Industry Categories

Data Scientist

Languages required

English

₹200 - ₹500

Cost

Location

India

Project ID: 00002514",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Autodesk,Data Engineer,"Job Requisition ID # 22WD66509 Job Description Position Overview In this role, you will be part of a highly energetic team of engineers working on Autodesk Enterprise Integration Platform to assemble, enrich and enhance the data as per the business needs. You would be part of the team working on providing advisory services and support to business in deriving sales strategies. you will be working to build robust and scalable self-service platform for Autodesk while using innovative technologies in big data space. Responsibilities Design and develop components of end-to-end Enterprise Integration Platform Work with the team to make the Enterprise Integration Platform an efficient, robust and scalable platform Enthusiastic and passionate member of a highly skilled and motivated agile development team Contribute to a team culture that values quality, robustness, and scalability while fostering initiatives and innovation Minimum Qualifications BS or MS in computer science or a related field with 5+ years of experience. 3+ years of experience with cloud ETL/ELTs Working experience in Agile methodologies Strong knowledge and experience in AWS technologies Experience in Matillion and Redshift Strong programming skills in either Java, Scala or Python Experience with Hive and one relational DB Strong problem-solving skills Strong communication skills Ability to learn new technical skills Preferred Qualifications Experience with Matillion Experience with Python, Scala Experience with SQL (Redshift) Experience with Kanban methodology At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. Are you an existing contractor or consultant with Autodesk Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact .",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Finxera,Data Engineer,"Company Description

Finxera, Powered by Priority Technology Holdings, Inc. (NASDAQ: PRTH), is headquartered in Alpharetta, Georgia USA. Our India office is located in Chandigarh, where our dynamic team builds state of the art, sophisticated Fintech products & solutions.

We are an emerging payments powerhouse that offer a single unified platform for Banking & Payments powering modern commerce.

Finxera offers a unique family of products which integrate into SMB Payments, B2B Payments and Enterprise Payments to help businesses thrive. We are on a mission to offer an industry agnostic platform that enables businesses to collect, store and send money using various new age payment methods.

Finxera is an employee-first organisation and we continually strive to ensure their professional and personal success supported by employee friendly policies and a positive work environment built on mutual respect and professionalism. We offer a dynamic work environment, with continuous growth & learning opportunities. We believe in growing together and our people are the driving force behind our success.

Summary Requirement

We are looking for a Data Engineer for a position in the Data and Analytics team.We need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively.

Responsibilities

1. Data Warehousing experience

2. Develop ETL pipelines against traditional databases and distributed systems to allow our team to remain agile in data requirements, and to flexibly produce data back to the business and analytics teams for analysis.

3.Dimensional Modelling experience

4.Database Skills - SQL / PLSQL

5.Python / Spark / Hive / Nifi Knowledge

6.Elasticsearch / Kafka knowledge would be plus

7.Data visualization tools knowledge

8.Participate in design reviews and code reviews

9.Trouble shoot and resolve production issues

10.Resolve performance related issues

11.Knowledge on Data Science / ML / Analytics would be plus

12.Build processes that analyze and monitor data to help maintain data accuracy and completeness.

13.Ability to work with colleagues across global locations remotely

14.Independent and able to work with little supervision

15.Able to mentor and supervise junior team members

16.Agile Methodology Experience

Required Skills & Qualifications

l **Should have scored 70% & above in 10th and 12th grade.

l SQL/PLSQL

l ETL/ETL

l Python

l Pyspark (Optional)

l Any BI Tool like Si Sense, Looker, Quick Sight, Tableau, Power BI, etc (Preferred LOOKER)

l Dimension Modelling (Kimbal) / Data warehousing

l Redshift (Good to have)

l OLAP Cubes",Chandigarh,True,False,True,False,False,False,False,False,True,True,False,False,True,False,False,False
Confidential,Associate - Data Engineer,"JOB DESCRIPTIONRole and responsibilitiesResponsible for overall data analysis (e.g wrangling, cleansing), tools, and technology implementationBuild data systems, pipelines, and workflowsAbility to organize structure/unstructured raw data sources of many types with the ability to combine into useable dataDevelop data wrangling and analytical applicationsCollaborate with data scientists and analyst to management data pipelinesExplore opportunities to enhance quality of data and processesManage a team of data analysts, MIS, and operations resourcesProactive communication with project manager, ensuring all client requirements are met and reports are submitted on timeOversee implementation of tools and technology to build efficiency and consistencyResolve and escalate issues with tools and applicationsDesired candidate profile3-5 years of experience as a data engineer and/or developerStrong background within the role of a data engineer for capabilities such as knowledge of various programming language with a strong emphasis on Python, SQL, Power Query and VBA/MacrosExcellent knowledge in Microsoft Excel and knowledge of advance functionsExperience with SQL set-up and advanced queries developmentStrong technical knowledge in data mining, wrangling, and structuringManaging large volumes of data with the ability to scale as neededAbility to develop and design end-to-end solutions with operations/analyst groups for deliverablesCreating custom scripts and applications to perform wrangling, cleansing, and storingTrouble shoot data issues at any level of project/structure pipeline(s)Presenting data/reports as neededExcellent people management skillsPassionate to drive business metrics - Productivity, Quality, and other key deliverablesAbility to prioritize between multiple complex projects/timelinesExcellent written and Verbal communicationHigh level of positive attitude with good listening skillsAbility to priorities between multiple complex projects/timelinesStrong attention to detail and the ability to conduct root cause analysisCandidates with demonstrated experience in Data Breach Response, or Incident Response will be preferredKnowledge and hands-on experience in breach notification and privacy laws around data breach scenarios is desirable but not must.UnitedLex is committed to preserving the confidentiality, integrity, and availability of all the physical and electronic information assets throughout the organization. Consistent with the UnitedLex ISMS policy and the ISO 27001 standard, every employee is responsible for complying with UnitedLex information security policies and reporting all security concerns, weaknesses, and breaches. Legal Services",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Oil Field Instrumentation (India),DATA ENGINEER,"Experience :

4+ years of Mud Logging experience as Data Engineer in India and abroad, both onshore and offshore rig operations. Deep Water experience will be an advantage. Proficient with real-time data monitoring and data management, WITSML transmission, gas detection and analysis, MLU maintenance.

Qualification :

Graduate or above in Geology, Applied Geology, Geoscience, Petroleum Technology or related, or B.E. in Petroleum Engineering.",Mumbai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
News Corp,Senior Data Engineer,"One of the most innovative and high-profile teams in News Corp is looking for a seasoned data engineer to help accelerate its vision. The role will involve combining all of News Corp’s data across categories, countries and organizations into a singular view of a customer enabling engagement, measurement and targeting. The Dow Jones Senior Data Engineer will partner closely with product, data & engineering teams to help make this data available and accessible to our businesses, teams and partners in order to drive revenue and improve the customer experience.

Responsibilities
• Design, implement and maintain high performance big data infrastructure/systems & big data processing pipelines scaling to billions of structured and unstructured events daily
• Design, implement and maintain deep integration with up-stream systems
• Monitor performance of the data platform and optimize as needed
• Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
• Support products with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
• Data analysis, understanding of business requirements and translation into logical pipelines & processes
• Conduct timely and effective research in response to specific requests (e.g. data collection, summarization, analysis, and synthesis of relevant data and information)
• Evaluate and prototype new technologies in the area of data processing
• Think quickly, communicate clearly and work collaboratively with product, data, engineering, QA and operations teams
• High energy level, strong team player and good work ethic

Technologies we use
• AWS (emr,ec2,glue,athena,redshift,lambda,s3,dynamodb,sns)
• Python, Spark (PySpark)
• Kubernetes, Docker
• Airflow
• Git for source code management
• Jira

Qualifications
• BS in Computer Science or other technical discipline
• 5+ years of experience designing and developing big data processing systems using distributed computing
• Fluency in Python and Spark
• Expert knowledge in optimizing complex SQL queries
• Experience with job orchestration tools like Airflow
• An affinity for automation
• Experience working with cloud platforms such as AWS & GCP
• Familiarity with networking and network application programming, including HTTP/HTTPS, JSON, and REST APIs
• Experience with at least one object oriented language (ex: Java)
• Strong OO design, data structure, and algorithm design skills
• Strong interest in emerging technologies: Hadoop, Hive

Nice to Have
• Experience in the digital advertising and marketing industry
• Contribution to Open Source projects",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,True,False
Saasvaap,Sr Data Engineer,"JOB DESCRIPTION
• Job Description (Data Engineer) ROLE AND RESPONSIBILITIES You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
• You will support our product, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
• You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
• You will be excited by the prospect of optimizing or even re-designing Peer Data architecture pipeline to support our next generation of products and data initiatives.
• Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implergent internal process improvements: automating manual processes, optimizing data delivery, re-designing hastructure for greater scalability, etc.
• Buld the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep our data separated and secure across national boundaries through multiple data centers and cloud regions.
• Create data tools for analytics assist in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.
• QUALIFICATIONS AND EDUCATION REQUIREMENTS • 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field PREFERRED SKILLS Experience with Big Data platforms such as Apache Hadoop and Apache Spark Deep understanding of REST, good API design, and OOP principles Experience with object-oriented/object function scripting languages: Python, C#, Scala, etc.
• Experience with relational SQL and NoSQL databases, including Postgres, Cosmos and Cassandra.
• Experience with data pipeline and workflow management tools: Keboola, Stitch, Azkaban, Luigi, Airflow, etc.

SKILLS REQUIRED

My SQL, AWS

EXPERIENCE

4-10

LOCATION

Kochi, Trivandrum

WORK TYPE

FullTime

TIME SHIFT

Day",,True,False,True,False,False,False,False,False,False,False,False,True,False,False,True,False
Confidential,Chistats - Senior Data Engineer - ETL/Data Pipeline,"Day-to-Day Responsibilities :- Create and manage ETL pipelines and job schedulers.- Handle unstructured data and work to automate data ingestion and validation.- Develop APIs with Flask and Python for data mining, transformation and ingestion to AWS.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Write clean, performant code to develop functional applications; build reusable code and libraries- Collaborate with team to evaluate technologies we can leverage, including open-source frameworks, libraries, and tools.- Support Business and application teams with respect to data-related requirements.- Build proactive data validation automation to catch data integrity issues.- Troubleshoot and resolve data issues using critical thinking.Required Qualifications :- Minimum of 3 years of relevant data platform experience (structured/non-structured database, data extraction, data ingestion)- A Degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines.- Experience in at least one of these databases: MS SQL, mySQL, PostGreSQL.- Experience in AWS would be ideal and basic cloud infrastructure knowledge.- Experience in Python is required for Web Scraping.- Strong fundamentals in data mining & data processing methodologies- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.- Build processes supporting data transformation, data structures, metadata, dependency and workload management.Good to have Critical Skills :- Ability to thrive in challenging situations and solve complex problems- Strong bias for action, and see yourself as an 'initiator' and 'problem solver'- Analytical and problem-solving skills- Customer centricity and Good communication (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Calix,Senior Data Engineer - Snowflake,"This position is based in Bangalore, India.

Calix is undergoing a growth transformation, and we are looking for the best and brightest engineers for our Data Engineering team. Our team is facilitating Calix’s transformation into a more data centric enterprise with our business operational leaders. We partner with our operational teams to identify the key points of decision. We create decision support tools that enable optimal data driven selection of business actions and we build out & maintain these decision support tools on a modern data technology stack using DataOps processes. We are building out the data foundations for the next phase of our growth journey. This is a great opportunity to join a rapidly scaling enterprise with a lot of opportunity for personal growth.

The Data Engineering team is seeking a Lead Data Engineer who will be an extraordinary addition to our growing team. You will build and maintain our cloud-based enterprise data platform. Key areas that you will own include data architecture, data modeling, data pipeline flow, data warehousing, security and governance protocols, data integrity processes, and data QA best practices. You will lead buildout of the end-to-end ETL/ELT data environment, integrating with new technologies, and the development of new processes to support the creation and deployment of trusted, accurate and secure decision support tools to the Calix operational business units.

The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

Responsibilities and Duties:
• Lead data modeling, data ingestion, ELT/ETL, and data integration development using our cloud-based tooling including Snowflake, AWS, Fivetran, dbt, Airflow and GitHub.
• Establish and maintain a DataOps approach for our data pipeline infrastructure and processes.
• Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
• Deploy production machine learning pipelines into business operations analytic tooling.
• Ensure data quality throughout all stages of acquisition and processing.
• Create and maintain secure and governed access to the enterprise data warehouse and reporting tools.
• Evaluate new technologies for continuous improvement in data engineering.
• Participate in project meetings, providing input to project plans and providing status updates.
• A desire to work in a collaborative, intellectually curious environment.
• Highly motivated self-starter with a bias to action and a passion for delivering high-quality data solutions.

Qualifications
• 5+ years of experience in related field; preferably experience building and delivering data pipelines, data lakes and ELT solutions at scale.
• Expert knowledge of data architecture, data engineering, data modeling, data warehousing, and data platforms.
• Experience with Snowflake, BigQuery, Redshift, AWS, and pipeline orchestration tools (Fivetran, Stitch, Airflow, etc.).
• Coding proficiency in at least one modern programming language (Python, Java, Ruby, Scala, etc.).
• Deep SQL expertise.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, operations, and technical documentation.
• Excellent verbal and written communication skills and technical writing skills.
• Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership to gain alignment, and drive progress.
• Bachelor’s degree or equivalent experience in Computer Science, Engineering, Management Information Systems (MIS), or related field.

Preferred Qualifications
• Experience with dbt SQL development environment.
• Experience developing and deploying machine learning models in a production environment.
• Experience with Power BI and/or SFDC Einstein/Tableau.
• Experience with Oracle ERP and Oracle Data Cloud tools.

Location:
• Bangalore, India",Bengaluru,True,False,True,True,False,False,False,True,True,True,False,False,True,True,True,True
Paradise Placement Consultancy,Azure Data Engineer (Mercedes-Benz)(BSL),"Job Description : Job Description: Job Description, Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, ETL,Big Data. Responsibilities: Touch base with customers to collect the requirements and analyze them Design and build end-to-end data pipelines to get the data for customers Unit testing of the pipelines and UAT support Deployment and post production support Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents. Adapt changes to the existing scripts, codes and pipelines. Reviewing design, code and other deliverables created by your team to guarantee high-quality results Capable enough to own the PoCs and deliver the results in reasonable time Accommodate and accomplish any ad-hoc assignments Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users. Job Qualifications Qualifications: Bachelor's or Master's degree in Computer Science, Information Technology or equivalent work experience 2+ years of full time data engineering experience 2+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks Skilled in ETL/ELT process. Good working knowledge with ETL tools. Experienced with Hadoop/Big data eco systems Data migration experience from on premise to cloud Expertise in structured query language and PL/SQL Exposure to log analytics and debugging Good to have DevOps, Continuous Deployment and testing techniques Agile development experience Fluent English in spoken and written Mercedes-Benz Research and Development India Private Limited. Preferred Qualifications: Microsoft Azure Certifications Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics Hands-on experience with Databricks notebooks Hands-on experience with Kafka/eventhubs Working experience in an international team or abroad.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Kaplan,Senior Data Engineer (Hybrid),"Job Title

Senior Data Engineer (Hybrid)

Job Description

For more than 80 years, Kaplan has been a trailblazer in education and professional advancement. We are a global company at the intersection of education and technology, focused on collaboration, innovation, and creativity to deliver a best-in-class educational experience and make Kaplan a great place to work.

Our offices in India opened in Bengaluru in 2018. Since then, our team has fueled growth and innovation across the organization, impacting students worldwide. We are eager to grow and expand with skilled professionals like you who use their talent to build solutions, enable effective learning, and improve students’ lives.

The future of education is here and we are eager to work alongside those who want to make a positive impact and inspire change in the world around them.

Job Impact and Scope Summary

The Senior Data Engineer at Kaplan North America (KNA) within the Analytics division will work with world class psychometricians, data scientists and business analysts to forever change the face of education. This role is a hands-on technical expert who will own the design and implementation of an Enterprise Data Warehouse powered by AWS RA3 as a key feature of our Lake House architecture.

The perfect candidate is an expert in data warehousing technical components (e.g. data modeling, ETL, reporting). You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be able to work with business customers in a fast-paced environment understanding the business requirements and implementing data & reporting solutions. Above all you should be passionate about working with big data and someone who loves to bring datasets together to answer business questions and drive change.

Responsibilities
• Hands-on technical leader. Continually raises the bar for the data engineering function.
• Leads the design, implementation, and successful delivery of large-scale, critical, or difficult data solutions. These efforts can be either a new data solution or a refactor of an existing solution and include writing a significant portion of the “critical-path” code.
• Sets an example through their code, designs and decisions. Provides insightful code reviews and take ownership of the outcome. (You ship it, you own it.)
• Proactively works to improve data quality and consistency by considering the architecture, not just the code for their solutions.
• Makes insightful contributions to team priorities and overall data approach, influencing the team’s technical and business strategy. Takes the lead in identifying and solving ambiguous problems, architecture deficiencies, or areas where their team bottlenecks the innovations of other teams. Makes data solutions simpler.
• Leads design reviews for their team and actively participates in design reviews of related development projects.
• Communicates ideas effectively to achieve the right outcome for their team and customer. Harmonizes discordant views and leads the resolution of contentious issues.
• Demonstrates technical influence over 1-2 teams, either via a collaborative development effort or by increasing their productivity and effectiveness by driving data engineering best practices (e.g. Code Quality, Data Quality, Logical and Physical Data Modelling, Operational Excellence, Security, etc.).
• Actively participates in the hiring process and is a mentor to others - improving their skills, their knowledge, and their ability to get things done.
• Hybrid Schedule: 3 days remote / 2 days in the office
• 30-day notification period preferred

Requirement:
• In-depth knowledge of the AWS stack (RA3, Redshift, Lambda, Glue, SnS).
• Expertise in data modeling, ETL development and data warehousing.
• 3+ years’ experience with Python, or Java, Scala
• Effective troubleshooting and problem-solving skills
• Strong customer focus, ownership, urgency and drive.
• Excellent verbal and written communication skills and the ability to work well in a team.

Preferred Qualification:
• 3+ years’ experience with AWS services including S3, RA3.
• Ability to distill ambiguous customer requirements into a technical design.
• Experience providing technical leadership and educating other engineers for best practices on data engineering.
• Familiarity with Airflow, Tableau & SSRS.

#LI-Remote

#LI-AK1

Location

Bangalore, KA, India

Additional Locations

Employee Type

Employee

Job Functional Area

Data Analytics/Business Intelligence

Business Unit

00092 Kaplan Health

Kaplan is an Equal Opportunity Employer. All positions with Kaplan are paid at least $15 per hour or$31,200 per year for full-time positions. Compensation for specific positions are based on job level, skills, years of experience, and education, among other factors. Additionally, certain positions are bonus or commission eligible. Information regarding benefits can be found here
.

Diversity & Inclusion Statement:

Diversity inspires innovation and growth in the Kaplan community. Kaplan strives to be a model employer for inclusiveness. Not only does Kaplan value its employees for their professionalism and skills, but also for the unique viewpoints they bring to the Organization. Kaplan's employees bring diverse perspectives, ideas, and backgrounds that give Kaplan a competitive edge in anticipating and exceeding our students' needs in today's global market. Learn more about our culture
.",Bengaluru,True,False,False,True,False,False,False,True,False,True,False,False,True,False,True,False
"Planview, Inc.",Data Engineer,"Overview

Planview is looking for a passionate data engineer to join our team innovating tools for connected work. You will work closely with other data engineers, data scientists and individual product teams to specify, validate, prototype, scale, and deploy features with a consistent customer experience across the Planview product suite.

Responsibilities
• Ability to work in a fast paced start up mindset. Should be able to manage all aspects of AI/ML activities
• Develop platforms that make data across applications/application deployments available for AI/ML-driven feature prototyping, proofs-of-concept, and general availability
• Refine data pipelines for analysis, while refining, automating, and scaling as needed for the use-case at hand
• Work on various aspects of the AI/ML ecosystem – data modelling, data pipelines, data observability, data documentation, scaling, deployment, monitoring and maintenance etc.
• Work closely with Data scientists and MLOps Engineers to come up with scalable system and model architectures for enabling real-time ML/AI services

Qualifications

Required qualifications
• Masters or equivalent experience in Informatics, CS, Data Science or a related field
• 2+ years of experience as a data engineer or data scientist with a focus on data engineering for ML applications
• Strong Python and SQL coding skills
• Familiar with AWS Data and ML Technologies (AWS Sagemaker, Data Pipeline, Glue, Athena, Redshift etc)

Preferred Qualifications
• Demonstrated experience building data and analytics pipelines/services that efficiently scale for cloud application usage, meeting a product team’s SLA for performance and resilience
• Exposure to database queries and strong in SQL
• Exposure to any of the libraries and frameworks in data science (Pandas, Numpy, Dask, PySpark etc)
• Exposure to data version control (DVC) and orchestration tools (Airflow, etc)
• AWS Certification is a plus
• Skilled at working as part of a global, diverse workforce of high-performing individuals
• AWS Certification is a plus",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Full Potential Solutions,Lead Data Engineering,"Overview:

About The Company

Full Potential Solutions (FPS Perch) is a global product company headquartered in the US. We help contact centers around the world better engage with their customers by combining our deep domain expertise with cutting-edge technology. Our AI enabled, cloud-scalable analytics products enhance the end-to-end customer journey via omnichannel engagement (voice, email, SMS, chat and conversational AI) and optimize agent performance. Our offices are spread across the US, India and Philippines.

Some exciting initiatives they are working on are:
• Designing and development amazing user experiences across data visualization and analytics in the customer engagement / contact center industry
• Salesforce based solutions for omni-channel customer engagement leveraging AI to enhance contact strategy and routing, guide agents on next best action (NBA) enable dynamic scripting, capture voice of customer (VOC)
• Developing standardized AI/ML models to optimize customer engagement and agent performance using various Python frameworks and AWS services. (Transcribe, Comprehend, Sagemaker, etc.)
• Developing a comprehensive Data, Analytics and Reporting platform to integrate, measure and analyze millions of daily customer interactions/metrics across all our clients.
• Building web/mobile apps to improve agent/manager performance across thousands of agents (metrics, gamification, collaboration).
• Deploying cutting-edge, tech-enablement services using the AWS ecosystem, including microservices and serverless architecture, CI/CD pipelines, event notification & search services, multi-tenant architecture ec.,

Our team has successfully built and scaled analytics-focused products at companies such as Salesforce, Demandware, and IBM. We have strong backgrounds in Computer Science, Math and Engineering from top universities such as IIT, Harvard and Yale. We strongly believe that empowered people are the key to building a great company, and our development process focuses on iteratively improving our products as well as ourselves as individuals and as a team. Our mission as a company is to create an environment where the people THRIVE.

Explore more at: FPS Website

Leadership Team: Explore here

About The Team

Our analytics team helps our clients leverage data to optimize their performance and results. Our Data Analysts possess a unique skill of understanding the data, making a clear sense of it, and telling a story to the business. We work closely with the Analytics Leaders and the rest of the Analytics product team to analyze and understand customer needs, explore granular data to identify key drivers that influence each KPI, measure the level of expression of attributes that move the KPI, and execute in a way that brings the best solutions to life. Most importantly our team exhibits our core values: Integrity, Excellence, Accountability and Grace.

Responsibilities:

Functional Requirements
• Data exploration skills to unravel data sources and identify the granularity, attributes & measure
• Discover data sources and determine the best EL approach and tools to bring data into the D
• Determine the data cleansing rules based on source data and ensure data replication accurac
• Coordinate & collaborate with data architects and BI engineers to ensure timely project deliver
• Understanding of data warehouse concepts and implementation methods of warehouse object
• Translate the data models from the ER diagram into physical models coded into data warehous
• Innovate reusable and configurable frameworks for data replication to build the bronze laye
• Well-versed with the data warehouse modeling concepts (star/snowflake/denormalized structures
• Define data quality rules, modeling standards, business metrics, standard processes and test case
• Understand the domain, business cases, objectives, and KPIs that need to be reported and analyse
• Participate in data discovery phase and capture the data sources, required KPIs with the formulae

Qualifications:

Technical Requirements
• Bachelor’s degree in a Technical/Quantitative subject such as Computer Science (B.E/B.Tech/MCA
• 7 - 9 years of relevant experience in the field of Data Engineering and Data Warehouse/BI solution
• Proficient in SQL, query optimization, coding stored procedures, and ad-hoc data analysis using SQ
• Develop data pipelines using some combination of ETL/ELT tools and data processing framework
• Develop DBT models as per the data model and implement the data transformations & test case
• Orchestrate data integration & transformation processing from sources to ssots in data warehous
• Design, develop, monitor and re-engineer database objects and processes/pipelines/schedules
• Use AWS services for data storage, access and retrieval and application setup and monitorin
• Design & document the data processing workflows and setup systems as per solution architecture
• Design the multi-dimensional data model in data warehouse in order to meet BI reporting needs
• Setup software engineering best practices for data pipelines with data-associated documentation
• CI/CD implementation with source code version control, QA checks and deployment automation
• Hands-on with languages like Spark, Python, Scala, R, Bash/Shell Scripting or stored procedures
• Must have worked with RDS or data lakes (MySQL, PostgreSQL, Oracle, Redshift, Snowflake, etc.)
• Hands-on experience in using ETL/ELT tools like Talend, Informatica, SSIS, Airbyte, Alteryx, Stitch etc.,
• Experience working with orchestration tools such as Airflow, Astronomer, Control-M, Prefect etc.,
• Experience working with DevOps tools such as Bitbucket, Github, Gitlab, Jenkins, CircleCI etc.,
• Experience working with AWS Cloud services such as S3, EC2, ECS, EFS, Lambda, Docker, Kubernetes
• Hands-on experience in using data-modeling tools like Erwin, DBSchema, MySQL Workbench, ER Studio

Leadership Responsibilities
• Lead a squad of data engineers to implement data warehouse solution across various projects
• Ensure adherence to the best practices of data engineering and testing across all projects
• Drive sprint planning/review and lead daily Kanban meetings in alignment with project plan
• Allocate work smartly and efficiently manage/ coach the team while monitoring the progress
• Develop systematic training plan for onboarding new joiners and upskilling existing resources",,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Kyndryl,Azure Data Engineer,"Why Kyndryl Kyndryl is a market leader that thinks and acts like a start-up. We design, build, manage, and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward - always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers, and our communities. We invest heavily in you - not only through learning, training, and career development, but also through the flexible working practices and stellar benefits that help you grow and progress long-term. And we give back - from planting 90,000 trees in our first 3 months as part of our One Tree Planted initiative to the Corporate Social Responsibility and Environment, Social and Governance practices embedded within everything we do, we are committed to powering human progress in an ethical, sustainable way. Your Role and Responsibilities Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Create and maintain clear and complete pipeline documentation, e.g., architecture diagrams and data transformations. Integrates data solutions with business processes. Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Provide actionable insights via compelling storytelling to drive business outcomes. Microsoft Certified: Azure Data Engineer Associate Required Technical and Professional Expertise Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Preferred Technical and Professional Experience Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Required Education Associate's Degree/College Diploma",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Role: AWS Data Engineer

Ex- 4 to 8 YRS

Locations- Delhi/NCR, Gurgaon, Bangalore, Ahmedabad, Pune, Indore, Mumbai, Kolkata

Must Have –
• Working on EMR, good knowledge of CDK and setting up ETL and Data pipeline
• Coding - Python
• AWS EMR, Athina, Supergule, Sagemaker, Sagemaker Studio
• Data security & encryption
• ML / AI
• Pipeline
• Redshift
• AWS Lambda

Expectations/Responsibility
• Industry experience in Data Engineering on AWS cloud with glue, redshift , Athena experience.
• Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python.
• 3+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
• Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.
• Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
• AWS engineer provides comprehensive systems administration functions on Amazon Web Services (AWS) infrastructure to include support of AWS products such as: AWS Console root user administration, Key Management, EC2 Compute, S3 Storage, Relational Database Service (RDS), AWS Networking & Content delivery (VPC, Route 53, ELB, etc.) Identity & Access Management, CloudWatch, CloudTrail, Cloud Formation, Auto Scaling, Cost and Usage Reports, and more.
• Train and guide the company’s HR engineering team on developing with aforementioned AWS tools, while also executing on specific deliverables (ingestion, Storage, integration, warehouse, visualization)
• Coach and mentor other technical resources on the team on AWS technologies
• Create ETL piplelines that are highly optimized with very large data sets
• Solve issues with data models and come up with solutions
• Developing and directing software system testing and validation procedures, programming and documentation
• Analyzing user needs and requirements to determine feasibility of design within time and cost constraints
• Provide technology expertise, direction, coordination, and consultation, in the development, integration, launch, scaling, and maintenance of new and existing products and solutions
• Establishes infrastructure technology architectures, standards, test plans, design templates and governance
• Works with the team to define standards and frameworks with regards to coding, programming, and the general development of applications for multiple platforms
• Work with business teams to understand customer issues and to investigate, prototype and deliver new and innovative solutions

FRESHERS DO NOT APPLY.",New Delhi,True,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False
Mercede,Azure Data Engineer-Karnataka-Bangalore,"Azure Data Engineer

Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, Big Data, Azure DevOps, Azure Boards, VSTS Git

Hey, do you want to change the world? Build #TheNextBigThing? Which means implementing ideas that are said to be unachievable? Like it was stated at first about smart chatbots or artificial intelligence. At MBRDI you are dealing with questions, for which there are no answers. Not yet.

About Us

The Corporate Center of Excellence (CoE) for AI, Advanced Analytics and Big Data is working on #TheNextBigThing. Besides conducting cool use cases in the field of Data Analytics and AI, we are inventing, building and running eXtollo a data analytics cloud platform based on Microsoft Azure for Daimler teams around the world. Our users trust in the cross-divisional platform to create secure and scalable cloud applications based on Machine Learning, Artificial Intelligence Big Data technologies. In addition to the provisioning and utilization of the technology we are also responsible for Daimler s data treasure the eXtollo Data Lake.

Role

We are searching you as a Data Engineer to continuously improve eXtollo within an agile working environment. You are expected to work directly with customers to understand their data demands and build end to end pipelines to get the data as they wish. Also be able to understand the current codes, pipelines and scripts which are already in production in a short time and support operations/changes.

Experience of data migration into Azure from various cloud and on-prem is a plus. You are expected to have good exposure to structured query language preferably MS Sql Server. Should be able to do PoC for the new adaptions and work independently with minimal guidance. Data warehouse/data engineering experience is appreciated. Most importantly the candidate should have real hands-on experience rather bookish/training experience.

Responsibilities
• Touch base with customers to collect the requirements and analyze them
• Design and build end-to-end data pipelines to get the data for customers
• Unit testing of the pipelines and UAT support
• Deployment and post production support
• Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents.
• Adapt changes to the existing scripts, codes and pipelines.
• Reviewing design, code and other deliverables created by your team to guarantee high-quality results
• Capable enough to own the PoCs and deliver the results in reasonable time
• Accommodate and accomplish any ad-hoc assignments
• Building CI/CD pipelines in Azure DevOps to deliver our services into various data centers worldwide
• Analyzing and solving incidents in productive environment while avoiding data loss and minimizing service outages
• Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users
Job Qualifications

Qualifications
• Bachelor s or Master s degree in Computer Science, Information Technology or equivalent work experience
• 3+ years of full time data engineering experience
• 3+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks
• Skilled in ETL/ELT process. Good working knowledge with ETL tools.
• Must be experienced with Hadoop/Big data eco systems
• Data migration experience from on premise to cloud
• Expertise in structured query language and PL/SQL
• Experienced in Powershell scripting for orchestration
• Exposure to log analytics and debugging
• Good knowledge in DevOps, Continuous Deployment and testing techniques
• Agile development experience
• Fluent English in spoken and written

Preferred Qualifications
• Microsoft Azure Certifications
• Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics
• Hands-on experience with Databricks notebooks
• Working experience in an international team or abroad
,

This job is provided by Shine.com",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
HARMAN International,Data Engineer,"What You Will Do :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis What You Need :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis",Bengaluru,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
GSPANN Technologies,Azure Data Engineer,"Should have experience in ADLS (Azure Data Lake storage) Experience implementing Azure Data Factory Pipelines using latest technologies and techniques Experience in working on Azure HDInsight Experience in working with Storage Strategy Azure developer should be able to ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements Expert in Azure Data Factory, Azure Data Lake Azure SQL Data Warehouse, Azure Functions, Databricks · Comfortable working with Spark, Python, and PowerShell Excellent problem solving, Critical and Analytical thinking skills Strong t-SQL skills with experience in Azure SQL DW DevOps, CI/CD, and Automation experience strongly preferred Able to interact with team members collaboratively Experience handling Structured and unstructured datasets Experience in Data Modelling and Advanced SQL techniques",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Uber,Data Engineer II,"What You'll Do
• Responsible for defining the Source of Truth (SOT), Dataset design for multiple Uber teams.
• Identify unified data models collaborating with Data Science teams
• Streamline data processing of the original event sources and consolidate them in source of truth event logs
• Build and maintain real-time/batch data pipelines that can consolidate and clean up usage analytics
• Build systems that monitor data losses from different sources and improve the data quality
• Owns the data quality and reliability of the Tier-1 & Tier-2 datasets including maintaining their SLAs, TTL and consumption
• Devise strategies to consolidate and compensate the data losses by correlating different sources
• Solve challenging data problems with cutting-edge design and algorithms.

What You'll Need
• 4+ years of extensive Data engineering experience working with large data volumes and different sources of data.
• Strong data modeling skills, domain knowledge, and domain mapping experience.
• Strong experience in using SQL language and writing complex queries.
• Experience with using other programming languages like Java, Scala, Python
• Good problem-solving and analytical skills
• Good communication, mentoring, and collaboration skills.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Cardinal Health,"Sr. Data Engineer, Data Engineering","Job function:

IT Quality Control is responsible for owning and implementing software testing and certification strategies for the enterprise. Debugs problems with software through standard tests and recommends solutions. Conducts defect trend analysis and continuous process improvement. Demonstrates knowledge of requirement and risk based testing principles, theories, concepts and techniques. Establishes internal IT service quality control standards, policies and procedures.

Job duties:

Create Test Strategy, define quality standards for Google Cloud Platform and define metrics to measure the efficiency and testing for applications on Google BigQuery, Dataflow and Airflow. Develop and maintain test automation frameworks, build regression test strategy and continuous testing process.

Skills:
• Ability to create test strategy balancing manual and automated testing
• 8+ years of experience with designing and implementing test frameworks in cloud
• Technical skills – SQL, Java/Python
• Good communication and collaboration skills across teams and business SMEs
• Should exhibit continuous testing mindset
• Knowledge on Devops and CI/CD process
• Develop automated test cases to be used in performance testing or as part of testing
• Identify and implement performance metrics to be measured
• Collaborate with functional and technical teams to identify test data or create through UI and database
• Generate automation or performance testing reports from execution
• Maintain record of test discrepancies, using designated QA tools
• Provide feedback of test results to development and infrastructure teams for resolution
• Review Business Requirements Documents and Functional and Technical Specifications towards determining test data scope
• Oversee defects from initial identification through post-deployment analysis
• Coordinate with other QA engineers, leadership, system administrators, architects and developers

What is expected of you and others at this level:
• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
• Participates in the development of policies and procedures to achieve specific goals
• Recommends new practices, processes, metrics, or models
• Works on or may lead complex projects of large scope
• Projects may have significant and long-term impact
• Provides solutions which may set precedent
• Apply design thinking mindset
• Independently determines method for completion of new projects
• Receives guidance on overall project objectives
• Acts as a mentor to less experienced colleagues

Cardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,True,True,False
Siemens Energy,Data Engineer (PMK Project),"A Snapshot ofYour Day

A Our team ofdata engineers supports several business teams to get needed data, prepare itproperly for the particular use cases and make it available in an efficientway. You are integrated in the business team and also work on business topics.

One strongexample is the Prescriptive Marketing team. They create forecasts for energymarkets about power prices, demand and other key factors. This is used to helpcustomers to improve their plants and thus their revenues. It is essential alsofor the data engineers to understand the big picture while fulfilling the datarequests.

You discussthe scope of the data and how to make it available, followed by implementingand maintaining data pipelines, normally Python based, with generic setups thatcan be re-used easily to scale up the market models. There is a wide range oftasks, from doing research about data sources up to presenting the data indashboards.

How You’ll Makean Impact
• As a data engineer you willsupport the team by using, building and maintaining ETL tools and pipelines forcollecting, transferring and preparing data for several use case like internalanalytics or consumer facing applications.
• You should be able to deliverrapidly in a reliable manner with the highest quality standards.
• You should be curious,passionate about problem solving, building and self-improving.

WhatYou Bring
• Master’s / bachelor’s degree in computer science or similar with a minimum 5 years of experience in the field of Software Development and Architecture.
• Deliver rapidly in a reliable manner with the highest quality standards
• Curious, passionate about problem solving, building and self-improving
• Experience in Agile development
• Fluent English Skills
• Experience working in and with international teams and stakeholders with different cultures

Technical Skills:
• Highly experienced in relational database setups and data warehouse environments
• Snowflake knowledge is a plus
• Solid experience with building ETL pipelines
• Strong SQL skills (aggregations, windows functions, pivoting etc.)
• Python knowledge
• Solid experience in software development, incl. design patterns is a plus
• Experience with OOP concepts (e. g. Java/C# background) is a plus

Working experience:
• version control systems (Gitlab) and tools like Confluence and Jira
• deployments with CI/CD pipelines
• AWS cloud environments
• Time series data
• integrating and managing structured and unstructured data in cloud based data management systems for example with PostgreSQL or Redshift, Snowflake is a plus
• Tableau dashboards is a plus

About The Team

""Let’s make tomorrowdifferent today"" is our genuine commitment at Siemens Energy to allcustomers and employees on the way to a sustainable future.

Ourteam belongs to the Software Engineering andProduct Development Function within Siemens Energy. Our missionis to grow the digital software business and develop solutions and products forour internal and external customers. This covers from edge data acquisition,data lake and processing up to development of digital twins and apps aroundcustomer assets.

Who is Siemens Energy?

At SiemensEnergy, we are more than just an energy technology company. We meet thegrowing energy demand across 90+ countries while ensuring our climate isprotected. With more than 92,000 dedicated employees, we not only generateelectricity for over 16% of the global community, but we’re also using ourtechnology to help protect people and the environment.

Ourglobal team is committed to making sustainable, reliable, and affordable energya reality by pushing the boundaries of what is possible. We uphold a 150-yearlegacy of innovation that encourages our search for people who will support ourfocus on decarbonization, new technologies, and energy transformation.

Our Commitment to Diversity

Lucky for us, we are not all the same.Through diversity we generate power. We run on inclusion and our combinedcreative energy is fueled by over 130 nationalities. Siemens Energy celebratescharacter – no matter what ethnic background, gender, age, religion, identity,or disability. We energize society, all of society, and we do not discriminatebased on our differences.

Rewards/Benefits
• The opportunity to engage inan exciting environment on challenging projects
• Strong professional supportand working with colleagues around the world
• Professional developmentopportunities within the company
• To be part of a growingfunction with a dynamic, informal, and inspiring working environment in aposition that entails a large responsibility
• Medical benefits
• Remote/Flexible work
• Time off/Paid holidays
• Parental leave
• Continual learning throughthe Learn@Siemens-Energy platform

https://jobs.siemens-energy.com/jobs",Gurugram,True,False,True,True,False,False,False,False,False,True,False,False,True,False,False,True
Snowflake,Data Engineer,"Build the future of data. Join the Snowflake team.

Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Because traditional on-premises and cloud solutions struggle with this, Snowflake developed an innovative product with a new built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.

In addition, Snowflake’s culture was built on the following values that are even more important to us today:
• Put Customers First. We only succeed when our customers succeed
• Integrity Always. Be open, honest, and respectful
• Think Big. Be ambitious and have big goals
• Be Excellent. Quality and excellence count in everything we do
• Get It Done. Results matter!
• Own It
• Make Each Other the Best
• Embrace each others’ Differences

Job Description
• Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data infrastructure that scales across the company
• Drive the design, building, and launching of new data models and data pipelines in production
• Build data expertise and own data quality for allocated areas of ownership
• Align to Product roadmap in building tools for data platform users
• Mature requirement and follow design develop and communicate model using Agile methodology for data ingestion and data tooling.

MINIMUM QUALIFICATION
• Expertise in SQL statements and modeling concepts.
• Must be aware of the cloud environment from data ingestion and modeling perspective.
• Must be strong in python
• Experience with Apache Airflow is highly desirable.
• schema design and dimensional data modeling.
• custom ETL design, implementation and maintenance.
• object-oriented programming languages.
• Understanding of API and connectors is highly desirable
• analyzing data to identify deliverables, gaps and inconsistencies.
• Experience in data warehouse space.

PREFERRED QUALIFICATION :
• BE/BTECH in Computer Science, IT or other technical field.
• Experience with data ingestions and data analytics.
• 4 years experience using Python and SQL, .",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,True
PepsiCo,Data Engineer,"Overview

This role is with the Global business services arm of Media Center of Excellence (CoE) at PepsiCo.

In this role, you will play a key role in shaping the future of media measurement strategy for PepsiCo, esp. with focus on building an understanding role of media as key Growth Driver thru ROI and related analytics.

You will be laying down strong data foundation through implementation of process & technology.

This role is the backbone of media measurement agenda at Pepsico and is responsible for building data systems and pipelines to feed into prescriptive and predictive modeling by establishing and enhancing process around data capture, storage, quality and reliability.

You will work with Media, Data engineering, Data Science and IT teams globally and within AMESA sector to identify best practices in the industry and across all PepsiCo’s brands, providing support to codify and scale best in-class methods that inspire continuous improvement in marketing effectiveness and ROIs.

Responsibilities
• Collect, structure, analyze, organize and maintain RAW data from various data sources needed for creating predictive models in structured databases in order to ensure faster model building
• Partner with PepsiCo functional teams, agencies and third parties to build seamless process for acquiring, tagging, cataloging and managing all media, Nielsen and internal data periodically in structured format as needed for measurement statistical models
• Design, build and codify data structures in efficient way to periodically feed in raw data from various internal and external sources and also manage and house model outputs for quick input to businesses;
• Build data systems and pipelines as per business needs and objectives, in this case prepare data to feed specifically to MMM and media measurement models or any descriptive or prescriptive analysis
• Promote data consistency globally to support common standards and analytics
• Establish periodic data verification processes to ensure data accuracy
• Build new technologies and algorithms to optimize any business process around creation and maintenance of databases/data lakes running of batch processes for data updation

Qualifications
• 3-6 years of experience in the field of data structures, building and managing data lakes
• Hands on experience in building database/Data Management Solutions
• Mandatory experience Python, Data modelling, data pipeline set up and meta data management
• Experience in relational databases as well as unstructured data streams
• Experience with schema design and dimensional data modeling (for ex: using data vault/snowflake/star schema)
• Hands on experience, Airflow (or any other Orchestration tools)
• Hands on experience in AWS (or any other cloud operator)
• Good to have experience on technologies like, DBT
• Good to have experience in data engineering teams in consulting or ecom/telecom sector
• Desirable - Experience optimizing larger applications to increase speed, scalability, and extensibility
• Educational Background- BE/B T ECH/ MS in computer science or related technical field",Peeramcheru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,True,True
HTC Global Services,Data Engineer-GCP,"Greetings from HTC Global Services

We are hiring Data Engineer- GCP

Skills Required:

Data Engineer- GCP

Experience with Big query, Terraform ,Hive

Experience:

3+ Years

Location:

Chennai

Notice:

Looking for candidates who can join within 15 days.

Interested candidates please drop your resume to sunitha.manohar@htcinc.com

Regards

Sunitha",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
HP,Senior Data Engineer,"The Company

HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations and dreams. We apply new thinking and ideas to create simpler, more valuable and trusted experiences with technology, continuously improving the way our customers live and work.

Position background

In the GTM analytics COE our mission is to deliver impact by building machine learning products to optimize pricing and marketing investments and provide guidance to our sales organization.

As a Big Data Engineer, you will be in a unique position to support the development one of our internal assets. You will work together with the project and asset team to understand the end state in which the data must be delivered and you will model the data using Big Data technologies like Spark.

We offer an international experience, collaborative culture, top rate experience in AI and ML and opportunity to create significant real-world impact.

What You Will Do

Create / Maintain ETL pipelines.

Ensure that processes are optimized.

Use Spark to model big volumes of data.

Contribute to the database architecture, design and implementation.

What You Will Need

Bachelor’s in computer engineering, Computer Science, Electrical Engineering, Robotics or a related field

2+ years on a similar role.Ability to work independently under a fast-paced environment, comfortable to deliver results under pressure.

You Have Strong Problem-solving Skills.Agile Experience.

Experience with modern application lifecycle management tools (Git, Visual Studio, Intellij, Code Reviews).

Proficient in at least one of the following languages: Python, PySpark, Scala, Spark, SparkR.

Experience with SQL & NoSQL databases is preferred (PostgreSQL, MongoDB, Elastic Search).

Experience in working with DataIku DSS software.

Strong analytical skills with demonstrated problem solving ability.

Who We Are

At HP, we believe in the power of ideas. We use ideas to put technology to work for everyone. And we believe that ideas thrive best in a culture of teamwork. That is why everyone – at every level in every function, is encouraged to think big, have original ideas and express and share them. We trust anything can be achieved if you really believe in it, and we will invest in your ideas to change lives and the way people work. This vision is what sets us apart as a company. At HP, we work across borders and without limits. Global virtual teams share resources, pool their big ideas to solve our biggest business opportunities. Everyone is valued for the unique skills, experiences and perspective they bring. That’s how we work at HP. And this is how ideas and people grow.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
DataTheta,Azure Data Engineer,"Experience Required: 5-10 Years

Location: Noida/Chennai

Azure Data Engineer | Job Description
• Dev / Architect
• 2+ years of experience in Azure cloud data stack such as Synapse/DW, Azure SQL DB, Azure Blob Storage
• 1+ years of experience in Logic Apps
• 3+ years of experience in Python
• 5+ years of experience in SQL
• 3+ years of experience in Databricks
• 2+ years of experience in Azure/AWS Lambda Functions
• 2+ years of experience in Microservices (REST) architecture
• Ability to project manage and work within an agile, flexible environment.
• Performs peer reviews for other data engineers’ work.
• Ensuring adherence to programming and documentation policies, software development, testing and release.
• Develop modeling, design, and coding practices.
• Experience with Lean / Agile development methodologies
• Positive attitude with great collaboration and communication skills",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Tech Mahindra,GCP Data Engineer,"Job Role - GCP Data Engineer
• Looking only GCP Data Engineers. (GCP Certification is not mandatory)
• Experience should be 4-8 Years Max.
• Candidates should be aware about BigQuery, Data Flow, Composures.
• GCP Services, SQL, Migration Process
• Migration Tools ( Plate spin, Cloud Physics, Stratozone)
• Work Location Pune/Bangalore/Noida/Chennai",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False,False
MOURI Tech,Sr. Data Engineer,"Hi Folks,

Greetings from Mouritech!!

We are hiring for Sr. Data Engineer

Mandatory Skill: Data Engineer, Python, SQL, DWH & GCP

Location: Gurgaon (Hybrid)

Exp: 4+ Yrs to 12 Yrs

Notice Period: Immediate to 30 Days Serving.

If interested pls share your profile on below mail id

surabhim.in@mouritech.com.",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
NAB,Senior Data Engineer [T500-7047],"Essential Skills:
• Advanced SQL skills (or equivalent database querying language), Database SQL skills (MySQL preferably), Able to write complex queries (subquery, window function, CTE, removing duplication), Able to analyse query bottleneck by “Explain” command.
• Construct RMDB database-Database modelling skills, Able to operate DDL (stored procedure, indexing, trigger, table, materialised view, view), Understand database modelling (normalisation)
• Develop, maintain and Implement Power BI dashboards,
• Understand batch job process, able to modify it, and solve batch job issue- Python (ODBC DB connection, Pandas, XML handling), PowerShell (General PS command), ETL experience.
• Extract meaningful insights out of the data.
• AWS experience -AWS EC2, RDS monitoring, parameter store
• Aware of GitHub skills- Merge/pull request/resolving conflicts, Pull/push.
• Translate business information requirements into a meaningful set of SQL queries.
• Develop and maintain key reporting metrics that drive business performance.
• Creating Views, Stored Procedures and Materialised Views in MySQL.
• Ability to parse XML tags into SQL human readable tables.

Job Requirements:
• Advanced SQL skills (or equivalent database querying language)
• 6+ years’ experience working in a similar role.
• 6+ years of experience working with BI tools such as Power BI
• Proficient in Python language
• Data Science enthusiast
• Understand Jenkin pipeline, Jira & Confluence
• Advanced MS Office skills, including Excel, PowerPoint, Access etc.
• Ability to deal with ambiguity, solve complex problems, and navigate large, global organisations.
• Self-motivated, assertive, analytical, and comfortable working in a fast-paced environment
• Stakeholder management

Department : Group Security

Sub Department : Cyber Security

Job code : AAZA01",Gurugram,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
SID Global Solutions,Senior Data Engineer(8+yrs),"Skillset: SQL, AWS Stack, Python, Redshift/MYSQL

Roles & Responsibilities:

Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift

Hands on Python programming.

Working knowledge on S3

AWS certification is a nice to have

Must be punctual and follow deadlines and deliver on time.

Must be able to clearly communicate with stakeholders and team",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
InVisions Ltd.,Data Engineer,"Hello people,

We are happy to assist the product and service company “Saras Analytics” in welcoming new Data Engineers to the team.

Now, a little bit about the company and the product:

Saras Analytics is a rapidly growing data management and analytics advisory firm with offices in Austin, USA and Hyderabad, India. We are a group of engineers and analysts focused on accelerating growth for e-commerce and digital businesses by setting up or transforming their data (analytics & BI) ecosystems and providing further analytics services. We are laser focused on providing the best ROI for our clients and leave no stone unturned in our quest to provide the best results for our customers.

We are an employee-centric organization and, to meet the ever-growing demand for our services, are looking for individuals who share our passion to make a difference and would be great additions to our analytics and growth consulting practice.

How Saras Analytics describes your role:

As a Data Engineer at Saras Analytics, you will be responsible for building and maintaining large-scale data pipelines as well as create and data pipelines that deal with large volumes of data.

You will deal with:
• Database programming using multiple flavors of SQL and Python.
• Understand and translate data, analytic requirements and functional needs into technical requirements.
• Build and maintain data pipelines to support large scale data management projects.
• Ensure alignment with data strategy and standards of data processing.
• Deploy scalable data pipelines for analytical needs.
• Big Data ecosystem - on-prem (Hortonworks/MapR) or Cloud (Dataproc/EMR/HDInsight).
• Work with Hadoop, Pig, SQL, Hive, Sqoop and SparkSQL.
• Experience in any orchestration/workflow tool such as Airflow/Oozie for scheduling pipelines.
• Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow.
• Understand and execute IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, writing optimized queries in Spark.
• Hands-on experience in using Spark Streaming, Kafka and Hbase.

What you bring with you:
• 4 to 6 years of experience in building data processing applications using Hadoop, Spark and NoSQL DB and Hadoop streaming. Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow is a plus.
• Expertise in data structures, distributed computing, manipulating and analyzing complex high-volume data from variety of internal and external sources.
• Experience in building structured and unstructured data pipelines.
• Proficient in programming language such as Python/Scala.
• Good understanding of data analysis techniques.
• Solid hands-on working knowledge of SQL and scripting.
• Good understanding of in relational/dimensional modelling and ETL concepts.
• Understanding of any reporting tools such as Looker, Tableau, Qlikview or PowerBI.
• Degree: Bachelor of Engineering - BE, Bachelor of Science - BS, Master of Engineering - MEng, Master of Science – MS or equivalent work experience.

Eligibility:
• Significant technical academic course work or equivalent work experience.
• Excellent communication and interpersonal skills.
• Willingness to work under labor contract, B2B contract is an option too.
• Dedicate 40 hours/weekly to Saras Analytics.

Let’s connect and check if we match!

You can state your interest by sending your CV and we will get in touch with the short-listed candidates.

We treat your personal information with respect and confidentiality, guaranteed and protected by the professional ethics, the Bulgarian and European law.

“InVisions” agency license № 2420 from 19.12.2017.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,False
LTIMindtree,Specialist Data Engineer- AZURE-ADF/ADB,"• Job Title- Specialist Data Engineer
• Primary skill- Azure+Databricks (ADF+ADB+Pyspark)
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 12 Years
• Notice Period- 0 to 30 Days
• Job Description-

Primary Skills

• 5+ years of experience in Python and Databricks.

• Deep understanding of data modelling techniques for analytical data (i.e. facts, dimensions, measures)

• Experience developing and managing reporting solutions, dashboards, etc. Design and architecture experience in data transformation.

• Should have experience with data platforms and in data transformation and extraction: some combination of ETL/ELT, table and database design, query design, performance analysis and optimization

• Worked as a data engineer or related specialty (Software Engineer/Developer, BI Engineer/Developer, DBA)

Secondary Skills

• Experience in Azure Data Factory and Azure Storage

• Hands on experience with handling of large amount of data using SQL, Azure Data Factory, Spark, Azure Cloud architecture

• Knowledge of cloud architecture and data solutions

• Proficiency in Snowflake would be added advantage.

• Excellent written and verbal communication skills",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
Rently,Data Engineer,"Must have:
• Overall experience of 4+ years
• Experience in AWS Cloud services & solutions
• Experience working with enterprise data warehouse
• Experience as an ETL/ELT Developer using various ETL/ELT tools
• Experience in SQL/NoSQL/DWH databases across SQL DB, Managed instance & Data warehouse
• Experience in AWS platform services such as S3, EMR, RedShift, Glue, Kinesis, OpenSearch, Athena, QuickSight
• Working on SnowFlake and pipeline tools like Fivetran or Matillion.
• Experience in Apache Spark, Databricks
• Experience in creating data structures optimized for storage and various query patterns like Parquet
• Experience in building secured visualization reports and dashboards with access controls
• Experience in working in an Agile SDLC methodology
• Experience in DevOps Services using Git Repos, deployment artifacts and release packages for Test & production environment
• Experience in building end-end scalable data solutions, from sourcing raw data, and transforming data to producing analytics reports
• Should have experience in developing a complete DWH ETL lifecycle
• Experience in Data Analysis, Data Modelling and Data Mart design
• Should have experience in developing ETL processes - ETL control tables, error logging, auditing, data quality, etc. - using ETL tools.
• Experience in Data Integrator Scripts, workflows, Dataflow, Data stores, Transforms, and Functions.
• Should have worked on at least 2 end-to-end implementations
• Worked on Change Data Capture on both SOURCE and TARGET levels and a good understanding of Slowly changing Dimension (SCD)
• Should be able to implement reusability, parameterization, workflow design
• Should have experience in interacting with customers in understanding business requirement documents and translating them into ETL specifications and Low/High-level design documents
• Strong database development skills like complex SQL queries, complex stored procedures
• Able to work in Agile Framework Should have exposure to Scrum meetings.

Good to have:
• Exposure to other ETL/ELT, DWT technologies
• Hands-on with Data visualization tools like Power BI, Tableau, Qlik, QuickSight etc.
• Exposure to Python on ETL and Data Visualization libraries

Additional Skills:
• Good Communication Skills.
• Able to deliver independently.
• Team player.

Professional Commitment:

Being a product based company we heavily invest in developing functional/ technology/ leadership skill sets in our team members. So candidates who are willing to commit to a minimum of 2 years need to apply.",Coimbatore,True,False,True,False,False,False,False,False,True,True,False,True,True,False,False,True
Embibe,Data Engineer,"Requirements
• Should have knowledge in Coding: Preferred Java.
• Good to have - ( Python / Scala).
• Should have Knowledge in Technologies: Spark, spark streaming, scala spark/py spark.
• Good to have Knowledge of Messaging buses like Apache Kafka/ Rabbit MQ.
• Good to have Knowledge of NoSQL databases like - MongoDB, ElasticSearch, Cassandra, Hive, Impala, ADX, Synapse, Redshift, Athena, etc.
• Should have Knowledge in building Microservices with Spring boot/ Fast-Api.",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Concentrix,Data Engineer Big Data,"Job Title:

Data Engineer Big Data

Job Description

Data Engineer Big Data

Keywords: RDBMS SQL & Spark/Hive SQL, Performance tuning, Modeling Design

Job Description

Develops and maintains scalable data pipelines

Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.

Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.

Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.

Designs data integrations and data quality framework.

Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.

Qualification:

Bachelor's Degree in Computer Science or related field

3+ years of work experience

Strong experience in SQL ( include complex SQL query , SQL performance tuning , Index , Lock )

Experience with schema design and dimensional data modeling

Experience with Hive SQL , Spark(Spark SQL, DataFrame)

Experience with near-realtime data warehouse (10-30 mins level)

Experience with data quality check

Experience in Java or Python or Shell Script

#CSS

Location:

India Bangalore - Divyashree

Language Requirements:

Time Type:

Full time

If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents

R1357932",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
CirrusLabs,AWS Data Engineer / Data Engineer / Lead AWS Data Engineer,"Job Role: Aws Data Engineer

Location: Bangalore / Hyderabad

Type: Fulltime

JOB DESCRIPTION

Data Engineer/Operational Support with Snowflake

Must-Have:
• 7+ years of experience in an Oracle/Informatica environment with knowledge of views, packages, stored procedures, functions, constraints, cursors, indexes, and table partitions.
• 7+ years of experience with an ETL tool such as SSIS, Azure Data Factory, or AWS Glue
• Strong background in a data warehouse, data management, and data analytics
• Monitor ETL production batch schedules to meet predefined SLAs.
• Resolve functional and system errors as identified by Business Partners
• Coordinate activity between Business Units and EIS to drive open action items to closure.
• Work with other technical teams to resolve infrastructure-related problems.
• Maintain a good relationship with other technology teams within the client enterprise.
• Generate, Control, and Resolve incident tickets relating to Production batch and Data availability issues.
• Serve as senior contact for production support issues and escalations.
• Enterprise L3 support to resolve production support issues in a timely manner.
• Candidate is expected to exude a take-charge attitude toward problems and thrive for excellence. This is a hands-on, delivery-focused role.
• Attempt to isolate, reproduce, and resolve problems using available systems and tools, and investigate potential workarounds for verified defects.
• Participate in the creation of Knowledge Base articles, solutions, and other related support collateral.
• To interface with Subject Matter Experts, where the problem cannot be resolved at a frontline support level.
• Good to have:
• Excellent written and verbal communication skills
• Detail-oriented; Analytical with problem-solving abilities",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Tata Technologies,Data Engineer,"Job Title : Data Engineer

Job Location : Thane(Mumbai)

Domain Knowledge:

Should be capable of carrying out the following operations on the data with any application.

• Familiarity with data loading tools like Flume, Sqoop.

• Analytical and problem-solving skills applicable to Big Data domain

• Proven understanding with Hadoop, PySpark, Hive, Hadoop

• Good aptitude in multi-threading and concurrency concepts",Thane,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Trademo,Data Engineer,"Position : Data engineer - Full Time and Intern

Role: Python/ Data scraping with Algo and Data structures with Automation

Experience: 0-1 years

Location: Gurgaon (Work from office)

About Trademo

Trademo is a Global Supply Chain Intelligence SaaS Company, headquartered in Palo-Alto, CA. Trademo collects public and private data on global trade transactions, sanctioned parties, trade tariffs, ESG and other events using its proprietary algorithms. Trademo analyzes and performs advanced data processing on billions of data points using technologies like Graph databases, NLP and Machine Learning to build end-to-end visibility on Global Supply Chains. Trademo's vision is to build a single truth on global supply chains to help large and small businesses - discover new commerce opportunities, ensure compliance with trade regulations and build operational resilience. Trademo last closed its $12.5 mn Seed Round from marquee Silicon Valley VCs.

Trademo has been founded by Shalabh Singhal who is a third-time tech entrepreneur. Shalabh last co-founded ZipLoan. ZipLoan is a leading fintech lending startup in India. He earlier founded Credence, a Data-driven Digital Marketing, CRM Product and Sales Solutions company. Shalabh is an Alumni of Goldman Sachs, IIT BHU, CFA Institute USA and Stanford GSB SEED. Trademo has recently closed a $12.5 mn Seed round from some of the marquee investors in Silicon Valley.

Website

https://www.trademo.com

Location

Gurgaon (Work from office)

Technical Skills Required
• Python 3.6+ version, Pandas
• Scraping → Selenium, Beautiful Soap
• Knowledge NOSQL/MYSQL Database
• Knowledge how to tackle the problems with optimal Solution
• Individual contributor role with eagerness to learn new technologies - Elasticsearch , BigData etc.
• Knowledge of Basics DS and algorithms",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Pracemo Global Solutions,Data Engineer,"We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Responsibilities
• Analyze and organize raw data
• Build data systems and pipelines
• Evaluate business needs and objectives
• Interpret trends and patterns
• Conduct complex data analysis and report on results
• Prepare data for prescriptive and predictive modeling
• Build algorithms and prototypes
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Requirements And Skills
• Previous experience as a data engineer or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills
• Degree in Computer Science, IT, or similar field; a Master’s is a plus
• Data engineering certification (e.g IBM Certified Data Engineer) is a plus
• Self-motivated with a results-driven approach
• Aptitude in delivering attractive presentations
• High school degree
Skills: data warehousing,etl,sql,python,java,hadoop,hive,spark,nosql databases,cloud computing,aws,azure,gcp,data modeling,data mining,data visualization,communication,project management,data lake,data quality,data architecture",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Thompsons HR Consulting LLP,Lead Data Engineer,"We are looking for Lead Data Engineer

with Strong experience in Python, Development, Business Intelligence (BI tools), AWS, Mysql

Experience: 10+ years

It is a Remote opportunity.

If interested, please share your resume at deepika.ashok@thompsonshr.com",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Anonymous,Data Engineer - Partime / Freelance,"Required skills: Pyspark, AWS-cloud, Hive
Good to have: streamsets, CICD
Experience: 2 - 5yr
Timing : 8pm to 12am on weekdays",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
New Era India,Data Engineer/Sr. Data Engineer/Lead Data Engineer - Data Axle,"Data Engineer / Sr. Data Engineer / Lead Data Engineer (Pune)

About Data Axle

Data Axle Inc. has been an industry leader in data, marketing solutions, sales and research for over 45 years in the USA. Data Axle has set up a strategic global center of excellence in Pune. This center delivers mission critical data services to its global customers powered by its proprietary cloud-based technology platform and by leveraging proprietary business & consumer databases. Data Axle is headquartered in Dallas, TX, USA.

Roles And Responsibilities
• Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
• Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
• Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency.
• Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
• Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning.
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

Basic Qualifications
• 3 to 12 years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
• Demonstrated strength in data modeling, ETL development, and data warehousing.
• Experience using big data processing technology using Spark.
• Knowledge of data management fundamentals and data storage principles
• Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, Power BI etc.)

Preferred Qualifications
• Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
• Experience working with AWS big data technologies (Redshift, S3, EMR, Spark)
• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience working with distributed systems as it pertains to data storage and computing.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.",Pune,False,False,True,False,False,False,False,True,True,True,False,False,True,False,False,False
Quadratyx,Lead Data Engineer,"About Quadratyx

We are a product-centric insight & automation services company globally. We help the world’s organizations make better & faster decisions using the power of insight & intelligent automation. We build and operationalize their next-gen strategy, through Big Data, Artificial Intelligence, Machine Learning, Unstructured Data Processing and Advanced Analytics. Quadratyx can boast of more extensive experience in data sciences & analytics than most other companies in India. We firmly believe in Excellence Everywhere.

Purpose of the Job/ Role:

As a Lead Data Engineer, your work is a combination of hands-on contribution, customer engagement and technical team management. Overall, you’ll design, architect, deploy and maintain big data solutions.

Key Requisites:

• Expertise in Data structures and algorithms.

• Technical management across the full life cycle of big data (Hadoop) projects from requirement gathering and analysis to platform selection, design of the architecture and deployment.

• Scaling of cloud-based infrastructure.

• Collaborating with business consultants, data scientists, engineers and developers to develop data solutions.

• Leading and mentoring a team of data engineers.

• Hands-on experience on test-driven development (TDD).

• Expertise in No SQL like Mongo, Cassandra etc., preferred is Mongo and strong knowledge of relational database.

• Good knowledge of Kafka and Spark Streaming internal architecture.

• Good knowledge of any Application Servers.

• Extensive knowledge on big data platforms like Hadoop; Hortonworks etc.

• Knowledge of data ingestion and integration on cloud services such as AWS; Google Cloud; Azure etc.

Skills/ Competencies Required

Technical Skills

• Strong expertise (9 or more out of 10) in at least one modern programming language, like Python, Java.

• Clear end-to-end experience in designing, programming, implementing large software systems.

• Passion and analytical abilities to solve complex problems.

Soft Skills

• Always speaking your mind freely.

• Communicating ideas clearly in talking and writing, integrity to never copy or plagiarize intellectual property of others.

• Exercising discretion and independent judgment where needed in performing duties; not needing micro-management, maintaining high professional standards.

Academic Qualifications & Experience Required

Required Educational Qualification & Relevant Experience

• Bachelor’s or Master’s in Computer Science, Computer Engineering, or related discipline from a well-known institute.

• Minimum 7 - 10 years of work experience as a developer in an IT organization (preferably Analytics /

Big Data/ Data Science / AI background.

Quadratyx is an equal opportunity employer - we will never differentiate candidates based on religion, caste, gender, language, disabilities or ethnic group.",Hyderabad,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Persistent Systems,Data Engineer (Immediate joiner),"About Persistent

We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above.

We are experiencing tremendous growth, with $701.1 million in trailing 12-month revenue, representing 29.8% year-over-year growth. Along with that growth, we onboarded over 4,500 new employees in the past year, bringing our total employee count to over 16,500 people located in 18 countries across the globe.

At Persistent, our values are more than a list of ideals to improve our corporate image. We’re dedicated to building an inclusive culture that reflects what’s important to our employees and is based on what they value. As a result, 95% of our employees approve of the CEO and 83% recommend working at Persistent to a friend.

For more details please login to www.persistent.com

About Position
• 4+ years of strong technology experience in the field of transactional data and analytics systems
• Lead client conversations and data discovery sessions
• Should understand and be able to command architecture design for transactional and analytics systems.
• Strong SQL skills
• Hands on experience in building end to end data / orchestration pipelines using Python.
• Cloud Experience- Should have experience with any cloud data products (AWS, GCP, Azure)
• Experience in Agile Methodologies
• Familiarity with source repositories (Git, BitBucket etc.)
• Excellent communication skills",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Niftel Resources,Senior Data Engineer,"Responsibilities:

 Design and build reusable components, frameworks and libraries at scale to support analytics

products

 Design and implement product features in collaboration with business and Technology

stakeholders

 Anticipate, identify and solve issues concerning data management to improve data quality

 Clean, prepare and optimize data at scale for ingestion and consumption

 Drive the implementation of new data management projects and re-structure of the current data

architecture

 Implement complex automated workflows and routines using workflow scheduling tools

 Build continuous integration, test-driven development and production deployment frameworks

 Drive collaborative reviews of design, code, test plans and dataset implementation performed by

other data engineers in support of maintaining data engineering standards

 Analyze and profile data for the purpose of designing scalable solutions

 Troubleshoot complex data issues and perform root cause analysis to proactively resolve product

and operational issues

 Mentor and develop other data engineers in adopting best practices

Qualifications:

Primary skillset:

 Experience working with distributed technology tools for developing Batch and

Streaming pipelines using SQL, Spark, Python [3+ years], Airflow [2+ years], Scala [1+

years].

 Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc.

 Able to quickly pick up new programming languages, technologies, and frameworks.

 Strong skills building positive relationships across Product and Engineering.

 Able to influence and communicate effectively, both verbally and written, with team members and

business stakeholders

 Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed

Spark jobs, build Docker images, etc.

 Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture

Good to have:

 Experience working with Data platforms, including EMR, Airflow, Databricks (Data Engineering &

Delta Lake components, and Lakehouse Medallion architecture), etc.

 Experience working in Agile and Scrum development process

 Experience in EMR/ EC2, Databricks etc.

 Experience working with Data warehousing tools, including SQL database, Presto, and

Snowflake

 Experience architecting data product in Streaming, Server less and Microservices Architecture

and platform.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,True,True
"Giant Eagle, Inc.",Senior Data Engineer,"Job Summary

As a Sr Data Engineer on the Marketing Data Platforms team, you will be working on a team to bring customer-centric personalization to life. In this role, you will be empowered to develop data solutions in support of analytics, data science, and business partners to understand capability requirements and develop data solutions based on priorities. This leading technical and architecture role will collaborate with product managers, architects, technology teams, analysts, marketing operations specialists, and monetization business partners to understand capabilities and that will be brought to life for Giant Eagle’s 4M+ customers. The ideal candidate will have experience within multiple technology platforms (e. g. GCP, Engagement Platforms, Customer Data Platform, Ad-Tech, etc.) while providing the vision and design for integrating customer data. Additional key skills and qualifications below

Job Description
• Primary Job Responsibilities:
• 5+ years of relevant technical experience working with various data engineering methodologies such as data integration and data pipelines (ETL/ELT) to activate against data at scale.
• 3+ years of experience of data modeling for analytic projects activities that include design, curation, and management of large datasets
• 3+ years of experience adeveloping on big data technologies with Spark and Hive, preferably leveraging such as DataBricks, Juypter notebooks, or GCP, AWS, and Azure equivalent technology.
• 2+ years of experience data solution design for data engineering pipelines
• Strong Experience building event driven systems using cloud technology: storage, Pub/Sub, cloud functions, API’s, and DataProc
• Expertise with databases experience such as BigQuery, Snowflake, and Synapse designing schema and dimensional data modeling
• Experience leveraging RESTful web services to collect and publish data.
• Experience in software engineering development and testing life cycles using but not limited to Python, R, Linux, Java, JavaScript, Lambda, and SQL programming
• Bachelor's degree in Computer Science, Mathematics, or other technical field or equivalent work experience. Advanced degree a plus
• Experience with Retail Media Networks and Ad Tech preferred

Role Requirements:
• Architect, develop and implement end-to-end complex data projects and technical solutions through translating business requirements into technical solutions and data-flow architectures.
• Architect, build and automate data pipelines that clean, transform, and aggregate unorganized data into data sources that are ready for analysis.
• Use expertise to apply various analytic methods to discover and interpret information about customer behavior from multiple data sources to implement analytics solutions
• Use expertise in database design to implement, operate stable and scalable dataflows from multiple marketing platforms into a cloud data lake for Ad Tech
• Experience building data visualization tools Tableau, PowerBI, and Looker with data modeling and Looker ML preferred
• Design, implement and deploy data applications and mechanisms using big data technology
• Provides subject matter expertise for multiple projects concurrently through all phases of the development lifecycle.
• Develop, enhance, govern, and administer for data platform to: collect data, transform, enrich, unify, segment, and integrate data
• Strong adherence data ethics rules around PII data sets
• Work collaboratively with IT teams, Performance Marketing team, and business leaders to ensure actionable is provided key stakeholders
• Research and analyze customer behavior data to improve customer experience
• Experience with agile or other rapid application development methods a plus
• Retail industry experience a plus

About The Company

Since our founding in 1931, Giant Eagle, Inc. has evolved into one of the top 40 largest private corporations in the U. S. and one of the country’s largest food retailers and distributors. With more than 37,000 Team Members and $9.7 billion in revenue, we are committed to investing in people, technology, and data to elevate our customer’s experience across multiple touchpoints. It helps us follow on our commitment to serving others and improving our communities.

About Giant Eagle Bangalore

The Giant Eagle GCC in Bangalore is our global capability center. Our team of more than 370 members at the GCC enables us to expand internal capabilities in the areas such as data analytics, merchandising and eCommerce, quality engineering, and automation to generate insights for faster decision-making and helping us accelerate our business strategy. Our team in India plays a pivotal role in helping the company transition to new ways of working by redefining the food and grocery shopping experience for over 4.6 million customers.

About Us

At Giant Eagle Inc., we’re more than just food, fuel and convenience. We’re one giant family of diverse and talented Team Members. Our people are the heart and soul of our company. It’s why we strive to create a nurturing environment that offers countless career opportunities to grow. Deep caring and solid family values are what makes us one of the top work places for jobs in the Greater Pittsburgh, Cleveland, Columbus and Indianapolis Areas. From our Warehouses to our GetGo’s, our grocery Stores through our Corporate home office, we are working together to put food on shoppers' tables and smiles on their faces. We’re always searching for the best Team Members to welcome to our family. We invite you to join our Giant Eagle family. Come start a lasting career with us.",,True,False,True,True,False,False,True,False,False,True,False,False,False,True,False,True
TMRW House of Brands,Data Engineer-III,"Responsibilties:

Create, implement, and operate the strategy for robust and scalable data pipelines for business intelligence and machine learning.

Develop and maintain core data framework and key infrastructures

Create and support the ETL pipeline to get the data flowing correctly from the existing and new sources to our data warehouse.

Data Warehouse design and data modeling for efficient and cost-effective reporting

Collaborate with data analysts, data scientists, and other data consumers within the business to manage the data warehouse table structure and optimize it for reporting.

Constantly striving to improve software development process and team productivity

Define and implement Data Governance processes related to data discovery, lineage, access control, and quality assurance

Perform code reviews and QA data imported by various processes

Qualifications

6-10 years of experience.

At least 3+ years of experience in data engineering and data infrastructure space on any of the big data technologies: Hive, Spark, Pyspark(Batch and Streaming), Airflow, and Delta Lake.

Experience in product-based companies or startups.

Strong understanding of data warehousing concepts and the data ecosystem.

Strong Design/Architecture experience architecting, developing, and maintaining solutions in AWS.

Experience building data pipelines and managing the pipelines after they’re deployed.

Experience with building data pipelines from business applications using APIs.

Previous experience in Databricks is a big plus.

Understanding of Dev Ops would be preferable though not a must

Working knowledge of BI Tools like Metabase, and Power BI is plus

Experience in architecting systems for data access is a major plus.",Bengaluru,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False
Impetus,GCP Data Engineer,"Qualification
• The candidate should have extensive production experience (3-5 Years ) in GCP, Other cloud experience would be a strong bonus.
• Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
• Exposure to enterprise application development is a must

Role
• 6-10 years of IT experience range is preferred.
• Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.
• Strong experience in Big Data technologies – Hadoop, Sqoop, Hive and Spark including DevOPs.
• Good hands on expertise on either Python or Java programming.
• Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
• Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
• Ability to drive the deployment of the customers’ workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
• Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
• Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
• Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
• Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.",इन्दौर,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Newell Brands,Cloud Data Engineer,"Job Title: Cloud Data Engineer

Report To: Sr. Manager, Data Engineering

Job Location: Guindy, Chennai, India

Job Duties
• Participates in the full lifecycle of cloud data architecture (Preferably Azure cloud) from gathering, understanding end-user analytics and reporting needs.
• Migrate On-Prem applications and build CI/CD pipeline in Cloud platform.
• Design, develop, test, and implement on Cloud platform (Ingestion, Transformation and export pipelines that are reliable and performant) .
• Ensures best practices are followed and business objectives are achieved by focusing on process improvements.
• Quickly adapt by learning and recommending new technologies and trends.
• Develop and Test Data engineering related activities on Cloud Data Platform.
• Work with dynamic tools within a BI/reporting environment.

Job Requirements
• B.E/B.Tech, M.Sc/MCA.
• 5+ years experience in Rapid development environment, preferably within an analytics environment.
• 3+ years experience with Cloud experience (Preferably Azure cloud but not mandatory).
• DB : T-SQL, SQL Scripts, Queries, Stored Procedures, Functions and Triggers
• Language : Python / C# or Scala
• Cloud: Azure / AWS / Google cloud
• Frameworks: Cloud ETL/ELT framework

Preferred
• Azure Data Factory, Azure Synapse Analytics, Azure SQL, Azure Data lakes, Azure Data bricks, Airflow and Power BI
• Data warehousing principles and frameworks.
• Knowledge in Cloud DevOps and CI/CD pipelines would be an added advantage.

Newell Brands (NASDAQ: NWL) is a leading global consumer goods company with a strong portfolio of well-known brands, including Rubbermaid, FoodSaver, Calphalon, Sistema, Sharpie, Paper Mate, Dymo, EXPO, Elmer's, Yankee Candle, Graco, NUK, Rubbermaid Commercial Products, Spontex, Coleman, Campingaz, Oster, Sunbeam and Mr. Coffee. Newell Brands' beloved, planet friendly brands enhance and brighten consumers lives at home and outside by creating moments of joy, building confidence and providing peace of mind.",Chennai,True,False,True,False,False,False,False,False,True,False,False,False,False,False,True,False
Zupee,Lead Data Engineer,"About Zupee

Zupee is India’s fastest growing Technology backed Behavioral Science company. We are innovating Skill-Based Gaming with a mission to become the most trusted and responsible entertainment company in the world. We have been constantly focusing on innovation of indigenous games to entertain the mass.

Our strategy is to invest in our people & user experience to drive profitable growth and become the market leader in our space. We have been experiencing phenomenal growth since inception and running profitable at EBT level since Q3, 2020. We have closed Series B funding at $102 million, at a valuation $600 million.

The company also announced a partnership with Reliance Jio Platforms, post which Zupee games will distribute its content across all customers using Jio phones. The partnership now gives Zupee the biggest reach of all gaming companies in India, transforming it from a fast-growing startup to a firm contender for the biggest gaming studio in India.

About The Job

Lead Data Engineer

We are looking for someone to develop the next generation of our Data platform

collaborating across functions like product, marketing design, growth, strategy, customer

experience and technology.

Core Responsibilities

●Understand, implement and automate ETL and data pipelines with up-to-date

industry standards

●Hands-on involvement in the design, development and implementation of optimal and

scalable AWS services

What are we looking for?

●S/he must have experience in Python

●S/he must have experience in Big Data – Spark, Hadoop, Hive, HBase and Presto

●S/he must have experience in Data Warehousing

●S/he must have experience in building reliable and scalable ETL pipelines

Qualifications and Skills

●6-12 years of professional experience in data engineering profile

●BS or MS in Computer Science or similar Engineering stream

●Hands-on experience in data warehousing tools

●Knowledge of distributed systems such as Hadoop, Hive, Spark and Kafka etc.

●Experience with AWS services (EC2, RDS, S3, Athena, data pipeline/glue, lambda, dynamodb etc.
•",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
UST Product Engineering,Data Engineer,"Job Description :

- 4-8 Years experience in data warehousing , ETL processes, and data analytics.

- Good experience in developing, maintaining, and testing infrastructures for data generation, verification and transformation.

- Good understanding database concepts (SQL, Cloud DBs)

- Strong SQL query, profiling and troubleshooting skills

- Good understanding AWS Data related concepts like big data, big query etc.

- Basic understanding AWS (or supported) ETL tools - Glue, Airflow etc etc. would be an added advantage

- Good Understanding of python programming

- Basic understanding of programming language like C# or similar

- Basic knowledge of working in scrum/agile teams and tools like JIRA, confluence etc.",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.

Apply for this job",Mumbai,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
ThousandEyes,Cloud Application and Data Engineer,"Cloud Application and Data Engineer

Who We Are

The name ThousandEyes was born from two big ideas: the power to see what’s not ordinarily possible, and the ability to collect intelligence from vantage points as diverse and global as the Internet. As organizations depend on cloud services, the Internet has become their defacto network connecting cloud applications to users. Our Internet and cloud intelligence platform is like a ‘Google maps of the Internet’, providing the only collectively powered view of digital experiences end-to-end. We enable our customers made up of the world’s largest and fastest-growing brands, to identify problems before they impact revenue, brand reputation, or employee productivity.

In August 2020, Cisco Systems completed the acquisition of ThousandEyes, which now forms the ThousandEyes Business Unit within Cisco’s Network Services Business Group,and is a foundational component of Cisco’s growing Observability business.About The Team

Digital experiences rely on a vast ecosystem of ISPs, cloud providers, SaaS applications, individual configurations, unique devices, and many other external services that are critically dependent on the Internet. Trying to identify the root cause of a problem or a deviation from normal is like finding a needle in a haystack. This leads to long downtimes and poor customer or employee experience.

The AI Analytics team at ThousandEyes is leveraging machine learning at scale, while working across several different business units, to our help customers answer tough questions like:
• What is normal in my network and how do I catch deviations from this normal?
• How do I understand the root cause of a problem in my network or application stack?
• How do I ensure that devices joining my network are who they say they are?
• How do I know when my networking gear is about to break?

The goal of the AI Analytics Team is to leverage different machine learning techniques to deliver actionable insights for our customers to solve real world problems in their complex environments.

What You’ll Do

You will be part of our data and platform team. A worldwide distributed team responsible for data collection, ingestion, processing, and quality. You will play an important role in helping to deliver new ML powered features to our customers as well as monitoring and improving the existing features for performance and quality. You will work in the AI Cloud hosted on AWS with Python, Go, Spark, Hive, Open Search, and other cutting-edge technologies.

Responsibilities
• Collaborating closely with ML engineer to bring new features to production.
• Create and maintain an optimal data pipeline architecture.
• Contribute and operate data quality tooling.
• Monitor and optimize compute and query performances.
• Troubleshoot and debug issues across our applications and services.

Who you are

Agile, pragmatic and hardworking. You also love to interact with data scientists, machine learning engineers and software engineers to develop pipelines that scale seamlessly at huge volumes of data. You love technology, innovation and building products at scale.

You hold a degree in computer science, or a related field and you can demonstrate a consistent track record in the following areas:
• At least 4 years of software development experience.
• 2 years of experience building and developing data-intensive systems at industrial scale.
• Dimensional data modeling and schema design for both SQL and NoSQL databases.
• Prior exposure to data science, machine learning or statistics is a plus.
• Previous experience developing applications running on a public cloud infrastructure is a plus.
• Strong Communication and documentation skills in English.
• Strong sense of ownership, drive, attention to detail and ability to work in a distributed team.

We are looking for candidates based in Bangalore to work hybrid

Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.

Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do –you can’t put us in a box! But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. So, you have colourful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us.

We recognize that diverse teams make the strongest teams, and we encourage people from all backgrounds to apply.

Cisco COVID-19 Vaccination Requirements

The health and safety of Cisco's employees, customers, and partners is a top priority. Our goal is to protect and mitigate the spread of COVID-19 infection for strong business resiliency during the pandemic. Therefore, Cisco may require new hires to be fully vaccinated against COVID-19 if the role requires business-related travel, meeting with customers/partners (including visiting third-party sites on behalf of Cisco), attending trade events, and Cisco office entry, unless otherwise prohibited by applicable law, and in countries where COVID-19 vaccination is legally required. The company will consider legally required accommodations/exceptions for medical, religious, and other reasons as per the requirements of the role and in accordance with applicable law. Additional information will be provided to candidates about the requirements and accommodation process at the offer time based on region.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Verizon,Principal Engineer - Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

You will be expected to architect solutions for business projects, work with enterprise architects to align application & system architecture to enterprise strategy and deliver individually and/or with the help of a team. You need to have passion to learn and educate fellow associates/subordinates and guide them to follow best practices. Principal consultant to the team that develops, maintains and enhances the service delivery and management for NS applications
• Architecting/Developing solutions for the application which deals with big data platforms.
• Engaging with Enterprise Architects on HLAs and defining new solutions that adhere to big data volume processing.
• Driving a Culture of Innovation: Champion a culture of innovation and drive as an example.
• Supporting customers with major platform issues and coordinating triage efforts to solve them.
• Identifying and aligning project requirements and conducting impact analysis.
• Working closely with the business team, and other internal IT teams to deliver projects on time.
• Preparing presentations and reports to internal and external customers, as well as internal Executives.
• Evaluating various new technical products based on changing business needs and making product recommendations to management keeping in mind the architecture of the entire list of applications supported.
• Providing technical leadership and business-related subject matter expertise on large, highly complex projects.
• Guiding the team on best practices for efficient and streamlined delivery of software to production. Guiding teams on maintaining security posture and code quality of applications keeping the tech debt in check.
• Identifying chronic production issues, pain points of customers by evaluating feedback and monitoring the NPS to maintain it above the required threshold.
• Working with Quality Assurance, UAT & Production Support teams to support releases, troubleshoot progression/regression issues, integration & E2E testing and implement deliverables as per the targeted timelines.
• Working with infrastructure teams to implement DevOps capabilities that help streamline the CICD process. Leverage innovative technologies to build proof-of-concepts that help build customer experiences, reduce pain points in the current experience, and provide a delight factor to customers.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You view technology through a lens of making things better and more effective. Understanding and building continual improvements to the digital value chain is something you flourish with. You enjoy the process of solving complex issues while empowering the team around you to do the same.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Experience in Hadoop, Hive, Pyspark , Spark Scala, PIG, Java, GCP, AWS, CICD (Jenkins/ Gitlab).
• Experience in Big query, Composer, Cloud Functions & Java script.
• Experience with any of RDBMS, Druid and MongoDB.
• Experience in Devops & automation.
• Experience in Docker/K8s & SRE Practice.
• Experience in Agile & SAFe methodologies.

Even better if you have one or more of the following:
• A Master's degree.
• Ability to design products which can scale up for large volumes of data.
• Knowledge in Wireless Domain.
• Knowledge of Security Vulnerabilities.
• Strong written and verbal communication skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False
lululemon India Tech Hub,Data Engineer - SQL & Python,"We are looking to hire dynamic Data Engineers for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.

Responsibilities
• Uses structured tools for analysis and presentation of concepts and models to enhance the BRD
• Develop, maintain and deliver training materials to the supply chain end-users
• Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs
• Support day to day reporting needs where required
• Support production issues as relate to application functionality and integrations
• Excellent spoken and written communication skills (verbal and non-verbal)
• Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)
• Solid scripting capability for analysis and reporting (ANSI SQL)
• Solid experience in RDBMS and NoSql technologies
• Strong analytical skills to support BAs.
• Strong problem-solving skills (Math skills required for data modeling)
• Ability to work as an integration / data engineer.
• Ability to manage and complete multiple tasks within tight deadlines
• Possess expert level understanding of software development practices and project life cycles.
• Working experience with Java batch spring boot/ python.
• Working Experience with cloud-native technologies
• Must have: Working experience in dealing with big data and data manipulation.
• Desired: Familiarity with Retail planning / merchandising systems/ supply chain.
• Desired: Familiarity with DevOps practices like CICD pipeline
• Desired: Retail experience is a plus. (fashion retail experience would be ideal)
• Must Have: Working experience with cloud platforms namely AWS
• Must Have: Working experience with large data sets (at least 80 – 100 GB data)

Requirements
• name : lululemon India Tech Hub
• location : Bengaluru, IN
• experience : 5 - 8 years
• Primary Skills: SQL or RDBMS or NoSQL,Python,AWS,Springboot,ETL",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Splunk,Senior Data Engineer - 27505,"The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will work as part of an evolving Enterprise Data Management (EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.

The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible for building and scaling the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineers and Infrastructure teams plus other data roles, including Data Governance, Quality, and Architecture Stewards). Strong technical experience within enterprise software is essential.

Responsibilities:
• Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
• Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
• Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
• Identify and document standard methodologies, standards, and architecture guidelines
• Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
• Identify ways to improve Data Reliability, Data Efficiency and Data Quality

Required Qualifications, Skills & Experience:
• 7+ years of data engineering related experience such as data analysis, data modeling, and data integration.
• Experience with Sales Operations, Partner Operations and customer success business processes and applications
• Experience in custom ETL design, implementation, and maintenance
• Strong knowledge of programming languages (e.g. Python and Object Oriented Programming)
• Hands-on experience with SQL database design
• Experience working on CI/CD processes and source control tools such as GitHub and GitLab
• Experience working in Snowflake and relational databases
• Extensive hands-on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake
• Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
• Experience with or exposure to data governance & quality principles and practices
• Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
• Passionate about data solutions, technologies, and frameworks
• Experience in Data Visualization tools such as Tableau

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Knowledge of enterprise systems such as Salesforce, Workday, SAP etc.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
Reverate,Senior Data Engineer - Remote,"Reverate Tech is a product and service-based start-up, working with International Clients. Our services include Data Engineering, Web Development, BI/Data Warehousing, Enterprise Application Implementation (ERP/CRM), and NetSuite. Our product portfolio has business apps in the domain of ERP, Auto Service, and Personal Safety.

This is an exciting opportunity to work as Senior Data Engineer for our client SellerX.

SellerX is the 3rd fastest growing company in the EU evaluated at more than 1 Billion Euros. It has an ambitious goal: to become a leading global acquirer and operator of a new generation of eCommerce businesses.

Your Job:
• You are responsible for all types of data management processes (collection, storage, cleansing, preparation, maintenance, accumulation, transfer to business reporting).
• You optimize and develop existing and new data warehouse applications using tools for data ingestion and data modeling
• You design and document new data models and best-practice solutions.
• You are responsible for prototyping and implementing new ETL jobs and modeling approaches.
• As a data engineer, you will deal with python programs and their configurations in order to create or improve automated data engineering tasks.
• In addition to technical project management, you advise our other tech teams in Data Management aspects.
• Ensuring data security (e.g. encryption) and improving the backup strategy.
• You work hand in hand with data architects, data analysts, and data scientists.
• You ensure that quality, stability, and robustness along the entire process chain meet our high standards.

Your Background:
• You have a bachelor's degree with a focus on software engineering.
• 5+ years of experience in data engineering.
• Strong with Algorithms and have worked on scaling pipelines/solutions
• Hands-on experience with data ingestion tools like Fivetran, Daton, Stitch, or Data Virtuality.
• Hands-on experience with ETL and orchestration tools like Apache Airflow or similar.
• Object-oriented Python programming is more than just a plus.
• Expert knowledge in the areas of data modeling and ETL processes on the SQL level (e.g. using DBT), as well as experience working with REST APIs, is beneficial.
• DevOps experience
• In addition to your ""hands-on"" mentality, you score points with a high technical affinity and a strong analytical mindset.
• Your working standards do not suffer in terms of quality, even in hectic times.

Benefits:
• Compensation: up to 40 LPA
• 100% remote-working;
• Flexible working hours;
• Development of your personal strengths in a dynamic environment;
• An attractive and varied job with a high level of personal responsibility;
• A collegial togetherness and a modern management style/startup;

Interested? Join us and start your learning and growth journey.

Reverate focuses on Software Engineering. Their company has offices in India. They have a small team that's between 11-50 employees.

You can view their website at https://reverate.tech/",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
"6221, Roche",Senior Data Engineer,"The Position

Roche sequencing solution is developing the next generation sequencing based on nanopore technology. This has the potential to make sequencing based diagnostics cheaper, faster and more accurate enabling precision medicine and early diagnosis of many diseases improving the health outcome.

As part of Data Science Automation group, you will get to work on key software technologies enabling research and development of sequencer. You will solve complex problems related to processing terabytes of data coming out sequencer and deriving useful insights from the data. This requires massively parallel computation locally on GPU as well as in the cloud. You will gain exposure to latest and greatest in data engineering and data pipeline tools and technologies. You will also work with advanced data visualization problems involving millions of data points.

You also will get to collaborate with multidisciplinary team of scientists and engineers working in fields ranging from protein engineering, bio chemistry, biophysics, stats modeling, bioinformatics and deep learning.

If you are excited to become part of the next generation sequencing research and development and revolutionize the healthcare, we have a rare opportunity for you to come and work with us.

We need an experienced Data/Workflow Engineer with a strong background in designing and developing highly scalable data management solutions and workflow pipelines. You will work across a variety of problems and application spaces involved in high availability systems, for data management and compute systems, at a very large scale. You will be working with a hardworking team of engineers and data-scientists who are passionate about building creative and novel solutions at the forefront of Sequencing research.

Required Qualifications:
• BS in CS or similar and 10+ years professional experience, or MS with 7+ years of experience in building highly scalable, performant software systems, in a Linux environment.
• Strong, hands on experience building and supporting Enterprise level Workflow management systems such as airflow, nexflow, kubeflow etc. Experience with building performant airflow pipelines with a large number of DAGs and dynamic DAGs is desirable.
• Working experience in deploying and managing airflow platforms, knowledge of Terraform, Kind, Helm etc. is a huge plus.
• At least 3+ years’ experience of developing solutions using container and cloud technologies. Preferably Kubernetes, Docker.
• Experience building with cloud native technologies (e.g. GCP, AWS), blob stores and knowledge of various data compression formats.
• Demonstrated skill with software development following current software engineering best practices using languages such as: Python, Java and BASH scripting.
• Have a strong understanding of modern software development practices and tools, including: version control systems (e.g., Git), issue trackers, and test frameworks.
• Experience building and using automation tools, CI/CD, unit testing.

You 'll go above and beyond our required requirements if you...
• Possess a PhD/MS in Computer Science, Computer Engineering, or another related, technical discipline.
• Have at least ten years of relevant experience in the development of software systems ideally in a Linux environment.
• Have experience using modern frontend and backend software frameworks for software applications.
• Knowledge of challenges involved in large-scale, high-availability data platforms. Experience with designing and implementing platforms providing secured access to large datasets.
• Have experience applying software expertise to full project lifecycles, including requirements analysis, design, implementation, and testing.

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,True,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False
Koch,Senior Data Engineer,"Description

Position Description/ Requirements

The Data Engineer will be a part of an international team that designs, develops and delivers Data Pipelines and Data Analytics Solutions for Koch Industries. Koch Industries is a privately held global organization with over 120,000 employees around the world, with subsidiaries involved in manufacturing, trading, and investments. Koch Global Solution India (KGSI) is being developed in India to extend its IT operations, as well as act as a hub for innovation in the IT function. As KSGI rapidly scales up its operations in India, it’s employees will get opportunities to carve out a career path for themselves within the organization. This role will have the opportunity to join on the ground floor and will play a critical part in helping build out the Koch Global Solution (KGS) over the next several years. Working closely with global colleagues would provide significant international exposure to the employees.

The Enterprise data and analytics team at Georgia Pacific is focused on creating an enterprise capability around Data Engineering Solutions for operational and commercial data as well as helping businesses develop, deploy, manage monitor Data Pipelines and Analytics solutions of manufacturing, operations, supply chain and other key areas.

A Day In The Life Could Include:

(job responsibilities)
• Partner/collaborate with Business stakeholders and build high-quality end-to-end data solutions.
• Build a data architecture for ingestion, processing, and surfacing of data for large-scale applications in the cloud (AWS/ Azure)
• Create and maintain optimal data pipeline architecture.
• Follow best practices of Agile and DevOps focusing on the delivering of high-quality products and providing the ongoing support to meet the customers' needs
• Implement processes for Continuous integration, Test automation and Deployment (CI/CD Pipelines)
• Provide quality documentation of your design (process and workflows) and implementation including experiment tracking / logs.
• Provide on-call support on an as-needed basis
• Handle support cases to ensure issues are recorded, tracked, resolved, and follow-ups finished in a timely manner.

What You Will Need To Bring With You:

(experience & education required)
• Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science). Master’s degrees preferred.
• 6+ years of IT experience.
• In depth knowledge of Data Engineering concepts and platforms - SQL based systems, Hadoop, Spark, Distributed computing, In-memory computing, real time processing, pub-sub, orchestration, etc.
• Expertise of building data pipelines using (Pyspark based) and Databricks utilising techniques in Azure or AWS.
• 4-5 years of experience in DevOps and CI/CD using tools like Git, Terraform, Jenkins, Ansible.
• 5+ year of experience in Data modeling, SQL, Data Warehouse skills are a MUST.
• A passion and fearlessness for learning new technologies and methods in the areas of Administration
• Ability to thrive in a team environment and juggle multiple priorities.
• Excellent written and verbal communication skills.

What Will Put You Ahead:

(experience & education preferred)
• In depth knowledge of entire suite of services in AWS/Azure Cloud Platform.
• Strong coding experience using Pyspark.
• Experience of designing and implementing ETL process using SSIS.
• Cloud Data Anaytics/Engineering certification.

Other Considerations:

(physical demands/ unusual working conditions)
• Some work may involve hours outside of normal KGS works hours.

Koch is proud to be an equal opportunity workplace",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Greetings from TCS !!!

TCS India presents excellent opportunities for IT professionals.

Role :- Data Engineer

Experience:- 7 to 10 years

Location- Bangalore / Mumbai / Chennai / Bhubaneswar

Required Technical Skill Set- Data Engineer – Big Data, Hadoop, Hive, Spark, Yarn

Must-Have:-

1. 4-8 Yrs of hands-on development experience

2. Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is mandatory.

3. Experience working with various data exchange formats (JSON, CSV, XML etc.).

4. Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred.

5. Excellent knowledge of SQL and Linux shell scripting.

6. Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)

Good-to-Have:-

1. Experience building real-time data pipelines using Kafka or spark streaming is preferred.

2. Exposure to Microsoft Azure (or other cloud) platforms is preferred.

3. Experience with Agile methodologies for project development.

4. Excellent diagnostic, analytical and problem-solving skills are preferred.

5. Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred

TCS Eligibility Criteria:
• BE/B.Tech/MCA/M.Sc/MS with minimum 3 years of relevant IT-experience post Qualification.
• B.Sc Graduates with minimum 4+ years of relevant IT-experience post qualification.
• Only Full Time courses would be considered.
• Consistent academic records class X onwards (Minimum 50%)
• Candidates who have attended TCS interview in the last 3 months need not apply.

Interested candidate can share their resumes with the mandatory details mentioned below.

Please update the details:

1. Total years of Exp:

2 Email ID :

3. Present Company:

4. Current & Preferred Location:

5. Mobile No.:

6. Current CTC:

7. Expected CTC:

8.Notice Period:

9: Working With TCS /CMC ( Direct Payroll) earlier (Yes/ NO):

10. No Of job change-

Interested candidate can share their resumes with hiba.fathima@tcs.com",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LTIMindtree,GCP Data Engineering POD Lead,"Primary Skill – GCP Data Engineering POD Lead

Total Exp – 3 to 14 Years

Notice Period – 0 to 30 Days

Job Location – Kolkata, Bangalore, Mumbai, Pune, Chennai, Hyderabad

Job Description:

Job Description:

TPrimary Skill – GCP

Secondary Skill – Python, Big query

Overall, more than 8+ Yrs of experience in Data Science Statistical Modeling and Projects to Develop and Deliver Data Science work Strong understanding of Machine Learning Statistics fundamentals Technology Skill Set Python R Pandas Scikit Learn R s

Desired Candidate Profile Technology & Engineering Expertise

• 5+ years of experience in implementing data solutions using GCP/SQL programming

• Proficient in dealing data access layer, RDBMS | NO-SQL.

• Experience in implementing and deploying Big data applications with GCP Big Data Services.

• Good to have SQL skills.

• Experience with different development methodologies (RUP | Scrum | XP) Soft skills

• Able to deal with diverse set of stakeholders

• Proficient in articulation, communication, and presentation

• High integrity

• Problem solving skills & learning attitude

• Team player Key Responsibilities

• Implement data solutions using GCP and need to be familiar in programming with SQL/python.

• Ensure clarity on NFR and implement these requirements.

• Work with Client Technical Manager by understanding customer’s landscape & their IT priorities

• Lead performance engineering and capacity planning exercises for databases",,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Arcadis,Azure Data Engineer,"ARCADIS is looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in “Everything Digital, Digital Everything”. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.

Technology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.

Role accountabilities:
• Possess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment
• You are excited about working with Azure Data Platform
• challenges while building the next wave of software engineering solutions
• Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform
• Leading the craftsmanship, security, availability, resilience, and scalability of your solutions
• Very strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.
• Strong experience in
• T-SQL, SSIS, SSAS, SSRS
• Azure Data Factory
• Azure Data Lake Store
• Azure Data Lake Analytics (Good to have, not mandatory)
• Azure SQL DB
• Azure SQL DW
• Azure Analysis Services, DAX
• Azure Data Bricks with Python/Scala
• Experience in building end to end solution using Azure data analytics platform.
• Experience in building generic framework solution which can be reused for upcoming similar use cases.
• Experience in building Azure data analytics solutions with DevOps (CI/CD) approach.
• Experience in using TFS, Azure Repos.
• Mentor peers to gain expertise on Azure data platform solutions skills.
• Experience in developing, maintaining, publishing, and supporting dashboards using Power BI.
• Strong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded

Qualifications & Experience:

Basic Qualifications:
• Bachelor in Engineering/Math/Statistics/Econometrics or related discipline
• Should have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.
• Preferred Qualifications:
• Master’s or Minor in Computer Science
• 3+ years of experience developing Data Engineering solutions
• Architecture, design experience with good knowledge of data model design & their implementation.

Why Become an Arcadian?

Our work with clients has a direct impact on people’s lives and on the planet. We make moving, living and belonging in cities safer, more resilient and more sustainable. By partnering with our clients as responsible custodians of our earth's resources, we can create a sustainable planet.

We continue to think of new ways to make positive impacts and create better experiences for people; data driven and digital solutions have become part of the Arcadis DNA. Working together with clients and using techniques like design thinking, we can get to the heart of our clients’ most pressing challenges and work together to solve them.

As a global business, we have committed to support five of the UN’s Sustainable Development Goals to ensure that our projects contribute to a better and more sustainable future for all. But it’s not just the work that we do on client projects that benefits communities and our planet. As a global business, we are committed to making a positive impact to society by supporting local communities where we operate.

To help protect our planet, we monitor and measure non-financial information to inform business decisions and reduce our own environmental impact as part of our commitment to be net zero carbon as a global company by 2030.

Our Commitment to Equality, Diversity, Inclusion & Belonging:

We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.

In accordance with the Colorado Equal Pay Transparency Rules:

Arcadis offers benefits for full time positions. These benefits include medical, dental, and vision coverage along with a 401K plan, STD and LTD, and Life Insurance as well as some additional optional benefits. Full time positions also come with annual PTO days and at certain levels a bonus program may apply. The Salary range for this role is $61,360 - $95,000 for Colorado based positions only. Other locations will vary in salary range

Transform Your World",Hyderabad,True,False,True,False,False,False,False,True,True,False,True,False,False,False,False,False
Dolby Laboratories,Data Engineer,"Join the leader in entertainment innovation and help us design the future. At Dolby, science meets art, and high tech means more than computer code. As a member of the Dolby team, you’ll see and hear the results of your work everywhere, from movie theaters to smartphones. We continue to revolutionize how people create, deliver, and enjoy entertainment worldwide. To do that, we need the absolute best talent. We’re big enough to give you all the resources you need, and small enough so you can make a real difference and earn recognition for your work. We offer a collegial culture, challenging projects, and excellent compensation and benefits, not to mention a Flex Work approach that is truly flexible to support where, when, and how you do your best work.

Play a key role as part of Dolby's new R+D Center in Bangalore as a Data Engineer in our Advanced Technology Group ""ATG"". ATG is the research and technology arm of Dolby Labs. It has multiple competencies that innovate technologies in audio, video, AR/VR, gaming, music, and movies. Many areas of expertise related to computer science and electrical engineering, such as AI/ML, computer vision, image processing, algorithms, digital signal processing, audio engineering, data science & analytics, distributed systems, cloud, edge & mobile computing, natural language processing, knowledge engineering and management, social network analysis, computer graphics, image & signal compression, computer networking, IoT are highly relevant to our research.

Responsibilities:

As a Data Engineer, you’ll be a part of a growing engineering team building and designing our core data infrastructure for our internal technology research and development efforts. You’ll have the chance to partner closely with our research and data science teams to understand data and functional requirements. We are looking for an experienced data professional who is a problem solver, logical thinker and passionate about everything relating to data and analytics. Your responsibilities include:
• Create and maintain optimal data pipeline architecture for data coming from different sources, in various formats and of different content type (text, audio, video etc.) allowing to standardize, clean and ingest data.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Design and develop solutions which are scalable, generic and reusable. Be responsible for collecting, storing, processing, and analyzing huge sets including, but not limited audio, video, and metadata.
• Develop techniques to analyze and enhance both structured/unstructured data and work with big data tools and frameworks.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Databricks, and AWS ‘big data’ technologies.
• Create data tools for research and data scientist teams.

What You Bring To The Role
• BsC/Msc degree in CS or EE. Work experienced desired, but not required.
• Experience building and optimizing streaming big data pipelines, architectures, and data sets.
• Deep understanding data pipeline frameworks including Databricks and Fivetran.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Experience or solid theoretical understanding of data workflows including:
• Ingestion
• Batch and stream processing
• Storage and archiving
• Visualization/Reporting and Dashboards
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Understanding of the current state of infrastructure automation, continuous integration/deployment - CI/CD, SQL/NoSQL, security, networking, and cloud-based delivery models.
• In-depth understanding of:
• NoSql databases (Kafka, HBase, Spark, Hadoop ,Cassandra, MongoDb etc). SQL development and any procedural extension language (T-SQL, PL/SQL, Pg/PLSQL etc.)
• Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Distributed data processing frameworks like Apache Spark, Apache Flink
• Scalable ML pipelines for image, video and audio modalities with tools such as Flyte, MLflow, Prefect, or AirFlow
• Data collection, labeling, cleaning, and generation tools such as LabelBox, SuperAnnontate, Scale Ai, or V7
• Scripting abilities with two or more general purpose programming languages including but not limited to Java, C/C++, C#, Objective C, Python, JavaScript.
• Data modeling and extraction of data from different sources
• Strong documentation skills, communication and client facing Experience
• Experience supporting and working with cross-functional teams in a dynamic environment.

Build your career profile, also within the Careers tab in Employee Central to open the possibility of new opportunities finding you. Express your interest. If you want to express your interest in a specific opportunity and be contacted by a recruiter, click the apply button associated with the relevant job description. The Recruiter is the only one who will see your application.

Please refer to the recruiting website for more information: https://jobs.dolby.com/careers

]]>",Bengaluru,True,False,True,True,False,True,True,True,False,False,False,True,False,False,True,False
Mercede,Positions for Data Engineer,"Technical Skills Competencies
• Deep hands-on expertise in Databricks (Scala or Python).
• Experience in Design and implementation of Big Data technologies (Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases) and familiarity with data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture).
• Experience in working as a Big Data Engineer: query tuning, performance tuning, troubleshooting, and debugging Spark and other big data solutions.
• Familiarity with a full range of data engineering approaches, covering theoretical best practices and the technical applications of these methods.
• Experience building and deploying a range of data engineering pipelines into production, including using automation best practices for CI/CD.
• Very good experience in writing SQL queries.
• Hands-on experience with any of the cloud providers such as AWS or Azure.
• Familiarity with databases and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Ability to juggle and prioritize multiple tasks within a collaborative team environment
• Desire to learn and grow both technical and functional skill sets, and drive team s potential
• Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience And Skills

Microsoft Azure and AWS Certifications
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Trained in Data Factory, Delta lake, Data bricks Notebooks
• Working experience in SAFe - Scaled agile framework
• Working experience in an international team environment
,

This job is provided by Shine.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
HuQuo,Interesting Job Opportunity: Azure Data Engineer - ETL/MDM,"Job Description
• To collaborate with various teams/regions in driving facilitating data design, identifying architectural risks and key areas of improvement in data landscape, and developing and refining data models and architecture frameworks
• Technical experience and knowledge in Cloud Data Warehousing, data migration and data transformation
• Develop and test ETL components to high standards of data quality and performance as a hands-on development lead
• Familiarity with Data Lakes, Data Warehouses, MDM, BI, Dashboards, AI, ML
• Design data architecture patterns and ecosystems including data stores (operational systems, data lakes, data warehouses, data marts), ingress patterns (API, streaming, ETL/ELT), and egress patterns (analytics/decision tools, BI tools). Lead, consult or oversee multiple architectural engagements
• Oversee and contribute to the creation and maintenance of relevant data artifacts (data lineages, source to target mappings, high level designs, interface agreements, etc.) in compliance with enterprise level architecture standards
• Experience in leading and delivering data centric projects with concentration on Data Quality and adherence to data standards and best practices.
• Experience in data modeling, metadata support, development and testing for enterprise wide data solutions
• Azure cloud experience is a must have with familiarity of the services: Azure Databricks, Azure Datafactory, Azure Datalake, Spark SQL, PySpark, Airflow, SQL server and Informatica MDM.
• Additional exposure to GCP and AWS is good to have.

Key Skill: Azure Databricks, ADF, ETL, Pipeline Dev, SQL, DWH, ADLS.

(ref:hirist.com)",Gurugram,False,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
AXA XL,Data Engineer,"Gurgaon, Haryana, India

The Application Developer plays a critical role within the Data and Analytics SDC as this person is responsible for designing and implementing data structures to support current and future analytical projects. We are looking for candidates that have experience working with data from a raw, unprocessed state and organizing it intuitively. Building this data pipeline enables our partners to analyze data better and faster – ultimately leading the organization in optimizing the decision-making process.

DISCOVER your opportunity

What will your essential responsibilities include?
• Candidates for this role should have experience developing data processes with source data in a variety of formats (structured / unstructured, databases, APIs) into a target state. This will involve building proper data pipelines to support initial exploration and real-time integration.
• Data development using appropriate tools and techniques to process data required for advanced analytics. A candidate would be expected to interact with Data Engineering Leads and Data Scientists to understand requirements and would be responsible for the development of the solution.
• Providing the right context of data required for a given analysis. This would require the candidate to work with data modelers/analysts to understand the business problems they are trying to solve and create data structures to feed into their analysis.
• Build upon learnings of internal and external data to become more proactive. This includes thinking ahead of what modelers will anticipate with their data needs and designing structures that are intuitive to use.
• Making sure quality and understanding of analytical data. This would require hands-on data experience to look into data issues and seek resolution or acceptance. Create the appropriate amount of documentation, leverage standards, and build upon them. Data should be reconciled and documented at various stages for integrity.
• Take part in developing governance and rigor of data management practice within the Data and Analytics SDC. This will also include partnering with enterprise IT groups and involvement in enterprise data-related functions.
• You will report to Data Manager/Principal Data Engineer.

SHARE your talent

We’re looking for someone who has these abilities and skills:
• Demonstrated ability to work through data complexities which include a variety of sources, formats, and structures. Robust preference for experience in the Insurance domain.
• Ability to see through ambiguous concepts and break down complex problems into manageable components.
• Detail-orientated, proven ability to recognize patterns in data.
• Demonstrated ability to incorporate data quality standards into data development.
• Possesses natural curiosity. Seek to understand the world around you, and question when appropriate.
• Robust SQL Skills required.
• 2-4 years of development experience using data development (visual ETL or coded) / analysis tools (ex. SAS, SPSS, R, Microsoft SSIS/SSAS, Informatica, DataStage, AbInitio).
• Experience in .NET, Python, or Java development is a plus.
• Experience in web extraction, unstructured data, advanced text parsing, machine learning, and NLP a plus.
• Familiarity with developer support tools (TFS/GIT, Jenkins) is a plus.
• College Degree in MIS, Information Technology, Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, or equivalent.

FIND your future

AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals, and even some inspirational individuals we don’t just provide re/insurance, we reinvent it.

How? By combining an effective and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business − property, casualty, professional, financial lines, and specialty.

With an innovative and flexible approach to risk answers, we partner with those who move the world forward.

Learn more at axaxl.com

Inclusion & Diversity

AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic.

At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That’s why we have made a strategic commitment to attract, develop, advance, and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It’s about helping one another — and our business — to move forward and succeed.
• Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability, and inclusion with 20 Chapters around the globe
• Robust support for Flexible Working Arrangements
• Enhanced family-friendly leave benefits
• Named to the Diversity Best Practices Index
• Signatory to the UK Women in Finance Charter

Learn more at axaxl.com/about-us/inclusion-and-diversity. AXA XL is an Equal Opportunity Employer.

Sustainability

At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called “Roots of resilience”, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations.

Our Pillars
• Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society – are essential to our future. We’re committed to protecting and restoring nature – from mangrove forests to the bees in our backyard – by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans.
• Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions.
• Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We’re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting.
• AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL’s “Hearts in Action” programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day – the Global Day of Giving.

For more information, please see axaxl.com/sustainability

Flexible Work Eligible

None

AXA XL is an Equal Opportunity Employer.

Location

IN-HR-Silokhera Gurgaon

Job Field

IT

Schedule

Full-time

Job Type

Standard",Gurugram,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,False
Inference Labs,Data Engineer,"Responsibilities for the job Key Responsibilities: - Data Model Designing, Developing and maintaining Data pipelines on cloud (AWS Platform) Translate business needs to technical specifications and framework Maintain and support data mart, data analytics platforms & application. Perform quality assurance to make sure the data correctness Develop sub-marts using SQL and OLAP function to fulfil immediate/ad-hoc need of the business users basis the comprehensive marts Monitoring of the performance of ETL and Mart Refresh processes, understand the problem areas and open a project to fix the performance bottlenecks. Other Responsibilities (If Any):- Availability during month-end Deck generation, may be sometime during week-end/holidays. Eligibility Criteria for the Job Education B.E/B.Tech in any specialization, BCA, M.Tech in any specialization, MCA Work Experience Data Engineer: 4+ years of experience in data engineering on cloud platforms like AWS, Azure, GCP Exposure with working on BFSI domain / big data warehouse project Exposure to manage multiple source of the information, both structured / unstructured data Manage data lake environment for point in time analysis (SCD Type 2), multiple refresh during the day, event based refresh Should have exposure on Managing environment having real time dashboard, data mart requirement. Primary Skill Must have orchestrated using any of the cloud platforms Expert in writing complex SQL Command using OLAP Working experience on BFSI Domain Technical Skills Must have orchestrated at least 3 projects using any of the cloud platforms (GCP, Azure, AWS etc.) is a must. Must have worked on any cloud PaaS/SaaS database/DWH such as AWS redshift/ Big Query/ Snowflake Python/Java Hands - on Exp from data engineering perspective is a must Experience with any of the object-oriented/object function scripting languages: Python, Java, Scala, Shell, .NET scripting, etc. is a must Experience in at least one of the major ETL tools (Talend + TAC, SSIS, Informatica) will be added advantage Management Skills Ability to handle given tasks and projects simultaneously in an organized and timely manner. Soft Skills Good communication skills, verbal and written. Attention to details. Positive attitude and confident.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
PwC,Data Engineer-Manager-P&T Labs,"Line of Service
Internal Firm Services

Industry/Sector
Not Applicable

Specialism
IFS - Internal Firm Services - Other

Management Level
Manager

Job Description & Summary
A career in National Special Functions, within Internal Firm Services, will provide you with the opportunity to support service, sector, and market leaders deliver the unique PwC client experience to our clients. You’ll play an important part in continuously innovating and improving Firm operations so that we can continue to provide the highest quality of services to our current and prospective clients.

Our team focuses on representing data as a strategic business asset to help serve our clients. You’ll focus on using data and information across PwC to drive change and improvements in data related operations to help enable the business as well as provide insights related to attendant risks.

Preferred Knowledge/Skills:

Demonstrates intimate knowledge and/or a proven record of success in the following areas:
• Understanding architectural design and data platform delivery in technologies that include, but are not limited to cloud, ETL, data streaming, data storage, data modeling, APIs/microservices, automation, continuous integration/continuous deployment;
• Showcasing work experience as a Data Engineer, Data Architect or similar role;
• Showcasing data engineering knowledge around complex efforts within established Software Development Lifecycles and methodologies including agile, scrum, iterative and waterfall;
• Showcasing technical knowledge that spans multiple platforms and portfolio of applications with demonstrated knowledge of the business strategic priorities in order to resolve complex problems;
• Utilizing IT processes and frameworks including, but not limited to, Identity Access Management (IdAM), Enterprise Application Integration, Data Warehousing, Business Intelligence, Reporting, Mobility, Master Data Management, and Search;
• Understanding of database structure principles;
• Showcasing advanced experience building and maintaining optimal data pipeline architecture and data streaming and integrations using tools such as ADF, SSIS, Informatica, API Management, Enterprise Service Bus (preferably Kafka);
• Showcasing advanced SQL knowledge and experience working with relational databases and performance optimization;
• Demonstrating data mining and segmentation techniques;
• Exhibiting knowledge in relational SQL, NoSQL and Big Data technologies;
• Understanding Data Federation/Virtualization technologies, such as PowerBI, Tableau, D3.js, and implementing Cloud based solutions;
• Assessing and analyzing system requirements;
• Showcasing analytical skills and a problem-solving attitude;
• Demonstrating virtual leadership and motivational skills;
• Recommending and participating in activities related to the design, development and maintenance of the Enterprise Data Architecture;
• developing internal relationships and PwC brand;
• Demonstrating time management skills with the ability to handle multiple projects simultaneously;
• Leveraging business knowledge and interpersonal skills to build, maintain, and influence relationships with leaders throughout the business and IT.

Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements
Not Specified

Available for Work Visa Sponsorship?
No

Government Clearance Required?
Yes

Job Posting End Date
May 10, 2023",Hyderabad,False,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Mindera,Data Engineer,"We are looking for an experienced Data Engineer to join our team.

Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.

As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.

This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.

National and international expected travelling time varies according to project/client and organisational needs: 0%-15% estimated

Requirements

You’re great at
• Python
• AWS like (Glue, S3, EMR, Athena and ECS/Fargate)
• SQL
• Airflow
• Data Modelling
• Pyspark

It also would be cool if you have
• Exposure to DBT would be preferable
• Experience working with modern data platforms such as redshift or snowflake would be preferable
• Experience working with Airflow, Docker, Terraform and CI/CD would be preferable
• Experience working with docker, Scala, and Kafka would be an added advantage

What You Will Be Doing
• Implement/support new data solutions in the data lake/warehouse built on the snowflake
• Develop and design data pipelines using python.
• Design and Implement Continuous Integration/Continuous Deployments pipelines.
• Perform Data Modelling using downstream requirements.
• Develop transformation scripts using advanced SQL and DBT.
• Write test cases/scenarios to ensure incident-free production release.
• Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
• Debug production and development issues and provide support to colleagues where necessary.
• Perform data quality checks to ensure the quality of the data exposed to the end users.
• Build strong relationships with team, peers and stakeholders.
• Contributes to overall data platform implementation.

Benefits

We offer
• Flexible working hours (self-managed)
• Competitive salary
• Annual bonus, subject to company performance
• Access to Udemy online training and opportunities to learn and grow within the role

At Mindera we use technology to build products we are proud of, with people we love.

Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.

We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world.

You get to work with a bunch of great people, and the whole team owns the project together.

Our culture reflects our lean and self-organisation attitude.

We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.

Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook

Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Rishabh Software,Big Data Engineer,"Job Description:

Roles and Responsibilities:

1) Work with BigData Practice Tech lead to Execute BigData Projects

2) Work with Techlead , helping him in Solutioning, Architecture and Technical Design

3) Analyze requirements and prepare low level design

4) Hands on implementation of Data ingestion, data processing and Data storage code and algorithms

5) Team management under Techlead guidance - including work distribution and delivery

6) Participate in potential client meetings and demos

Required Skills:

Any one programming Language - Java or Scala

Good Experience with Apache Spark

Any one Data integration platform - Kafka or similar

Any one No Sql data storage - S3 or No Sql Database

One live BigData project - Data ingestion , processing and Storage

Basic Cloud Exposure - AWS preferred

Excellent Analytical and problem solving skills.

Excellent Communication Skills",Vadodara,False,False,True,True,False,False,False,True,False,False,False,True,False,False,False,False
Cloud Software Group,Senior Data Engineer,"About Cloud Software Group

Cloud Software Group combines the capabilities of both Citrix and TIBCO, creating one of the world’s largest cloud software providers, serving more than 100 million users around the globe. When you join Cloud Software Group, you are making a difference for real people, each of whom count on our suite of cloud solutions to get work done – from anywhere. Members of our team will tell you that we value diverse lived experiences, varied perspectives, and having the courage to take risks. Our teams are encouraged to learn, dream, and build the future of work. We are on the brink of another Cambrian leap - a moment of immense evolution and growth. And we need your expertise and experience to do it. Now is the perfect time to move your skills to the cloud.

Position Summary

This is an individual contributor role with responsibility for supporting all data warehouse processes including technical analysis, design, development, implementation, and support of ETL solutions. The ideal candidate needs to have at least 5 years of experience developing with Microsoft SQL applications in an implementation and support role of a business intelligence organization.

Primary Duties / Responsibilities

Responsibilities will include, but are not limited to:

• Supporting the designs, tasks, and continuous improvements to maintain a scalable data warehouse

• Analyzing and validating data to ensure that business requirements are satisfied

• Creating data flow diagrams to depict business logic relating to data transformations

• Creating conceptual, logical, and physical data models for relational and dimensional solutions

• Breaking down, estimating, and executing increments of work

• Developing ETL packages of high complexity to fulfill all the business requirements

• Supporting deployment and delivery of defined technical solutions

• Communicating accurate and timely project status, issues, risks, and scope changes

• Performing root cause analysis of data discrepancies

• Creating data dictionaries and business glossaries to document data lineages, data definitions and metadata for all business-critical data domains

• Documenting all work (both technical and procedural) and ensuring that co-workers understand how to support system from an operational perspective

• Working in a highly collaborative team environment following the Agile Framework for planning and executing deliverables

Qualifications (include knowledge, skills, abilities, and related work experience)

• Bachelor’s degree in computer science or related field, or equivalent combination of education and recent, relevant work experience

• Minimum 5 years of experience in developing T-SQL Queries, Stored Procedures, and ETL packages with Microsoft SQL databases

• Strong understanding of data warehouse design and report development principles

• Experience in creating data flow diagrams and data models pertaining to business intelligence

• Experience in analyzing and developing reporting output such as Power BI, Tableau, or SSAS

• Strong interpersonal and problem resolution skills

• Strong teamwork and customer support focus

• Strong written (technical documentation) and verbal communication skills

• Ability to handle numerous conflicting priorities in a professional manner

Cloud Software Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all federal, state and local laws that prohibit employment discrimination on the basis of age, race, color, gender, sexual orientation, gender identity, ethnicity, national origin, citizenship, religion, genetic carrier status, disability, pregnancy, childbirth or related medical conditions, marital status, protected veteran status, and other protected classifications.",Bengaluru,False,False,True,False,False,False,False,False,True,True,True,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"Roles and responsibilities:
• Mandatory: Strong in Azure, ADF, Data Lake, Databricks, Pyspark
• Hands-on-experience in developing data lake solutions using Azure (Azure data factory for ingestion, Data Lake gen 2 and Azure SQL server for storage, Azure analysis service for transformations, Azure data bricks)
• Implement a robust data pipeline using Microsoft Stack.
• Create reusable and scalable data pipelines.
• Development and deployment of new data platforms.
• Leverage Azure BI services for development of Big Data Platforms.
• Work closely with the Product Owners and Architects to develop Azure Data Platforms.
• Work with the leadership to set the standards for software engineering practices within the team and support across other disciplines.
• Produce high-quality code that allows us to put solutions into production.
• Refactor code into reusable libraries, APIs, and tools.",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Affine,Data Engineer,"Company Description

About Company

http://www.affine.ai

""AFFINE"" cited by GARTNER as a SPECIALIST MIDSIZE CONSULTANCY in ANALYTICS and MACHINE LEARNING solutions and services. Click to Read More ""

Affine is a provider of high-end analytics services to solve complex business problems with offices in NJ, USA & Bangalore, India. We combine data driven algorithmic analysis with heuristic domain expertise to provide actionable solutions that empower organizations make better and informed decisions. Affine's value proposition is enabling clients to implement and realize ROI of the recommendations.

Affine has a group of people with significant experience in Analytics industry along with solid pedigree, deep business understanding and strong problem solving acumen. Our group primarily consists of Statisticians, Operations Researchers, Econometricians, MBAs and Engineers. Our employees have experience of working for many Fortune 500 companies.

Job Description

What the candidate will do:
• Contribute to adoption of cloud & cloud-based technologies and good design practices, while finding opportunities to simplify and scale
• Resolve problems and roadblocks as they occur with peers and help unblock junior members of the team. Follow through on details and drive issues to closure
• Define, develop, and maintain artifacts like technical design or partner documentation
• Drive for continuous improvement in Data engineering process within an agile development team
• Own and deliver assigned sprint tasks and help drive the team forward.
• Communicate and work effectively with geographically distributed cross functional teams

Experience

4 to 6 Years in Deploying models, Sage Maker or TensorFlow

Required skillset.
• Big Data: Spark, Kafka, Hadoop, Hive, SQL and NoSQL
• Cloud: AWS, EMR, Qubole/Databricks, VPC
• Devops: Docker containers and Jenkins. Spinnaker is preferred but not required.
• Programming languages: Scala and Pyspark is mandatory
• Agile and scrum experience and working with a remote team (nice to have, not required)

Must Have Skills
• Spark, AWS, Scala/Python, SQL, Java
• ML ops tools:Tensorflow or Sagemaker

Additional Information

Others
• Quick learner
• Excellent written and oral communication skills
• Excellent interpersonal & organizational skills
• Good listening and comprehension skills",Bengaluru,True,False,True,True,True,False,False,True,False,False,False,False,False,False,False,False
DISH Careers,Data Engineer,"About DISH:

DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world’s best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH’s largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market

Job Duties and Responsibilities:
• Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production
• Creation and optimization of data engineering pipelines for analytic projects
• Strong analytic capability and the ability to create innovative solutions
• Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues
• Provide technical guidance to L1 team members and help to resolve ETL related issues
• Need to work as on call-support

Skills, Experience and Requirements:
• Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment
• Experience in developing/debugging and fixing data ingestion pipelines both real time and batch
• Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb
• Experience in transforming/integrating data in Redshift/Snowflake
• Strong in writing complex SQLs to ingest data into cloud data warehouses
• Good hands on experience in shell scripting or python
• Experience with scheduling tools - ControlM, Airflow , StepFunction
• Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements
• Perform root cause analysis (RCA) for failures
• Good Communication skills – written and verbal with the ability to understand and interact with the diverse range of stakeholders
• Capable of working without much supervision",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
MPOWER Financing,"Data Engineer - Data and Analytics - Bangalore, India","THE COMPANY

MPOWER’s borderless loans and scholarships enable students from around the world to realize their full academic and career potential by attending top universities in the U.S and Canada.

As a mission-oriented fintech/edtech company, we move extremely quickly and leverage the latest technologies, global best practices, and heavy analytics to tackle one of the biggest challenges in financial inclusion. We’re backed by over $150 million in equity capital from top global investors, which enables fast growth and provides our company with financial stability and a clear path to an IPO over the coming years.

Our global team is composed of former management consultants, financial service and technology professionals, and other experts in their respective fields. We work hard, have fun, and believe strongly in our cause. For us, MPOWER’s mission is personal.

As a member of our team, you’ll be challenged to think quickly, act autonomously, and constantly grow creatively in an environment where fast change and exponential growth are the norm. Ideation and implementation happen very quickly. We value feedback and emphasize personal and professional development by providing the resources you need to further your skills and grow with the company. MPOWER is committed to cultivating your strengths and curiosity and helping you make an immediate impact.

MPOWER has been named one of the best fintechs to work for by American Banker for 2018, 2019, 2020, and 2021. We pride ourselves on being a “growth company for grown-ups,” where there are no pool tables but rather great health, education, and maternity/paternity benefits instead. Our team diversity has been recognized as well; we’re one of the most diverse workforces in the world in terms of nationality, gender, religion, age, sexual orientation, and educational background.

THIS IS A FULL-TIME POSITION, BASED IN OUR BANGALORE OFFICE

THE ROLE

You will be tasked with building and maintaining MPOWER’s data infrastructure. You’ll also play a key role in acquiring, organising and analysing data to provide insights that enable the company in making sound business decisions. This includes, but is not limited to:
• Maintaining MPOWER’s database and building on the existing database infrastructure
• Establishing the needs of different users and monitoring user access and security
• Capacity planning and refining the physical design of the database to meet system storage requirements
• Creating efficient queries and tools to obtain data for different business needs
• Building data models to identify, analyze and interpret trends or patterns in data sets that inform business decisions and strategy
• Working with various internal and external stakeholders to maintain and develop enhanced data collection systems
• Performing periodic data analyses, creating and presenting findings and insights
• Performing scheduled data audits in order to locate and correct code errors and maintain data integrity
• Collaborating with MPOWER’s global tech team to build data collection and data analysis tools

THE QUALIFICATIONS
• Undergraduate degree in computer science; advanced degree preferred
• 5+ years of experience in database programming, database administration and data analysis
• Must have prior experience in building high quality databases in accordance with end users information needs and views
• Proficiency in Big Data and Hadoop ecosystems.
• Deep familiarity with database design and documentation
• Hands-on expertise and exposure to at least one database technology (MySQL, PostgreSQL)
• Advanced knowledge of R/Python, PySpark, or Scala is a plus
• Prior experience building data pipelines and data orchestration is a plus.
• Superior analytical and problem solving skills
• Proven ability to create and present comprehensive reports
• Ability to multitask and own several key responsibilities at a given time
• Passion for excellence: constantly striving to improve professional skills and business operations

A passion for financial inclusion and access to higher education is a must, as well as comfortable working with a global team across multiple-time zones and sites!

In addition, you should be comfortable working in a fast growth environment, meaning a small agile team, fast-evolving roles and responsibilities, variable workload and tight deadlines, a high degree of autonomy, and 80-20 everything.

MPOWER Financing focuses on Financial Services, Finance, Finance Technology, Higher Education, and Education Technology. Their company has offices in New York City, Washington DC, and Washington. They have a small team that's between 11-50 employees. To date, MPOWER Financing has raised $7.291M of funding; their latest round was closed on October 2016.

You can view their website at http://www.mpowerfinancing.com/ or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,False,False,False
Cortex Consultants LLC,Data Engineer,"Hi,

Welcome to Cortex

Job Title: Data Engineer

Job Description

2+ years of Data Engineer experience in Snowflake (on Azure Cloud Preferred).

Strong knowledge of SQL to build queries and Optimization techniques.

Strong Knowledge of the ETL process using SSIS / ADF (Azure Data Factory) / Matillion

Experience of Python programming is an added advantage.

Location: Chennai

Work type-Hybrid

Immediate joiners

Interested candidates share your resume to

Deepak.g@cortexconsultants.com

Contact No: 9080100600",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Roche,Data Engineering Manager,"The Position
Engineering Manager is a critical leadership role in our Data Engineering team. This is a people management role that needs the ability to hire and grow top engineering talent and to manage multiple teams. It includes responsibility to deliver and operate high quality, scalable, and extensible products & solutions, including making appropriate design and technology choices. The role requires strong strategic thinking and making build/buy/partner decisions for technical capabilities. Effective Communication is critical, as you will be working closely with a variety of stakeholders to understand and address their needs. A healthcare background with experience in integrating healthcare IT systems would be good to have.

KEY RESPONSIBILITIES
• Manage team of Data Engineers working on multiple data analytics products.
• Work with different agile product teams, understand and fulfill their staffing needs.
• Work with business stakeholders to develop high level project plans and roles and responsibilities.
• Prepare training and development plans for the team.
• Understand and create a career path for the team members.
• Evolve and develop a long-term roadmap for team and projects.
• Apply data engineering best practices in terms of quality, security, scalability and maintainability.
• Participate in how the budget and staff is allocated for the projects.
• Maintain project time frames, budget estimates and status reports.
• Create management, communication plans and processes. Analyze and develop process for management and technical duties.
• Foster team bonding and trust within the team. Responsible for hiring, growing and motivating engineers on your team, ensuring you recruit and retain top talent.

REQUIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• BS degree in Computer Science, Computer Engineer or a related technical discipline with 10+ years of IT industry experience.
• At least 4-6 years of proven managerial experience developing a high-performing team.
• Experience in Agile Solution Delivery and Operations Management and people management.
• Quick learner with the ability to understand complex workflows and develop and validate innovative solutions to solve difficult problems.
• Strong communication, with the ability to explain complex technical problems to non-technical audiences and the ability to translate customer requirements to technical designs.
• Strong interpersonal skills, with proven ability to navigate complex corporate environments and influence stakeholders and partners.

DESIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• Proven work experience in AWS or other cloud related technologies.
• Experience of working in product based organization
• Proven work experience as an Engineering Manager or similar role
• Communication skills for overseeing staff and working with other management personnel
• Organizational skills for keeping track of various budgets, employees, and schedules simultaneously
• Leadership, team-building, and mentoring skills
• Personnel and project management skills
• Ability to work on multiple projects in various stages simultaneously
• Experience in the Healthcare Laboratory domain is a plus.

EDUCATION

Bachelor’s degree in Engineering

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Data.Ai,DNA Team - Data Engineer,"data.ai is the mobile standard and the trusted source for the digital economy. Our vision is to be the first Unified Data AI company that combines consumer and market data to provide insights powered by artificial intelligence. We passionately serve enterprise clients to create winning digital experiences for their customers.

We care deeply about our high-performance culture and operate as a global team. We put our customers at the center of every decision [Customer First], follow through with what we say we are going to do [Own It & Deliver] and propose solutions, not just issues [Challenge, Them Commit] to Win As A Team.

We are a remote-first company and we trust our people to get it done from the location that works for them.

What can you tell your friends when they ask you what you do?

As a DNA Team Data Engineer, I’ll be a key contributor to DNA team data services. I’ll help the DNA team to build and enhance internal processes of data production and transaction/transformation, as well as internal tools. And help colleagues from other teams and/or external clients to better experience the DNA team services.

You will be responsible for and take pride in…
• Exciting Projects using technical expertise across Python, SQL, Spark, DataBricks
• Build data pipeline across different data sources/databases such as AWS S3, PG database, and Snowflake
• Produce and maintain relevant documentation
• Support internal and external customers
• Becoming better at what you do every day

You should recognize yourself in the following…
• Bachelor’s degree in Computer Science, Engineering, or equivalent experience
• At least 5 years of related work experience in building data pipelines
• Strong skills in Python and PL/SQL
• Deep understanding and experience in building data pipelines across different data sources/databases such as AWS S3, PG database, and Snowflake
• Experience in data processing such as ETL
• Knowledge of machine learning and AI is preferred
• Familiarity with specific app markets (e.g.: Gaming, Entertainment, Finance, etc.) is a big plus
• Strong problem-solving, analytical, and troubleshooting skills
• A self-starter who identifies and solves problems before anyone has noticed
• Fluent in English, both written and oral

data.ai are in the process of establishing an entity in India, in the interim the employees will be on the rolls of Leap 29 our Global Employer of Record",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Danske Bank,Senior Data Engineer-ETL Datastage,"Experience 5-8Years

The ideal applicant should have the following skills:

- Strong technical experience in Data Warehousing and Experience in working with ETL tools (Datastage, Informatica etc) for the purpose of creating data marts for analytical purposes

- Strong understanding of relational database concepts & technology. Exposure to Big Data technologies is an added advantage.

- Strong analytical and problem solving skills with the ability to collect, organize, analyse and process large volumes of data in a complex environment

- Good written and verbal communication skills with the ability to communicate and articulate one's thought process clearly.

- Be self driven and work closely with business stakeholders, in a global environment, to gather enough context to translate the business
objective into an analytical solution.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Splunk,Data Engineer - 27516,"As a Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have a deep understanding of the architecture for enterprise-level data lake solutions using multiple platforms (RDBMS, AWS, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.

What you'll do: Yeah, I want to and can do that.
• As a Data Engineer, you will be responsible for engineering data pipelines for Splunk’s enterprise data platform, democratizing datasets, enabling advanced analytics capabilities, integrating data from various systems, and applications. You will work as part of an evolving Enterprise Data Management(EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. The role will collaborate closely with other specialists, Product Managers & key stakeholders across the company.
• Build large-scale batch and real-time data pipelines using the cloud data technologies, such as Snowflake, Matillion, Kubernetes, Python, Apache Airflow and Apache Kafka
• Serve as a resource for data management implementations on other technology teams and collaborate with data owners, business owners, and leaders.
• Supports the design and development of framework based data integration and interoperability across multiple Splunk Business applications.
• Advanced level skills in Python, SQL, data integration, data modeling and data architecture.

Requirements: I’ve already done that or have that!
• A minimum of 5 years of related experience
• 3+ years of experience as a Data Warehouse Architect or Data Engineer.
• 2+ years of experience driving adoption and building automation of data management services and tools.
• 2+ years of experience with API based ELT automation framework, data management, or interface design, development and maintenance.
• Large scale design, implementation and operations of Cloud data storage technologies such as AWS Redshift, Snowflake, Kubernetes, etc.
• 3+ years of experience with programming scripting and data science languages such as Python, SQL, etc.
• Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.
• Advanced knowledge of Big Data concepts in organising both structured and unstructured data

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Experience with Sales Operations, Partner Operations and customer success business processes and applications

Education: Got it!
• Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Verizon,Manager-Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

As a Manager for Data Engineering team, you will be managing data platforms and implementing new technologies and tools to further enhance and enable data science/analytics, focus to drive scalable data management and governance practices. Leading the team of data engineers & solutions architects to deliver solutions to business teams.
• Driving the vision with leadership team for data platforms enrichment covering the areas like Data Warehousing/Data Lake/BI across the portfolio.
• Defining and executing on a plan to achieve that vision.
• Building a high-quality Data engineering team and continue to drive to scale up.
• Ensuring the team adheres to the standard methodologies on data engineering practices.
• Building cross-functional relationships with Data Scientists, Data Analysts and Business teams to understand data needs and deliver data for insight solution.
• Driving the design, building, and launching of new data models and data pipelines.
• Driving data quality across all data pipelines and related business areas.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You are curious and passionate about Data and highly scalable data platforms. People count on you for your expertise in data management in all phases of the software development cycle. You create environments where teams thrive and feel valued, respected and supported. You enjoy the challenge of managing resources and competing priorities in a dynamic, complex and deadline-oriented environment. Building effective working relationships with other managers across the organization comes naturally to you.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Two or more years of experience in leading the team and tracking the end-to-end deliverables.
• Experience in end-to-end delivery of Data Platform Solutions and working on large scale data transformation.
• Knowledge of Spark, Hive, Scala, Pig, Kafka, Pulsar, Nifi, Python, Shell scripting.
• Knowledge of Google Cloud Platform/BigQuery.
• Knowledge of Teradata.
• Experience in working with DevOps tools like Bitbucket, Artifactory, Jenkins.
• Knowledge of Data Governance and Data Quality.
• Experience in building / mentoring the team.

Even better if you have one or more of the following:
• Master’s degree.
• Experience in data engineering, big data, hadoop and DevOps technologies.
• Certifications in any Data Warehousing/Analytical solutioning.
• Certification in program/project management.
• Experience in technical leadership in architecture, design, implementation and support of large-scale data and analytics solutions that are highly reliable, flexible, and scalable.
• Ability to meet tight deadlines, multi-task, and prioritize workload.
• Experience in collaborating with cross-functional teams and managing stakeholder expectations.
• Experience in working with globally distributed teams.
• Good Communication and Presentation skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
FairMoney,Senior Data Engineer,"About FairMoney

FairMoney is a credit-led mobile bank for emerging markets. The company was launched in 2017, operates in Nigeria & India, and raised close to €50m from global investors like Tiger Global, DST & Flourish Ventures. The company has offices in France, Nigeria, and India.

Role and responsibilities

At FairMoney, we are making a lot of data driven decisions in real time: risk scoring, fraud detection as examples.

Our data is mainly produced by our backend services, and is being used by data science team, BI team, and management team. We are building more and more real time data driven decision making processes, as well as a self serve data analytics layer.

As a senior data engineer at FairMoney, you will help building our Data Platform:

• Ensure data quality and availability for all data consumers, mainly data science and BI teams.
• Ingest raw data into our DataWarehouse (BigQuery / Snowflake)
• Make sure data is processed and stored efficiently:
• Work with backend teams to offload data from backend storage
• Work with data scientists to build a machine learning feature store
• Spread best practices in terms of data architecture across all tech teams
• Effectively form relationships with the business in order to help with the adoption of data-driven decision-making.

You will be part of the Datatech team, sitting right between data producers and data consumers. You will help building the central nervous system of our real time data processing layer by building an ecosystem around data contracts between producers and consumers.

Our current stack is made of

• Batch processing jobs (Apache Spark in Python or Scala)
• Streaming jobs (Apache Flink deployed on Kinesis Data Analytics - Apache Beam deployed on Google Dataflow)
• REST apis (Python FastApi)

Our tool stack

• Programming language: Python, SQL
• Streaming Applications: Flink, Kafka
• Databases: MySQL, DynamoDB
• DWH: BigQuery, Snowflake
• BI: Tableau, Metabase, dbt
• ETL: Hevo, Airflow
• Production Environment: Python API deployed on Amazon EKS (Docker, Kubernetes, Flask)
• ML: Scikit-Learn, LightGBM, XGBoost, shap
• Cloud: AWS, GCP

Requirements

You will work on a daily basis with the below tools, so you need working experience on

• Languages: Python and Scala.
• Big data processing frameworks: all or one of Apache Spark (batch/streaming) - Apache Flink (streaming) - Apache Beam.
• Streaming services: Apache Kafka / AWS Kinesis.
• Managed cloud services: one of AWS EMR / AWS Kinesis Data Analytics / Google Dataflow.
• Docker.
• Building REST APIs.

Ideally, you have experience with:

• deployment/management of stateful streaming jobs.
• the Kafka ecosystem: Kafka connects mainly.
• infrastructure as code frameworks (Terraform).
• architecture around data contracts: Avro Schemas management, schema registries (Confluent Kafka / AWS Glue).
• Kubernetes.

Overall experience required for this role: 6+ Years.

Benefits

• Training & Development
• Family Leave (Maternity, Paternity)
• Paid Time Off (Vacation, Sick & Public Holidays)
• Remote Work

Recruitment Process • A screening interview with one of the members of the Talent Acquisition team for 30 minutes.
• Takeaway assignment to be done at home.
• Technical design interview for 60-90 minutes.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,True,False,True,True,True
Boston Consulting Group,IT Senior Data Engineer,"WHAT YOU'LL DO
Under the general supervision of senior management and the Data Engineering Chapter Lead in the Enterprise Data Tribe, you will be working with key customers to deliver timely and accurate data engineering pipelines in a secure manner. You are expected to provide guidance on proper engineering design ensuring that our architectural guidelines are met, and the appropriate support model is in place for production deployments. This role will work in a multi-functional agile squad and support the product owner. You will also be supporting the Chapter Lead and other team members of the Data Engineering chapter in proof-of-concept activities and other Data Engineering chapter related work.
YOU'RE GOOD AT
You have experience in data warehousing, data modelling, and the building of data engineering pipelines. You are well versed in data engineering methods, such as ETL and ELT techniques through scripting and/or tooling. You are good in analysing performance bottlenecks and providing enhancement recommendations; you have a passion for customer service and a desire to learn and grow as a professional and a technologist.
• Viewed as subject matter expert for stakeholders, possessing in-depth knowledge and specialized technical skill set
• Able to work independently with minimal supervision
• Proactively identify and independently solve non-routine problems by applying expertise
• Perform research of viable technical and/or non-technical solutions
• Develop internal network with senior leaders within the chapter and key stakeholders in the tribe.
• Develop strategies for data engineering in Snowflake using DBT and Talend.
• Architect, design, and implement data pipelines to feed data models for subsequent consumption
• Actively monitor and resolve user support issues, working closely with your assigned squad and other squads as part of the chapter.
• Develop and maintain architectural standards, best practices, and measure compliance

YOU BRING (EXPERIENCE & QUALIFICATIONS)
You bring to us experience in data engineering technologies, database development, and data model design; both in IaaS and PaaS Cloud (AWS and/or Azure) environments.
• Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
• 5-7+ years of project experience, preferably as a Data Engineer/Developer and minimum of 3 years of agile project experience is a must (preferred tool - JIRA)
• Essential: Must have exposure to technologies such as DBT, Talend and Apache airflow
• Essential: SQL is heavily focused. An ideal candidate must have hands-on experience with SQL database design
• Essential: Extremely talented in applying SCD, CDC and DQ/DV framework
• Essential: Experience in data platforms: Snowflake, Oracle, SQL Server, PostgreSQL, and MySQL
• Essential: Lead R&D efforts to find solutions for data engineering requirements not addressed by existing technology standards
• Essential: Demonstrate ability to write new code i.e., well-documented and stored in a version control system (we use GitHub & Bitbucket)
• Essential: Develop metrics that illuminate the flow of data across the organization
• Essential: Experience in data modelling and relational database design
• Preferred: Experience in AWS and Azure data platforms.
• Preferred: Experience in Qlik Compose, Fivetran and HVR
• Preferred: Strong programming/ scripting skills (Python, Powershell, etc.)

YOU'LL WORK WITH
As part of the Enterprise Data Tribe, you don t have to fit into a mould at BCG. We seek people with strong drive, relentless curiosity, desire to create their own path, ability to work collaboratively, and the passion and leadership to make an impact. You ll collaborate on challenging projects with team members from many backgrounds and disciplines, increasing your understanding of complex business problems from diverse perspectives and developing new skills and experience to help you at every stage of your career. You ll be able to experience business on a genuinely global scale and learn how to bring together people from different cultures to uncover insights that challenge the status quo. As a member of the Product Engineering Group, you will work closely with a cross functional team that is collaborative, passionate and that holds themselves to a high standard.",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
General Mills,Data Engineer,":

India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.

Job Description:

Job Overview

The Enterprise Data Development team is responsible for designing & architecting solutions to integrate & transform business data into Data Lake to deliver data layer for the Enterprise using cutting edge technologies like Big Data - Hadoop. We design solutions to meet the expanding need for more and more internal/external information to be integrated with existing sources; research, implement and leverage new technologies to deliver more actionable insights to the enterprise. We integrate solutions that combine process, technology landscapes and business information from the core enterprise data sources that form our corporate information factory to provide end to end solutions for the business.

This position will develop solutions for the Enterprise Data Lake & Data Warehouse. You will be responsible for developing data lake solutions for business intelligence and data mining.

Job Responsibilities

70% of time Create, code, and support a variety of Hadoop, ETL & SQL solutions

Experience with agile techniques or methods

Work effectively in a distributed global team environment.

Works on pipelines of moderate scope & complexity

Effective technical & business communication with good influencing skills

Analyze existing processes and user development requirements to ensure maximum efficiency

Participates in the implementation and deployment of emerging tools and processes in the big data space

Turn information into insight by consulting with architects, solution managers, and analysts to understand the business needs & deliver solutions

20% of time Support existing Data warehouses & related jobs.

Job Scheduling experience (Tidal, Airflow, Linux)

10% of time Proactive research into up to date technology or techniques for development

Should have automation mindset to embrace a Continuous Improvement mentality to streamline & eliminate waste in all processes.

Desired Profile

Education:

Minimum Degree Requirements: Bachelors

Preferred Degree Requirements: Bachelors

Preferred Major Area of Study: Engineering

Experience:

Minimum years of Hadoop experience required: 2 years

Preferred years of Data Lake/Data warehouse experience: 2-4+ years

Total Experience required : 4-5 years

Specific Job Experience or Skills Needed

Skills Level: Beginner  Intermediate Expert  Advance

HDFS, Map reduce

Beginner

Hive, Impala & Kudu

Beginner

Python

Beginner

SQL, PLSQL

Proficient

Data Warehousing Concepts

Beginner

Other Competencies:
• Demonstrate learning agility & inquisitiveness towards latest technology
• Seeks to learn new skills via experienced team members, documented processes, and formal training
• Ability to deliver projects with minimal supervision
• Delivers assigned work within given parameter of time and quality
• Self-motivated team player and should have ability to overcome challenges and achieve desired results",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Fidelity India Careers,Lead - Software Engineering - Data Engineering,"Job Description:

Job Title – Lead Data Engineer [Data CoE]

The Purpose of This Role

At Fidelity, we use data and analytics to personalize incredible customer experiences and develop solutions that help our customers live the lives they want. As part of our digital transformation, we have significant investments to create innovative big data capabilities and platforms. One of them is to build various enterprise data lakes by gathering data across Business Units. We are looking for a hands-on data engineer who can help us design and develop our next generation, cloud enabled data capabilities.

The Value You Deliver
• You will be participating in end to end development which includes design, development, testing and deployment.
• You will be working closely with Technical Lead/Architects to ensure that solutions are consistent with IT Roadmap.
• You will be participating in technical life cycle processes, which include impact analysis, design review, code review, and peer testing.
• You will be participating in hands on development of application framework code in Oracle PL-SQL, pySpark, Python, NiFi, Informatica Power Center, along with Control-M and UNIX shell scripts.
• You will be troubleshooting and fixing any issues reported on data issues and performance.
• You will be presenting the findings and outcome to Senior Leadership teams and provide insights from the data to the business.
• You will be helping business teams optimize their current tasks and increase their productivity.

The Skills that are Key to this role

Technical / Behavioral
• You must be an expert in using SQL and PLSQL on Oracle or Netezza with UNIX shell scripting skills.
• You should be having working knowledge in Hadoop, HDFS, Hive, Spark, NoSQL DBs,
• Good knowledge on Python, JavaScript, Java and Scala
• You should have experience of using AWS services like RDS, EC2, S3, EMR and IAM to move data onto cloud platform
• Experience/Knowledge on Kubernetes, Containerization and building applications in Containers
• Knowledge of Logging, Telemetry and Data Security on AWS / Azure
• Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
• Experience of query tuning and optimization in one of the RBMS (oracle or DB2)
• You should be having experience in Control-M or similar scheduling tools.
• You should have proven analytical and problem-solving skills
• You should be strong in Database and Data Warehousing concepts.
• You must be able to work independently in a globally distributed environment
• You should have clear understanding of the business needs and incorporate these into technical solutions.

The Skills that is good to have for this role
• Experience in performance tuning and optimization techniques on SQL (Oracle and Netezza) and Informatica Power Center.
• Having strong inter-personal and communication skills including written, verbal, and technology illustrations.
• Having adequate knowledge on DevOps, JIRA and Agile practices.

How Your Work Impacts the Organization

Cloud Enablement and Data Model ready for Analytics.

The Expertise we’re looking for
• 3+[SE] / 7+ [Lead] years of experience in Data Warehousing, Big data, Analytics and Machine Learning
• Graduate / Post Graduate

Location: Bangalore , Chennai

Shift timings: 11:00 am - 8:00pm

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following:

For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2
For roles based in Ireland: Contact AccommodationsIreland@fmr.com
For roles based in Germany: Contact accommodationsgermany@fmr.com

Fidelity Privacy policy

Certifications:

Company Overview

At Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. Fidelity India has been the Global Inhouse Center of Fidelity Investments since 2003 with offices in Bangalore and Chennai. For information about working at Fidelity, visit India.Fidelity.com.

Fidelity Investments is an equal opportunity employer.",Bengaluru,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
EMERSON,Data Engineer - Sustainability,"JOB DESCRIPTION AS A PROFFESSIONALYOU WILL: Work closely with key stakeholders to understand business needs and translate them into technical requirements that would feed into developing effective data analytics solutions Design and implement end-to-end data solutions in collaboration with other technical and functional teams. Review and revise existing software development lifecycle andcode standards. Work closely with the data Architect onproduct roadmaps. Work on SharePoint and Power BI tools to manage, analyse and deduce data insights. Act as a point of escalation for complex operational issues to ensure optimal performance of analytics systems. WHO YOU ARE: You anticipate customer needs and provide services that are beyond customer expectations. You understand interpersonal and group dynamics and react in an effective manner. You encourage others to learn and adopt new technologies. You show a tremendous amount of initiative in tough situations and are exceptional at spotting and seizing opportunities. You promote high visibility of shared contributions to goals. REQUIRED EDUCATION, EXPERIENCE, & SKILLS: Bachelor's degree in Computer Science/Information Technology or equivalent Must have a minimum of 6+ years of experience in a Engineering role with experiences with: SharePoint Online and Power BI Experience in Visualization and Interpreting Data in various forms Technical expertise in data modelling, data mining, and segmentation techniques Experience with building new and troubleshooting existing data pipelines using Experience with batch and real-time data ingestion and processing frameworks Experience with languages such asPython andJava Knowledge of additional cloud-based analytics solutions Hands-on experience working on Linux and Windows systems Using Agile development methods Ability to work in a large, global corporate structure Ability to lead, manage and deliver large scale projects Advanced English level Demonstrated ability to clearly isolate and define problems, effectively evaluate alternative solutions, and make decisions in a timely manner Good decision-making ability, ability to operate in ambiguous situations, and high analytical ability to judge pros/cons of approaches against objectives PREFERRED EDUCATION, EXPERIENCE, & SKILLS: Expert level knowledge of data analytics and warehousing frameworks, including Snowflake and Cloud-based data integration solutions Experience with DevOps andCI/CD development practices Advanced level of software development knowledge",Chandigarh,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 24 2023

Come join us as we supercharge GSK’s data capability!

At GSK we are building a best-in-class data and prediction powered team that is ambitious for patients.

Scientific Digital and Tech’s goal is to power the discovery, development and supply of medicines and vaccines to patients. This means new tools to discover new medicines and vaccines, predictive capability for pre-clinical research, accelerated CMC and supply chain and an improved day-to-day laboratory experience for our scientists. Our Digital & Tech solutions will automate workflows and speed up decisions; freeing hands and releasing minds to focus on science.

As R&D enters a new era of data driven science, we are building a data engineering capability to ensure we have high quality data captured with context and aligned data models, so that the data is useable and reusable for a variety of use cases.

GSK R&D and Digital and Tech’s collective goal is to deliver business impact, including the acceleration of the discovery and development of medicines and vaccines to patients. The R&D Digital and Tech remit has expanded over the past 2 years, and to position GSK for the future, The change will strengthen R&D Tech, to provide more strategic impact, focus, accountability, and improved decision making in the use of Digital, Data and Analytics (DDA) to strengthen the pipeline.

Job Purpose

This role contributes to the construction of the development data fabric and data strategy. This role will interact with architects, engineers, data modelers, product owners as well as other team members in Clinical Solutions and R&D. This role will actively participate in creating technical solutions, designs, implementations & participate in the relentless improvement of R&D Tech systems in alignment with agile and DevOps principles.

The Data Engineer demonstrates both depth and breadth across key data engineering competencies e.g. Software Development, Testing, DevOps, Data Science/Analytics, and cloud. Can collaborate with experts from other subject domains. Primary responsibilities include using Azure cloud services and GSK data platform tools to ingest, egress, and transform data from multiple sources.

In addition, the role will demonstrate core engineering knowledge/experience of industry technologies, practices, and frameworks such as data fabric and scaling data platforms, containerization, cloud-based platforms, data analytics, machine learning, and data streaming. Examples of technologies include Java/C#/Python, Denodo, GIT, Azure Devops, Data Bricks, Presto, Spark, Azure Data Factory, ADLS V2, Kafka, Selenium, JUnit/NUnit, SAFe, Kanban, Docker, AI/ML, Azure/GCP Cloud Architecture including networking principles and scaling applications.

The Data Engineer, Clinical Solutions role is a senior technical role and will provide you the opportunity to lead key activities to progress your career. These responsibilities include the following:
• Working with other teams that are defining devops and data platform practices to meet the requirements of clinical solutions.
• Supporting engineering teams in the adoption and creation of data fabric best practices.
• Conducting PoCs of new technologies and helping to embed them in product teams
• Being part of a cutting-edge team creating the Development Data Fabric
• Ensures that technical delivery is fully compliant with GSK Security, Quality and Regulatory standards
• Ensures use of relevant R&D Tech / central services and collaborating with service partners in identification and delivery of service improvements
• Maintains best practices for engineering and architecture on our Confluence site. This requires hands on experience with cutting edge technology.
• Pro-actively engages in experimentation and innovation to drive relentless improvement
• Provides leadership, technical direction and GSK expertise to architecture and engineering teams composed of GSK FTEs, strategic partners and software vendors.

Why you?

Basic Qualifications:

Are you ready to work in an environment where you are continuously expected to work on projects with new technology and expected to use this technology to deliver real business value?

We are looking for professionals with these required skills to achieve our goals:
• Total 15+ years of experience and proficient with at least 3 of the below skills and can demonstrate knowledge and value with relevant experience in all the following competencies:
• Must have experience in Spark, Python and Databricks
• Software development, architecture design & technology platforms/frameworks
• Data Platforms and Domain-driven design
• Agile, DevOps & Automation [of testing, build, deployment, CI/CD, etc.]
• Data science (e.g. AI/ML), data analytics & data quality/integrity
• Testing strategies & frameworks
• Role requires:
• Demonstrated skill in delivering high-quality engineered data products
• Knowledge of industry standards and technology platforms aligned to GSK and R&D roadmaps
• Excellent communication, negotiation, influencing and stakeholder management skills
• Customer focus and excellent problem-solving skills
• Computer Science or related bachelor’s degree – MS in Computer Science is preferred
• Familiarity and use of various open-source ecosystems including JavaScript, Bigdata, java, python etc.
• Good understanding of various software paradigms: domain-driven, procedural, data-driven, object-oriented, functional
• Familiar with .Net Core (C#), Java, Python
• Demonstrable knowledge depth in more than one area of software engineering and technology

Preferred Qualifications:

If you have the following characteristics, it would be a plus:
• Experience in agile software development and DevOps, relevant technology platforms [e.g., Kubernetes] and frameworks [e.g. Docker] including cloud technologies & data structures (i.e. information management), data models or relational database design
• Subject matter expertise in clinical development
• R&D Tech requires Engineers with understanding of the relevant technical and scientific domains. Able to deliver continuous change to meet rapidly evolving R&D strategy and ambition.
• Experience with agile development methods, with security strategies and best practices, data integration mechanisms, architectural design tools, delivering and integrating COTS applications, areas of Service Oriented Architecture (SOA), Application Integration, Business Process Management and Data Quality.
• Experience in applying AI/ML, data curation, virtualization, predictive modelling, workflow, and advanced visualization techniques to enable decision support across multiple products and assets to drive results across R&D business operations.

At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.
#LI-GSK

GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in “gsk.com”, you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,True,False,False,True,False,False,True,True,False,False,False,False,False,False,False,False
Bloom Consulting Services,Data Engineer,"Data Engineer ( Job ID : 815310498 )

data engineer

NA

Contract

Experience

06.0 - 08.0 years

Offered Salary

10.00 - 14.00

Notice Period

Not Disclosed

Job Description

Total Experience6 to 8 years

Min Relevant Experience: 3 to 5 years

Location :Bangalore

JD: Data Engineer

Role Description:

In this role, you will be part of a growing, global team of data engineers, who collaborate in DevOps mode, in order to enable business with state-of-the-art technology to leverage data as an asset and to take better informed decisions.

The Life Science Data Engineering Team is responsible for designing, developing, testing, and supporting automated end-to-end data pipelines and applications on Life Science’s data management and analytics platform (Palantir Foundry, Hadoop and other components).

The Foundry platform comprises multiple different technology stacks, which are hosted on Amazon Web Services (AWS) infrastructure or own data centers. Developing pipelines and applications on Foundry requires:
• Proficiency in SQL / Java / Python (Python required; all 3 not necessary)
• Proficiency in PySpark for distributed computation
• Familiarity with Postgres and ElasticSearch
• Familiarity with HTML, CSS, and JavaScript and basic design/visual competency
• Familiarity with common databases (e.g. JDBC, mySQL, Microsoft SQL). Not all types required

This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.

Roles & Responsibilities:
• Develop data pipelines by ingesting various data sources – structured and un-structured – into Palantir Foundry
• Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application
• Acts as business analyst for developing requirements for Foundry pipelines
• Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline
• Document technical work in a professional and transparent way. Create high quality technical documentation
• Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)
• Deploy applications on Foundry platform infrastructure with clearly defined checks
• Implementation of changes and bug fixes via change management framework and according to system engineering practices (additional training will be provided)
• DevOps project setup following Agile principles (e.g. Scrum)
• Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java
• Work closely with business users, data scientists/analysts to design physical data models

Education
• Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields

Professional Experience
• 5+ years of experience in system engineering or software development
• 3+ years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.

Required Knowledge, Skills, and Abilities

Data engineer",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"• Experience with Azure Data Bricks, Data Factory
• Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics
• Experience in Python/Pyspark/Scala/Hive Programming.
• Experience with Azure Databricks/ADB
• Good understanding of SQL queries, joins, stored procedures, relational schemas
• Experience with NoSQL databases, such as HBase, Cassandra, MongoDB",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Genpact,Data Engineer,"With a startup spirit and 90,000+ curious and courageous minds, we have the expertise to go deep with the world's biggest brands--and we have fun doing it! We dream in digital, dare in reality, and reinvent the ways companies work to make an impact far bigger than just our bottom line. We're harnessing the power of technology and humanity to create meaningful transformation that moves us forward in our pursuit of a world that works better for people.

Now, we're calling upon the thinkers and doers, those with a natural curiosity and a hunger to keep learning, keep growing. People who thrive on fearlessly experimenting, seizing opportunities, and pushing boundaries to turn our vision into reality. And as you help us create a better world, we will help you build your own intellectual firepower.

Welcome to the relentless pursuit of better.

In this role, resource will be expert in designing, building and maintaining data infrastructure. Work will help people with unmet medical needs, including those who wish to quit smoking, those with major depression disorder, and those with schizophrenia--ultimately improving lives through engineering. Help design and build a data infrastructure using state-of-the-art technologies with data security at utmost importance and employ elegant solutions to help ensure Client's data products meet compliance needs (e.g., GDPR and HIPAA) in different regions of the world.

Responsibilities!
• Design, build and maintain analytical data infrastructure which includes both data processing and data reporting.
• Onboarding data from both internal and external systems.
• Collaborate with Product, Engineering, Science, Data analysts and Data scientists to implement rich and re-usable datasets/metrics.
• To make data infrastructure and applications scalable, reliable, and secure.
• Strong attitude towards automating routine tasks via coding/scripting.
• Research on security and privacy requirements and provide solutions.

Qualifications we seek in you!
• B Tech/M Tech/BCA/MCA
• Experience in building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience writing complex, highly optimized SQL queries.
• Experience with reporting to enable explanatory and exploratory analytics.
• Python development experience.
• Have experience with dbt, Airflow, Snowflake and AWS infrastructure.
• Have experience implementing APIs to share data with internal / external vendors.
• Experience implementing streams.
• Understanding of privacy and security regulations (e.g., GDPR, HiTrust, HIPA)",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
Vanderlande Careers,Lead Data Engineer,"Lead Data Engineer at DSF

Vanderlande provides baggage handling systems for 600 airports around the globe, capable of moving over 4 billion pieces of baggage around the world per year. For the parcel market our systems handle 52 million parcels per day. All these systems generate data. Do you see a challenge in building data-driven services for our customers using that data? Do you want to contribute to the fast growing Vanderlande Technology Department on its journey to become more data driven? If so, then join our Digital Service Factory team!

Your Position

As a lead data engineer you will be leading the data engineering efforts in a product team. You will work together with product/solution architecture to provide technical necessities to design and develop end-to-end data ingestion pipelines and well tested and monitored data services. You will assess the technical dependency between different functional components and define a resolution. You will also provide technical guidance and coaching to the junior/medior data engineers in the team, set technical standards and best practices.

Your responsibilities:
• You will be designing, developing, testing, and documenting the data collection framework. The data collection consists of (complex) data pipelines with data from (IoT) sensors and low/high level control components to our Digital Service and Data Science platform.
• You will build monitoring solutions for data pipelines which enable data quality improvement.
• You will develop scalable data pipelines to transform and aggregate data for business use, following software engineering and Data Mesh best practices. For these data pipelines you will make use of the best and most applicable frameworks available for data processing.
• You develop our data services and data products for customer sites towards a product, using (test & deployment) automation, componentization, templates, and standardization to reduce delivery time of our projects for customers. The product provides insights in the performance of our material handling systems at customers all around the globe.
• You design and build a CI/CD pipeline, including (integration) test automation for data pipelines. In this process you strive for an ever-increasing degree of automation and high levels of security.
• You will work with infrastructure engineers to extend storage capabilities and types of data collection (e.g. streaming)
• You have experience in developing APIs.
• You will coach and train the junior data engineer with the state of art big data technologies.
• You will lead the Data Engineering Guild where passionate members discuss current trends, short term development, and solutions for ongoing issues that span multiple teams.

Your Profile
• Total experience of 10+ years (with at least 7+ years of programming exp)
• Experience programming in Python and/or Scala (Java programming exp is a plus)
• You are familiar with DevOps practices and have relevant experience in automation (CI/CD), measurement, applying lean practices and what DevOps culture entails
• You know how to achieve high performing secure pipelines, maintain and test them
• You are familiar with different storage formats (e.g. Azure Blob, SQL, noSQL)​
• Experience with scalable data processing frameworks (e.g. Spark)​
• Experience with event processing tools like Splunk or the ELK stack​
• Deploying services as containers (e.g. Docker and Kubernetes)​
• You have experience with streaming data platforms (e.g. Kafka )​ and messaging formats (e.g. Apache AVRO)
• Strong experience with cloud services (preferably with Azure)

Diversity & Inclusion

Vanderlande is an equal opportunity employer. Qualified applicants will be considered without regards to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Pune,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Visa,Sr. Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.
Job Description

This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. You will be an integral part of the Payment Products Development team focusing on design and development of software solutions that leverage data to solve business problems. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and testing of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Responsible for the design, development, and implementation
• Work on development of new products iteratively by building quick POCs and converting ideas into real products
• Design and develop mission-critical systems, delivering high-availability and performance
• Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology
• Develop code to ensure deliverables are on time, within budget, and with good code quality
• Have a passion for delivering zero defect code and be responsible for ensuring the team's deliverables meet or exceed the prescribed defect SLA
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
• Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.
Qualifications

We are seeking team members that are passionate, visionary and insatiably inquisitive. Successful candidates frequently have a mix of the following qualifications:

• Bachelor’s Degree or an Advanced Degree (e.g. Masters) in Computer Science/ Engineering, Information Science or a related discipline
• Minimum of 3 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies
• Extensive experience with SQL and Big Data technologies (Hadoop, Java, Spark, Kafka, Hive, Python) for large scale data processing and data transformation
• Deep knowledge of Unix/Linux
• Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
• Familiar with software design patterns
• Experience working in an Agile and Test-Driven Development environment
• Strong knowledge of API development is highly desired
• Strategic thinker and good business acumen to orient data engineering to the business needs of internal and external clients
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
• Previous exposure to financial services is a plus, but not required
Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
Shell,"Senior Data Engineer- Azure (ADF, Data lake)","Join the number One Global Lubricants supplier in the world and be part of the team that helps in shaping up the digital and the IDT strategy which delights our customers in over 100 countries across every sector.

If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

Where you fit

Shell's Projects and Technology (P&T) business exists to make the delivery of our strategies and the growth of our company possible. Our team develops the advanced products and technologies Shell needs to meet customer demand. Our solutions help our partners grow the LNG, Gas and Power businesses, deepen the integration of Manufacturing, Chemicals and Trading, and maximise the competitiveness of our Upstream business.

What's the role?

As a Data Engineer in Shell, you will create and maintain optimal data pipeline architecture and also will a ssemble large, complex data sets that meet functional / non-functional business requirements.

You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

More specifically, your role will include:
• Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.
• Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

What we need from you

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree and at least have a Seniority level in their previous workplace.

They should also have experience using the following software/tools:
• Experience with Azure: ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.
• Experience with relational SQL/NoSQL databases, file handlings and API integrations
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODS

Skills - Nice to Have
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing data pipelines using ADF
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of transforming, processing and extracting value from large disconnected datasets
• Strong team player with organizational and communication skills
• Experience supporting and working with cross-functional teams in a dynamic environment",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fibe India,Data Engineer - SQL,"Responsibilities:
• The candidate is expected to lead one of the key analytics areas end-to-end. This is a pure hands-on role.
• Ensure the solutions are built to meet the required best practices and coding standards.
• Ability to adapt to any new technology if the situation demands.
• Requirement gathering with business and getting this prioritized in the sprint cycle.
• Should be able to take end-to-end responsibility for the assigned task
• Ensure quality and timely delivery.

Requirements:
• Experience: 3- 6 years.
• Strong at PySpark, Python, and Java fundamentals
• Good understanding of Data Structure
• Good at SQL query/optimization
• Strong fundamental of OOPs programming
• Good understanding of AWS Cloud, Big Data.
• Nice to have Data Lake, AWS Glue, Athena, S3 Kinesis, SQL/NoSQL DB",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer,"Role: Data Engineer Job Description
• Design, build, and maintain distributed batch and real-time data pipelines and data models.
• Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
• Be curious and eager to work across a variety of engineering specialties (i.e., Data Science, and Machine Learning to name a few).
• Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
• Enforce privacy and security standards by design.
• Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Qualifications
• +3 years experience building complex data pipelines and working with both technical and business stakeholders.
• Experience in at least one primary language (e.g., Java, Scala, Python) and SQL (any variant).
• Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
• Experience creating and maintaining ETL processes.
• Experience designing, building, and operating a DataLake or Data Warehouse.
• Experience with DBMS and SQL tuning.
• Strong fundamentals in big data and machine learning.

Preferred Qualifications
• Experience with RESTful APIs, Pub/Sub Systems, or Database Clients.
• Experience with analytics and defining metrics.
• Experience with measuring data quality.
• Experience productionalizing a machine learning workflow; MLOps
• Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
• Language ability in Japanese and English is a plus (We have a professional translator but it is nice to have language skills).
• Experience with AWS services.
• Experience with microservices.
• Knowledge of Data Security and Privacy.

experience

6",Hyderabad,True,False,True,True,False,False,False,False,False,False,False,False,True,True,False,False
deloitte,Consulting - BO - Cloud Engineering - Manger - Azure Data Engineer,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organisations, Deloitte is where you will find numerous opportunities to succeed and realise your full potential.

The team

Deloitte is working with global customers on cloud technologies to help unlock growth, stability, and sustainability by enabling them to spot unseen business trends through curation, transformation, and blending of data. In our endeavors for continued expansion, we’re searching for like-minded individuals to help us ‘take it to the next level’.

In this exciting opportunity for an experienced developer, you will join a team delivering a transformative cloud hosted data platform for some of the world’s biggest organizations. The candidate we seek, needs to have a proven track record in implementing data ingestion and transformation pipelines on Microsoft Azure. Deep technical skills and experience with working on Azure Databricks. Familiarity with data modelling concepts and exposure to Synapse.

You will also be required to participate in stakeholder management, highlight risks, propose deliver plans and estimate for time and team size based on requirements. Hence, adequate levels of communication skills and relevant experience in handling such situations is desired.

Scope of work

Your main responsibilities will be:
• Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
• Delivering and presenting proofs of concept of key technology components to project stakeholders.
• Developing scalable and re-usable frameworks for ingesting and enriching datasets
• Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
• Working with event based / streaming technologies to ingest and process data
• Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
• Evaluating the performance and applicability of multiple tools against customer requirements
• Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.

Qualifications
• Strong knowledge of Data Management principles
• 9+ years of total years of experience
• Experience in building ETL / data warehouse transformation processes
• Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably Databricks).
• Experience using Apache Spark and associated design and development patterns
• Microsoft Azure Big Data Architecture certification is an advantage.
• Hands-on experience designing and delivering solutions using Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
• Experience with Apache Kafka / Nifi for use with streaming data / event-based data (Nice to have but not mandatory)
• Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)
• Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
• Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Terraform etc.

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters.

In addition to living our purpose, managers across our organisation:
• Develop self by actively seeking opportunities for growth, share knowledge and experiences with others, and act as a strong brand ambassadors
• Understand objectives for clients and Deloitte, align own work to objectives and set personal priorities
• Seek opportunities to challenge self
• Collaborate with others across businesses and borders to deliver and take accountability for own and team results
• Identify and embrace our purpose and values and put these into practice in their professional life
• Build relationships and communicate effectively in order to positively influence peers and other stakeholders

Professional growth

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn.From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Our Purpose

Deloitte is led by a purpose: To make an impact that matters.

Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and work—always striving to be an organisation that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description

insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.

Job Description
• Develops and maintains scalable data pipelines for bulk data movement between systems of record and systems of reference
• Develops and maintains scalable application to application integrations
• Aligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
• Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes
• Writes appropriate unit or integration tests to implement test-driven development
• Continually contributes to and enhances data team documentation
• Performs data analysis required to troubleshoot and resolve data related issues
• Works closely with a team of frontend and backend engineers, product managers, and analysts
• Defines company data assets, artifacts and data models

Qualifications

Required qualifications:
• 5 years of Data Engineering and Data Integration
• 5 Years of Data Warehousing
• 3 Years of Data Architecture and Modeling
• 2 years of Cloud Data Engineering
• Agile Methodologies

Preferred skills:
• AWS or Azure Data Certifications
• Experience with databricks, spark, python
• Experience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)
• Experience with Salesforce

Additional Information

All your information will be kept confidential according to EEO guidelines.
• * At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. **

insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Northern Tool + Equipment, India",Senior Data Engineer,"Are you an individual who wants to play a game changing role and make an impact in a fast-growing organization? We at Northern are waiting for you. Join us and unleash your potential!!

We are hiring <>!!

Join the core group of founding members at the NTE India to build an organization from the ground up.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Role Objective

PRIMARY OBJECTIVE OF POSITION:

We are looking for an Experienced Data Engineer who will partner with a specific business function and understand the requirements, builds data model, creates data pipelines and stored procedures. Also work with Data Analysts/Modelers, Data Visualization Engineers to deliver high performing analytics.

MAJOR AREAS OF ACCOUNTABILITY:
• SME for data structures and data models for specific line of business.
• Analyze and understand various source systems and related data structures.
• Build and automate creation of ETL pipelines and stored procedures to move data from source system to consumption layer using variety of ETL methods.
• Collaborate with Data Analysts/Data Architect/Data Visualization Engineers to provide them with Data mapping documents and ensure adherence to a common data model.
• Responsible for administration and security of data and analytics assets in Azure.
• Works collaboratively and effectively communicates with others across departments in order to perform and complete necessary tasks and projects.
• Follows established Software Development Life Cycle (SDLC) to enable CI/CD in relevant areas.
• Follow established change control, release management and incident management processes.
• Responsible for performance and tuning, scaling of Azure resources to optimize costs
• Builds and maintains relationships cross-functionally in order to stay current with the needs and operations of the business functional areas supported.
• Supports the day-to-day operation of the reporting and analytic solutions by troubleshooting ETL and other errors encountered during data processing.
• Keeps manager informed of important developments, potential problems, and related information necessary for effective management. Coordinates and communicates plans and activities with others, as appropriate to ensure a coordinated work effort and team approach.

Job Description

Performs related work as apparent or assigned.

QUALIFICATIONS:
• To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
• Bachelor’s Degree in Computer Science, Statistics, Mathematics, Business or related field.
• At least 6 years relevant work experience in Data and Analytics field.
• In-depth understanding of Data warehousing concepts.
• Hands-on experience in writing complex, highly optimized SQL queries across large data sets
• Hands-on experience in building performance optimized data pipelines (ETL/ELT)
• Experience in configuring, deploying, and provisioning of IaaS, PaaS with Terraform and PowerShell using Azure DevOps and GIT.
• Specific familiarity with the Microsoft Azure Data Stack - ADF, Azure SQL DB, Azure Synapse (SQL Data Warehouse), Azure Data Lake, Azure Storage and Analysis Services.
• Azure Security & Identity: Azure Active Directory App Permissions, Key Vaults.
• Hands-on experience in creating user groups, creating security policy and implementation of Row-Level Security(RLS) to restrict the data access to the users.
• Hands-on experience with data cataloging and data profiling concepts.
• Diversity of perspective for various tools and technologies like Azure Stream Analytics, Azure Databricks, NoSQL databases, read or write optimized databases to advocate for their appropriate adoption at Northern Tool.
• Basic programming experience using .NET, Python, or any scripting language.
• Must be willing to work as a team and possess the skills to work independently.
• Demonstrated ability to take initiative and utilize creativity on assigned projects.
• Must possess strong analytical, problem-solving, and technical design skills.
• Demonstrates Northern Tool + Equipment’s 12 Core Competencies.
• Sounds interesting? Here’s your chance to join our family at Northern.

About the Company

Northern Tool + Equipment is a retailer and manufacturer that specializes in offering superior quality tools at great prices, along with the knowledge and support needed to help customers get the job done right.

They’ve been in business for over 40 years, recently reaching revenues over $1.5 billion. The company not only supplies over 100,000 tools from the top brands in the industry but also designs, manufactures, and tests an extensive lineup of premium private label products that customers can’t get anywhere else.

Northern Tool’s far-reaching customer base includes handy men and women, weekend hobbyists, serious do-it-yourselfers, full-fledged contractors, trade professionals, and more. The company’s products can be found in over 120 retail stores in the USA, on its comprehensive international website, and via numerous catalogs throughout the year. Recently Northern Tool has expanded operations to offices in India to serve its global distribution better.

We are recently named as one of the Top Workplaces for MidSize Employers by Forbes in the US.

We have also been recognized as the “Top GCC to work for in AI and analytics” and our India HR team as the “Top HR Professionals in AI and Analytics” by 3AI which is a professional firm associated with analytics within India.

About NTE India

Northern Tool is making a significant investment in business transformation. We are committed to providing our customers with an exceptional experience. The team in India will enable Northern Tool to expand its internal capabilities in Finance, Merchandising, Product Engineers, Manufacturing Ops, Marketing, Contact Center, and Information Technology.

Why Northern?

True Northern: We know that our strength is our people. The distinct abilities they bring into the system are the key to our success. We seek talented people who wish to share their initiative, ideas, and expertise; we develop and support our teams, and we put them in a position to succeed. We know our customer; we provide value, and we act with integrity. We are True Northern.

Build Lasting Relationships: At Northern Tool + Equipment, we’re far more interested in building relationships than we are in simply making transactions. Our purpose is building a long-lasting relation with our customers and employees.

We care for our customers, employees and society. Our customer base is exceptionally loyal because customers know that we will give them the right solution.

Accelerate Decision Making: by collaborating with the brightest minds, bring ideas to life across our value chain of business operations across our vast network of over 120 stores across the US.

Lead with Innovation: Join us to elevate our customer experience?with cutting-edge products, technology, and business processes and?drive our business forward.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Does this sound interesting?? Be an early applicant!!

Northern Tool is an Equal Opportunity Employer. We encourage and empower everyone and support diversity in experience, and point of view. We are pledged to a fair and a transparent hiring process with no discrimination of race, color, ancestry, religion, gender, national origin, age, citizenship, marital status, disability, or veteran status.

Requirements
• name : Northern Tool + Equipment, India
• location : Hyderabad, IN
• experience : 6 - 9 years
• employmentType : Full-Time
• Primary Skills: ETL or ELT,Python,Data Warehousing,Azure Data Lakes or Data Factory,SQL",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Comcast India Engineering Center I, LLP",Data Engineer 3,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary About Sky We’re Sky, Europe’s biggest entertainment brand. Think top-quality shows. Breaking news. Innovative tech. Must-have products. Careers here mean the freedom and support you need to make an impact – pushing boundaries, creating solutions, hitting targets. And as part of our close-knit team, you’ll enjoy plenty of benefits. Plus, experiences you’ll only find at Sky. We love telling the world we work at Comcast . We’re fans too. We move fast and embrace pace. We have the freedom to be brilliant. And we work collaboratively because together we can. This is how we work at Comcast and why we love it. Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Documents all development activity. Assists with training non-technical personnel. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as a resource for colleagues with less experience. Job Description Core Responsibilities Create and maintain an optimal data pipeline architecture focussed upon network data, including real-time and batch data sources. Assemble large, complex data sets that meet functional and non-functional business requirements. Build batch/streaming ELT/ETL solutions from a wide variety of data sources in varying formats (SQL, JSON, AVRO, HTTP, API, etc.) using the right blend of tools. Keep our data compliant, relevant and secured across multiple data centres and regions. Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery and evolving current solutions whilst ensuring continuity of service. Create data tools for data scientist team members that assist them in building and optimizing into an innovative industry leader. Guide and collaborate with data consumers on analytics, tooling and platform related queries. Employees at all levels are expected to: Graduate degree BSc in Computer Science, Electrical Engineering or similar. Strong communications skills. SQL knowledge and experience working with relational databases. Strong analytic skills related to working with structured and unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Hands-on experience in building scalable data platforms. Awareness of security practices and privacy concerns when working with data across both in-house and cloud platforms. Ideally an awareness of network technologies and concepts or an insatiable desire to learn. Key technologies Apache Airflow / NiFi / Kafka / ZooKeeper Confluent ecosystem (Connect / Schema Registry / ksqlDB) GCP (BigQuery, Dataflow, Pub/Sub, IAM) Linux / Terraform / Ansible Python / Docker (Nice to have). Experience: 5 Years to 7.5 Years Location: Chennai Disclaimer: This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. Comcast is an EOE/Veterans/Disabled/LGBT employer. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools that are personalized to meet the needs of your reality—to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the benefits summary on our careers site for more details. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Certifications (if applicable) Relative Work Experience 5-7 Years Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. At Comcast , you have the power to connect the world. Your career options are endless as you grow in your career. Explore your future with access to a variety of teams, locations, and resources in an expanding network. You can also explore additional opportunities at our company, NBCUniversal.",,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
Revolo Infotech,Data Engineer - SQL/Python,"Job Description :

- Design, develop, and maintain data pipelines and architecture for data storage, processing, and analysis

- Work with cross-functional teams to understand and implement data requirements

- Build and optimize data pipelines using various cloud-based technologies such as AWS, Azure, or Google Cloud Implement data visualization solutions using cloud-based tools such as Tableau, Power BI, or Looker Monitor and troubleshoot data pipeline issues, and implement solutions to improve performance and scalability

- Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis

- Stay up-to-date with the latest technologies and industry trends in data engineering and data visualization

Requirements :

- 2+ years of experience as a data engineer with a focus on cloud-based data pipelines and visualization

- Strong experience with cloud-based technologies such as AWS, Azure, or Google Cloud

- Experience with data visualization tools such as Tableau, Power BI, or Looker

- Strong knowledge of SQL and programming languages such as Python or Java

- Familiarity with big data technologies such as Hadoop, Spark, or Hive

- Strong problem-solving and analytical skills

- Experience working in an Agile development environment Bachelor's degree in Computer Science or related field.

Preferred Qualifications :

- Experience with data warehousing concepts and technologies

- Experience with data governance and data management best practices.

- Experience with machine learning and AI technologies Strong communication and teamwork skills.

Job Types : Full-time, Regular / Permanent, Contractual / Temporary

Salary : 1,000,000.00 - 1,200,000.00 per year

Benefits :

- Health insurance

- Internet reimbursement

- Paid sick time

- Paid time off

Schedule :

- Day shift

- Monday to Friday

Power BI: 2 years (Preferred)

Tableau: 2 years (Preferred)

AWS: 2 years (Preferred)
(ref:hirist.com)",Navi Mumbai,True,False,True,True,False,False,False,False,True,True,False,False,False,False,False,False
MediaMath,Data Engineer,"About Us

MediaMath is the leading technology pioneer on a mission to make advertising better. We deliver outstanding results through powerful ad tech, partnership and a curiosity for what’s next. We help more than 3,500 advertisers solve complex marketing problems so they can deepen their customer relationships across screens and around the world.

Key Responsibilities

MediaMath’s Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by Analytics to discover and deliver insights that drive value for our clients. The Analytics team fulfils customers’ advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. As the Data Engineer on the Analytics Engineering team within the Analytics team, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:
• Become an expert in MediaMath data flows and the Analytics data infrastructure.
• Build, maintain, and own scalable data pipelines to support client data integration.
• Become a team SME in data munging and automated ETL processes.
• Work with Analysts to understand and leverage big data to solve client problems and needs.
• Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
• Be a team player, and bring the team and company forward by solving team and company priorities.

You are:
• Experienced in writing readable, re-usable code SQL and Python (our entire team uses Jupyter Notebook and Pandas!)
• Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
• Experienced in implementing data pipeline health monitoring, alerting
• Experienced with data infrastructure troubleshooting and working with system logs
• Experienced developing data flow schematics/blueprints
• Advocate for automation and building efficient, scalable solutions
• Self-driven, with a hunger to learn and spread knowledge by teaching others
• Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:
• Bachelor’s Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
• 3 - 5 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
• Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
• Experience partnering with client-facing teams to understand client needs and translate them to technical requirements

Nice-to-have’s:
• Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)
• Experience working with REST APIs, web services, object-oriented technologies like Java, C++
• Public GitHub repos or notebooks that illustrate the way you think about data
• Exposure to ad-tech, digital marketing, or e-commerce industries

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are three values that provide a framework for how we approach our work and the world: Win Together, Obsess Over Growth, and Do Good, Better. These values inform how we energize one another and engage with our clients. They get us amped to come to work.

Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath is recognized as a Leader in the Gartner 2020 Magic Quadrant for Ad Tech and has won Best Account Support by a Technology Company for two years in a row in the AdExchanger Awards.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristic as established by law.

MediaMath focuses on Digital Media, Internet, Advertising, Software, and Marketing. Their company has offices in New York City, San Francisco, Chicago, Durham, and Singapore. They have a large team that's between 501-1000 employees. To date, MediaMath has raised $617.877M of funding; their latest round was closed on July 2018.

You can view their website at http://www.mediamath.com or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,False
"Atlassian, Inc.",Senior Data Engineer,"Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.",,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
Motilal oswal,Data Engineer,"Job Description : Strong AWS Data Engineering skills. Exposure to SSIS, SSRS, SSAS will be an advantage,Handson experience working with S3, Redshift, Glue, EMR, RDS, Athena, Aurora,Strong development skills and experience coding with SQL, Pyspark, Python,High on ownership and accountability,Comfortable with change, initial hiccups and small failures,Experience with understanding designs, creating low level designs, unit test cases, unit testing and assisting with Integration and User acceptance testing,Experience of 2-6yrs with AWS Data technologies.",,True,False,True,False,False,False,False,False,False,False,True,False,True,False,False,False
Fisker Inc.,Data Engineer,"Responsibilities
• Work with leaders, engineering and data scientists to understand data needs.
• Design, build and launch efficient and reliable data pipelines to best utilize connected vehicle data for real-time systems and within data warehouses.
• Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
• Help insure that best practices are followed when storing, retrieving and accessing data.

Qualifications
• 3+ years of Python development experience.
• 3+ years of SQL experience.
• 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
• 3+ years experience with Data Modeling.
• Experience in organizing queries, tables and pipelines with proper indexing, partition and sharding.
• 3+ years experience in custom ETL design, implementation and maintenance.
• Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Clickhouse, Spark, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience with more than one coding language, ideally Go or C++ and java.
• Experience with designing and implementing real-time pipelines.
• Experience with data quality and validation.
• Experience with SQL performance tuning and E2E process optimization.
• Experience with notebook-based Data Science workflow.
• Experience with Airflow.
• Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,True,True,False
Poshmark,"Software Developer, Data Engineering","The Big Data team is a central player in the Poshmark organization. Our mission is to build a world-class big data platform to bring value out of data for us and for our customers. Our goal is to democratize data, support exploding business, provide reporting and analytics self-service tools, and fuel existing and new business critical initiatives.

The Data Engineering team at Poshmark is looking for an experienced software engineer to take care of Poshmak’s growth data, ensuring real-time access to quality data for all the stakeholders. The role requires strong understanding of software engineering best practices and excellent software development skills to build and maintain real-time and batch data pipelines with a focus on scalability and optimizations. In addition, the role also requires collaborating with Data Science, Analytics and other Engineering teams to build newer ETLs analyzing terabytes of data.

The role also requires being able to write clean and scalable code to pull datasets from disparate sources involving External APIs, S3 transfers, Web Scraping. You will work with cutting edge technologies and frameworks like Scala, Ruby, Apache Spark, Airflow, Redshift, Databricks, Docker. You will also manage the growth data infrastructure comprising ETL pipelines, Hive tables, Redshift tables, BI tools. We are looking for a software engineer who can help us define the next phase of growth data systems in terms of scalability and stability.

Responsibilities
• Design, Develop & Maintain growth data pipelines and integrate paid media sources like Facebook and Google to drive insights for business.
• Build highly scalable, available, fault-tolerant data processing systems using AWS technologies, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 100s of terabytes of data ingested every day and a petabyte-sized data warehouse.
• Responsible for architecting/designing/developing critical data pipelines at Poshmark.
• Productionizing ML models in collaboration with the Data Science and Engineering teams.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.
• Participate and contribute to constantly improving best practices in development.

Desired Skills & Experience
• Excellent technical problem solving using data structures and algorithms, with emphasis on optimization and code quality.
• 1-3 years of relevant software engineering experience using object oriented programming languages like Scala / Java / Ruby / Python / C++ etc.
• Expertise in architecting and building large-scale data processing systems using Big Data technologies like Spark, Hadoop, EMR, Kafka/ Kinesis, Flink, Druid.
• Expertise in SQL with knowledge on any existing data warehouse technology like Redshift
• Expertise in Google Apps Script, Databricks or API Integrations is a plus.
• Be self-driven, take complete ownership of initiatives, make pragmatic technical decisions and collaborate with cross-functional teams.",Chennai,True,False,True,True,True,True,False,True,False,False,False,True,True,False,True,False
Confidential,Data Engineer - AWS/ETL,"Role and responsibilities :- The Data Engineer will be responsible for leading design, development, transformation, deployment, and maintenance of Data Warehousing stack on AWS- Work with BI and dev team to build data pipelines using AWS Glue and similar tools.- Develop custom data ingestion jobs and ETL scripts using Python/Spark scripts- Perform data modelling and schema design activities in Data Lake and Data Warehouse environments as per the standard practices- Advanced SQL knowledge and experience working with relational databases, able to write/debug complex SQL queriesYour profile must have :- 4+ years of experience in building data pipelines and data warehouse architectures on cloud platforms such as AWS- Exposure to agile methodology- Experience in developing Python or Spark jobs- Strong understanding of data modelling principles- Good communication and collaborative skillsExtra points if you have :- Built processes supporting data transformations, data structures and workload management in Database/Data Warehouse- Experience in performance tuning of Redshift databases and implement recommendations- Experience with sourcing data using APIs from external systems- Experience working with teams across the globe, in a fast-paced, high-tech and customer-obsessed environment- Exposure to Shopify, Amazon and Syndicated data (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
ANI Calls India Private Limited,Azure Data Engineer with Big Data,"Anicalls Industry:

IT
Total Positions: 3

Job Type:
Full Time/

Permanent Gender:
No Preference Salary: 900000 INR - 1400000 INR ( Annually )

Education:
Bachelor′s degree Experience: 8 -12

Years Location:
Hyderabad, India . Azure Data Factory . Azure Databricks . Python, Scala, PySpark, Spark . HIVE / HIVE LLAP / HBASE / CosmoDb . Azure Active Directory Domain Services . Apache Ranger / Apache Ambari . Azure Key Vault . Expertise in HDInsight ( Minimum 2 -3 years ' experience with multiple implementations ) . Expertise in Cloud Native and Open Cloud Architecture",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
DAZN,Senior Data Engineer,"Are you an engineer who loves to make things that just work better? Do you love to work with cutting edge technologies and think about how can this run faster, be deployed quicker or fail less and deliver killer streaming applications that add business value and stick with customers?

DAZN is a tech-first sport streaming platform that reaches millions of users every week. We are challenging a traditional industry and giving power back to the fans. Our new Hyderabad tech hub will be the engine that drives us forward to the future. We’re pushing boundaries and doing things no-one has done before. Here, you have the opportunity to make your mark and the power to make change happen - to make a difference for our customers. When you join DAZN you will work on projects that impact millions of lives thanks to your critical contributions to our global products

This is the perfect place to work if you are passionate about technology and want an opportunity to use your creativity to help grow and scale a global range of IT systems, Infrastructure and IT Services. Our cutting-edge technology allows us to stream sports content to millions of concurrent viewers globally across multiple platforms and devices. DAZN’s Cloud based architecture unifies a range of technologies in order to deliver a seamless user experience and support a global user base and company infrastructure.

Join us in India’s beautiful “City of Pearls” and bring your ambition to life.

Benefits will include access to DAZN, an annual performance related bonus, family friendly community, free access for you and one other to our workplace mental health platform app (Unmind), learning and development resources, opportunity for flexible working, and access to our internal speaker series and events.
As our new Data Engineer, you'll have the opportunity to:

• Support building real-time user-facing analytics and data driven operations applications
• Be responsible with the rest of the team for the availability, performance, monitoring, emergency response, and capacity planning
• Use your love of big data systems, thinking about how to make them run as smoothly and securely as possible, support operational endpoints
• Have a strong sense of teamwork and put team’s / company’s interests first

You'll be set up for success if you have

• 5+ years’ experience writing clean, robust and testable code, preferably in Typescript
• Experience building high performant, low latency and high velocity data pipelines
• Working knowledge in AWS services, such as Kinesis, EventBridge, SQS, SNS Topic, S3, Lambda, Kinesis, EKS, Firehose
• Experience with infrastructure-as-code (preferably Terraform) and CI/CD processes
• Comfortable building & maintaining production level data pipelines; streaming or event driven.
• Strong analytical and communication skills.

Even better if you have:

• Exposure to streaming technologies such as Apache Kafka / Google PubSub, Apache Beam, Google Dataflow.
• Having worked in an agile environment with scrum / kanban delivery methodologies

At DAZN, we bring ambition to life. We are innovators, game-changers and pioneers. So if you want to push boundaries and make an impact, DAZN is the place to be.

As part of our team you'll have the opportunity to make your mark and the power to make change happen. We're doing things no-one has done before, giving fans and customers access to sport anytime, anywhere. We're using world-class technology to transform sports and revolutionise the industry and we're not going to stop.

If you're ambitious, inventive, brave and supportive, then you're the kind of person who's going to enjoy life at DAZN.

We are committed to fostering an inclusive environment, both inside and outside of our walls, that values equality and diversity and where everyone can contribute at the highest level and have their voices heard. For us, this means hiring and developing talent across all races, ethnicities, religions, age groups, sexual orientations, gender identities and abilities. We are supported by our talented Employee Resource Group communities: proud@DAZN, women@DAZN, disability@DAZN and ParentZONE.

If you’d like to include a cover letter with your application, please feel free to. Please do not feel you need to apply with a photo or disclose any other information that is not related to your professional experience.

Our aim is to make our hiring processes as accessible for everyone as possible, including providing adjustments for interviews where we can.

We look forward to hearing from you.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer: Enterprise Content Management,"Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As Enterprise Content Management, you will be working as an application developer on projects in OpenText Process suite BPM. Your role would also involve in playing a critical role in design of a new system

Responsibilities:
• As a Business Process Management (BPM) Developer, you will manage asset services and application development while collaborating with global team in harmonizing the development of asset management applications.
• You will focus on improving corporate performance by managing business processes.
• Identification and driving of related service quality improvements and engineering deliverables.
• Management and progression of Action items on time with prompt response
• Automation and process improvement, if applicable
• Client communication

Required Technical and Professional Expertise
• Minimum 4 years of core development experience as OpenText Process Suite Developer
• Proficient in OpenText Process Suite BPM and having knowledge to design and develop the workflow
• Experience in Xform, HTML5, Angular JS. Javascript, & SQL
• Working knowledge of Core Java, Web Services & Ws APP integration is an added advantage
• Knowledge on Rest API's and SOAP' API's

Preferred Technical and Professional Expertise
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications

About Business UnitIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients' businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBMIn a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you'll be able to learn and develop yourself and your career, you'll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBMIBM's greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we're also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it's time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location StatementWhen applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBMIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Bengaluru,False,False,True,True,False,False,True,False,False,False,True,False,False,False,False,False
Digital Mapout Solutions India Private Limited,Azure Data Engineer,"Role : Azure Data Engineer

Location : Bangalore / Hyderabad

Experience : 4+

M.O.H : Full Time

M.O.W : Work From Office

NP Immediate / 15 days

Education : ÂBE / BTech, ME / MTech / MCA.

Skillsets : Azure Data Factory+Azure Data Lake + Azure SQL+ Azure Synapse+PowerBI

JD : -

Azure data / Lead Engineer :

Mandatory Skill sets : T SQL, Data Warehousing (DW) , ADF, Synapse Analytics

Optional : Power BI-DAX,Data Bricks, Python,PySpark

experience : 3 to 15 years

Senior developer to leads
• Candidate must have a strong experience background in database, Data warehousing & ETL / ELT design and development
• Exposure to complex & large scale enterprise development environment
• Excellent communication & collaboration skills required. Ability to work with internal and external stakeholders is must.
• Good business acumen
• Good problem solving and analytical ability
• Ability & willingness to learn new skills

Core technical skills
• Azure data lake Gen 2
• Synapse Analytics
• Python / scala programming
• Synapse pipeline / Azure data factory
• Azure SQL
• T-SQL programming
• Power BI-DAX",Bengaluru,True,False,True,False,True,False,False,True,True,False,False,False,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.Job DescriptionDevelops and maintains scalable data pipelines for bulk data movement between systems of record and systems of referenceDevelops and maintains scalable application to application integrationsAligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organizationImplements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processesWrites appropriate unit or integration tests to implement test-driven developmentContinually contributes to and enhances data team documentationPerforms data analysis required to troubleshoot and resolve data related issuesWorks closely with a team of frontend and backend engineers, product managers, and analystsDefines company data assets, artifacts and data modelsQualificationsRequired qualifications:5 years of Data Engineering and Data Integration5 Years of Data Warehousing3 Years of Data Architecture and Modeling2 years of Cloud Data EngineeringAgile MethodologiesPreferred skills:AWS or Azure Data CertificationsExperience with databricks, spark, pythonExperience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)Experience with SalesforceAdditional InformationAll your information will be kept confidential according to EEO guidelines.** At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. ** insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Factspan,Factspan Analytics - Azure Data Engineer - Big Data/Hadoop,"Job Description :- 7+ Years of deep experience with complex data systems and good instincts around data modelling and usage.- Knowledge of data engineering technologies, architecture, and processes. Specifically, Azure Data Lake, Hadoop ecosystem, Kafka, and common third-party integration and orchestration tools.- Good knowledge of multi-cloud data ecosystem and build scalable solutions on cloud (Azure)- Good knowledge of Big Data Ecosystem-Spark, Hadoop, Databricks- Work across 3-4 teams to develop practices which lead to the highest quality products and contribute transformation change within the cloud- Experience building large scale data processing ecosystems with real time and batch style data as input using big data technologies- Experience in any programming language like Scala or Python.- Exposure to agile methodology and proven ability to technically lead a team of engineers across geographies.- Implement Data Quality, Data Governance on Azure Cloud ecosystem- Good instincts around technical architecture, including metadata, Rest API Integrations, Data API and Solution design of NoSQL and File systems.- Willingness and ability to invest in engineering growth.- Strong communication skills and ability to coordinate across a diverse group of technical and non-technical stakeholders.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Big Data Engineer,"DATA ENGINEER - JD

The role will be part of the Data and Analytics Team responsible for expanding and optimizing AECOM’s data and data pipeline architecture, data flow, and collection for cross functional teams. The role will support software developers, database architects, data analysts, and data scientists on data initiatives and will ensure consistent optimal data delivery architecture throughout ongoing projects.

Responsibilities & Duties
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep AECOM’s data separated and secure across national boundaries through multiple data centres and regions.
• Create data tools for analytics and data scientist team members -to assist them in building and optimizing our product into an innovative industry leader.
• Collaborate with data and analytics experts to strive for greater functionality in our data systems.
• Escalate issues and recommend resolutions to the Team Lead for timely. May support junior members of the team in addressing routine issues within the assigned processes.
• Maintain the SOP/DTP of current processes and incorporate documentation updates as required.
• Perform moderately complex tasks in compliance with service level agreement, process, policies, and procedures.
• Propose alternatives in identified issues and assist in investigating and in resolving common and unusual issues.
• Contribute in various and simultaneous process improvement initiatives to streamline processes, improve customer experience, and increase productivity. This includes automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Contribute specialized expertise to different assigned projects and may provide key updates to Team Lead and Manager.

Qualifications & Requirements

Minimum Requirements:
• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or relevant discipline in the quantitative field
• 6-10years
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Advanced working SQL/nosql, ADLS, Databricks, ADF, Azure DevOps
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency,and workload management.
• Demonstrated ability to manipulate, process, and extract value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.

Preferred Qualifications
• Experience with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Attributes
• Demonstrated ability to champion and drive ideas/programs/solutions
• Excellent organizational and time management skills, able to work under pressure and prioritize effectively
• Able to demonstrate passion, energy and drive, especially in the face of resistance
• Ability to effectively communicate and collaborate within a varied audience and internal and external customers. (Communication)
• Ability to maintain good customer relationship with the ability to suggest ways to improve customer support customer experience (Customer Service)
• Ability to be thorough and meticulous in completing assigned tasks and identifying errors, duplicates, & discrepancies through defined methods. (Attention to Detail)
• Ability to identify and resolve simple to moderate with the ability to provide resolution alternatives by following defined policies and procedures. (Problem Solving)

experience

10",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,True,False,True,False
Tiger Analytics India Consulting Private Limited,Senior Data Engineer - Denodo,"Job Title: Senior Data Engineer – Denodo

Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 2800+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Curious about the role? What your typical day would look like?
· Engage with clients to understand their business context.
· Translate business needs to technical specifications.
· Define Data Virtualization architecture, deployments, and standards.
· Support the development of data architecture principles, standards, and processes and applies these to deliverables
· Involving in data exploitation and the development of (advanced) analytical data models with multiple data sources using Denodo/Tibco or AtScale semantic layer.
· Developing integrated data solutions, modernizing, consolidating, and coordinating business needs across several applications.
· Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Senior Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Senior Data Engineer

Senior Data Engineer, Delivery Engineering Platform
Delivery Engineering Platforms is part of MasterCard Data & Services group and one of the most rapidly growing organization in the space. Platform Teams provides cloud-based analytic software tools that enable large, consumer-focused businesses to seize the Big Data analytics opportunity by triangulating between business strategy, algorithmic math, and large databases to improve decisions.

100 of the largest corporations in the world uses these products. Test & Learn™ for Sites, Test & Learn™ for Customers, Test & Learn™ for Ads, and other similar products employ patented algorithms and workflow to design and interpret business experiments that evaluate, target, and refine proposed business programs

The Delivery Engineering Platform team is a core component to consulting services, managing the data acquisition, integration and transformation of client provided data within the Test & Learn platform for global engagements.

Role

The Senior Data Engineer will lead and participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace. A Senior Data Engineer will:
• Independently lead projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base
• Act as an expert technical resource within the team and region
• Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors
• Build trust-based working relationships with peers and clients across local and global teams
• Implement best practices and collaborate in the design of effective streamlined processes for a complex global solutions group
• Leverage industry best practices including proper use of source control, participation in code reviews, data validation and testing
• Plays a lead role where he/she oversees the activities of the data engineers and ensures the efficient execution of their duties
• Act as an advisor/mentor and helps in managing careers for junior team members
• Comply and uphold all MasterCard internal policies and external regulations

All about you:
• BE/BTech in a quantitative field (e.g., Computer Science, Statistics, Econometrics, Engineering, Mathematics, Operations Research). ME/MTech preferred
• Excellent English quantitative, technical, and communication (oral/written) skills; is an excellent listener
• Expertise with hands-on experience with RDMS technologies, preferably with Microsoft SQL Server, the SSIS Stack and .Net; Proficiency with at least one scripting language (VB Script, Perl, Python)
• Proven self-motivated leader with experience working in teams
• Demonstrate excellent skills in the ability to innovate, think critically and disaggregate problems. Able to provide oversight, validation and quality control to own and team work product
• Ability to easily move between business, data management, and technical teams; ability to quickly intuit the business use case and identify technical solutions to enable it
• Able to balance multiple projects and differing project priorities
• Flexible to work with global offices across several time zones

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LodgIQ,Data Engineer,"About LodgIQ

Headquartered in New York, LodgIQ delivers a revolutionary SaaS platform for Algorithmic Pricing and Revenue Management for the hospitality industry by incorporating machine learning and artificial intelligence. For more information, visit http://www.lodgiq.com.

Backed by Highgate Ventures and Trilantic Capital Partners, LodgIQ is a well-funded company, seeking for a motivated and entrepreneurial Developer to join its Product/ Engineering team. Qualified candidates will be offered an excellent compensation and benefit package.

Title: Data Engineer

Location: India

Requirements:
• In-depth knowledge of Python.
• Understanding of Django/Flask, Pandas.
• Familiarity with AWS Environment (EC2, S3, IAM, Athena).
• Working knowledge of NoSQL databases such as MongoDB.
• Proficiency in consuming and developing REST APIs with JSON data.
• Ability to perform data mining and data exploration with intuitive sense for problem solving and strong desire for craftsmanship.

Specific Job Knowledge, Skills & Abilities:
• Real world experience with large-scale data on AWS or similar platform.
• Must be a self-starter and an effective data wrangler.
• Intellectual curiosity and strong desire to learn new Big Data and Machine Learning technologies.
• Deadline driven, and capable of delivering projects on time under a fast paced, high growth environment.
• Willingness to work with unstructured and messy data.
• Bachelor’s degree or Masters degree in relevant quantitative fields (e.g. Computer Science, Statistics, Electrical Engineering, Applied Mathematics, etc).",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Edu Angels India Private Limited,Data Engineer (PySpark),"Responsibilities
• Develop process workflows for data preparations, modeling, and mining Manage configurations to build reliable datasets for analysis Troubleshooting services, system bottlenecks, and application integration.
• Designing, integrating, and documenting technical components, and dependencies of big data platform Ensuring best practices that can be adopted in the Big Data stack and shared across teams.
• Design and Development of Data pipeline on AWS Cloud
• Data Pipeline development using Pyspark, AWS, and Python.
• Developing Pyspark streaming applications

Eligibility
• Hands-on experience in Spark, Python, and Cloud
• Highly analytical and data-oriented
• Good to have - Databricks",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Zepto,Data Engineer III (Lead Data Engineer),"Responsibilities:
• Collaborate with Tech and Analytics team to build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
• Oversee and govern the expansion of the current data architecture as the business grows and ensure best practices are followed.
• Design and build best-in-class architecture for data tables to ensure optimal querying performance in relational databases.
• Create and maintain connectors that expose the data securely for consumption by downstream systems and services in near real-time.
• Create and maintain data architecture docs to communicate data requirements that are important to business stakeholders and work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that.
• Build data governance and security protocols and ensure adherence from analytics, tech, and business teams.
• Build and mentor the data engineering team, recognize their strengths, and lead them to take ownership of end-to-end data architecture.
• Stay on top of the latest developments in the tech stack and propose potential upgrades to existing systems.

Requirements:
• 6 to 10 years of experience in Data Engineering - Designing databases, building data pipelines, and maintaining data governance protocols in cloud platforms.
• A visionary in technical architecture, with experience building and maintaining Data.
• Engineering Products, along with the demonstrated ability to take accountability for achieving results.
• Hands-on working experience with Python, ETL pipelines, and advanced SQL.
• Strong understanding of AWS Services - Redshift, Lambda, Glue, Athena, and security protocols.
• Experience in any Cloud DW Redshift/Snowflake/BigQuery/Synapse.
• Strong data Modelling and database design experience with Redshift or other relational databases.
• Experience working with Agile methodologies, Test Driven Development, and implementing CI/CD pipelines using Gitlab and Docker.
• Good understanding of ETL/ELT technology and processes.
• Experience in gathering and processing raw data at scale including writing scripts, web scraping, and calling APIs.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,True,False,True
Confidential,Data Engineer 3 - AWS & Python (Contractual),"IntroductionThe Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed. At our heart is a 50 year forward look, a global forecast of the majority of the world's economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions. We're changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products. What will you experience At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to. Accountabilities How you will contribute: Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment. Work in an agile environment: within a collaborative agile product team using Kanban Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements. Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives. Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals. Experience, skills and professional attributesTo succeed in this role it would be an advantage if you possess: Experience with programing in Python, and Lambda functions Knowledge of building bespoke ETL solutions, and extracting data using Data APIs MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting Prior experience in design and developing microservice architecture Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. A combination of IT skills, data governance skills, analytics skills and economics knowledge An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience. Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. This employer is a corporate member of myGwork - LGBTQ+ professionals, the business community for LGBTQ+ professionals, students, inclusive employers & anyone who believes in workplace equality. PRB",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Axtria - Ingenious Insights,Data Engineer,"• 5-8 years of experience in data engineering, consulting, and/or technology implementation roles
• Expertise in the design, data modeling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation and delivery
• Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena
• Strong proficiency in SQL and database design, development and maintenance
• Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence
• Good understanding of modern architecture patterns like serverless and microservices
• Expertise with analytics and business intelligence solutions (e.g. 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation and Agile Methodology
• Experience working with some of the following marketing data sources
• Traditional > TV / Print / Email
• Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data
• Experience leading project teams with members with different roles and skills
• Experience working in hybrid onshore/offshore team models
• Strong communication skills",New Delhi,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Valiance Solutions,Big Data Engineer,"About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc. Our team comprises 150+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire a Senior Data Engineer to help our customers create scalable data engineering pipelines and infrastructure for downstream analytics workloads. You should be good at understanding client data needs, the landscape of various heterogeneous data sources, identifying a set of services for data ingestion & transformation workloads, and timely execution of projects.

Roles & Responsibilities:
• As a data engineer with Pyspark & SQL skills you will be required to highly scalable, robust, and resilient data engineering pipelines .
• You will be working closely with business stakeholders & the data science team to understand their data requirements and underlying business logic.
• Deploy and monitor pyspark jobs on cloud infrastructure.
• Troubleshoot job failures and ensure system recovery at earliest.
• Attending regular client calls, communicating work status and pro-actively highlighting any delays to the product release.

Technical Skills :
• Hands on experience on pyspark for at least 3 years
• Solid programming experience in Python & SQL is required.
• Working experience of any one cloud platform; AWS, GCP or Azure
• Intermediate plus proficiency in shell scripting
• Experience deploying ML algorithms in production is preferred

Personal Skills :
• Excellent communication skills, both written & oral.
• Ability to learn new skills quickly, adjust to the changing needs of the project.
• You are highly enthusiastic about your work
• Ability to multi-task, manage high-pressure release scenarios occasionally.

Valiance Solutions focuses on Financial Services, Cloud Computing, Artificial Intelligence, Internet of Things, and Big Data Analytics. Their company has offices in Noida and Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://valiancesolutions.com or find them on Twitter and LinkedIn.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
DarioHealth,Data Engineer - Hybrid,"About The Position

At Dario, Every Day is a New Opportunity to Make a Difference.

﻿We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career.

DarioHealth is looking for an experienced Data Engineer who will join our team and create new data solutions, maintain existing solutions and be a focal point of all technical aspects of our data activity. As part of this position, you will develop advanced data and analytics solutions to support our analysts and production units with validated and reliable data.

Responsibilities
• Develop and maintain DarioHealth data infrastructure.
• Develop in-house applications for providing self-service tools.
• Develop real-time data applications for production.
• Provide analysts and data scientists technical support related to data infrastructure.
• Design, build and launch new data models and visualizations in production, leveraging common development toolkits.

Requirements
• At least 4 years of proven experience with Python - a must.
• Very high level of SQL and data warehouse modeling.
• Experience with 24/7 systems and real-time analytics.
• Experience developing data pipelines with Airflow or similar - a must
• Experience with big data solutions like Kinesis/Sparks - an advantage
• Experience with NoSQL databases like MongoDB/Redis.
• Experience with web development using Django/javascript/react - an advantage.
• Experience in the online industry.
• B.A./B.Sc. in industrial/information systems engineering, computer science, statistics, or equivalent.
• **DarioHealth promotes diversity of thought, culture and background, which connects the entire Dario team. We believe that every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions. We are passionate about building and sustaining an inclusive and equitable working and learning environments for all people, and do not discriminate against any employee or job candidate.***",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False
AlphaGrep Securities,Data Engineer,"About the Company

AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes (equities, commodities, currencies, fixed income) that trade on global exchanges..

We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.

Roles & Responsibilities
• Build infrastructure tools and applications to support trading teams across the firm.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Coordinate with global teams to understand their requirements and work alongside them.
• Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
• Set up practices on data reporting and continuous monitoring
• Write a highly efficient and optimized code that is easily scalable.
• Adherence to coding and quality standards.

Required Skills
• Strong working knowledge in Python.
• Strong working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have
• Experience with web crawling and scraping, text parsing
• Experience working in Linux Environment
• Experience with Stock Market Data

Why You Should Join Us
• Great People. We’re curious engineers, mathematicians, statisticians and like to have fun while achieving our goals
• Transparent Structure. Our employees know that we value their ideas and contributions
• Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.
• Health & Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Robert Bosch,Azure Data Engineer,"Job Description

Location : Bengaluru
Experience : 6 to 8 years
Requirements
• Overall 6+ IT experience out of which 3+ years of working experience in Azure, architecting data soultions with good experience on Azure SQL, Azure Data Lake, ADF, Azure DataBricks/Synapse etc.
• 5+ years experience with data modelling,implementing backends and data optimization.
• Good experience working with with Automotive domain usecases.
• Experience implementing compliances like GDPR,HIPAA etc.
• Enforcing data security at rest and in transit.
• Good experience implemeting data security in Azure storage systems.
• Ability to thrive in a fast-paced, dynamic, client-facing role where delivering solid work products to exceed high expectations is a measure of success
• Excellent leadership and interpersonal skills
• Eager to contribute in a team-oriented environment
• Ability to be creative and analytical in a problem-solving environment
• Effective verbal and written communication skills

Skills
• Must have skills : SQL,ADF,other Azure storage services.
• Good to have:Synapse,spark or any big data framework or data warehouse.
• Key Responsibilities : conceptualizaing ,modelling Optimizing databases.Designing data flows
• Knowledge or basic experience with Nosql,parquet,Predictive modelling etc
• Working knowledge, creating ETL packages and deploying them

Qualifications

BE,MCA,MSC,MS,MTech
Experience : 6 to 8 years

Additional Information

Additional information
• Nice to have : Power BI experience, Azure DevOps, Cost Monitoring, Azure AD, Azure Synapse.
• Ability to quickly ramp up on new Azure Components which comes in Azure Roadmap.
• Excellent communication skills (English)

Experience: 6.00-8.00 Years",,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
CX Customer Experience,Salesforce - Data Engineer,"Come create the technology that helps the world act together

Nokia is committed to innovation and technology leadership across mobile, fixed and cloud networks. Your career here will have a positive impact on people’s lives and will help us build the capabilities needed for a more productive, sustainable, and inclusive world.

We challenge ourselves to create an inclusive way of working where we are open to new ideas, empowered to take risks and fearless to bring our authentic selves to work.

The team you'll be part of

You will work as part of the CX Global Sales Operation team. You will work making our Data strategy come alive across CX. You will be involved with integrating data across multiple platforms and especially the integration of the Advanced Analytic platform and the use cases being developed on it. The Advance Analytic projects will enable the CX organization to increase speed and efficiency, and assure the right things are done in the right way to maximize Nokia business.

What you will learn and contribute to

Be responsible for the data engineering of the Advanced Analytics Platform and CX AI use cases

Data integration from different source data platform to the advanced analytics platform

Data integration from the advanced analytic platform to different platform with UI

Ongoing support for the integration

Potentially doing data cleaning and wrangling on the advanced analytics platform

Potential involvement in dashboard and UI development

The type of use case you will work on will center around one or more of the following:
• Machine learning prediction models,
• The use of machine learning to automate the currently manual business planning process
• Knowledge mining and recommendation systems to improve the likelihood to win new business.

Your skills and experience
• Deep experience end data engineering including but not limited to experience using SQL and other types of databases and database languages
• Proficiency developing software probably in python using jupyter notebooks
• Proven competency in agile and lean software development
• Competency in SCM (Git), Automation tools, infrastructure automation,
• Good knowledge about Azure cloud infrastructure, security and application development.
• Experience with Python / Spark and Delta Lake. Familiar with big data patterns like lake house.
• Experience with Azure DevOps, CI/CD pipelines, version control tools like GIT / VSTS. Familiar with IDE’s like Visual Studio
• Nokia Business and process understanding
• Bachelor’s degree or higher in information technology, data science or related disciplines
• Effective communication in English (written and verbal)

Nice to have:
• Experience in app development

What we offer

Nokia offers flexible and hybrid working schemes, continuous learning opportunities, well-being programs to support you mentally and physically, opportunities to join and get supported by employee resource groups, mentoring programs and highly diverse teams with an inclusive culture where people thrive and are empowered.

Nokia is committed to inclusion and is an equal opportunity employer

Nokia has received the following recognitions for its commitment to inclusion & equality:
• One of the World’s Most Ethical Companies by Ethisphere
• Gender-Equality Index by Bloomberg
• Workplace Pride Global Benchmark
• LGBT+ equality & best place to work by HRC Foundation

At Nokia, we act inclusively and respect the uniqueness of people.

Nokia’s employment decisions are made regardless of race, color, national or ethnic origin, religion, gender, sexual orientation, gender identity or expression, age, marital status, disability, protected veteran status or other characteristics protected by law.

We are committed to a culture of inclusion built upon our core value of respect.

Join us and be part of a company where you will feel included and empowered to succeed.

Additional Information",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 20 2023

Ready to help shape the future of healthcare?

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organization where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to impact the health of 2.5 billion people around the world in the next 10 years.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves - feeling welcome, valued and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.

The Senior Principal Data Engineer is a vital technical role in the successful design and delivery of Data and Analytics (D&A) initiatives for the GSK's Pharmaceutical and Vaccines Supply Chains. The primary purpose of this role is to ensure that D&A Products have an optimal solution design and that the technical development work to then deliver them into production and support is smooth and successful. This requires deep expertise in data and analytics platforms and technologies as well as domain understanding of Pharmaceutical & Vaccines manufacturing and quality processes. This also requires close collaboration with D&A Product Managers, D&A Development Squads and the D&A Platform & Architecture team as well as with business stakeholders and other Digital and Tech teams.

The MSAT & Quality D&A team currently has a portfolio of around 15 D&A products across 4 product groups with over 100 people (GSK employees plus contractors) working in agile squads to deliver these. The Sr Principal Engineer will oversee and be accountable for the technical success of all of these products.

Key Responsibilities:
• Accountable for optimal solution designs for D&A Products that facilitates an agile, product management approach, can be rapidly and cost-effectively delivered to meet the true business requirements and are robust, sustainable and supportable throughout their lifecycle
• Work closely with D&A Product Mangers, using deep technical expertise and domain understanding to effectively influence (and when needed challenge) business and architectural stakeholders to arrive at the right design
• Steer solution design through D&A Architecture Review process, aligning with enterprise platforms and architectural patterns by first intent
• Oversee technical work of development teams ensuring it is remains aligned with agreed design, is of high quality, complies with relevant standards and policies and will meet agreed business objectives
• Provide hands-on technical problem-solving expertise to address technical challenges during development and, where needed, during lifecycle support
• Act as mentor for more junior technical roles, supporting their development and promoting adoption of best practice across development teams
• Lead discovery / proof-of-concept activities to establish early technical feasibility of new Products or Product Features
• Input to, review and approve key technical documents (e.g. design spec, validation plan)
• Drive adoption by development teams of existing and future best-practice approaches from D&A Platform team (e.g. implementation of DevOps CI/CD pipelines, automated testing)
Why you?

Basic Qualifications:
• Computer Science or related Bachelor's degree
• 16+ years of experience and track record of engineering and delivery of flexible, scalable, and supportable data and analytics applications for large complex, global organizations
• End-to-end / 'full stack' D&A experience from data ingestion through transformation to user interaction (visualisation, analytics, etc.)
• Track record of designing and delivering solutions in a cloud environment using modern data architectures and engineering technologies
• Experience designing with DataOps and FinOps in mind to ensure solutions are flexible/future-proof and can scale to handle growing demand, while remaining cost effective
• Experience in Agile development
• Track record of designing and delivering solutions compliant with industry regulations and legislation
• Ability to oversee and matrix manage GSK and 3rd party technical resources
• Excellent communication, negotiation, influencing and stakeholder management skills.
• Customer focus and excellent problem-solving skills.
Preferred Qualifications:
• Computer Science or related Master's degree
• Microsoft Azure accreditation and experience
• SAFe (Scaled Agile) accreditation experience
• Experience developing and delivering GxP-validated solutions for the Pharma/Vaccines industry
• Experience with specific technologies in GSK stack: Talend, Databricks/DeltaLake, Azure Synapse, Snowflake, PowerBI, Azure Functions, Azure App Services,
At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.

#LI-GSK

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We're committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in ""gsk.com"", you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Confidential,Data Engineer,"Job purpose We are hiring a Data Engineer to join our Enterprise Analytics team. As a Data Engineer, you will be responsible for building, maintaining, and optimizing the data infrastructure needed to support our company's data-driven initiatives. You will work closely with our data analysts, data scientists, and business intelligence developers to ensure that our data is accurate, complete, and secure.Job Responsibilities:Design, build, and maintain the data infrastructure needed to support our company's data-driven initiatives.Develop and maintain data pipelines and ETL processes that move data from source systems to our data warehouse.Implement data quality checks to ensure that our data is accurate, complete, and consistent.Work closely with our data analysts, data scientists, and business intelligence developers to understand their data needs and ensure that our data infrastructure meets those needs.Optimize our data infrastructure to ensure that it can handle large amounts of data and support complex queries.Develop and maintain documentation for our data infrastructure and processes.Stay up to date with the latest technologies and trends in data engineering and recommend new tools and techniques as appropriate.Collaborate with other members of the BI and Data team to ensure that our data infrastructure is aligned with our company's strategic goals.Background and experience:Bachelor's degree in Computer Science, Engineering, or a related field.3+ years of experience in data engineering or a related field.Competencies and skills:· Strong knowledge of SQL and experience working with relational databases.· Experience with data modeling and schema design.· Experience building and maintaining data pipelines and ETL processes.· Experience with cloud-based data warehousing technologies, such as Azure Data Lake, Data Factory, Synapse Analytics.· Strong problem-solving skills and attention to detail.Excellent communication and collaboration skills. Transportation, Logistics and Storage,IT Services and IT Consulting,Truck Transportation",,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Loop Health,Data Engineer - Remote,"About Loop

Looking for a great mission? Help build a customer focused healthcare company.

Loop wants to create an inspirational healthcare and insurance company. We believe in the transformative nature of empathetic primary care, proactive financial coverage and want to bring that to our members. We want to fundamentally change how healthcare assurance is designed and delivered. We believe in the power of incentives. We are successful when we deliver health outcomes — when our members and their families get healthier.

“Why exactly are we building a new revolutionary healthcare system? The obvious answer is India deserves better care for its people. Not enough of it around, and what exists can be tough to navigate.

Imagine if doctors were paid to actually make you better. What a concept! What if they didn't have to worry about finishing consults in 10 minutes to meet their daily quota. What if they could take their time, really understand the symptoms, the family history & the lifestyle to come up with a plan, rather than just a prescription.

Imagine if hospital admissions, treatments, billing and insurance were as easy as ordering food home and your care doesn't end when they send you home from the hospital. It goes till you are back on your feet. And further, now imagine if your family had access to this great care anytime they wanted. From serious conditions to the smallest questions. So that they live longer. Wouldn't you worry less?

At the end of it. It's not why you would build this system... Why wouldn't you?”

Here’s how we are going about it:

- We built a high quality concierge and primary care program that allows members and their families to access unlimited care when they need it.
- We use technology to deliver this through highly engaging care.
- We work with insurers to bring financial protection to our members so that they do not worry about their families’ well being.
- As a healthcare insurance broker, we provide companies with the best coverage and claims service for their employees and dependents.

Doing this will mean that we create great products and services that work on changing behaviors and mindsets. This will need a deep understanding of design of products and services through an empathetic lens of what members need for their health and technology will play a very pivotal role in enabling our members to use our programs . We are looking for folks in our ‘EngineeringTeams’ to work with us to take Loop to this future.

If you'd like to learn more about what we are building at Loop, there are tons of resources. Here are some of our favorites:
https://yourstory.com/2021/06/loop-health-insurance-plans-improve-
healthcare/amphttps://yourstory.com/2022/04/loop-health-raises-25m-elevation-capital-general-catalyst/amp

Join us in making healthcare simple, reliable, and human.

Roles and Responsibilities

• Work in collaboration with engineers and stakeholders to build a platform for enabling data-driven decisions.
• Build reliable, scalable, CI/CD driven streaming and batch data engineering pipelines.
• Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.
• Create a conceptual data model to identify key business entities and visualize their relationships.
• Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and relationships
• Storage (cloud data warehouse, S3 data lake), orchestration (Airflow), processing (Spark, Flink), streaming services (Kafka), BI tools, graph database, and real-time large scale event aggregation store are all examples of data architecture to design and maintain.
• Work on cloud data warehouses, data as a service, business intelligence, and machine learning solutions.
• Data wrangling in a diverse environment.
• Ability to provide data and analytics solutions that are cutting-edge.
• Identify strategic and Operational KPIs for the team and drive the team to deliver the committed targets.

Qualifications

• SQL knowledge, as well as programming skills in Scala or Python.5+ years of applicable data warehousing, data engineering, or data architecture experience
• Experience with the GCP stack (BigQuery, GCP Databricks) is a plus
• Ability to design data analytics solutions to meet performance and scaling requirements.
• Demonstrated analytical and problem-solving abilities, particularly in the context of large data.
• Data warehousing concepts and modern data warehouse/Lambda architecture are well-understood.
• Good understanding of the Machine Learning and Artificial Intelligence (AI) solution space.
• Communication and interpersonal skills at all levels of management
• You are a detail-oriented person with excellent communication skills and a strong sense of teamwork.

What you can expect from us

  ‍  ‍   Loop Family Healthcare Health insurance for you and your family for all medical emergencies.

   High agency You'll always have the agency to shape projects, processes and outcomes independently.

  Learning Budget If there's a workshop, book or event you think will help you learn, we'll cover your bill.

   Work from home setup We'll help you set up your office the way you want to with the best equipment around.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,True,True,False
Visa,Sr. Data Engineer - Big Data Testing,"This position is ideal for an engineer who is passionate about solving challenging business problems. You will be an integral part of the Payment Products Development team focusing on test automation. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, and testing of new and existing functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Develop systems and processes to refine efficiency of automated testing solutions
• Design and execute tests for applications and services
• Develop and maintain tools for automation tracking and reporting
• Review product requirements and specifications and recommend improvements to ensure product testability
• Recommend areas of applications and services where automation would be beneficial
• Present technical solutions, capabilities, considerations, and features in business terms
• Effectively communicate status, issues, and risks in a precise and timely manner
• Perform other tasks on data governance, system infrastructure, and other cross team functions on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
PayPal,"MTS 1, Data Engineer","Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
The MTS 1 ? Data Engineer will directly report to and support Sr. Manger of Finance Technologies in the development and execution of strategic transformation programs & initiatives, strategic engineering architecture design, resource allocation, and platform performance monitoring. Ideal candidate is a technologist who believes that use of technology is in its infancy and the best is yet to come. The Regulatory Reporting Hadoop product owner (Business System Analyst) will be part of the Global Regulatory Reporting, and Merger & Acquisition Integration support. The nature of role is strategic, analytical and highly collaborative, working with team members across World and also as a liaison for Global projects.
• Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
• Undergraduate/ Master degree in Computer Engineering or equivalent from a leading university.
• 11+ years of post-college working experience as a Business System Analyst and leading large scale projects end to end.?
• Minimum 4+ years? experience working with large data sets, experience working with distributed computing a plus (Map/Reduce, Hadoop, Hive, Spark, etc.)
• Experience in Data Analysis, Data Validation.
• Strong knowledge in writing complex queries for validation of ETL process.
• Preferred/Basic understanding of Payments/Finance/Accounting Industry Background.
• Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations.
• Experience with databases, systems integration, application development and reporting.?
• Works independently and able to make decisions quickly when necessary.
• Quick Learner with an ability to ramp up in technologies and modules to meet business needs.
• Works in an Agile environment and continuously reviews the business needs, refines priorities, outlines milestones and deliverables, and identifies opportunities and risks.
• Maintain, track and collaborate with dev teams to ensure project estimation for delivery.
• Experience using JIRA and Confluence, or similar User Story workflow and management tool is a must.
• Highlight the bugs and blockers and coordinate with the development and operations team to come up with the best solutions/fixes and document them.
• Work across internal team in various geo-locations across the world
• ?Drive For Results? - Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.
• ?Priority Setting? - Spends his/her time and the time of others on what’s important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.
• Weekly and Monthly status reporting to leadership.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Software Engineer II | Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Software Engineer II | Data Engineer

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Overview
The Enterprise Data Solutions team is looking for a Big Data Engineer to drive our mission to unlock potential of data assets by consistently innovating, eliminating friction in how users access data from its Big Data repositories and enforce standards and principles in the Big Data space. The candidate will be part of an exciting, fast paced environment developing Data Engineering solutions in the data and analytics domain.

Role
• Develop high quality, secure and scalable data pipelines using spark, Scala/ python on Hadoop or object storage.
• Leverage new technologies and approaches to innovate with increasingly large data sets.
• Drive automation and efficiency in Data ingestion, data movement and data access workflows by innovation and collaboration.
• Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency.
• Perform assigned tasks and support production incidents.

All About You
• 4+ years of experience in Data Warehouse related projects in product or service-based organization
• Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
• Experience of building data pipelines through Spark with Scala/Python/Java on Hadoop or Object storage
• Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
• Experience of working on Nifi will be an added advantage
• Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
• Strong communication skills - both verbal and written
• Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
• Flexibility to work as a member of a matrix based diverse and geographically distributed project teams

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Plume Design,Senior Data Engineer,"Plume’s Cloud Platform team is looking for engineers to build and operate data pipelines that power the gamut of Plume products and analytics. Due to the massive scale and performance requirements of many of our use cases, you will be solving challenging problems on a daily basis using a variety of cutting edge technologies.

What you will do:
• Interact with stakeholders to gather and understand data requirements
• Design and implement data pipelines with high data quality goals
• Maintain up-to-date documentation of data warehouse schemas
• Write clean, maintainable code, and perform peer code-reviews
• Refactor code as needed to improve performance and simplify operations
• Provide production support in triaging and fixing issues relating to data quality and availability
• Mentor and assist junior team members and new hires to become successful and productive
• Adhere to data protection requirements including data access, retention, residency and de-identification
• Play an integral role in driving the technology roadmap and enhancing best practices

What You’ll Bring
• Education Requirements: BS/MS/PhD in Computer Science, Electrical Engineering or related technical field
• 5+ years of software development experience with a proven track record of building, scaling, and supporting production data pipelines
• High proficiency in writing idiomatic code, preferably in Java or Scala
• High proficiency in writing SQL in data warehousing technologies
• Strong understanding of large-scale data processing technologies, e.g. Apache Spark (preferred) or Apache Flink
• Strong understanding of data warehousing concepts
• Strong analytical and problem-solving skills
• Strong oral and written communication skills

Plume Design focuses on Internet Service Providers and Cloud Data Services. Their company has offices in Palo Alto. They have a mid-size team that's between 51-200 employees. To date, Plume Design has raised $37.5M of funding; their latest round was closed on June 2017.

You can view their website at https://platform.plume.com or find them on Twitter, Facebook, and LinkedIn.",Hyderabad,False,False,True,True,False,False,False,False,False,False,False,True,False,False,False,False
Lilly,Data Engineer - (DT) Business Insights & Analytics,"At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 35,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world.

Business Insights and Analytics: Data Engineer

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease. We’re looking for people who are determined to make life better for people around the world.

The LCCI (Lilly Capability Center India), BI&A (Business Insights & Analytics) team was started in 2017 with the objective of supporting business decisions for the commercial and marketing functions in the US and ex-US affiliates. This team is part of the LCCI - Commercial Services organization and works very closely with business analytics team based in Indianapolis (HQ). The team currently comprises of more than 100 staff members, with varied backgrounds and skills across data management, analytics and data sciences, business and commercial operations etc.

To better meet the evolving analytics needs, the LCCI BI&A team is ramping up the data engineering pillar. We are looking for data engineers who can be play integral role in developing, maintaining, and testing infrastructures for data generation, processing and storage; work closely with data scientists and help architecting solutions with the objective of driving right KPIs for the business.

Core Responsibilities:
• Create and maintain optimal data pipeline architecture ETL/ ELT into Structured data
• Assemble large, complex data sets that meet functional / non-functional business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.
• Expert level experience creating Fact tables, Dimensional tables and ingest datasets into Cloud based tools. Job Scheduling, automation experience is must.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Setup and maintain data ingestion, streaming, scheduling, and job monitoring automation. Connectivity between Lambda, Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and “big data” technologies like AWS and Google
• Build analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics
• Work with cross-functional teams including external consultants and IT teams to assist with data-related technical issues and support their data infrastructure needs
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

Experience Required
• 4-8 years of in-depth hands-on experience in data warehousing (Redshift or any OLAP) to support business/data analytics, business intelligence (BI)
• Advanced knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouses
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience building and optimizing “big data” data pipelines, architectures and data sets
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores
• Strong analytical and problem-solving skills to be able to structure and solve open ended business problems (pharma experience is highly preferred)

Education
• Bachelor’s/ Master’s degree in Technology OR Computer Sciences

Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response.

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,True,False,False,True
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications

• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
GE,Senior Data Engineer,"Job Description Summary
GE HealthCare is on a transformational journey leveraging Data and Analytics to drive business growth. GE HealthCare is looking for Senior Data Engineer who will be responsible for building and implementing the data ETL pipelines for Finance function data (from data ingestion to consumption).The Data Engineering team helps solve our customers' toughest challenges leveraging data and analytics. The Senior Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE HealthCare to drive business analytics to a new level of predictive analytics while leveraging On-prem, Cloud Platform, Big data tools and technologies.

GE HealthCare is a leading global medical technology and digital solutions innovator. Our purpose is to create a world where healthcare has no limits. Unlock your ambition, turn ideas into world-changing realities, and join an organization where every voice makes a difference, and every difference builds a healthier world.

Job Description

In this role you will:
• Responsible for building data and analytical engineering solutions with standard end to end design & ETL patterns, implementing data pipelines, data modelling and overseeing overall data quality.
• Responsible to work with cross functional teams in GEHC to make the data usable for functional users, data scientists and application users to enable delivery of business values to customers.
• Responsible to enable access of data in AWS storage layers and transformations in AWS Datawarehouse and further transporting in respective databases, consumers, data marts etc.
• As a Senior Data Engineer, you will be part of a data engineering or cross-disciplinary team on Finance facing development projects, typically involving large, complex data sets. These teams typically include data engineers, data visualization engineers, architects, data scientists, product managers, and end users, working in cohorts with partners in GE business units.
• Implement Data warehouse entities with common re-usable data model designs with automation and data quality capabilities.
• Demonstrate proficiency at industry standard data modeling tools (e.g., Erwin, ER Studio, etc.).
• Integrate domain data knowledge into development of data requirements.
• Develop processing codebase using pySpark and implement medium to complex transformations, business logics.
• Look across multiple systems, understands the purpose of each system and defines data requirements by systems.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Lead other horizontal improvement initiatives to benefit technology and leap further on a problem area or Hackathon etc
• Establish and maintain as a trusted advisor relationship within GE Healthcare Data & Analytics (Finance Function)
• Establish and maintain close working relationships with teams responsible for delivering solutions to the businesses and functions
• Engage collaboratively with project teams to support project objectives through the application of sound data engineering principles
• Identify risks and assumptions for the in scope Data & Analytics solutions
• Work with the contract/vendor resources to deliver the solution and manage the technical resources work

Qualifications
• Bachelor's Degree in Computer Science, Information Technology or equivalent (STEM)
• A minimum of 6 year of similar experience working on Database(s), SQL, Python, Datawarehouse, Java, ETL and AWS cloud platform is required. AWS certifications would be added advantage
• Experienced in Deployment process on-prem and on-cloud using Kubernetes, Dockers, Jenkins
• Ability to drive projects in big data (structured/unstructured/machine/logs/streaming data types)
• 3+ Year of Data modelling & Data warehousing experience with MPP systems (Teradata, Netezza, Greenplum etc.)
• 3+ years in AWS Services Like Redshift, RDS, S3, Glue, Step Function, Lambda etc.
• Hands on experience in delivering analytics in modern data architecture (Massively Parallel Processing Database Platforms and Semantic Modelling)
• Demonstrable knowledge of ETL and ELT patterns and when to use either one; experience selecting among different tools that could be leveraged to accomplish this. (i.e. Informatica, HVR, Talend etc)
• Demonstrable knowledge of and experience with different scripting languages (python, shell)
• Understands data quality and solves for application-level needs
• Understanding of DaaS, Data management tools / solutions
• Strong verbal & written communication
• Experience working with solutions delivery teams using Agile/Scrum ore similar methodologies
• Added advantage if experienced in working on Finance data

Desired skills:
• Delivers results when working on shorter-term (weeks-months), outcome-focused service engagements
• Proactively learning new technology, predicts trends, and identifies new opportunities based on trends
• Leverages knowledge about technology trends, and changing business needs across the broad environment to bring new ideas to the team
• Articulates the value proposition of existing technology capabilities and maps them to customer requirements to minimize incremental cost of development
• Experienced in working with On-prem (Teradata) data warehouse – Dimensional and data modelling. Experienced in one of the ETL like Informatica.
• Identifies the customer’s business and strategic needs, concerns, and desires for the value delivery capabilities of the Product
• Functional understanding of finance - Close Book, Treasury, Cash, Controllership, Credit, Account Payables, Account Receivables, Cash Forecasting, Balance sheet exposure, Debt, Forex etc.

Inclusion and Diversity

GE Healthcare is an Equal Opportunity Employer where inclusion matters. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

We expect all employees to live and breathe our behaviors: to act with humility and build trust; lead with transparency; deliver with focus, and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.
#LI-Hybrid
#LI-GM2

Additional Information

Relocation Assistance Provided: Yes",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,False
Concinnity Media Technologies,Senior Data Engineer,"Preferred Experience:

• 8+ years’ experience building mobile, web and/or API-based applications

• Follow engineering standards and best practices

• Knowledge of databases: MySQL, PostgreSQL, SQL, etc...

• 4+ years of Python server development experience

• Django, Flask, Bottle, or similar framework experience

• 2+ years industry experience

• Knowledge of cloud deployment strategies using AWS, Azure, Rackspace, etc.

• Expertise working with and building RESTful APIs

• Ability to operate in Agile / Scrum development environments

• Understanding of OOP and Data Structures and know when to apply them in daily coding scenarios

• Knowledge in the following web-technologies:
• JavaScript
• HTML & HTML5
• CSS3
• JavaScript frameworks (React, Angular, Next.js, etc.)

• Understand the development of the following:
• Responsive Web Development
• Accessibility
• Secure web applications

• Message queue implementations (RabbitMQ, ZeroMQ, Kafka, etc.)

• Background task processing (Celery, etc.)

• Experience configuring container like systems (Vagrant, Docker, etc.)

• Container orchestration with Kubernetes

• Ability to self-organize with minimal guidance/competing priorities and work effectively within a team

• Ability to provide innovative, creative solutions to tasks/problems

• Ability to complete work following engineering standards and best practices

• Experience with GIT and Gitlab is a plus

• Experience with JSON is a plus",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Hewlett Packard Careers,Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.
We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

The Data Engineer will develop, test, and maintain Big Data solutions for a company. Gather large amounts of data from multiple sources and ensure that downstream users can access the data quickly and efficiently. Essentially, the company’s data pipelines are scalable, secure, and able to serve multiple users.

Job description
• Meeting with managers to determine the company’s Big Data needs.
• Developing Hadoop systems.
• Loading disparate data sets and conducting pre-processing services using Spark, Hive or Pig.
• Finalizing the scope of the system and delivering Big Data solutions.
• Managing the communications between the internal system and the vendor.
• Collaborating with the software research and development teams.
• Building cloud platforms for the development of company applications.
• Maintaining production systems.
• Training staff on data management.

Big Data Engineer Requirements:
• Bachelor’s degree in computer engineering or computer science.
• Previous experience as a big data engineer.
• In-depth knowledge of Hadoop, Spark, and similar frameworks.
• Knowledge of scripting languages is preferred .
• Knowledge of NoSQL and RDBMS databases including Redis and MongoDB.
• Familiarity with Mesos, AWS, and Docker tools.
• Excellent project management skills.
• Good communication skills.
• Ability to solve complex data, and software issues.

Education and Experience Required:
• Typically, 6+ years of progressive professional experience as a big data engineer.
• Bachelor’s degree in computer engineering or computer science.

We love our work environment. We think you will too:
• It’s a friendly atmosphere with supportive leaders to bring your creativity to the max.
• Work-life balance support including flex-time arrangements and work from home opportunities.
• Corporate Social Responsibility initiatives to help you make an impact to communities at large.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Omnivio,Data Engineer - Full Time,"Job Profile

Omnivio is a startup in the Supply Chain and Logistics domain. We help retailers deliver an 'Amazon like' shopping experience to their customers. We optimize and manage delivery times and cost, inventory, geo-distributed stores etc. One of the core pieces of our infrastructure is the data engineering required to get data from various upstream systems into a data model that we use for intelligence. If you've worked in data engineering before, you might imagine that this has a good number of challenging engineering problems. We are looking for junior to mid level data engineers to join our team.

Experience / Skills required

We are looking for previous experience in data engineering for this role. Here's a list of tools and technologies that you can expect to be working with in this job. The listed examples are not necessarily all a part of our stack, but they are solid indicators of your skills being a good fit for the job.
• Relational Databases, both OLTP and OLAP, such as MySQL, Postgres, Redshift, BigQuery, CLickhouse etc
• Solid software engineering fundamentals, and experience with one or more general purpose programming languages such as Python, Typescript
• Data engineering programming libraries such as Pandas, NumPy etc
• Building data engineering pipelines using orchestration tools such as Airflow, Airbyte, Temporal, or other commercial offerings
• Transformations using dbt, or a similar alternative
• Experience with AWS's data engineering stack is definitely a plus
• Deployment/operating experience with any of these tools would be really interesting too

Work culture
• No ego anywhere in the team, including higher management.
• Remote, asynchronous, flexible work timings.
• Collaboration and team-thinking. No single person owns the failure.
• Ample time and attention to help developers level up.

Work Ex - 3+ years

Omnivio focuses on Supply Chain Management, Logistics, Cloud Infrastructure, Logistics Software, and Logistics / Transportation / Shipping. Their company has offices in Noida. They have a small team that's between 11-50 employees. To date, Omnivio has raised $400k of funding; their latest round was closed on July 2022 at a valuation of $5M.

You can view their website at https://omnivio.io or find them on LinkedIn.",,True,False,True,False,False,False,False,False,False,False,False,False,True,True,True,False
Mercedes-Benz Research and Development India Private Limited,Big Data Engineer,"AufgabenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team playerQualifikationenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team player",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Confidential,Data Engineer - SQL/Data Pipeline,"Job Description : : - Hands on working knowledge on building and optimizing 'big data' data pipelines, architectures, and data sets- Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases- Strong know-how on data ingestion tools and APIs to prioritize data sources, validate them, and dispatch data to ensure an effective ingestion process. Knowledge on data ingestion tools such as Apache Kafka/ Apache Storm/Apache Flume/Apache Sqoop/Wavefront, and more.- Working knowledge on data mining tools such as Apache Mahout/KNIME/Rapid Miner/Weka,- Strong know-how on ETL tools such as Talend/Informatica PowerCenter/AWS Glue/Stitch,- Ability to handle various types of data in the form of text, speech, image, video, or live stream from IoT/ Sensors/ Web- Ability to manipulate, process and extract value from large, disconnected datasets and articulate the same in business contextPreferred : - Skilled in the use of business intelligence and visualization tools, such as PowerBI, Tableau - Experience with stream-processing systems such as Storm, Spark-Streaming, etc.- Ability to leverage MLOps Platforms such as Teraform, Ansible, Kubeflow, Google AI Platform- Know-how on software development languages such as Python, Java, C++, Scala, etc (ref:hirist.com) IT",,True,False,True,True,False,True,False,False,False,True,False,False,False,False,False,False
NIRA,Sr Data Engineer/Architect (3y-7y),"About the job

Starting with credit, NIRA aspires to be the pre-eminent financial brand for the mass market or ""Middle India"". We already have customers in over 5,000 towns and cities, and we're growing quickly (15% MoM for the last 20 months!). It's a very exciting time to join us. We have over 200+ employees.

Currently, only 10% of Indians can use banks when they need credit: banks typically require a high credit score or collateral, something most people don't have. It need not be this way. Using a combination of traditional data and the vast amount of digital data available, it is now possible to score the unscored.

Today, we receive 15000 new loan applications daily from across 4000 cities in India, and we are growing 15-20% MoM. People reach us at their time of need, and we offer them credit via our app. Money reaches their bank account within 24 hrs of application.

We are addressing head-on a big challenge. It's also a great opportunity from both a commercial and societal impact perspective. We can improve lives for millions. It is no exaggeration to say that our addressable market will be 400mm within 5 years. It's pretty exciting, we think.

If our mission resonates with you, and you are a talented and hardworking individual that wants to commit yourself to an incredible challenge, then we want to hear from you.

As one of the senior data engineers on the team, you’ll be working on our core data platform and infrastructure powering business decisions and data science workloads.

Job Overview

We are looking for an experienced Data Engineer to join our engineering team. The hire will be responsible for building our data and data pipeline architecture. You will optimise our data flow starting with the collection of data for cross functional teams and purposes. You will support our key data science initiatives while maintaining consistency of data delivery architecture throughout ongoing projects. Ideal candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities

• Create and maintain optimal data pipeline architecture

• Assemble large, complex data sets that meet performance and business requirements

• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies

• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, credit risk, operational efficiency and other business KPIs.

• Create data tools for analytics and data scientist team members that assist them in building and scaling our core products

• Work with cross domain data and analytics experts to strive for stronger data driven outcomes for the business.

Qualification

• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science. They should also have experience using the following software/tools:

• Experience with big data tools: Hadoop, Spark / PySpark, Kafka.

• Experience with relational SQL and NoSQL databases.

• Experience with object-oriented/object function scripting languages: Python.

• Expertise in data modelling and buiding data driven systems.

• Well versed with Shell Scripting.

• Hands on experience on various AWS services like EMR,EC2,Glue.

• Deploy existing data projects using CICD pieplines.

• Knowledge on Docker is a plus.

What we offer:

• Competitive salary

• Medical Insurance

You can learn more about NIRA here:

Press:

https://yourstory.com/2019/05/startup-fintech-nira-entrepreneur-loans/

https://www.livemint.com/companies/news/muthoot-finance-partners-with-nira-to-provide-personal-loans-11620309871295.html

https://www.financialexpress.com/money/personal-loan-collection-rates-return-to-pre-covid-levels-data-from-nira-reveals/2313170/

NIRA focuses on Consumer Lending and Fin Tech. Their company has offices in Bengaluru. They have a large team that's between 201-500 employees. To date, NIRA has raised $3.1M of funding; their latest round was closed on April 2020.

You can view their website at https://www.nirafinance.com or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Referrals Only,Consultant-Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.
Job responsibilities• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will collaborate with Data Scientists in order to design scalable implementations of their models
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Assure effective collaboration between Thoughtworks' and the client's teams, encouraging open communication and advocating for shared outcomes
Job qualificationsTechnical skills• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Professional skills• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more
Other things to knowL&DThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.
About ThoughtworksThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 28+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.

Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
ANI Calls India Private Limited,Senior Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Bengaluru, India
Candidate should have:
Worked collaboratively with cross-functional teams and stakeholders to achieve an organizational goal.
Worked in an agile environment and are comfortable running an agile process for the data and analytics team.
Strong experience in data pipelines, ETL design (both implementation and maintenance), data warehousing, and data modeling (preferably in dbt).
Implementation and tuning experience in the Big Data Ecosystem,
(such as Data Analytics (Dataproc, Airflow, Hadoop, Spark, Hive),
Google Cloud Platform AI and ML Services and Data Warehousing (such as BigQuery, schema design,
query tuning and optimization) and data migration and integration.
End to end hands-on to carry out complex POC, Pilot, Limited production rollout, assignments requiring the development of new or improved techniques and procedures.
Participated in deep architectural discussions to build confidence and ensure customer, success when building new, or migrating existing, applications, software, and services on the Google Cloud Platform.
advanced skills in SQL, data modeling, ETL/ELT development, and data warehousing.
Strong skills in Optimization - performance, pipeline, spark.
Experience on Pyspark.
5+ years of design & implementation experience with distributed applications.
5+ years of experience architecting/operating solutions built on Google Cloud Platform.
. Bachelor's degree.

Experience: 5.00-10.00 Years",,False,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
deloitte,Consulting- SAMA- A&C-Azure Data Engineer- AD,"JD:
• Location - Mumbai OR Pune
• Experience range:
• 12-15 yrs for AD
• Strong experience in Python programming and related skills like PySpark
• Strong SQL skills
• Strong experience with any of the data engineering platforms like Hadoop, Spark, Synapse, Databricks, Apache Airflow, etc.
• Preferred: Knowledge of any cloud platform like Azure/AWS/Google
• For AD level: Need technology leadership experience in terms of architecture/solutioning and team leading",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Comcast,"Data Engineer, Data Products Engineering","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary INTRODUCTION: At Comcast, we believe in the talent of our people. It's our passion and commitment to excellence that drives Comcast's vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It's what makes us uniquely Comcast. Here you can create the extraordinary. Join us. ABOUT THE ROLE: Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered Job Description Responsibilities Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc) 2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas. Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications. Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data. Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time. REQUIREMENTS: Bachelor's degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience. Strong Computer Science/Engineering/Information Systems background 3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies. Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas Strong problem-solving skills. Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences. PREFERRED QUALIFICATIONS: 1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills Ability to design concise and visually appealing reports, user interfaces, mockups and documentation Ability to read external API documentation and write pipelines to extract data from our partners systems Ability to write and stand up internal API endpoints to share data with other internal teams. Strong analytical focus, results-oriented and execution driven. Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures. Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed. Flexibility to adjust to changing requirements, schedules and priorities. Ability to work independently under minimum supervision and proactive in solving issues Energetic, committed and solution focused with the ability to perform under pressure and meeting targets Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Relevant Work Experience 2-5 Years Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,False,True
Versor Investments,Data Engineer,"India

Versor Investments (“Versor”) is a quantitative investment boutique headquartered in Midtown Manhattan. The Firm currently has an AUM of $1.8 billion*. Versor creates diversified sources of absolute returns across multiple asset classes. Within a scientific, hypothesis-driven framework, Versor leverages modern statistical methods and vast datasets to drive every step of the investment process. Alpha forecast models, portfolio construction, and the trading process rely on the ingenuity and mathematical expertise of 60+ investment professionals. Versor offers two categories of investment products – Hedge Funds and Alternative Risk Premia. Both are designed to provide superior risk-adjusted returns while exhibiting low correlation to traditional and alternative asset classes. Each invests in liquid, scalable markets. On average, Versor’s partners have spent over 20 years researching, investing and trading systematic alternative investment strategies.

Role Summary

The Data Engineer position will be based in Mumbai and be part of the Portfolio Analytics team. They will collaborate closely with senior researchers to design and develop a large-scale data lake. We are seeking candidates who have excelled in engineering (specifically computer science). Prior experience in investments and finance is beneficial but not mandatory.

This role is ideal for candidates who are passionate about technology and excited about building a data platform.

Responsibilities
• Design architecture for a data platform.
• Design data pipelines based on business and functional requirements.
• Extract, transform, and load logic to automate data collection and manage data processes/pipelines. This includes data quality and monitoring.
• Develop data access tools to allow researchers to access data seamlessly.
• Develop integration tools and analytical reports for the databases and data warehouse.
• Write and review technical documents. This includes requirements and design documents for existing and future data systems, as well as data standards and policies.
• Collaborate with analysts, support/system engineers, and business stakeholders to ensure data infrastructure meets constantly evolving requirements.

Requirements
• E., B.Tech., M.Tech., or M.Sc. in Computer Science, Computer Engineering or similar discipline from a top tier institute.
• 2+ years direct experience working as a data engineer.
• Experience in design, architecture and implementation of data lake, data pipelines and flows.
• Experience with developing software code and APIs in one or more languages such as Python and C#.
• Experience designing and deploying large scale distributed data processing systems with one or more technologies such as MS SQL Server, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, Hive, Teradata, or MicroStrategy.
• A high-level understanding of automation in a cloud environment (AWS experience preferred).
• Excellent communication, presentation, and problem-solving skills.
• Data as of December 31, 2022. AUM reflects regulatory AUM as per SEC definition for the purposes of Item 5.F on the Form ADV Part 1a.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tredence Inc.,Senior Data Engineer,"Associate Manager – Data Engineering (8-11 Years)

This position requires someone with good problem solving, business understanding and client presence.

Overall professional experience of the candidate should be above 8 years. A minimum of 4 years of experience in Data Engineering space. Should have good understanding of business operations, challenges faced, and business technology used across business functions.

The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.

Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.

Role Description:
• Engages with Leadership of Tredence' s clients to identify critical business problems, define the need for data engineering solutions and build strategy and roadmap
• S/he possesses a wide exposure to complete lifecycle of data starting from creation to consumption
• S/he has in the past built repeatable tools / data-models to solve specific business problems
• S/he should have hand-on experience of having worked on projects (either as a consultant or with in a company) that needed them to –
• Provide consultation to senior client personnel
• Implement and enhance data warehouses or data lakes.
• Worked with business teams or was a part of the team that implemented process re-engineering driven by data analytics / insights
• Should have deep appreciation of how data can be used in decision making
• Should have perspective on newer ways of solving business problems. E.g. external data, innovative techniques, newer technology
• S/he must have a solution creation mindset. Ability to design and enhance scalable data platforms to address the business need
• Working experience on data engineering tool for one or more cloud platforms -Snowflake, AWS/Azure/GCP
• Engage with technology teams from Tredence and Clients to create last mile connectivity of the solutions -
• Should have experience of working with technology teams
• Demonstrated ability in thought leadership – Articles/White Papers/Interviews

Mandatory Skills

Program Management, Data Warehouse, Data Lake, Analytics, Cloud Platform

Job Location - Bangalore , Chennai , Pune , Gurugram.

Experience Level - 8-11 yrs.

Expected Joining Time - Immediate to Max 30 days.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
Mercedes-Benz Research and Development India Private Limited,Big Data CoE - Engineering.IT Data Engineer,"TasksOverview
A highly motivated and technically proficient professional, capable of delivering solution on Data Engineering in Cloud platform. He or she must be skilled in developing all rituals for Data Engineering especially using pyspark.
Job Responsibilities
Data Engineer:

· Development, Enhancement, testing and release of existing application.
· Develop and enhance end to end data pipeline.
· Interpret data to analyze results to a specific business problem or bottleneck that needs to be solved using statistical techniques.
· Ensure data efficiency and reliability.
Qualification
Mandatory:
· Bachelor's /post graduate degree in Computer Science, Computer Engineering or Data Science, Data Engineering with strong knowledge in below technologies,
· Pyspark
· Databricks,
· ADLS
· SQL
Desired:
· Work experience on Agile/SDLC process.
· Experience in building Knowledge Base (KB)
· Experience in taking over Knowledge Transfer
· Open to work/explore new technology areas.
· Automotive and Aviation domain is plus.Qualifications",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
D2C Ecommerce India Pvt Ltd,D2C Ecommerce - Data Engineer,"What is D2cecommerce :D2C Ecommerce is India's first multi-D2C brand online platform that sells its own homegrown brands across multiple home and lifestyle categories, including - apparel, cosmetics, beauty, jewelry, accessories, fitness, sports, shoes, bags, books, kitchen, food, auto accessories, electronics, kids and travel packages. Along with selling these products on its own portal, D2CEcommerce also has these items listed on leading e-commerce sites.Job Summary : We are seeking a highly motivated and skilled Data Engineer to join our team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our databases and reports. You will work closely with our team of developers, data scientists, and analysts to ensure that our data systems are accurate, efficient, and scalable.What would be your responsibilities :- Design and develop databases that are scalable, efficient, and accurate- Develop and maintain ETL pipelines to move data from various sources into our databases- Create and manage database reports to provide insights to our team of data scientists and analysts- Collaborate with our team of developers to integrate our databases into our applications and services- Continuously monitor and optimize our databases for performance and security- Ensure that our data systems adhere to industry best practices and compliance regulations- Stay up-to-date with the latest database technologies and trendsWhat are we looking for in the candidate :- Ability to build things from scratch.- Self-starter and motivated individuals who can drive processes on their own- A bachelor's or associate degree in management information systems, computer science, or a related field- 0-1 years of experience in database management or a similar role- 0-1 years of experience designing, developing, and producing database reports- Proficiency in SQL and experience with relational databases such as MySQL, Postgre SQL, or Oracle- Experience with ETL tools and techniques- Understanding of data modelling concepts- Familiarity with database administration and management tools- Strong analytical and problem-solving skills- Excellent verbal and written communication skillsWhy you should join D2cecommerce :In addition to an attractive compensation package, you own a piece of the company through ESOPs. Also you will have the opportunity to take up a role in a rapidly growing, highly disruptive organization, and shape the face of ecommerce for the future.Interested? What to do next :If reading the details above excited you and made you feel you can help us take D2Cecommerece to the next level, all you have to do is let us know!",Gurugram,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Thoughtworks Inc.,Senior Consultant - Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Aryng,Sr. Data Engineer,"Welcome You made it to the job description page

Aryng is looking for a cloud data engineer with experience in developing
enterprise-class distributed data engineering solutions on the cloud. We are seeking
an entrepreneurial and technology-proficient Data Engineer who is an expert in the
implementation of a large-scale, highly efficient data platform, batch, and real-time
pipelines and tools for Aryng clients. This role is based out of India. You will work
closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength
is creative and effective problem-solving.

Key Responsibilities:

● Should have implemented asynchronous data ingestion, high volume stream data
processing, and real-time data analytics using various Data Engineering
Techniques.
● Implement application components using Cloud technologies
and infrastructure.
● Assist in defining the data pipelines and able to identify bottlenecks to enable
the adoption of data management methodologies.
● Implementing cutting edge cloud platform solutions using the latest tools and
platforms offered by GCP, AWS, and Azure.

Requirements
• Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
5+ years of data engineering experience is a must.
• 2+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers
• 2+ years' experience in Python.
• Must be strong in SQL and its concepts.
• Experience in Big Query, Snowflake, Redshift, DBT.
• Strong understanding of data warehousing, data lake, and cloud concepts.
• Excellent communication and presentation skills
• Excellent problem-solving skills, highly proactive and self-driven
• Consulting background is a big plus.
• Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study
Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
This role requires mandatory overlap hours with clients in the US from 8 am - 1
pm PST.

Benefits
• Direct Client Access
• Flexible work hours
• Rapidly Growing Company
• Awesome work culture
• Learn From Experts
• Work-life Balance
• Competitive Salary
• Executive Presence
• End to End Problem Solving
• 50%+ Tax Benefit
• 100% Remote company
• Flat Hierarchy
• Opportunity to become a thought leader

Why Join Aryng: Click on the Youtube link",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,True,True
Emerson,Data Engineer - Sustainability,"AS AN Data Engineer, YOU WILL:

· Architect and design platform solutions to meet and exceed expectations of Projects.

· Proactively evolve and apply DevSecOps methodologies, standards and leading practices

· Apply architectural standards/principles, security standards, usability design standards, as approprioate.

· Lead a project from delivery perspective, including giving periodic updates to all stakeholders.

Skills Requirements:

· Must be able to communicate fluently in English, both written and verbal

· Excellent interpersonal communication and organizational skills

· Able to distil complex technical challenges to actionable and explainable decisions

· Inspire DevSecOps teams by building consensus and mediating compromises when necessary

· Demonstrate excellent technical & architecture skills, service management and product lifecycle management

· Demonstrate ability to rapidly learn new and emerging technologies

· Operational abilities including early life support and driving root cause analysis and remediation

·Any Azure/Microsoft Big Data related certifications is highly preferred.

REQUIRED EXPERIENCE :

· Bachelor’s Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline)

· 10+ years overall IT industry experience

· 3+ years in a solution design role using service and hosting solutions such as private/public cloud IaaS, PaaS and SaaS platforms.

· Large scale design, implementation and operations of OLTP, OLAP, DW and NoSQL data storage technologies such as SQL Server, Azure SQL, Azure SQL DW, PostgreSQL, CosmosDB, RedisCache, Azure Data Lake Store, Hadoop, Hive, MongoDB, MySQL, Neo4j, Cassandra, HBase

· Creation of descriptive, predictive and prescriptive analytics solutions using Azure Stream Analytics, Azure Analysis Services, Data Lake Analytics, HDInsight, HDP, Spark, Databricks, MapReduce, Pig, Hive, Tez, SSAS, Watson Analytics, SPSS

· Design and configuration of data movement, streaming and transformation (ETL) technologies such as Azure Data Factory, HDF, Nifi, Kafka, Storm, Sqoop, SSIS, LogicApps, Signiant, Aspera, MoveIT, Alteryx, Pentaho, IDQ,

· Enablement of data reuse through Data Catalog/Marketplace, Metadata, Search and Governance technologies such as including Azure Data Catalog, Waterline, Apache Atlas, Apache Solr, Azure Search, Alteryx Connect, Datawatch Monarch Swarm, Collibra Catalog, Enigma Councourse, Adaptive, Cambridge Semantics, Data Advantage Group (DAG), Global IDs, Alation

· Experience with any of the following: Azure, O365, Azure Stack, Azure AD

· Delivery using modern methodologies especially SAFe Agile.

Requisition ID : 23000590

Emerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.",Pune,False,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Confidential,Fractal.ai - Azure Data Engineer - SQL/PySpark,"Mandatory Skills :- Azure Databricks (ADB)- Azure DataFactory (ADF)- Python or Pyspark- SQLResponsibilities :- Be an integral part of large scale client business development and delivery engagements- Develop the software and systems needed for end-to-end execution on large projects- Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions- Build the knowledge base required to deliver increasingly complex technology projectQualifications & Experience :- A bachelor's degree in Computer Science or related field with 3-12 years of technology experience- Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space- Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks- Database programming using any flavours of SQL- Expertise in relational and dimensional modelling, including big data technologies Exposure across all the SDLC process, including testing and deployment- Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.- Good knowledge of Python and Spark are required- Good understanding of how to enable analytics using cloud technology and ML Ops- Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus- Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)- Characteristics of a forward thinker and self-starter - Ability to work with a global team of consulting professionals across multiple projects- Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems- Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to success. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LatentView,Principal Data Engineer,"About LatentView:
• LatentView Analytics is a leading global analytics and decision sciences provider, delivering solutions that help companies drive digital transformation and use data to gain a competitive advantage. With analytics solutions that provide 360-degree view of the digital consumer, fuel machine learning capabilities and support artificial intelligence initiatives., LatentView Analytics enables leading global brands to predict new revenue streams, anticipate product trends and popularity, improve customer retention rates, optimize investment decisions and turn unstructured data into a valuable business asset.
• We specialize in Predictive Modelling, Marketing Analytics, Big Data Analytics, Advanced Analytics, Web Analytics, Data Science, Data Engineering, Artificial Intelligence and Machine Learning Applications.
• LatentView Analytics is a trusted partner to enterprises worldwide, including more than two dozen Fortune 500 companies in the retail, CPG, financial, technology and healthcare sectors.

Job Description:
As a Manager - data engineer, they should build and maintain scalable, rock solid self-serve data pipelines for data analysts and data Scientists and support them by understanding the content and context of data and collaborating with them to figure out the best way to Extract, Transform, Load, and access it.
• Be an SME in DE and should know how to set up the process, requirement gathering for any movement or data ingestion and data platform creation
• Has to be the end-to-end project manager, allocating work, monitoring progress, providing feedback, and taking necessary steps to ensure agreed timelines are met
• Scripting skills: SQL and Python or PySpark
• Excellent understanding of Data Warehouse and its architecture
• Excellent understanding of Issue Tracking System like JIRA/ Dev-Ops
• Excellent experience in Version control like Github or Gitlab
• Ideal to have knowledge of other DE platforms like Azure databricks, Snowflake etc.

Education: UGEmployment Type: CONTRACTOR",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
CIEL HR Services,Data engineer,CTC-6LPA EXP-Freshers Area of Expertise - A) Data Hygiene B) Application of ML DL Tools If interested please call 9000338173 or forward cv to [Confidential Information],Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer - ETL,"Design, build, and maintain the data infrastructure that supports our data-
driven applications and services.• Develop and maintain ETL (Extract, Transform, Load) processes to ensure
data accuracy and completeness.
• Optimize data pipeline performance and scalability.
• Collaborate with data scientists, analysts, and developers to ensure that our
data pipeline meets their needs.• Implement data governance policies and procedures to ensure data quality
and consistency.
• Design and develop data models that support data-driven applications and
services.
• Troubleshoot and resolve issues related to the data pipeline.
• Participate in the evaluation and selection of data management and analytics
tools and technologies.
• Keep up to date with emerging trends and technologies in data engineering
and big data.

experience

8",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Swift,Data Engineer,"About us:

Swift is building a next generation checkout stack for India - a platform rolling up payments and logistics solution for all fulfillment needs. We give businesses the opportunity to provide a customer experience at par with the likes of Amazon and Flipkart, all the while saving money and time.

Its basically Amazon without the website listing - we let our sellers design their own sales channel :-)

We believe there are many things a seller or small business has to worry about when selling online, logistics/payments/etc shouldn't be one of them. With our solution, SMBs and D2C brands get access to technologies and services like next day delivery, same day delivery, live package tracking, Card/Cash on delivery, scheduled delivery etc, making parcel delivery just as simple as collecting payment.

We also provide robust APIs which makes it easy for developers to add shipping capabilities to their multichannel online store.

We want to be the #1 checkout platform that’s reliable, easy to use and affordable.

About you:

You have experience in working with data pipelines and ETL sets (programmatically and using tools) – say MongoDB, Spark streaming, Python/Java, Apache Beam, PARQR, Delta Lake, Airflow, etc. You are looking for challenges in growing a data backed company to deal with from hundreds to millions of visitors data points per month.

You like working with streaming/reactive architectures and have experience/interest in setting up data pipelines on cloud infra from scratch. You generally prefer to use a minimal set of simple tools to a diverse range of complex ones. We are looking to build a back-end cloud infrastructure (Google Cloud Platform preferably) which will be a fault-tolerant real-time stream processing system on the cloud - Our system will need to meet liveliness guarantees from a big data/ETL perspective.

You like to work on a variety of projects - at this job, you’ll be developing a complex ETL infra, a reactive streaming architecture and a cloud-native, highly available API for our customers.

You are someone who is:
• Experienced in any JVM based language or Python.
• Have worked on NoSQL (MongoDB)/ SQL databases.
• Have worked on creating data pipelines (both programmatically, say using spark streaming) or using tooling like Airflow, dataflow)
• Strong verbal and written communication skills and the ability to work well cross-functionally.
• We offer: *
• You to be a part of a small, but a super capable team.
• The opportunity to work closely with founders to define, scope, estimate and plan various aspects of the product.
• Being one of the first hires at Swift, you will be involved in both high and low-level decision making. This means a lot of ownership, which we cultivate by having a flat structure.

Swift focuses on E-Commerce, B2B, Small and Medium Businesses, Logistics, and D2C. Their company has offices in Bengaluru. They have a mid-size team that's between 51-200 employees. To date, Swift has raised $2.34M of funding; their latest round was closed on July 2021.

You can view their website at https://goswift.in or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
CarbyneTech India Pvt Ltd,CarbyneTech - Azure Data Engineer - IoT,"- Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions - Azure Synapse/Azure SQL DWH, Azure Data Lake, Azure Blob Storage, Spark, HDInsights, Databricks, CosmosDB, EventHub/IOTHub.- Experience in migrating on-premise data warehouses to data platforms on AZURE cloud.- Designing and implementing data engineering, ingestion, and transformation functions- Azure Synapse or Azure SQL data warehouse- Spark on Azure is available in HD insights and data bricks- Good customer communication.- Good Analytical skill",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Brillio,GCP Data Engineer - R01523647,"About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

GCP Data Engineer
Primary Skills

• BigQuery, Cloud Logging, Cloud Storage, Cloud Trace, Composer, Data Catalog, Data Modelling Fundamentals, Data Warehousing, Dataflow, Datafusion, Dataproc, ETL Fundamentals, Modern Data Platform Fundamentals, PLSQL, T-SQL, Stored Procedures, Python, SQL, SQL (Basic + Advanced)

Specialization

• GCP Data Engineering Basic: Senior Data Engineer

Job requirements

• Job description below. • 6 years of experience in software design and development • 5 years of experience in the data engineering field is preferred • 3 years of Hands-on experience in GCP cloud data implementation suite such as Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage, • Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms. • Mandatory 1 year of software development skills using Python • Extensive hands-on experience working with data using SQL and Python • Cloud Functions. Comparable skills in AWS and other cloud Big Data Engineering space is considered. • Experience in DevOps(CI/CD) pipeline facilitating automated deployment and testing • Experience with agile development methodologies • Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions • Bachelor's Degree in Computer Science, Information Technology, or closely related discipline

Know what it’s like to work and grow at Brillio: Click here",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,True,False
Latent View Analytics Private Limited,Data Engineer,"Job Title :
Data Engineer Experience : 2.5-5 Location : INDIA
• Chennai Job Description: Experience working with Cloud Data Platforms, especially AWS and its services, must be strongly experienced in building data pipelines.
Experience with big data tools like Python, Pyspark, and Spark SQL. Focus on scalability, performance, service robustness, and cost trade-offs.

A continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs.
Attention to detail, coupled with the ability to think abstractly. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product. Keen to learn new technologies and apply the knowledge in production systems.

AWS skills:
AWS Glue AWS EMR (any two AWS services)

Data Engineer Skills:
Python Pyspark Spark SQL HIVE HQL Scala Good to have: Snowflake querying Databricks AWS API Gateway AWS Lambda",Chennai,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,True
Whiteforce,Data Engineer - Java,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20246

LocationIndia

Job Descriptions

Job Purpose and Primary Objectives : Develop and Deploy data and analytics-led solutions on GCP Key responsibilities (please specify if the position is an individual one or part of a team): Data engineering solution on GCP using Cloud Bigquery, Cloud Dataflow, Pu-Sub, Cloud BigTable and AI/Ml solutions Key Skills/Knowledge : - Good Experience in GCP. - Python/Java, PySpark/Spark Java. - GCP BigQuery. - GCP Pub-Sub. - Secondary Skill - DataFlow, Compute Engine, Cloud Fusion. (ref:hirist.com)

Skills",,True,False,False,True,False,False,False,False,False,False,False,False,False,True,False,False
Confidential,Big Data Engineer - Python/AWS,"JOB_DESCRIPTION :- Has experience in the following #Python, #AWS_Athena, #Glue #Pyspark, #EMR, #DynamoDB, #Redshift, #Kinesis, #Lambda, #Snowflake.- Proficient in #AWS_Redshift, #S3, #Glue, #Athena, #DynamoDB.- Design, build and operationalize large-scale enterprise data solutions and applications using one or more of #AWS_data and analytics services in combination with 3rd parties - #PySpark, #EMR, #DynamoDB, #RedShift, #Kinesis, #Lambda, #Glue, #Snowflake. - Analyze, re-architect, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services. - Design and build production data pipelines from ingestion to consumption within a big data architecture, using Python/PySpark. - Design and implement data engineering, ingestion, crawling, manipulation and curation functions on AWS cloud using AWS native or custom programming. - Must have strong knowledge of databases like Postgres, MySQL, MongoDB, Cassandra, etc. - Must have experience in using AWS SDKs and libraries for interacting with different AWS services. - Experience in building REST APIs. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Varite India Private Limited,Data Engineer,"snowflake- Data Engineer Data engineering, integration, and data modeling experience . Can write scalable/performant pipelines, queries, and summaries of data . Has worked with various data systems and tools . Understands analytics and data science workflows and common use cases that leverage their work . Python . SQL . Datawarehouse experience . AWS experience . Data QA / validation skills (to check their work) . Snowflake experience (MUST) . Matillion, DBT, or other Data tech experience (ideal) . Marketing technology experience (ideal)",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
deloitte,Consulting-SAMA- A&C-Data Engineer/Architect-Associate Director,"Location: No Preference

Years of Exp: 10-15 Years

Hybrid: Yes

Mandatory Skill: MS Azure, Analytics, Delivery Management & Operations, People Management

Responsibilities:
• 10 - 15 years of deep delivery experience in cloud data engagements, with proven experience to lead teams sizes of 10+ resources
• Experience with platforms such as Azure Cloud Services, Solution Design & Review, Data Management, Data Visualization Tools
• Deep experience handling large volumes of data across multiple data sources like csv, relational data, SAP, json, parquet, flat files, streaming data, etc.
• Strong conceptual understanding of data warehousing, data modeling principles
• Strong Experience with visualization / reporting solutions
• Good understanding of data quality, data management and data governance principles
• Depth in data transformation / data modernization / ETL and ELT based data pipeline development
• Hands on with Agile/Scrum Methodology based implementations and end to end software delivery lifecycle
• Exposure working with international clients / geographies
• Excellent client handling, team handling and interpersonal skills
• Excellent written and spoken communication skills

Ability to understand and appreciate business / domain context and develop data and analytics solutions",Hyderabad,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Wipro Technologies,Data Engineer,"Share resume to akshara.raju@wipro.com

Location - Bangalore , Chennai , Pune , Hyd

JD :
• Azure data factory
• Azure data bricks with PySpark coding experience
• Experience in Snowflake
• Good to have knowledge in data visualization tool Tableau or PowerBI
• Good to have knowledge in data warehousing & data analytics

Data Analyst / Data Engineer
• 5 yrs. experience in working with large, complex data sets
• Create reports for internal teams and/or external clients
• Hands on Experience on Azure Data Factory and Azure
• Need to be able to code the data pipelines from ADF standpoint / Data Ingestion.
• Collaborate with team members to collect and analyze data
• Knowledge of Snowflake will be a big plus
• Need to ensure they can work on Data Mapping / Data Enrichment and Data Transformations.
• Use graphs and other methods to visualize data
• Establish KPIs to measure the effectiveness of business decisions
• Structure large data sets to find usable information
• Reporting and Data Visualization skills
• Experience in Data Mapping, data cleansing.",Bengaluru,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
ANI Calls India Private Limited,Lead Snowflake Data Engineer,"Anicalls

Industry : IT

Total Positions : 3

Job Type : Full Time / Permanent

Gender : No Preference

Salary : 900000 INR - 1800000 INR (Annually)

Education : Bachelor s degree

Experience : 5-10 Years

Location : Noida, India

Candidate should have :

Knowledge and experience in Big data and Data Vault methodology

Experience in power shell, shell scripting, and python

Exposure to data modeling for Snowflake

Experience in working with agile / scrum methodologies.

Experience in building data pipelines for large volumes of data across disparate data sources

Experience in DBT for Snowflake

Good experience on Azure Databricks

experience in Confluent cloud platform

Experience in building pipelines through Confluent Kafka and Knowledge of Azure Kubernetes Service

Good communication and presentation skills

Expertise in building data pipelines for Snowflake using Snowpipe, Snowpark, SnowSQL's and stored procedures.

Azure experience must be focused on Azure Data Factory, Azure storage solutions (such as Blob and Azure Data lake Gen2), and Azure data pipelines

Good experience on Snowflake and Snowflake architecture

7+ years of total experience in data projects with a focus on data integration and ingestion

3+ years of experience working primarily on Snowflake",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Impetus Technologies India Pvt. Ltd,GCP Data Engineer,"Role : GCP Data Engineer

Job Description :

- The candidate should have extensive production experience in GCP, Other cloud experience would be a strong bonus.

- Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.

- Exposure to enterprise application development is a must.

Roles & Responsibilities :

- 6-10 years of IT experience range is preferred.

- Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.

- Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOps.

- Good hands on expertise on either Python or Java programming.

- Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.

- Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.

- Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.

- Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.

- Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.

- Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.

- Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.

,",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Stantec Technology International,Senior Data Engineer,"Description
Grow with the best. Join a smart, creative, and inspired team that works behind the scenes to support operational excellence. As part of the Innovation Office, the Digital Technology & Innovation team is composed of digital experts who conduct research and development to keep our teams and our client's projects ahead of the technological curve. They implement established technologies and find emerging solutions for all business lines (Buildings, Energy & Resources, Environmental Services, Infrastructure, and Water), bridging existing knowledge domains and facilitating the integration of powerful tools and methods. The team's goal is to make projects more efficient and help provide higher-quality results to our clients. The ideal candidate will be a self-starter, a critical thinker, and highly interested in the application of new technologies and methods. The candidate will become a member of the Innovation Office, however, he or she will also be accessible to Stantec's project teams to support project work as needed.

Your Opportunity
The Innovation Office's Digital Technology & Innovation (DTI) team has an opportunity for a Senior Data Engineer. This position requires a person who is technically savvy, experienced in data engineering, and enjoys working with data to solve business problems, shaping & creating solutions, and helping to champion implementation. As a member of the Innovation Office, the Digital Technology Development group, as part of the DTI team, also engages in research & development and provides guidance and oversight as a center of excellence for the business. This group also engages in new product research and testing and the incubation of new ideas. The candidate will be responsible for the delivery of professional services and will recommend solutions to achieve complex strategic objectives across our large global team spanning Stantec IT, Business Lines, and the Office of Innovation.

Your Key Responsibilities
Serve at the direction of the Digital Technology Development Leader to:
- Take ownership of the project, work independently in a team environment, and mentor others as needed.
- A passion for solving problems and providing workable solutions, flexible to learn new technologies to meet the business needs.
- Translate complex functional and technical requirements into detailed designs.
- Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining.
- Implement access governance of production data systems to ensure compliance with our privacy and security policies.
- Oversee, design, and develop algorithms for real-time data processing within the business units and to create the frameworks that enable quick and efficient data acquisition.
- Build and maintain best practices to support the Continuous Integration and Delivery (CI/CD) of data engineering solutions.
- Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets (batch and stream data) into a readable and accessible format for end-user facing reports, data science, and ad-hoc analyses.
- Build and maintain reliable and scalable ETL on big data platforms as well as work with varied forms of data infrastructure inclusive of relational databases and NoSQL databases.
- Work collaboratively with DTI's Data & Analytics group, Stantec's internal business units, and clients to define problem statements, collect data and define solution approaches.
- Deliver highly reliable software and data pipelines using Software Engineering best practices like automation, version control, continuous integration/continuous delivery, testing, security, etc.
- Possess excellent time-management skills, a thorough understanding of task assignments and schedules, and efficient use of time and available resources.
- Perform other miscellaneous tasks associated with being a member of the Digital Technology & Innovation team and those typical of a data engineer.",Pune,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
TensorGo Technologies,TensorGo Technologies - Senior Data Engineer - Python,"Skillset : Python, PySpark, Kafka, Airflow, Sql, NoSql, API Integration,Data pipeline, Big Data, AWS/ GCP/ OCI/ AzureRequirements :Understanding our data sets and how to bring them together.Working with our engineering team to support custom solutions offered to the product development.Filling the gap between development, engineering and data ops.Creating, maintaining and documenting scripts to support ongoing custom solutions.Excellent organizational skills, including attention to precise detailsStrong multitasking skills and ability to work in a fast-paced environment3+ years experience with Python to develop scripts.Know your way around RESTFUL APIs.[Able to integrate not necessary to publish]You are familiar with pulling and pushing files from SFTP and AWS S3.Experience with any Cloud solutions including GCP / AWS / OCI / Azure.Familiarity with SQL programming to query and transform data from relational Databases.Familiarity to work with Linux (and Linux work environment).Excellent written and verbal communication skillsExtracting, transforming, and loading data into internal databases and HadoopOptimizing our new and existing data pipelines for speed and reliabilityDeploying product build and product improvementsDocumenting and managing multiple repositories of codeExperience with SQL and NoSQL databases (Casendra, MySQL)Hands-on experience in data pipelining and ETL. (Any of these frameworks/tools: Hadoop, BigQuery, RedShift, Athena)Hands-on experience in AirFlowUnderstanding of best practices, common coding patterns and good practices aroundstoring, partitioning, warehousing and indexing of dataExperience in reading the data from Kafka topic (both live stream and offline)Experience in PySpark and Data framesResponsibilities :You'll :Collaborating across an agile team to continuously design, iterate, and develop big data systems.Extracting, transforming, and loading data into internal databases.Optimizing our new and existing data pipelines for speed and reliability.Deploying new products and product improvements.Documenting and managing multiple repositories of code.",,True,False,True,False,False,False,False,True,False,False,False,False,True,True,True,False
Confidential,Data Engineer (Data Bricks) Manager,"EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL's proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients' decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries. Please visit www.exlservice.com for more information about EXL Analytics. Location - Bangalore/ Gurgaon Note- Currently Remote & expecting candidates to relocate to either Bangalore/Gurgaon once office reopens.Requirements: 7+ years of Data engineering exp with 3+ years hands on Databricks (DB) experience. Should be able to create New Clusters, Cluster Pools and attach existing clusters to pool in DB. Should have some pool management experience. Should be good in Datalakehouse concepts. Should have good experience in Data Engineering in Databricks Batch process, Streaming is good to have. Should have good experience in creating Workflows & scheduling the pipelines. Should have good exposure on how to make packages or libraries available in DB. Should have good experience in Databricks default runtimes, Photon & Light is good to have. Some experience in Databricks SQL / DW in Databricks. Delta Live Tables experience is good to have. IT Services and IT Consulting",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Streamline Digital,Sr. Data Engineer- Azure/ Snowflake/ Databricks,"Sr. Data Engineer

Who We Are

At Streamline, we are experts in Enterprise Mobility, Product Engineering, and IT Transformation. We help organizations navigate the constantly evolving landscape of IT. Our sole focus is ensuring that our client’s organization is armed with the strategies, products and solutions that are transformative to their business. Streamline works closely with our clients, takes pride in developing genuine relationships and embraces open communication and collaboration with our clients. We become a part of our client’s team, working together to achieve short-term goals and enable long-term success. Our team is comprised of world-class strategists, architects, engineers, and developers.

In our new flagship product, iEnterprise, we are taking things to the next level, using our collective experience and customer input to create new enterprise mobility management products that reduce operational costs, prevent issues before they happen, and resolve issues faster than with traditional tools and approaches.
Role Summary

This position is full time remote position. The Data Engineer will work with a team of other software developers to define, design, develop, integrate, and re-engineer the Enterprise Data warehouse. Data Marts, Data Virtualization and Data Visualization components in different environments which meet customers’ analytical and business intelligence requirements, scales easily and supports deployment in highly available environments. The Senior engineer acts as the development and technical lead and individual contributor on complex projects, contributing to strategic vision and technical decisions, participating in vendor analysis and selection projects, introducing process improvements, completing proof-of-concept projects for the introduction of changes to our architecture, and providing oversight on the work of other technical staff.

Role Responsibilities
• Build data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective
• Create and maintain highly scalable data pipelines using Databricks, Azure, AWS, Kafka.
• Design and build ETL/ELT data pipelines to extract and process data from a variety of external/internal data sources.
• Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery
• Build data insights, data analytics, ML models, fraud and anomaly detection using Snowflake
• Build and deploy modern data solutions.
• Define, implement and build jobs to populate data models.
• Relational and NoSQL Database management
• DevOps building CI/CD pipelines

Technical System Expertise: Understands data warehouse development practices and processes including ETL design, Dimensional models, slowly changing dimensions, Data Security components, Data quality methods, how MPP databases operate, and data flows. Aware of current technology benefits. Expected to independently develop full stack ETL solution from source to staging to presentation layers. Understands the building blocks, interactions, dependencies, and tools required to complete BI Data warehousing software and automation work. An Independent study of current technology is expected. Interact with system engineers to define continuous delivery and/or DevOps solutions, data security, and/or necessary requirements for automation.

Technical Engineering Services: Supports BI Data Warehousing and Enterprise Data Solutions projects by analyzing, designing, and developing ETL solutions; conducting tests and inspections; creating BI reports and virtualization components. Create interface jobs/APIs for data sharing with internal and external stakeholders Ensure we use accurate and secure methods to extract data. Analyze Big Data and MPP Databases to discover trends and patterns. Participates in reviews (walkthroughs) of technical specifications and program code with other members of the DevOps team. Expected to supervise associate engineers on occasion.

Innovation: Presents new ideas which improve existing BI Data Warehousing systems/processes/services. Presents new ideas which utilize new frameworks and reusable components to improve existing ETL jobs/processes/services. Express new perspectives based on an independent study of the industry. Review current company processes to highlight questions that may drive process refinement and optimization.

Technical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the engineer level. Develop application support documentation as required by the application support teams for acceptance of systems changes into production.

Technical Leadership: Collaborates with other engineering, development teams and utilizes data engineering/analyst expertise to deliver technical solutions. Continuously learn and mentor on new technologies.
Qualifications & Skills
• Bachelor’s degree in CS, Statistics, Information systems or equivalent experience.
• Strong expertise working with relational databases. Exceptional ability to author and optimize complex SQL queries/workflows Strong analytical skills to work with unstructured data.
• Experience building and optimizing highly scalable data pipelines, architectures, and data sets.
• A successful history of manipulating, processing, and extracting value from large, disconnected datasets and provide valuable insights.
• Proficiency in workflow orchestration (Databricks, Azure data factory).
• Experience working with streaming data solutions such as Kafka, IoT hub and Spark Streaming.
• Working experience building insights, ML models, fraud and anomaly detection using Snowflake/Databricks.
• Experience working with NoSQL databases like MongoDB, Cassandra, Cosmos DB
• Proficiency in Java/Python/Scala, pandas, pySpark, NumPy
• 8+ years of ETL Development experience using Azure Databricks, Azure data factory, SSIS, Talend.
• 8+ years Professional experience in designing and building cost-effective large scale data marts, data warehousing, distributed big data processing MPP Systems (Databricks, Spark, Snowflake)
• 5+ years of Expertise in Logical and Physical Data Model design using various modelling Tools like Erwin 7.3 and Power Designer.
• 5+ years of experience with Azure and AWS cloud stack
• 3+ years BI Visualization experience developing reports using Snowflake, PowerBI, Grafana, Qlik and analytic cubes utilizing SQL Server Analysis Services (SSAS).

Preferred Skills
• Certifications
• DevOps experience
• Bash, Golang scripting experience
• Docker/Kubernetes experience
• CI/CD pipelines

Powered by JazzHR",Hyderabad,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,True
Whiteforce,Data Engineer - ETL/,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20453

LocationIndia

Job Descriptions

Key Responsibilities - ETL pipeline design and implementation - Streaming and batch data processingStorage optimization - Storage optimization - DataOps / MLOps - Preparing, cleaning, structuring data for statistical modeling / machine learning - Deployment, packaging, versioning of Machine Learning models that operate on data Educational Qualification & Experience - BE/BTech in Computer Science or Information Systems - (Preferred) ME/MTech in Computer Science or Information Systems - Minimum 5 years total experience working in the industry - Minimum 3-5 years experience in Data Engineering Knowledge and Skills Required - Experience with Big Data technologies: Presto / Trino / Hive / Hadop - Cloud data storage, query and analytics technologies, esp. on AWS - S3 / Athena / Glue or Elastic Stack (specifically for data/ML workflows) - Message passing / data broker systems: Kafka / RabbitMQ

Skills",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Apple,Cloud Data Engineer,"Summary

The people here at Apple don’t just build products — they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Imagine what you could do here. Are you passionate about handling large & complex data problems, want to make an impact and have the desire to work on groundbreaking big data technologies? Then we are looking for you. At Apple, phenomenal ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Business Intelligence team is looking for passionate, technical savvy, energetic leader who like to think creatively. Someone who is self motivated and ready to lead team of most hardworking engineers building high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse team deals with Petabytes of data catering to a wide variety of real- time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We leverage a diverse technology stacks such as Snowflake, AWS, Teradata, HANA, Vertica, Single Store, Dremio, Hadoop, Kafka, Spark, Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.

Key Qualifications

High expertise in modern cloud data lakes and implementation experience on any of the cloud platforms like AWS/GCP/Azure - preferably AWS.

Good Experience in cloud based data warehouse - Snowflake.

Hands on Experience in developing and building data pipelines on Cloud & Hybrid infrastructure for analytical needs- Preferably having Cloud certifications.

Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.

Database development experience with Relational or MPP/distributed systems such as Teradata/ SingleStore/ Hadoop

Experience working with data at scale (peta bytes) with big data tech stack and sophisticated programming languages is a plus e:g Python, Scala.

Description

As a Cloud Development Engineer you will design, develop and implement modern cloud data warehouse/ datalakes and influence overall data strategy for the organization

Translate sophisticated business requirements into scalable technical solutions meeting data warehousing/analytics design standards

Strong understanding of analytics needs and proactive-ness to build solutions to improve the efficiency along with that help execute leading data practices & standards

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business

Ability to communicate effectively, both written and verbal, with technical and non- technical multi-functional teams

You will engage with many other group’s & internal/external teams to deliver best-in-class products in an exciting constantly evolving environment

Education & Experience

Bachelors or Masters Degree in Computer Science or equivalent in Engineering.

Role Number: 200154652",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
SecureKloud,Data Engineer,"The candidate will work in a cloud development team using Informatics Data Architecture and Data Engineering concepts to deliver foundational data capabilities such as Data Lakes and Master Data Management. The candidate will be part of the DevOps Team and responsible for building data platforms, test cases, deployment documentation, and support documentation in accordance with internal and industry standards. This is a highly technical and hands-on role.

Responsibilities
• Defining database design, data flows, and data integration techniques
• Design and build data solutions ensuring data quality, reliability, and availability
• Create data processing routines for managing enterprise master data throughout the data lifecycle (capture, processing, and consumption)
• Maximize business outcomes using Mobile Device Management via improved data integrity, visibility, and accuracy
• Manage and evolve global data lakes platforms
• Develop data ingestion, data enrichment, data deidentification routines, and RESTful APIs for consumption of data lakes data",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Varite India Private Limited,Data Engineer II (India - Contract),"Description: Location: Gurgaon / Bangalore Contract Duration: 8 months (extension and conversion based on project and performance basis) Shift: Early US (over lapping with India) Please provide 2-3 values or traits that are important to this role: Strong data warehouse and modeling skills You are a self-starter who is highly organized and communicative Deals well with ambiguity Please list 3-4 functional activities the resource should be capable of: Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) What technical skills will successful candidates possess Bachelor's or Master's degree in Computer Science or Engineering with 6+ years of proven experience in related field Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Experience building low-latency data product APIs You value strong pull request reviews, understand when to stand your ground and when to let go You are a self-starter who is highly organized, communicative, quick learner, and team-oriented Successful background as a technical leader driving cross-organizational data initiatives to completion Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Deep expertise in data access patterns, data validation, data modeling, database performance, and cost optimization Hands-on knowledge with BI tools, modern OLAP engines such as Presto, Clickhouse, Druid, Pinot, and data processing frameworks such as Spark and Flink Experience establishing and applying standards for operational excellence, code quality, and software engineering best-practices Effective verbal and written communication skills with the ability to think strategically about technology decisions and manage relationships with stakeholders and senior leadership Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage Experience with BI tools like Tableau, DataDog Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) Experience working with SQL & NoSql databases along with programming experience in Python and/or JAVA Strong business acumen with an understanding of business drivers and of how to drive value by supporting data discoverability across the organization Experience building low-latency data product APIs Experience working in an agile environment Tell us about your team: Clientis seeking a data engineer to join the Business Enablement Technology team. As we challenge the status quo and take the CTO's agenda to the next level, your focus will be on building a data platform to enable analytics and reporting capabilities across Product & Technology. This is an exciting, high-impact, and cross-functional role. This role requires a highly inventive individual with strong technical and analytical skills, execution drive, and self-motivation. Is there any industry specific experience that would separate one candidate from another Experience migrating data/apps from on-prem to cloud, engineering degree, tech industry experience",Gurugram,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
ITC Infotech India Ltd,Azure Data Engineer,"• Azure Data Engineer
• Ability to understand the functional & technical specification to develop the Notebooks
• Perform Data Loads and Transformations
• Schedule the Jobs with the Azure Data Factory and monitoring the jobs
• Strong in write complex SQL queries",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
bp,Senior Data Engineer - dataWorx,"Job Profile Summary
Role Synopsis:
As part of bp “reinvent”, we have created a major new business line called “Innovation & Engineering” (I&E). One key remit of this group is to drive the transformation of the company through its use of digital and data. A major digital sub-team within I&E is Digital Production & Business Services (DP&BS). DP&BS are responsible for all digital and data initiatives and operations across the following areas of the bp business:
• Production & Projects including Health, Safety, Environment & Carbon
• Refining & Operations
• Wells & Subsurface
• Business Services including Finance, Procurement, People & Culture, Performance Management
• Strategy & Sustainability
• “DataWorx” is the name of the data team that is responsible for all data within these areas and we are developing deep data capabilities to transform the access, supply, control and quality to our vast and ever growing data reserves that are measured in Petabytes. The DataWorx team covers many data sub-disciplines, including data science, data analytics, data engineering and data management as well as specialist areas such as geospatial, remote sensing, knowledge management and digital twin. The DataWorx team works with a wide variety of data from structured data to unstructured data & we also work on Real-time streaming data processing along with Batch data processing. Key Responsibilities :
• Architects, designs, implements and maintains reliable and scalable data infrastructure
• Leads the team to write, deploy and maintain software to build, integrate, manage, maintain, and quality-assure data
• Architects, designs, develops, and delivers large-scale data ingestion, data processing, and data transformation projects on the Azure cloud
• Mentors and shares knowledge with the team to provide design reviews, discussions and prototypes
• Leads customer discussions from a technical standpoint to deploy, manage, and audit best practices for cloud products
• Leads the team to follow software & data engineering best practices (e.g. technical design and review, unit testing, monitoring, alerting, source control, code review & documentation)
• Leads the team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline
• Leads the team in following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Actively contributes to improve developer velocity
• Part of a cross-disciplinary team working closely with other data engineers, software engineers, data scientists, data managers and business partners in a Scrum/Agile setup
• responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Work closely with other data engineers, software engineers, data scientists, data managers and business partners

Job Advert
Job Requirements :
Education :
Bachelor or higher degree in computer science, Engineering, Information Systems or other quantitative fields

Experience :
• Years of experience: 8 to 12 years with minimum of 5 to 7 years relevant experience
• Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
• Hands on experience with:
• Databricks and using Spark for data processing (batch and/or real-time)
• Configuring Delta Lake on Azure Databricks
• Languages : Python, Scala, SQL
• Cloud platforms : Azure (ideally) or AWS
• Azure Data Factory
• Azure Data Lake, Azure SQL DB, Synapse, and Cosmos DB
• Data Management Gateway, Azure Storage Options, Stream Analytics and Event Hubs
• Designing data solutions in Azure incl. data distributions and partitions, scalability, disaster recovery and high availability
• Data modeling with relational or data-warehouse systems
• Advanced hand-on experience with different query languages
• Azure Devops (or similar tools) for source control & building CI/CD pipelines
• Understanding Data Structures & Algorithms & their performance
• Experience designing and implementing large-scale distributed systems
• Deep knowledge and hands-on experience in technologies across all data lifecycle stages
• Stakeholder management and ability to lead large organizations through influence

Desirable Criteria :
• Strong stakeholder management
• Continuous learning and improvement mindset
• Boy Scout mindset to leave the system better than you found it

Key Behaviours :
• Empathetic: Cares about our people, our community and our planet
• Curious: Seeks to explore and excel
• Creative: Imagines the extraordinary
• Inclusive: Brings out the best in each other

Entity
Innovation & Engineering

Job Family Group
IT&S Group

Relocation available
Yes - Domestic (In country) only

Travel Required
Yes - up to 10%

Time Type
Full time

Country
India

About BP
INNOVATION & ENGINEERING
Join us in creating, growing, and delivering innovation at pace, enabling us to thrive while transitioning to a net zero ‎world. All without compromising our operational risk management.

Working with us, you can do this by:
• deploying our integrated capability and standards in service of our net zero and ‎safety ambitions
• driving our digital transformation and pioneering new business models
• collaborating to deliver competitive customer-focused energy solutions
• originating, scaling and commercialising innovative ideas, and creating ground-breaking new ‎businesses from them
• protecting us by assuring management of our greatest physical and digital risks

Because together we are:
• Originators, builders, guardians and disruptors
• Engineers, technologists, scientists and entrepreneurs‎
• Empathetic, curious, creative and inclusive

Experience Level
Intermediate",Pune,True,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Domnic Lewis Private Limited,Big Data Engineer,"we are hiring for Big Data Engineer with the experience in spark , python , SQL , AWS glue Big Data Engineer: Spark, Python, SQL, AWS Cloud (Glue, Lambda, Athena) Hyderabad Location A big data engineer is an information technology (IT) professional who is responsible for designing, building, testing and maintaining complex data processing systems that work with large data sets.",Secunderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Invsto,Data Engineer Intern (2024/2025 graduates),"You are
• A self-starter who is fueled by a desire to improve customer experiences in the moments that matter most, approaching your work with a bias toward accountability, decision-making and action
• Inquisitive and creative, with an ability to listen to identify customer needs and dig deeper to understand the reasons behind those needs
• Able to transform conceptual thinking into deliverables that generate excitement, feedback and alignment among stakeholders
• Thrive in a collaborative environment, partnering across the company with business experts, software developers, data engineers, and marketers

You have
• Enthusiasm to take the initiative to tackle problems and work with others to expand on your experience and expertise
• Experience with Python, AWS and databases such as Postgres
• Know-how to build data engineering pipelines using services such as Airflow

You will

Work closely with and learn from a team of engineers to contribute to a build a variety of features and infra.

Must-Have skills
• Python, Database experience
• Django (in lieu of Django, an equivalent tech stack)

Qualification and Experience:
• 0-2 Years software engineering
• Education: BE/BTech or equivalent (in lieu of academics, equivalent software developer experience required)

Benefits
• Fully remote, forever
• Annual retreats

About Us

Invsto is building the future of financial engineering.

We believe in hiring the best and providing complete autonomy to our employees to build stuff that they think would make a difference to the world

How to Apply

Does this role sound like a good fit? Email us at [hello@invsto.com].
• Include the role's title in your subject line.
• Send along links that best showcase the relevant things you've built and done (Github, Behance, Dribbble etc)

Invsto focuses on Hedge Funds and Stock Exchanges. Their company has offices in Bangalore Urban. They have a small team that's between 11-50 employees.

You can view their website at https://invsto.com/ or find them on LinkedIn.",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False
Everyday Health Group,Data Engineer,"Description

Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.

Health eCareers, a property of Everyday Health Group, is looking for a Data Engineer to support our growing business.

Key Responsibilities
• Understand overall data collection strategies and implement them in data tables and connections.
• Program Python-based APIs and web services per implementation schema, such as creating applications to access Facebook, Twitter, Salesforce, and other data sources.
• Use SQL Server, MySQL, HDFS and AWS to design, develop and deploy data processing.
• Analyze and organize raw data from various ETL tools.
• Monitor/Forecast computing resources usage (data lakes, AWS, EC2, etc.).
• Collaborate with ETL developers located at various offices (US and India)
• Implement coding standards, procedures and techniques, concluding writing technical code base.
• Detect, repair, prevent data pipeline failures; identify systematic weaknesses and provide pre-emptive remedies with programming or processes.
• Build and optimize data storage systems.
• Prepare data for prescriptive and predictive modeling.
• Recommending and implement emerging database technologies.
• Create automation for repeating database tasks.

Job Qualifications
• 4+ years of programming experience with an emphasis on data processing.
• Coding skills: Python, SQL.
• Ability to create object-oriented programming and data architectures
• Knowledge of new, leading technology strategy in data engineering and management
• Understanding of industry technologies in scalability, performance, delivery pipeline and maintenance of these tools and systems.
• Hands-on experience with SQL database design, AWS or other cloud systems.
• 2+ year’s experience in Linux environments.
• Experience with SQL Server, MySQL or other popular database management tools.
• Good/Expert knowledge of API services
• Familiarity with Agile process
• Clear documentation requirements and specifications
• Bachelors or Advanced Degree in Information Management, Computer Science or related field.

Desirables:
• Experience in AWS, Tableau
• Experience in developing custom data pipelines in HDFS
• Experience working with social, Double Click, Adobe APIs
• Working with teams across multiple locations
• Skills: Pig; Hive; Spark; Impala; EMR

Our Culture and Values

We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.

Life at Everyday Health

At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work.

#HealtheCareers",,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Unusual Hire,Data Engineer,"Job type: Partial Onsite

Expert Level

Project detail

General:

– Ability to architect Data Science solutions

– Ability to lead a team

– Ability to gather requirements

Must have :

– 6-8 Years Data Science Experience

– At least 4 Data Science solutions

– At least 2 NLP / ML Solutions

– At least 1 solution with Deep learning framework Deep

– Python / PySpark

– Knowledge in Statistical Algorithms like classification, Regression , recommendation and Clustering , Neural Networks

– Azure ML , Databricks , Delta Lake , Data science workspace

Industry Categories

Data Scientist

Languages required

English

₹200 - ₹500

Cost

Location

India

Project ID: 00002514",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Autodesk,Data Engineer,"Job Requisition ID # 22WD66509 Job Description Position Overview In this role, you will be part of a highly energetic team of engineers working on Autodesk Enterprise Integration Platform to assemble, enrich and enhance the data as per the business needs. You would be part of the team working on providing advisory services and support to business in deriving sales strategies. you will be working to build robust and scalable self-service platform for Autodesk while using innovative technologies in big data space. Responsibilities Design and develop components of end-to-end Enterprise Integration Platform Work with the team to make the Enterprise Integration Platform an efficient, robust and scalable platform Enthusiastic and passionate member of a highly skilled and motivated agile development team Contribute to a team culture that values quality, robustness, and scalability while fostering initiatives and innovation Minimum Qualifications BS or MS in computer science or a related field with 5+ years of experience. 3+ years of experience with cloud ETL/ELTs Working experience in Agile methodologies Strong knowledge and experience in AWS technologies Experience in Matillion and Redshift Strong programming skills in either Java, Scala or Python Experience with Hive and one relational DB Strong problem-solving skills Strong communication skills Ability to learn new technical skills Preferred Qualifications Experience with Matillion Experience with Python, Scala Experience with SQL (Redshift) Experience with Kanban methodology At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. Are you an existing contractor or consultant with Autodesk Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact .",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Finxera,Data Engineer,"Company Description

Finxera, Powered by Priority Technology Holdings, Inc. (NASDAQ: PRTH), is headquartered in Alpharetta, Georgia USA. Our India office is located in Chandigarh, where our dynamic team builds state of the art, sophisticated Fintech products & solutions.

We are an emerging payments powerhouse that offer a single unified platform for Banking & Payments powering modern commerce.

Finxera offers a unique family of products which integrate into SMB Payments, B2B Payments and Enterprise Payments to help businesses thrive. We are on a mission to offer an industry agnostic platform that enables businesses to collect, store and send money using various new age payment methods.

Finxera is an employee-first organisation and we continually strive to ensure their professional and personal success supported by employee friendly policies and a positive work environment built on mutual respect and professionalism. We offer a dynamic work environment, with continuous growth & learning opportunities. We believe in growing together and our people are the driving force behind our success.

Summary Requirement

We are looking for a Data Engineer for a position in the Data and Analytics team.We need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively.

Responsibilities

1. Data Warehousing experience

2. Develop ETL pipelines against traditional databases and distributed systems to allow our team to remain agile in data requirements, and to flexibly produce data back to the business and analytics teams for analysis.

3.Dimensional Modelling experience

4.Database Skills - SQL / PLSQL

5.Python / Spark / Hive / Nifi Knowledge

6.Elasticsearch / Kafka knowledge would be plus

7.Data visualization tools knowledge

8.Participate in design reviews and code reviews

9.Trouble shoot and resolve production issues

10.Resolve performance related issues

11.Knowledge on Data Science / ML / Analytics would be plus

12.Build processes that analyze and monitor data to help maintain data accuracy and completeness.

13.Ability to work with colleagues across global locations remotely

14.Independent and able to work with little supervision

15.Able to mentor and supervise junior team members

16.Agile Methodology Experience

Required Skills & Qualifications

l **Should have scored 70% & above in 10th and 12th grade.

l SQL/PLSQL

l ETL/ETL

l Python

l Pyspark (Optional)

l Any BI Tool like Si Sense, Looker, Quick Sight, Tableau, Power BI, etc (Preferred LOOKER)

l Dimension Modelling (Kimbal) / Data warehousing

l Redshift (Good to have)

l OLAP Cubes",Chandigarh,True,False,True,False,False,False,False,False,True,True,False,False,True,False,False,False
Confidential,Associate - Data Engineer,"JOB DESCRIPTIONRole and responsibilitiesResponsible for overall data analysis (e.g wrangling, cleansing), tools, and technology implementationBuild data systems, pipelines, and workflowsAbility to organize structure/unstructured raw data sources of many types with the ability to combine into useable dataDevelop data wrangling and analytical applicationsCollaborate with data scientists and analyst to management data pipelinesExplore opportunities to enhance quality of data and processesManage a team of data analysts, MIS, and operations resourcesProactive communication with project manager, ensuring all client requirements are met and reports are submitted on timeOversee implementation of tools and technology to build efficiency and consistencyResolve and escalate issues with tools and applicationsDesired candidate profile3-5 years of experience as a data engineer and/or developerStrong background within the role of a data engineer for capabilities such as knowledge of various programming language with a strong emphasis on Python, SQL, Power Query and VBA/MacrosExcellent knowledge in Microsoft Excel and knowledge of advance functionsExperience with SQL set-up and advanced queries developmentStrong technical knowledge in data mining, wrangling, and structuringManaging large volumes of data with the ability to scale as neededAbility to develop and design end-to-end solutions with operations/analyst groups for deliverablesCreating custom scripts and applications to perform wrangling, cleansing, and storingTrouble shoot data issues at any level of project/structure pipeline(s)Presenting data/reports as neededExcellent people management skillsPassionate to drive business metrics - Productivity, Quality, and other key deliverablesAbility to prioritize between multiple complex projects/timelinesExcellent written and Verbal communicationHigh level of positive attitude with good listening skillsAbility to priorities between multiple complex projects/timelinesStrong attention to detail and the ability to conduct root cause analysisCandidates with demonstrated experience in Data Breach Response, or Incident Response will be preferredKnowledge and hands-on experience in breach notification and privacy laws around data breach scenarios is desirable but not must.UnitedLex is committed to preserving the confidentiality, integrity, and availability of all the physical and electronic information assets throughout the organization. Consistent with the UnitedLex ISMS policy and the ISO 27001 standard, every employee is responsible for complying with UnitedLex information security policies and reporting all security concerns, weaknesses, and breaches. Legal Services",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Oil Field Instrumentation (India),DATA ENGINEER,"Experience :

4+ years of Mud Logging experience as Data Engineer in India and abroad, both onshore and offshore rig operations. Deep Water experience will be an advantage. Proficient with real-time data monitoring and data management, WITSML transmission, gas detection and analysis, MLU maintenance.

Qualification :

Graduate or above in Geology, Applied Geology, Geoscience, Petroleum Technology or related, or B.E. in Petroleum Engineering.",Mumbai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
News Corp,Senior Data Engineer,"One of the most innovative and high-profile teams in News Corp is looking for a seasoned data engineer to help accelerate its vision. The role will involve combining all of News Corp’s data across categories, countries and organizations into a singular view of a customer enabling engagement, measurement and targeting. The Dow Jones Senior Data Engineer will partner closely with product, data & engineering teams to help make this data available and accessible to our businesses, teams and partners in order to drive revenue and improve the customer experience.

Responsibilities
• Design, implement and maintain high performance big data infrastructure/systems & big data processing pipelines scaling to billions of structured and unstructured events daily
• Design, implement and maintain deep integration with up-stream systems
• Monitor performance of the data platform and optimize as needed
• Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
• Support products with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
• Data analysis, understanding of business requirements and translation into logical pipelines & processes
• Conduct timely and effective research in response to specific requests (e.g. data collection, summarization, analysis, and synthesis of relevant data and information)
• Evaluate and prototype new technologies in the area of data processing
• Think quickly, communicate clearly and work collaboratively with product, data, engineering, QA and operations teams
• High energy level, strong team player and good work ethic

Technologies we use
• AWS (emr,ec2,glue,athena,redshift,lambda,s3,dynamodb,sns)
• Python, Spark (PySpark)
• Kubernetes, Docker
• Airflow
• Git for source code management
• Jira

Qualifications
• BS in Computer Science or other technical discipline
• 5+ years of experience designing and developing big data processing systems using distributed computing
• Fluency in Python and Spark
• Expert knowledge in optimizing complex SQL queries
• Experience with job orchestration tools like Airflow
• An affinity for automation
• Experience working with cloud platforms such as AWS & GCP
• Familiarity with networking and network application programming, including HTTP/HTTPS, JSON, and REST APIs
• Experience with at least one object oriented language (ex: Java)
• Strong OO design, data structure, and algorithm design skills
• Strong interest in emerging technologies: Hadoop, Hive

Nice to Have
• Experience in the digital advertising and marketing industry
• Contribution to Open Source projects",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,True,False
Saasvaap,Sr Data Engineer,"JOB DESCRIPTION
• Job Description (Data Engineer) ROLE AND RESPONSIBILITIES You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
• You will support our product, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
• You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
• You will be excited by the prospect of optimizing or even re-designing Peer Data architecture pipeline to support our next generation of products and data initiatives.
• Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implergent internal process improvements: automating manual processes, optimizing data delivery, re-designing hastructure for greater scalability, etc.
• Buld the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep our data separated and secure across national boundaries through multiple data centers and cloud regions.
• Create data tools for analytics assist in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.
• QUALIFICATIONS AND EDUCATION REQUIREMENTS • 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field PREFERRED SKILLS Experience with Big Data platforms such as Apache Hadoop and Apache Spark Deep understanding of REST, good API design, and OOP principles Experience with object-oriented/object function scripting languages: Python, C#, Scala, etc.
• Experience with relational SQL and NoSQL databases, including Postgres, Cosmos and Cassandra.
• Experience with data pipeline and workflow management tools: Keboola, Stitch, Azkaban, Luigi, Airflow, etc.

SKILLS REQUIRED

My SQL, AWS

EXPERIENCE

4-10

LOCATION

Kochi, Trivandrum

WORK TYPE

FullTime

TIME SHIFT

Day",,True,False,True,False,False,False,False,False,False,False,False,True,False,False,True,False
Confidential,Chistats - Senior Data Engineer - ETL/Data Pipeline,"Day-to-Day Responsibilities :- Create and manage ETL pipelines and job schedulers.- Handle unstructured data and work to automate data ingestion and validation.- Develop APIs with Flask and Python for data mining, transformation and ingestion to AWS.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Write clean, performant code to develop functional applications; build reusable code and libraries- Collaborate with team to evaluate technologies we can leverage, including open-source frameworks, libraries, and tools.- Support Business and application teams with respect to data-related requirements.- Build proactive data validation automation to catch data integrity issues.- Troubleshoot and resolve data issues using critical thinking.Required Qualifications :- Minimum of 3 years of relevant data platform experience (structured/non-structured database, data extraction, data ingestion)- A Degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines.- Experience in at least one of these databases: MS SQL, mySQL, PostGreSQL.- Experience in AWS would be ideal and basic cloud infrastructure knowledge.- Experience in Python is required for Web Scraping.- Strong fundamentals in data mining & data processing methodologies- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.- Build processes supporting data transformation, data structures, metadata, dependency and workload management.Good to have Critical Skills :- Ability to thrive in challenging situations and solve complex problems- Strong bias for action, and see yourself as an 'initiator' and 'problem solver'- Analytical and problem-solving skills- Customer centricity and Good communication (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Calix,Senior Data Engineer - Snowflake,"This position is based in Bangalore, India.

Calix is undergoing a growth transformation, and we are looking for the best and brightest engineers for our Data Engineering team. Our team is facilitating Calix’s transformation into a more data centric enterprise with our business operational leaders. We partner with our operational teams to identify the key points of decision. We create decision support tools that enable optimal data driven selection of business actions and we build out & maintain these decision support tools on a modern data technology stack using DataOps processes. We are building out the data foundations for the next phase of our growth journey. This is a great opportunity to join a rapidly scaling enterprise with a lot of opportunity for personal growth.

The Data Engineering team is seeking a Lead Data Engineer who will be an extraordinary addition to our growing team. You will build and maintain our cloud-based enterprise data platform. Key areas that you will own include data architecture, data modeling, data pipeline flow, data warehousing, security and governance protocols, data integrity processes, and data QA best practices. You will lead buildout of the end-to-end ETL/ELT data environment, integrating with new technologies, and the development of new processes to support the creation and deployment of trusted, accurate and secure decision support tools to the Calix operational business units.

The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

Responsibilities and Duties:
• Lead data modeling, data ingestion, ELT/ETL, and data integration development using our cloud-based tooling including Snowflake, AWS, Fivetran, dbt, Airflow and GitHub.
• Establish and maintain a DataOps approach for our data pipeline infrastructure and processes.
• Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
• Deploy production machine learning pipelines into business operations analytic tooling.
• Ensure data quality throughout all stages of acquisition and processing.
• Create and maintain secure and governed access to the enterprise data warehouse and reporting tools.
• Evaluate new technologies for continuous improvement in data engineering.
• Participate in project meetings, providing input to project plans and providing status updates.
• A desire to work in a collaborative, intellectually curious environment.
• Highly motivated self-starter with a bias to action and a passion for delivering high-quality data solutions.

Qualifications
• 5+ years of experience in related field; preferably experience building and delivering data pipelines, data lakes and ELT solutions at scale.
• Expert knowledge of data architecture, data engineering, data modeling, data warehousing, and data platforms.
• Experience with Snowflake, BigQuery, Redshift, AWS, and pipeline orchestration tools (Fivetran, Stitch, Airflow, etc.).
• Coding proficiency in at least one modern programming language (Python, Java, Ruby, Scala, etc.).
• Deep SQL expertise.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, operations, and technical documentation.
• Excellent verbal and written communication skills and technical writing skills.
• Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership to gain alignment, and drive progress.
• Bachelor’s degree or equivalent experience in Computer Science, Engineering, Management Information Systems (MIS), or related field.

Preferred Qualifications
• Experience with dbt SQL development environment.
• Experience developing and deploying machine learning models in a production environment.
• Experience with Power BI and/or SFDC Einstein/Tableau.
• Experience with Oracle ERP and Oracle Data Cloud tools.

Location:
• Bangalore, India",Bengaluru,True,False,True,True,False,False,False,True,True,True,False,False,True,True,True,True
Paradise Placement Consultancy,Azure Data Engineer (Mercedes-Benz)(BSL),"Job Description : Job Description: Job Description, Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, ETL,Big Data. Responsibilities: Touch base with customers to collect the requirements and analyze them Design and build end-to-end data pipelines to get the data for customers Unit testing of the pipelines and UAT support Deployment and post production support Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents. Adapt changes to the existing scripts, codes and pipelines. Reviewing design, code and other deliverables created by your team to guarantee high-quality results Capable enough to own the PoCs and deliver the results in reasonable time Accommodate and accomplish any ad-hoc assignments Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users. Job Qualifications Qualifications: Bachelor's or Master's degree in Computer Science, Information Technology or equivalent work experience 2+ years of full time data engineering experience 2+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks Skilled in ETL/ELT process. Good working knowledge with ETL tools. Experienced with Hadoop/Big data eco systems Data migration experience from on premise to cloud Expertise in structured query language and PL/SQL Exposure to log analytics and debugging Good to have DevOps, Continuous Deployment and testing techniques Agile development experience Fluent English in spoken and written Mercedes-Benz Research and Development India Private Limited. Preferred Qualifications: Microsoft Azure Certifications Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics Hands-on experience with Databricks notebooks Hands-on experience with Kafka/eventhubs Working experience in an international team or abroad.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Kaplan,Senior Data Engineer (Hybrid),"Job Title

Senior Data Engineer (Hybrid)

Job Description

For more than 80 years, Kaplan has been a trailblazer in education and professional advancement. We are a global company at the intersection of education and technology, focused on collaboration, innovation, and creativity to deliver a best-in-class educational experience and make Kaplan a great place to work.

Our offices in India opened in Bengaluru in 2018. Since then, our team has fueled growth and innovation across the organization, impacting students worldwide. We are eager to grow and expand with skilled professionals like you who use their talent to build solutions, enable effective learning, and improve students’ lives.

The future of education is here and we are eager to work alongside those who want to make a positive impact and inspire change in the world around them.

Job Impact and Scope Summary

The Senior Data Engineer at Kaplan North America (KNA) within the Analytics division will work with world class psychometricians, data scientists and business analysts to forever change the face of education. This role is a hands-on technical expert who will own the design and implementation of an Enterprise Data Warehouse powered by AWS RA3 as a key feature of our Lake House architecture.

The perfect candidate is an expert in data warehousing technical components (e.g. data modeling, ETL, reporting). You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be able to work with business customers in a fast-paced environment understanding the business requirements and implementing data & reporting solutions. Above all you should be passionate about working with big data and someone who loves to bring datasets together to answer business questions and drive change.

Responsibilities
• Hands-on technical leader. Continually raises the bar for the data engineering function.
• Leads the design, implementation, and successful delivery of large-scale, critical, or difficult data solutions. These efforts can be either a new data solution or a refactor of an existing solution and include writing a significant portion of the “critical-path” code.
• Sets an example through their code, designs and decisions. Provides insightful code reviews and take ownership of the outcome. (You ship it, you own it.)
• Proactively works to improve data quality and consistency by considering the architecture, not just the code for their solutions.
• Makes insightful contributions to team priorities and overall data approach, influencing the team’s technical and business strategy. Takes the lead in identifying and solving ambiguous problems, architecture deficiencies, or areas where their team bottlenecks the innovations of other teams. Makes data solutions simpler.
• Leads design reviews for their team and actively participates in design reviews of related development projects.
• Communicates ideas effectively to achieve the right outcome for their team and customer. Harmonizes discordant views and leads the resolution of contentious issues.
• Demonstrates technical influence over 1-2 teams, either via a collaborative development effort or by increasing their productivity and effectiveness by driving data engineering best practices (e.g. Code Quality, Data Quality, Logical and Physical Data Modelling, Operational Excellence, Security, etc.).
• Actively participates in the hiring process and is a mentor to others - improving their skills, their knowledge, and their ability to get things done.
• Hybrid Schedule: 3 days remote / 2 days in the office
• 30-day notification period preferred

Requirement:
• In-depth knowledge of the AWS stack (RA3, Redshift, Lambda, Glue, SnS).
• Expertise in data modeling, ETL development and data warehousing.
• 3+ years’ experience with Python, or Java, Scala
• Effective troubleshooting and problem-solving skills
• Strong customer focus, ownership, urgency and drive.
• Excellent verbal and written communication skills and the ability to work well in a team.

Preferred Qualification:
• 3+ years’ experience with AWS services including S3, RA3.
• Ability to distill ambiguous customer requirements into a technical design.
• Experience providing technical leadership and educating other engineers for best practices on data engineering.
• Familiarity with Airflow, Tableau & SSRS.

#LI-Remote

#LI-AK1

Location

Bangalore, KA, India

Additional Locations

Employee Type

Employee

Job Functional Area

Data Analytics/Business Intelligence

Business Unit

00092 Kaplan Health

Kaplan is an Equal Opportunity Employer. All positions with Kaplan are paid at least $15 per hour or$31,200 per year for full-time positions. Compensation for specific positions are based on job level, skills, years of experience, and education, among other factors. Additionally, certain positions are bonus or commission eligible. Information regarding benefits can be found here
.

Diversity & Inclusion Statement:

Diversity inspires innovation and growth in the Kaplan community. Kaplan strives to be a model employer for inclusiveness. Not only does Kaplan value its employees for their professionalism and skills, but also for the unique viewpoints they bring to the Organization. Kaplan's employees bring diverse perspectives, ideas, and backgrounds that give Kaplan a competitive edge in anticipating and exceeding our students' needs in today's global market. Learn more about our culture
.",Bengaluru,True,False,False,True,False,False,False,True,False,True,False,False,True,False,True,False
"Planview, Inc.",Data Engineer,"Overview

Planview is looking for a passionate data engineer to join our team innovating tools for connected work. You will work closely with other data engineers, data scientists and individual product teams to specify, validate, prototype, scale, and deploy features with a consistent customer experience across the Planview product suite.

Responsibilities
• Ability to work in a fast paced start up mindset. Should be able to manage all aspects of AI/ML activities
• Develop platforms that make data across applications/application deployments available for AI/ML-driven feature prototyping, proofs-of-concept, and general availability
• Refine data pipelines for analysis, while refining, automating, and scaling as needed for the use-case at hand
• Work on various aspects of the AI/ML ecosystem – data modelling, data pipelines, data observability, data documentation, scaling, deployment, monitoring and maintenance etc.
• Work closely with Data scientists and MLOps Engineers to come up with scalable system and model architectures for enabling real-time ML/AI services

Qualifications

Required qualifications
• Masters or equivalent experience in Informatics, CS, Data Science or a related field
• 2+ years of experience as a data engineer or data scientist with a focus on data engineering for ML applications
• Strong Python and SQL coding skills
• Familiar with AWS Data and ML Technologies (AWS Sagemaker, Data Pipeline, Glue, Athena, Redshift etc)

Preferred Qualifications
• Demonstrated experience building data and analytics pipelines/services that efficiently scale for cloud application usage, meeting a product team’s SLA for performance and resilience
• Exposure to database queries and strong in SQL
• Exposure to any of the libraries and frameworks in data science (Pandas, Numpy, Dask, PySpark etc)
• Exposure to data version control (DVC) and orchestration tools (Airflow, etc)
• AWS Certification is a plus
• Skilled at working as part of a global, diverse workforce of high-performing individuals
• AWS Certification is a plus",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Full Potential Solutions,Lead Data Engineering,"Overview:

About The Company

Full Potential Solutions (FPS Perch) is a global product company headquartered in the US. We help contact centers around the world better engage with their customers by combining our deep domain expertise with cutting-edge technology. Our AI enabled, cloud-scalable analytics products enhance the end-to-end customer journey via omnichannel engagement (voice, email, SMS, chat and conversational AI) and optimize agent performance. Our offices are spread across the US, India and Philippines.

Some exciting initiatives they are working on are:
• Designing and development amazing user experiences across data visualization and analytics in the customer engagement / contact center industry
• Salesforce based solutions for omni-channel customer engagement leveraging AI to enhance contact strategy and routing, guide agents on next best action (NBA) enable dynamic scripting, capture voice of customer (VOC)
• Developing standardized AI/ML models to optimize customer engagement and agent performance using various Python frameworks and AWS services. (Transcribe, Comprehend, Sagemaker, etc.)
• Developing a comprehensive Data, Analytics and Reporting platform to integrate, measure and analyze millions of daily customer interactions/metrics across all our clients.
• Building web/mobile apps to improve agent/manager performance across thousands of agents (metrics, gamification, collaboration).
• Deploying cutting-edge, tech-enablement services using the AWS ecosystem, including microservices and serverless architecture, CI/CD pipelines, event notification & search services, multi-tenant architecture ec.,

Our team has successfully built and scaled analytics-focused products at companies such as Salesforce, Demandware, and IBM. We have strong backgrounds in Computer Science, Math and Engineering from top universities such as IIT, Harvard and Yale. We strongly believe that empowered people are the key to building a great company, and our development process focuses on iteratively improving our products as well as ourselves as individuals and as a team. Our mission as a company is to create an environment where the people THRIVE.

Explore more at: FPS Website

Leadership Team: Explore here

About The Team

Our analytics team helps our clients leverage data to optimize their performance and results. Our Data Analysts possess a unique skill of understanding the data, making a clear sense of it, and telling a story to the business. We work closely with the Analytics Leaders and the rest of the Analytics product team to analyze and understand customer needs, explore granular data to identify key drivers that influence each KPI, measure the level of expression of attributes that move the KPI, and execute in a way that brings the best solutions to life. Most importantly our team exhibits our core values: Integrity, Excellence, Accountability and Grace.

Responsibilities:

Functional Requirements
• Data exploration skills to unravel data sources and identify the granularity, attributes & measure
• Discover data sources and determine the best EL approach and tools to bring data into the D
• Determine the data cleansing rules based on source data and ensure data replication accurac
• Coordinate & collaborate with data architects and BI engineers to ensure timely project deliver
• Understanding of data warehouse concepts and implementation methods of warehouse object
• Translate the data models from the ER diagram into physical models coded into data warehous
• Innovate reusable and configurable frameworks for data replication to build the bronze laye
• Well-versed with the data warehouse modeling concepts (star/snowflake/denormalized structures
• Define data quality rules, modeling standards, business metrics, standard processes and test case
• Understand the domain, business cases, objectives, and KPIs that need to be reported and analyse
• Participate in data discovery phase and capture the data sources, required KPIs with the formulae

Qualifications:

Technical Requirements
• Bachelor’s degree in a Technical/Quantitative subject such as Computer Science (B.E/B.Tech/MCA
• 7 - 9 years of relevant experience in the field of Data Engineering and Data Warehouse/BI solution
• Proficient in SQL, query optimization, coding stored procedures, and ad-hoc data analysis using SQ
• Develop data pipelines using some combination of ETL/ELT tools and data processing framework
• Develop DBT models as per the data model and implement the data transformations & test case
• Orchestrate data integration & transformation processing from sources to ssots in data warehous
• Design, develop, monitor and re-engineer database objects and processes/pipelines/schedules
• Use AWS services for data storage, access and retrieval and application setup and monitorin
• Design & document the data processing workflows and setup systems as per solution architecture
• Design the multi-dimensional data model in data warehouse in order to meet BI reporting needs
• Setup software engineering best practices for data pipelines with data-associated documentation
• CI/CD implementation with source code version control, QA checks and deployment automation
• Hands-on with languages like Spark, Python, Scala, R, Bash/Shell Scripting or stored procedures
• Must have worked with RDS or data lakes (MySQL, PostgreSQL, Oracle, Redshift, Snowflake, etc.)
• Hands-on experience in using ETL/ELT tools like Talend, Informatica, SSIS, Airbyte, Alteryx, Stitch etc.,
• Experience working with orchestration tools such as Airflow, Astronomer, Control-M, Prefect etc.,
• Experience working with DevOps tools such as Bitbucket, Github, Gitlab, Jenkins, CircleCI etc.,
• Experience working with AWS Cloud services such as S3, EC2, ECS, EFS, Lambda, Docker, Kubernetes
• Hands-on experience in using data-modeling tools like Erwin, DBSchema, MySQL Workbench, ER Studio

Leadership Responsibilities
• Lead a squad of data engineers to implement data warehouse solution across various projects
• Ensure adherence to the best practices of data engineering and testing across all projects
• Drive sprint planning/review and lead daily Kanban meetings in alignment with project plan
• Allocate work smartly and efficiently manage/ coach the team while monitoring the progress
• Develop systematic training plan for onboarding new joiners and upskilling existing resources",,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Kyndryl,Azure Data Engineer,"Why Kyndryl Kyndryl is a market leader that thinks and acts like a start-up. We design, build, manage, and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward - always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers, and our communities. We invest heavily in you - not only through learning, training, and career development, but also through the flexible working practices and stellar benefits that help you grow and progress long-term. And we give back - from planting 90,000 trees in our first 3 months as part of our One Tree Planted initiative to the Corporate Social Responsibility and Environment, Social and Governance practices embedded within everything we do, we are committed to powering human progress in an ethical, sustainable way. Your Role and Responsibilities Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Create and maintain clear and complete pipeline documentation, e.g., architecture diagrams and data transformations. Integrates data solutions with business processes. Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Provide actionable insights via compelling storytelling to drive business outcomes. Microsoft Certified: Azure Data Engineer Associate Required Technical and Professional Expertise Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Preferred Technical and Professional Experience Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Required Education Associate's Degree/College Diploma",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Role: AWS Data Engineer

Ex- 4 to 8 YRS

Locations- Delhi/NCR, Gurgaon, Bangalore, Ahmedabad, Pune, Indore, Mumbai, Kolkata

Must Have –
• Working on EMR, good knowledge of CDK and setting up ETL and Data pipeline
• Coding - Python
• AWS EMR, Athina, Supergule, Sagemaker, Sagemaker Studio
• Data security & encryption
• ML / AI
• Pipeline
• Redshift
• AWS Lambda

Expectations/Responsibility
• Industry experience in Data Engineering on AWS cloud with glue, redshift , Athena experience.
• Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python.
• 3+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
• Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.
• Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
• AWS engineer provides comprehensive systems administration functions on Amazon Web Services (AWS) infrastructure to include support of AWS products such as: AWS Console root user administration, Key Management, EC2 Compute, S3 Storage, Relational Database Service (RDS), AWS Networking & Content delivery (VPC, Route 53, ELB, etc.) Identity & Access Management, CloudWatch, CloudTrail, Cloud Formation, Auto Scaling, Cost and Usage Reports, and more.
• Train and guide the company’s HR engineering team on developing with aforementioned AWS tools, while also executing on specific deliverables (ingestion, Storage, integration, warehouse, visualization)
• Coach and mentor other technical resources on the team on AWS technologies
• Create ETL piplelines that are highly optimized with very large data sets
• Solve issues with data models and come up with solutions
• Developing and directing software system testing and validation procedures, programming and documentation
• Analyzing user needs and requirements to determine feasibility of design within time and cost constraints
• Provide technology expertise, direction, coordination, and consultation, in the development, integration, launch, scaling, and maintenance of new and existing products and solutions
• Establishes infrastructure technology architectures, standards, test plans, design templates and governance
• Works with the team to define standards and frameworks with regards to coding, programming, and the general development of applications for multiple platforms
• Work with business teams to understand customer issues and to investigate, prototype and deliver new and innovative solutions

FRESHERS DO NOT APPLY.",New Delhi,True,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False
Mercede,Azure Data Engineer-Karnataka-Bangalore,"Azure Data Engineer

Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, Big Data, Azure DevOps, Azure Boards, VSTS Git

Hey, do you want to change the world? Build #TheNextBigThing? Which means implementing ideas that are said to be unachievable? Like it was stated at first about smart chatbots or artificial intelligence. At MBRDI you are dealing with questions, for which there are no answers. Not yet.

About Us

The Corporate Center of Excellence (CoE) for AI, Advanced Analytics and Big Data is working on #TheNextBigThing. Besides conducting cool use cases in the field of Data Analytics and AI, we are inventing, building and running eXtollo a data analytics cloud platform based on Microsoft Azure for Daimler teams around the world. Our users trust in the cross-divisional platform to create secure and scalable cloud applications based on Machine Learning, Artificial Intelligence Big Data technologies. In addition to the provisioning and utilization of the technology we are also responsible for Daimler s data treasure the eXtollo Data Lake.

Role

We are searching you as a Data Engineer to continuously improve eXtollo within an agile working environment. You are expected to work directly with customers to understand their data demands and build end to end pipelines to get the data as they wish. Also be able to understand the current codes, pipelines and scripts which are already in production in a short time and support operations/changes.

Experience of data migration into Azure from various cloud and on-prem is a plus. You are expected to have good exposure to structured query language preferably MS Sql Server. Should be able to do PoC for the new adaptions and work independently with minimal guidance. Data warehouse/data engineering experience is appreciated. Most importantly the candidate should have real hands-on experience rather bookish/training experience.

Responsibilities
• Touch base with customers to collect the requirements and analyze them
• Design and build end-to-end data pipelines to get the data for customers
• Unit testing of the pipelines and UAT support
• Deployment and post production support
• Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents.
• Adapt changes to the existing scripts, codes and pipelines.
• Reviewing design, code and other deliverables created by your team to guarantee high-quality results
• Capable enough to own the PoCs and deliver the results in reasonable time
• Accommodate and accomplish any ad-hoc assignments
• Building CI/CD pipelines in Azure DevOps to deliver our services into various data centers worldwide
• Analyzing and solving incidents in productive environment while avoiding data loss and minimizing service outages
• Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users
Job Qualifications

Qualifications
• Bachelor s or Master s degree in Computer Science, Information Technology or equivalent work experience
• 3+ years of full time data engineering experience
• 3+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks
• Skilled in ETL/ELT process. Good working knowledge with ETL tools.
• Must be experienced with Hadoop/Big data eco systems
• Data migration experience from on premise to cloud
• Expertise in structured query language and PL/SQL
• Experienced in Powershell scripting for orchestration
• Exposure to log analytics and debugging
• Good knowledge in DevOps, Continuous Deployment and testing techniques
• Agile development experience
• Fluent English in spoken and written

Preferred Qualifications
• Microsoft Azure Certifications
• Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics
• Hands-on experience with Databricks notebooks
• Working experience in an international team or abroad
,

This job is provided by Shine.com",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
HARMAN International,Data Engineer,"What You Will Do :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis What You Need :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis",Bengaluru,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
GSPANN Technologies,Azure Data Engineer,"Should have experience in ADLS (Azure Data Lake storage) Experience implementing Azure Data Factory Pipelines using latest technologies and techniques Experience in working on Azure HDInsight Experience in working with Storage Strategy Azure developer should be able to ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements Expert in Azure Data Factory, Azure Data Lake Azure SQL Data Warehouse, Azure Functions, Databricks · Comfortable working with Spark, Python, and PowerShell Excellent problem solving, Critical and Analytical thinking skills Strong t-SQL skills with experience in Azure SQL DW DevOps, CI/CD, and Automation experience strongly preferred Able to interact with team members collaboratively Experience handling Structured and unstructured datasets Experience in Data Modelling and Advanced SQL techniques",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Uber,Data Engineer II,"What You'll Do
• Responsible for defining the Source of Truth (SOT), Dataset design for multiple Uber teams.
• Identify unified data models collaborating with Data Science teams
• Streamline data processing of the original event sources and consolidate them in source of truth event logs
• Build and maintain real-time/batch data pipelines that can consolidate and clean up usage analytics
• Build systems that monitor data losses from different sources and improve the data quality
• Owns the data quality and reliability of the Tier-1 & Tier-2 datasets including maintaining their SLAs, TTL and consumption
• Devise strategies to consolidate and compensate the data losses by correlating different sources
• Solve challenging data problems with cutting-edge design and algorithms.

What You'll Need
• 4+ years of extensive Data engineering experience working with large data volumes and different sources of data.
• Strong data modeling skills, domain knowledge, and domain mapping experience.
• Strong experience in using SQL language and writing complex queries.
• Experience with using other programming languages like Java, Scala, Python
• Good problem-solving and analytical skills
• Good communication, mentoring, and collaboration skills.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Cardinal Health,"Sr. Data Engineer, Data Engineering","Job function:

IT Quality Control is responsible for owning and implementing software testing and certification strategies for the enterprise. Debugs problems with software through standard tests and recommends solutions. Conducts defect trend analysis and continuous process improvement. Demonstrates knowledge of requirement and risk based testing principles, theories, concepts and techniques. Establishes internal IT service quality control standards, policies and procedures.

Job duties:

Create Test Strategy, define quality standards for Google Cloud Platform and define metrics to measure the efficiency and testing for applications on Google BigQuery, Dataflow and Airflow. Develop and maintain test automation frameworks, build regression test strategy and continuous testing process.

Skills:
• Ability to create test strategy balancing manual and automated testing
• 8+ years of experience with designing and implementing test frameworks in cloud
• Technical skills – SQL, Java/Python
• Good communication and collaboration skills across teams and business SMEs
• Should exhibit continuous testing mindset
• Knowledge on Devops and CI/CD process
• Develop automated test cases to be used in performance testing or as part of testing
• Identify and implement performance metrics to be measured
• Collaborate with functional and technical teams to identify test data or create through UI and database
• Generate automation or performance testing reports from execution
• Maintain record of test discrepancies, using designated QA tools
• Provide feedback of test results to development and infrastructure teams for resolution
• Review Business Requirements Documents and Functional and Technical Specifications towards determining test data scope
• Oversee defects from initial identification through post-deployment analysis
• Coordinate with other QA engineers, leadership, system administrators, architects and developers

What is expected of you and others at this level:
• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
• Participates in the development of policies and procedures to achieve specific goals
• Recommends new practices, processes, metrics, or models
• Works on or may lead complex projects of large scope
• Projects may have significant and long-term impact
• Provides solutions which may set precedent
• Apply design thinking mindset
• Independently determines method for completion of new projects
• Receives guidance on overall project objectives
• Acts as a mentor to less experienced colleagues

Cardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,True,True,False
Siemens Energy,Data Engineer (PMK Project),"A Snapshot ofYour Day

A Our team ofdata engineers supports several business teams to get needed data, prepare itproperly for the particular use cases and make it available in an efficientway. You are integrated in the business team and also work on business topics.

One strongexample is the Prescriptive Marketing team. They create forecasts for energymarkets about power prices, demand and other key factors. This is used to helpcustomers to improve their plants and thus their revenues. It is essential alsofor the data engineers to understand the big picture while fulfilling the datarequests.

You discussthe scope of the data and how to make it available, followed by implementingand maintaining data pipelines, normally Python based, with generic setups thatcan be re-used easily to scale up the market models. There is a wide range oftasks, from doing research about data sources up to presenting the data indashboards.

How You’ll Makean Impact
• As a data engineer you willsupport the team by using, building and maintaining ETL tools and pipelines forcollecting, transferring and preparing data for several use case like internalanalytics or consumer facing applications.
• You should be able to deliverrapidly in a reliable manner with the highest quality standards.
• You should be curious,passionate about problem solving, building and self-improving.

WhatYou Bring
• Master’s / bachelor’s degree in computer science or similar with a minimum 5 years of experience in the field of Software Development and Architecture.
• Deliver rapidly in a reliable manner with the highest quality standards
• Curious, passionate about problem solving, building and self-improving
• Experience in Agile development
• Fluent English Skills
• Experience working in and with international teams and stakeholders with different cultures

Technical Skills:
• Highly experienced in relational database setups and data warehouse environments
• Snowflake knowledge is a plus
• Solid experience with building ETL pipelines
• Strong SQL skills (aggregations, windows functions, pivoting etc.)
• Python knowledge
• Solid experience in software development, incl. design patterns is a plus
• Experience with OOP concepts (e. g. Java/C# background) is a plus

Working experience:
• version control systems (Gitlab) and tools like Confluence and Jira
• deployments with CI/CD pipelines
• AWS cloud environments
• Time series data
• integrating and managing structured and unstructured data in cloud based data management systems for example with PostgreSQL or Redshift, Snowflake is a plus
• Tableau dashboards is a plus

About The Team

""Let’s make tomorrowdifferent today"" is our genuine commitment at Siemens Energy to allcustomers and employees on the way to a sustainable future.

Ourteam belongs to the Software Engineering andProduct Development Function within Siemens Energy. Our missionis to grow the digital software business and develop solutions and products forour internal and external customers. This covers from edge data acquisition,data lake and processing up to development of digital twins and apps aroundcustomer assets.

Who is Siemens Energy?

At SiemensEnergy, we are more than just an energy technology company. We meet thegrowing energy demand across 90+ countries while ensuring our climate isprotected. With more than 92,000 dedicated employees, we not only generateelectricity for over 16% of the global community, but we’re also using ourtechnology to help protect people and the environment.

Ourglobal team is committed to making sustainable, reliable, and affordable energya reality by pushing the boundaries of what is possible. We uphold a 150-yearlegacy of innovation that encourages our search for people who will support ourfocus on decarbonization, new technologies, and energy transformation.

Our Commitment to Diversity

Lucky for us, we are not all the same.Through diversity we generate power. We run on inclusion and our combinedcreative energy is fueled by over 130 nationalities. Siemens Energy celebratescharacter – no matter what ethnic background, gender, age, religion, identity,or disability. We energize society, all of society, and we do not discriminatebased on our differences.

Rewards/Benefits
• The opportunity to engage inan exciting environment on challenging projects
• Strong professional supportand working with colleagues around the world
• Professional developmentopportunities within the company
• To be part of a growingfunction with a dynamic, informal, and inspiring working environment in aposition that entails a large responsibility
• Medical benefits
• Remote/Flexible work
• Time off/Paid holidays
• Parental leave
• Continual learning throughthe Learn@Siemens-Energy platform

https://jobs.siemens-energy.com/jobs",Gurugram,True,False,True,True,False,False,False,False,False,True,False,False,True,False,False,True
Snowflake,Data Engineer,"Build the future of data. Join the Snowflake team.

Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Because traditional on-premises and cloud solutions struggle with this, Snowflake developed an innovative product with a new built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.

In addition, Snowflake’s culture was built on the following values that are even more important to us today:
• Put Customers First. We only succeed when our customers succeed
• Integrity Always. Be open, honest, and respectful
• Think Big. Be ambitious and have big goals
• Be Excellent. Quality and excellence count in everything we do
• Get It Done. Results matter!
• Own It
• Make Each Other the Best
• Embrace each others’ Differences

Job Description
• Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data infrastructure that scales across the company
• Drive the design, building, and launching of new data models and data pipelines in production
• Build data expertise and own data quality for allocated areas of ownership
• Align to Product roadmap in building tools for data platform users
• Mature requirement and follow design develop and communicate model using Agile methodology for data ingestion and data tooling.

MINIMUM QUALIFICATION
• Expertise in SQL statements and modeling concepts.
• Must be aware of the cloud environment from data ingestion and modeling perspective.
• Must be strong in python
• Experience with Apache Airflow is highly desirable.
• schema design and dimensional data modeling.
• custom ETL design, implementation and maintenance.
• object-oriented programming languages.
• Understanding of API and connectors is highly desirable
• analyzing data to identify deliverables, gaps and inconsistencies.
• Experience in data warehouse space.

PREFERRED QUALIFICATION :
• BE/BTECH in Computer Science, IT or other technical field.
• Experience with data ingestions and data analytics.
• 4 years experience using Python and SQL, .",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,True
PepsiCo,Data Engineer,"Overview

This role is with the Global business services arm of Media Center of Excellence (CoE) at PepsiCo.

In this role, you will play a key role in shaping the future of media measurement strategy for PepsiCo, esp. with focus on building an understanding role of media as key Growth Driver thru ROI and related analytics.

You will be laying down strong data foundation through implementation of process & technology.

This role is the backbone of media measurement agenda at Pepsico and is responsible for building data systems and pipelines to feed into prescriptive and predictive modeling by establishing and enhancing process around data capture, storage, quality and reliability.

You will work with Media, Data engineering, Data Science and IT teams globally and within AMESA sector to identify best practices in the industry and across all PepsiCo’s brands, providing support to codify and scale best in-class methods that inspire continuous improvement in marketing effectiveness and ROIs.

Responsibilities
• Collect, structure, analyze, organize and maintain RAW data from various data sources needed for creating predictive models in structured databases in order to ensure faster model building
• Partner with PepsiCo functional teams, agencies and third parties to build seamless process for acquiring, tagging, cataloging and managing all media, Nielsen and internal data periodically in structured format as needed for measurement statistical models
• Design, build and codify data structures in efficient way to periodically feed in raw data from various internal and external sources and also manage and house model outputs for quick input to businesses;
• Build data systems and pipelines as per business needs and objectives, in this case prepare data to feed specifically to MMM and media measurement models or any descriptive or prescriptive analysis
• Promote data consistency globally to support common standards and analytics
• Establish periodic data verification processes to ensure data accuracy
• Build new technologies and algorithms to optimize any business process around creation and maintenance of databases/data lakes running of batch processes for data updation

Qualifications
• 3-6 years of experience in the field of data structures, building and managing data lakes
• Hands on experience in building database/Data Management Solutions
• Mandatory experience Python, Data modelling, data pipeline set up and meta data management
• Experience in relational databases as well as unstructured data streams
• Experience with schema design and dimensional data modeling (for ex: using data vault/snowflake/star schema)
• Hands on experience, Airflow (or any other Orchestration tools)
• Hands on experience in AWS (or any other cloud operator)
• Good to have experience on technologies like, DBT
• Good to have experience in data engineering teams in consulting or ecom/telecom sector
• Desirable - Experience optimizing larger applications to increase speed, scalability, and extensibility
• Educational Background- BE/B T ECH/ MS in computer science or related technical field",Peeramcheru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,True,True
HTC Global Services,Data Engineer-GCP,"Greetings from HTC Global Services

We are hiring Data Engineer- GCP

Skills Required:

Data Engineer- GCP

Experience with Big query, Terraform ,Hive

Experience:

3+ Years

Location:

Chennai

Notice:

Looking for candidates who can join within 15 days.

Interested candidates please drop your resume to sunitha.manohar@htcinc.com

Regards

Sunitha",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
HP,Senior Data Engineer,"The Company

HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations and dreams. We apply new thinking and ideas to create simpler, more valuable and trusted experiences with technology, continuously improving the way our customers live and work.

Position background

In the GTM analytics COE our mission is to deliver impact by building machine learning products to optimize pricing and marketing investments and provide guidance to our sales organization.

As a Big Data Engineer, you will be in a unique position to support the development one of our internal assets. You will work together with the project and asset team to understand the end state in which the data must be delivered and you will model the data using Big Data technologies like Spark.

We offer an international experience, collaborative culture, top rate experience in AI and ML and opportunity to create significant real-world impact.

What You Will Do

Create / Maintain ETL pipelines.

Ensure that processes are optimized.

Use Spark to model big volumes of data.

Contribute to the database architecture, design and implementation.

What You Will Need

Bachelor’s in computer engineering, Computer Science, Electrical Engineering, Robotics or a related field

2+ years on a similar role.Ability to work independently under a fast-paced environment, comfortable to deliver results under pressure.

You Have Strong Problem-solving Skills.Agile Experience.

Experience with modern application lifecycle management tools (Git, Visual Studio, Intellij, Code Reviews).

Proficient in at least one of the following languages: Python, PySpark, Scala, Spark, SparkR.

Experience with SQL & NoSQL databases is preferred (PostgreSQL, MongoDB, Elastic Search).

Experience in working with DataIku DSS software.

Strong analytical skills with demonstrated problem solving ability.

Who We Are

At HP, we believe in the power of ideas. We use ideas to put technology to work for everyone. And we believe that ideas thrive best in a culture of teamwork. That is why everyone – at every level in every function, is encouraged to think big, have original ideas and express and share them. We trust anything can be achieved if you really believe in it, and we will invest in your ideas to change lives and the way people work. This vision is what sets us apart as a company. At HP, we work across borders and without limits. Global virtual teams share resources, pool their big ideas to solve our biggest business opportunities. Everyone is valued for the unique skills, experiences and perspective they bring. That’s how we work at HP. And this is how ideas and people grow.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
DataTheta,Azure Data Engineer,"Experience Required: 5-10 Years

Location: Noida/Chennai

Azure Data Engineer | Job Description
• Dev / Architect
• 2+ years of experience in Azure cloud data stack such as Synapse/DW, Azure SQL DB, Azure Blob Storage
• 1+ years of experience in Logic Apps
• 3+ years of experience in Python
• 5+ years of experience in SQL
• 3+ years of experience in Databricks
• 2+ years of experience in Azure/AWS Lambda Functions
• 2+ years of experience in Microservices (REST) architecture
• Ability to project manage and work within an agile, flexible environment.
• Performs peer reviews for other data engineers’ work.
• Ensuring adherence to programming and documentation policies, software development, testing and release.
• Develop modeling, design, and coding practices.
• Experience with Lean / Agile development methodologies
• Positive attitude with great collaboration and communication skills",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Tech Mahindra,GCP Data Engineer,"Job Role - GCP Data Engineer
• Looking only GCP Data Engineers. (GCP Certification is not mandatory)
• Experience should be 4-8 Years Max.
• Candidates should be aware about BigQuery, Data Flow, Composures.
• GCP Services, SQL, Migration Process
• Migration Tools ( Plate spin, Cloud Physics, Stratozone)
• Work Location Pune/Bangalore/Noida/Chennai",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False,False
MOURI Tech,Sr. Data Engineer,"Hi Folks,

Greetings from Mouritech!!

We are hiring for Sr. Data Engineer

Mandatory Skill: Data Engineer, Python, SQL, DWH & GCP

Location: Gurgaon (Hybrid)

Exp: 4+ Yrs to 12 Yrs

Notice Period: Immediate to 30 Days Serving.

If interested pls share your profile on below mail id

surabhim.in@mouritech.com.",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
NAB,Senior Data Engineer [T500-7047],"Essential Skills:
• Advanced SQL skills (or equivalent database querying language), Database SQL skills (MySQL preferably), Able to write complex queries (subquery, window function, CTE, removing duplication), Able to analyse query bottleneck by “Explain” command.
• Construct RMDB database-Database modelling skills, Able to operate DDL (stored procedure, indexing, trigger, table, materialised view, view), Understand database modelling (normalisation)
• Develop, maintain and Implement Power BI dashboards,
• Understand batch job process, able to modify it, and solve batch job issue- Python (ODBC DB connection, Pandas, XML handling), PowerShell (General PS command), ETL experience.
• Extract meaningful insights out of the data.
• AWS experience -AWS EC2, RDS monitoring, parameter store
• Aware of GitHub skills- Merge/pull request/resolving conflicts, Pull/push.
• Translate business information requirements into a meaningful set of SQL queries.
• Develop and maintain key reporting metrics that drive business performance.
• Creating Views, Stored Procedures and Materialised Views in MySQL.
• Ability to parse XML tags into SQL human readable tables.

Job Requirements:
• Advanced SQL skills (or equivalent database querying language)
• 6+ years’ experience working in a similar role.
• 6+ years of experience working with BI tools such as Power BI
• Proficient in Python language
• Data Science enthusiast
• Understand Jenkin pipeline, Jira & Confluence
• Advanced MS Office skills, including Excel, PowerPoint, Access etc.
• Ability to deal with ambiguity, solve complex problems, and navigate large, global organisations.
• Self-motivated, assertive, analytical, and comfortable working in a fast-paced environment
• Stakeholder management

Department : Group Security

Sub Department : Cyber Security

Job code : AAZA01",Gurugram,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
SID Global Solutions,Senior Data Engineer(8+yrs),"Skillset: SQL, AWS Stack, Python, Redshift/MYSQL

Roles & Responsibilities:

Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift

Hands on Python programming.

Working knowledge on S3

AWS certification is a nice to have

Must be punctual and follow deadlines and deliver on time.

Must be able to clearly communicate with stakeholders and team",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
InVisions Ltd.,Data Engineer,"Hello people,

We are happy to assist the product and service company “Saras Analytics” in welcoming new Data Engineers to the team.

Now, a little bit about the company and the product:

Saras Analytics is a rapidly growing data management and analytics advisory firm with offices in Austin, USA and Hyderabad, India. We are a group of engineers and analysts focused on accelerating growth for e-commerce and digital businesses by setting up or transforming their data (analytics & BI) ecosystems and providing further analytics services. We are laser focused on providing the best ROI for our clients and leave no stone unturned in our quest to provide the best results for our customers.

We are an employee-centric organization and, to meet the ever-growing demand for our services, are looking for individuals who share our passion to make a difference and would be great additions to our analytics and growth consulting practice.

How Saras Analytics describes your role:

As a Data Engineer at Saras Analytics, you will be responsible for building and maintaining large-scale data pipelines as well as create and data pipelines that deal with large volumes of data.

You will deal with:
• Database programming using multiple flavors of SQL and Python.
• Understand and translate data, analytic requirements and functional needs into technical requirements.
• Build and maintain data pipelines to support large scale data management projects.
• Ensure alignment with data strategy and standards of data processing.
• Deploy scalable data pipelines for analytical needs.
• Big Data ecosystem - on-prem (Hortonworks/MapR) or Cloud (Dataproc/EMR/HDInsight).
• Work with Hadoop, Pig, SQL, Hive, Sqoop and SparkSQL.
• Experience in any orchestration/workflow tool such as Airflow/Oozie for scheduling pipelines.
• Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow.
• Understand and execute IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, writing optimized queries in Spark.
• Hands-on experience in using Spark Streaming, Kafka and Hbase.

What you bring with you:
• 4 to 6 years of experience in building data processing applications using Hadoop, Spark and NoSQL DB and Hadoop streaming. Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow is a plus.
• Expertise in data structures, distributed computing, manipulating and analyzing complex high-volume data from variety of internal and external sources.
• Experience in building structured and unstructured data pipelines.
• Proficient in programming language such as Python/Scala.
• Good understanding of data analysis techniques.
• Solid hands-on working knowledge of SQL and scripting.
• Good understanding of in relational/dimensional modelling and ETL concepts.
• Understanding of any reporting tools such as Looker, Tableau, Qlikview or PowerBI.
• Degree: Bachelor of Engineering - BE, Bachelor of Science - BS, Master of Engineering - MEng, Master of Science – MS or equivalent work experience.

Eligibility:
• Significant technical academic course work or equivalent work experience.
• Excellent communication and interpersonal skills.
• Willingness to work under labor contract, B2B contract is an option too.
• Dedicate 40 hours/weekly to Saras Analytics.

Let’s connect and check if we match!

You can state your interest by sending your CV and we will get in touch with the short-listed candidates.

We treat your personal information with respect and confidentiality, guaranteed and protected by the professional ethics, the Bulgarian and European law.

“InVisions” agency license № 2420 from 19.12.2017.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,False
LTIMindtree,Specialist Data Engineer- AZURE-ADF/ADB,"• Job Title- Specialist Data Engineer
• Primary skill- Azure+Databricks (ADF+ADB+Pyspark)
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 12 Years
• Notice Period- 0 to 30 Days
• Job Description-

Primary Skills

• 5+ years of experience in Python and Databricks.

• Deep understanding of data modelling techniques for analytical data (i.e. facts, dimensions, measures)

• Experience developing and managing reporting solutions, dashboards, etc. Design and architecture experience in data transformation.

• Should have experience with data platforms and in data transformation and extraction: some combination of ETL/ELT, table and database design, query design, performance analysis and optimization

• Worked as a data engineer or related specialty (Software Engineer/Developer, BI Engineer/Developer, DBA)

Secondary Skills

• Experience in Azure Data Factory and Azure Storage

• Hands on experience with handling of large amount of data using SQL, Azure Data Factory, Spark, Azure Cloud architecture

• Knowledge of cloud architecture and data solutions

• Proficiency in Snowflake would be added advantage.

• Excellent written and verbal communication skills",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
Rently,Data Engineer,"Must have:
• Overall experience of 4+ years
• Experience in AWS Cloud services & solutions
• Experience working with enterprise data warehouse
• Experience as an ETL/ELT Developer using various ETL/ELT tools
• Experience in SQL/NoSQL/DWH databases across SQL DB, Managed instance & Data warehouse
• Experience in AWS platform services such as S3, EMR, RedShift, Glue, Kinesis, OpenSearch, Athena, QuickSight
• Working on SnowFlake and pipeline tools like Fivetran or Matillion.
• Experience in Apache Spark, Databricks
• Experience in creating data structures optimized for storage and various query patterns like Parquet
• Experience in building secured visualization reports and dashboards with access controls
• Experience in working in an Agile SDLC methodology
• Experience in DevOps Services using Git Repos, deployment artifacts and release packages for Test & production environment
• Experience in building end-end scalable data solutions, from sourcing raw data, and transforming data to producing analytics reports
• Should have experience in developing a complete DWH ETL lifecycle
• Experience in Data Analysis, Data Modelling and Data Mart design
• Should have experience in developing ETL processes - ETL control tables, error logging, auditing, data quality, etc. - using ETL tools.
• Experience in Data Integrator Scripts, workflows, Dataflow, Data stores, Transforms, and Functions.
• Should have worked on at least 2 end-to-end implementations
• Worked on Change Data Capture on both SOURCE and TARGET levels and a good understanding of Slowly changing Dimension (SCD)
• Should be able to implement reusability, parameterization, workflow design
• Should have experience in interacting with customers in understanding business requirement documents and translating them into ETL specifications and Low/High-level design documents
• Strong database development skills like complex SQL queries, complex stored procedures
• Able to work in Agile Framework Should have exposure to Scrum meetings.

Good to have:
• Exposure to other ETL/ELT, DWT technologies
• Hands-on with Data visualization tools like Power BI, Tableau, Qlik, QuickSight etc.
• Exposure to Python on ETL and Data Visualization libraries

Additional Skills:
• Good Communication Skills.
• Able to deliver independently.
• Team player.

Professional Commitment:

Being a product based company we heavily invest in developing functional/ technology/ leadership skill sets in our team members. So candidates who are willing to commit to a minimum of 2 years need to apply.",Coimbatore,True,False,True,False,False,False,False,False,True,True,False,True,True,False,False,True
Embibe,Data Engineer,"Requirements
• Should have knowledge in Coding: Preferred Java.
• Good to have - ( Python / Scala).
• Should have Knowledge in Technologies: Spark, spark streaming, scala spark/py spark.
• Good to have Knowledge of Messaging buses like Apache Kafka/ Rabbit MQ.
• Good to have Knowledge of NoSQL databases like - MongoDB, ElasticSearch, Cassandra, Hive, Impala, ADX, Synapse, Redshift, Athena, etc.
• Should have Knowledge in building Microservices with Spring boot/ Fast-Api.",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Concentrix,Data Engineer Big Data,"Job Title:

Data Engineer Big Data

Job Description

Data Engineer Big Data

Keywords: RDBMS SQL & Spark/Hive SQL, Performance tuning, Modeling Design

Job Description

Develops and maintains scalable data pipelines

Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.

Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.

Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.

Designs data integrations and data quality framework.

Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.

Qualification:

Bachelor's Degree in Computer Science or related field

3+ years of work experience

Strong experience in SQL ( include complex SQL query , SQL performance tuning , Index , Lock )

Experience with schema design and dimensional data modeling

Experience with Hive SQL , Spark(Spark SQL, DataFrame)

Experience with near-realtime data warehouse (10-30 mins level)

Experience with data quality check

Experience in Java or Python or Shell Script

#CSS

Location:

India Bangalore - Divyashree

Language Requirements:

Time Type:

Full time

If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents

R1357932",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
CirrusLabs,AWS Data Engineer / Data Engineer / Lead AWS Data Engineer,"Job Role: Aws Data Engineer

Location: Bangalore / Hyderabad

Type: Fulltime

JOB DESCRIPTION

Data Engineer/Operational Support with Snowflake

Must-Have:
• 7+ years of experience in an Oracle/Informatica environment with knowledge of views, packages, stored procedures, functions, constraints, cursors, indexes, and table partitions.
• 7+ years of experience with an ETL tool such as SSIS, Azure Data Factory, or AWS Glue
• Strong background in a data warehouse, data management, and data analytics
• Monitor ETL production batch schedules to meet predefined SLAs.
• Resolve functional and system errors as identified by Business Partners
• Coordinate activity between Business Units and EIS to drive open action items to closure.
• Work with other technical teams to resolve infrastructure-related problems.
• Maintain a good relationship with other technology teams within the client enterprise.
• Generate, Control, and Resolve incident tickets relating to Production batch and Data availability issues.
• Serve as senior contact for production support issues and escalations.
• Enterprise L3 support to resolve production support issues in a timely manner.
• Candidate is expected to exude a take-charge attitude toward problems and thrive for excellence. This is a hands-on, delivery-focused role.
• Attempt to isolate, reproduce, and resolve problems using available systems and tools, and investigate potential workarounds for verified defects.
• Participate in the creation of Knowledge Base articles, solutions, and other related support collateral.
• To interface with Subject Matter Experts, where the problem cannot be resolved at a frontline support level.
• Good to have:
• Excellent written and verbal communication skills
• Detail-oriented; Analytical with problem-solving abilities",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Tata Technologies,Data Engineer,"Job Title : Data Engineer

Job Location : Thane(Mumbai)

Domain Knowledge:

Should be capable of carrying out the following operations on the data with any application.

• Familiarity with data loading tools like Flume, Sqoop.

• Analytical and problem-solving skills applicable to Big Data domain

• Proven understanding with Hadoop, PySpark, Hive, Hadoop

• Good aptitude in multi-threading and concurrency concepts",Thane,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Trademo,Data Engineer,"Position : Data engineer - Full Time and Intern

Role: Python/ Data scraping with Algo and Data structures with Automation

Experience: 0-1 years

Location: Gurgaon (Work from office)

About Trademo

Trademo is a Global Supply Chain Intelligence SaaS Company, headquartered in Palo-Alto, CA. Trademo collects public and private data on global trade transactions, sanctioned parties, trade tariffs, ESG and other events using its proprietary algorithms. Trademo analyzes and performs advanced data processing on billions of data points using technologies like Graph databases, NLP and Machine Learning to build end-to-end visibility on Global Supply Chains. Trademo's vision is to build a single truth on global supply chains to help large and small businesses - discover new commerce opportunities, ensure compliance with trade regulations and build operational resilience. Trademo last closed its $12.5 mn Seed Round from marquee Silicon Valley VCs.

Trademo has been founded by Shalabh Singhal who is a third-time tech entrepreneur. Shalabh last co-founded ZipLoan. ZipLoan is a leading fintech lending startup in India. He earlier founded Credence, a Data-driven Digital Marketing, CRM Product and Sales Solutions company. Shalabh is an Alumni of Goldman Sachs, IIT BHU, CFA Institute USA and Stanford GSB SEED. Trademo has recently closed a $12.5 mn Seed round from some of the marquee investors in Silicon Valley.

Website

https://www.trademo.com

Location

Gurgaon (Work from office)

Technical Skills Required
• Python 3.6+ version, Pandas
• Scraping → Selenium, Beautiful Soap
• Knowledge NOSQL/MYSQL Database
• Knowledge how to tackle the problems with optimal Solution
• Individual contributor role with eagerness to learn new technologies - Elasticsearch , BigData etc.
• Knowledge of Basics DS and algorithms",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Pracemo Global Solutions,Data Engineer,"We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Responsibilities
• Analyze and organize raw data
• Build data systems and pipelines
• Evaluate business needs and objectives
• Interpret trends and patterns
• Conduct complex data analysis and report on results
• Prepare data for prescriptive and predictive modeling
• Build algorithms and prototypes
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Requirements And Skills
• Previous experience as a data engineer or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills
• Degree in Computer Science, IT, or similar field; a Master’s is a plus
• Data engineering certification (e.g IBM Certified Data Engineer) is a plus
• Self-motivated with a results-driven approach
• Aptitude in delivering attractive presentations
• High school degree
Skills: data warehousing,etl,sql,python,java,hadoop,hive,spark,nosql databases,cloud computing,aws,azure,gcp,data modeling,data mining,data visualization,communication,project management,data lake,data quality,data architecture",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Thompsons HR Consulting LLP,Lead Data Engineer,"We are looking for Lead Data Engineer

with Strong experience in Python, Development, Business Intelligence (BI tools), AWS, Mysql

Experience: 10+ years

It is a Remote opportunity.

If interested, please share your resume at deepika.ashok@thompsonshr.com",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Anonymous,Data Engineer - Partime / Freelance,"Required skills: Pyspark, AWS-cloud, Hive
Good to have: streamsets, CICD
Experience: 2 - 5yr
Timing : 8pm to 12am on weekdays",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
New Era India,Data Engineer/Sr. Data Engineer/Lead Data Engineer - Data Axle,"Data Engineer / Sr. Data Engineer / Lead Data Engineer (Pune)

About Data Axle

Data Axle Inc. has been an industry leader in data, marketing solutions, sales and research for over 45 years in the USA. Data Axle has set up a strategic global center of excellence in Pune. This center delivers mission critical data services to its global customers powered by its proprietary cloud-based technology platform and by leveraging proprietary business & consumer databases. Data Axle is headquartered in Dallas, TX, USA.

Roles And Responsibilities
• Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
• Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
• Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency.
• Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
• Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning.
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

Basic Qualifications
• 3 to 12 years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
• Demonstrated strength in data modeling, ETL development, and data warehousing.
• Experience using big data processing technology using Spark.
• Knowledge of data management fundamentals and data storage principles
• Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, Power BI etc.)

Preferred Qualifications
• Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
• Experience working with AWS big data technologies (Redshift, S3, EMR, Spark)
• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience working with distributed systems as it pertains to data storage and computing.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.",Pune,False,False,True,False,False,False,False,True,True,True,False,False,True,False,False,False
Quadratyx,Lead Data Engineer,"About Quadratyx

We are a product-centric insight & automation services company globally. We help the world’s organizations make better & faster decisions using the power of insight & intelligent automation. We build and operationalize their next-gen strategy, through Big Data, Artificial Intelligence, Machine Learning, Unstructured Data Processing and Advanced Analytics. Quadratyx can boast of more extensive experience in data sciences & analytics than most other companies in India. We firmly believe in Excellence Everywhere.

Purpose of the Job/ Role:

As a Lead Data Engineer, your work is a combination of hands-on contribution, customer engagement and technical team management. Overall, you’ll design, architect, deploy and maintain big data solutions.

Key Requisites:

• Expertise in Data structures and algorithms.

• Technical management across the full life cycle of big data (Hadoop) projects from requirement gathering and analysis to platform selection, design of the architecture and deployment.

• Scaling of cloud-based infrastructure.

• Collaborating with business consultants, data scientists, engineers and developers to develop data solutions.

• Leading and mentoring a team of data engineers.

• Hands-on experience on test-driven development (TDD).

• Expertise in No SQL like Mongo, Cassandra etc., preferred is Mongo and strong knowledge of relational database.

• Good knowledge of Kafka and Spark Streaming internal architecture.

• Good knowledge of any Application Servers.

• Extensive knowledge on big data platforms like Hadoop; Hortonworks etc.

• Knowledge of data ingestion and integration on cloud services such as AWS; Google Cloud; Azure etc.

Skills/ Competencies Required

Technical Skills

• Strong expertise (9 or more out of 10) in at least one modern programming language, like Python, Java.

• Clear end-to-end experience in designing, programming, implementing large software systems.

• Passion and analytical abilities to solve complex problems.

Soft Skills

• Always speaking your mind freely.

• Communicating ideas clearly in talking and writing, integrity to never copy or plagiarize intellectual property of others.

• Exercising discretion and independent judgment where needed in performing duties; not needing micro-management, maintaining high professional standards.

Academic Qualifications & Experience Required

Required Educational Qualification & Relevant Experience

• Bachelor’s or Master’s in Computer Science, Computer Engineering, or related discipline from a well-known institute.

• Minimum 7 - 10 years of work experience as a developer in an IT organization (preferably Analytics /

Big Data/ Data Science / AI background.

Quadratyx is an equal opportunity employer - we will never differentiate candidates based on religion, caste, gender, language, disabilities or ethnic group.",Hyderabad,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Persistent Systems,Data Engineer (Immediate joiner),"About Persistent

We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above.

We are experiencing tremendous growth, with $701.1 million in trailing 12-month revenue, representing 29.8% year-over-year growth. Along with that growth, we onboarded over 4,500 new employees in the past year, bringing our total employee count to over 16,500 people located in 18 countries across the globe.

At Persistent, our values are more than a list of ideals to improve our corporate image. We’re dedicated to building an inclusive culture that reflects what’s important to our employees and is based on what they value. As a result, 95% of our employees approve of the CEO and 83% recommend working at Persistent to a friend.

For more details please login to www.persistent.com

About Position
• 4+ years of strong technology experience in the field of transactional data and analytics systems
• Lead client conversations and data discovery sessions
• Should understand and be able to command architecture design for transactional and analytics systems.
• Strong SQL skills
• Hands on experience in building end to end data / orchestration pipelines using Python.
• Cloud Experience- Should have experience with any cloud data products (AWS, GCP, Azure)
• Experience in Agile Methodologies
• Familiarity with source repositories (Git, BitBucket etc.)
• Excellent communication skills",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Niftel Resources,Senior Data Engineer,"Responsibilities:

 Design and build reusable components, frameworks and libraries at scale to support analytics

products

 Design and implement product features in collaboration with business and Technology

stakeholders

 Anticipate, identify and solve issues concerning data management to improve data quality

 Clean, prepare and optimize data at scale for ingestion and consumption

 Drive the implementation of new data management projects and re-structure of the current data

architecture

 Implement complex automated workflows and routines using workflow scheduling tools

 Build continuous integration, test-driven development and production deployment frameworks

 Drive collaborative reviews of design, code, test plans and dataset implementation performed by

other data engineers in support of maintaining data engineering standards

 Analyze and profile data for the purpose of designing scalable solutions

 Troubleshoot complex data issues and perform root cause analysis to proactively resolve product

and operational issues

 Mentor and develop other data engineers in adopting best practices

Qualifications:

Primary skillset:

 Experience working with distributed technology tools for developing Batch and

Streaming pipelines using SQL, Spark, Python [3+ years], Airflow [2+ years], Scala [1+

years].

 Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc.

 Able to quickly pick up new programming languages, technologies, and frameworks.

 Strong skills building positive relationships across Product and Engineering.

 Able to influence and communicate effectively, both verbally and written, with team members and

business stakeholders

 Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed

Spark jobs, build Docker images, etc.

 Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture

Good to have:

 Experience working with Data platforms, including EMR, Airflow, Databricks (Data Engineering &

Delta Lake components, and Lakehouse Medallion architecture), etc.

 Experience working in Agile and Scrum development process

 Experience in EMR/ EC2, Databricks etc.

 Experience working with Data warehousing tools, including SQL database, Presto, and

Snowflake

 Experience architecting data product in Streaming, Server less and Microservices Architecture

and platform.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,True,True
"Giant Eagle, Inc.",Senior Data Engineer,"Job Summary

As a Sr Data Engineer on the Marketing Data Platforms team, you will be working on a team to bring customer-centric personalization to life. In this role, you will be empowered to develop data solutions in support of analytics, data science, and business partners to understand capability requirements and develop data solutions based on priorities. This leading technical and architecture role will collaborate with product managers, architects, technology teams, analysts, marketing operations specialists, and monetization business partners to understand capabilities and that will be brought to life for Giant Eagle’s 4M+ customers. The ideal candidate will have experience within multiple technology platforms (e. g. GCP, Engagement Platforms, Customer Data Platform, Ad-Tech, etc.) while providing the vision and design for integrating customer data. Additional key skills and qualifications below

Job Description
• Primary Job Responsibilities:
• 5+ years of relevant technical experience working with various data engineering methodologies such as data integration and data pipelines (ETL/ELT) to activate against data at scale.
• 3+ years of experience of data modeling for analytic projects activities that include design, curation, and management of large datasets
• 3+ years of experience adeveloping on big data technologies with Spark and Hive, preferably leveraging such as DataBricks, Juypter notebooks, or GCP, AWS, and Azure equivalent technology.
• 2+ years of experience data solution design for data engineering pipelines
• Strong Experience building event driven systems using cloud technology: storage, Pub/Sub, cloud functions, API’s, and DataProc
• Expertise with databases experience such as BigQuery, Snowflake, and Synapse designing schema and dimensional data modeling
• Experience leveraging RESTful web services to collect and publish data.
• Experience in software engineering development and testing life cycles using but not limited to Python, R, Linux, Java, JavaScript, Lambda, and SQL programming
• Bachelor's degree in Computer Science, Mathematics, or other technical field or equivalent work experience. Advanced degree a plus
• Experience with Retail Media Networks and Ad Tech preferred

Role Requirements:
• Architect, develop and implement end-to-end complex data projects and technical solutions through translating business requirements into technical solutions and data-flow architectures.
• Architect, build and automate data pipelines that clean, transform, and aggregate unorganized data into data sources that are ready for analysis.
• Use expertise to apply various analytic methods to discover and interpret information about customer behavior from multiple data sources to implement analytics solutions
• Use expertise in database design to implement, operate stable and scalable dataflows from multiple marketing platforms into a cloud data lake for Ad Tech
• Experience building data visualization tools Tableau, PowerBI, and Looker with data modeling and Looker ML preferred
• Design, implement and deploy data applications and mechanisms using big data technology
• Provides subject matter expertise for multiple projects concurrently through all phases of the development lifecycle.
• Develop, enhance, govern, and administer for data platform to: collect data, transform, enrich, unify, segment, and integrate data
• Strong adherence data ethics rules around PII data sets
• Work collaboratively with IT teams, Performance Marketing team, and business leaders to ensure actionable is provided key stakeholders
• Research and analyze customer behavior data to improve customer experience
• Experience with agile or other rapid application development methods a plus
• Retail industry experience a plus

About The Company

Since our founding in 1931, Giant Eagle, Inc. has evolved into one of the top 40 largest private corporations in the U. S. and one of the country’s largest food retailers and distributors. With more than 37,000 Team Members and $9.7 billion in revenue, we are committed to investing in people, technology, and data to elevate our customer’s experience across multiple touchpoints. It helps us follow on our commitment to serving others and improving our communities.

About Giant Eagle Bangalore

The Giant Eagle GCC in Bangalore is our global capability center. Our team of more than 370 members at the GCC enables us to expand internal capabilities in the areas such as data analytics, merchandising and eCommerce, quality engineering, and automation to generate insights for faster decision-making and helping us accelerate our business strategy. Our team in India plays a pivotal role in helping the company transition to new ways of working by redefining the food and grocery shopping experience for over 4.6 million customers.

About Us

At Giant Eagle Inc., we’re more than just food, fuel and convenience. We’re one giant family of diverse and talented Team Members. Our people are the heart and soul of our company. It’s why we strive to create a nurturing environment that offers countless career opportunities to grow. Deep caring and solid family values are what makes us one of the top work places for jobs in the Greater Pittsburgh, Cleveland, Columbus and Indianapolis Areas. From our Warehouses to our GetGo’s, our grocery Stores through our Corporate home office, we are working together to put food on shoppers' tables and smiles on their faces. We’re always searching for the best Team Members to welcome to our family. We invite you to join our Giant Eagle family. Come start a lasting career with us.",,True,False,True,True,False,False,True,False,False,True,False,False,False,True,False,True
TMRW House of Brands,Data Engineer-III,"Responsibilties:

Create, implement, and operate the strategy for robust and scalable data pipelines for business intelligence and machine learning.

Develop and maintain core data framework and key infrastructures

Create and support the ETL pipeline to get the data flowing correctly from the existing and new sources to our data warehouse.

Data Warehouse design and data modeling for efficient and cost-effective reporting

Collaborate with data analysts, data scientists, and other data consumers within the business to manage the data warehouse table structure and optimize it for reporting.

Constantly striving to improve software development process and team productivity

Define and implement Data Governance processes related to data discovery, lineage, access control, and quality assurance

Perform code reviews and QA data imported by various processes

Qualifications

6-10 years of experience.

At least 3+ years of experience in data engineering and data infrastructure space on any of the big data technologies: Hive, Spark, Pyspark(Batch and Streaming), Airflow, and Delta Lake.

Experience in product-based companies or startups.

Strong understanding of data warehousing concepts and the data ecosystem.

Strong Design/Architecture experience architecting, developing, and maintaining solutions in AWS.

Experience building data pipelines and managing the pipelines after they’re deployed.

Experience with building data pipelines from business applications using APIs.

Previous experience in Databricks is a big plus.

Understanding of Dev Ops would be preferable though not a must

Working knowledge of BI Tools like Metabase, and Power BI is plus

Experience in architecting systems for data access is a major plus.",Bengaluru,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False
Impetus,GCP Data Engineer,"Qualification
• The candidate should have extensive production experience (3-5 Years ) in GCP, Other cloud experience would be a strong bonus.
• Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
• Exposure to enterprise application development is a must

Role
• 6-10 years of IT experience range is preferred.
• Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.
• Strong experience in Big Data technologies – Hadoop, Sqoop, Hive and Spark including DevOPs.
• Good hands on expertise on either Python or Java programming.
• Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
• Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
• Ability to drive the deployment of the customers’ workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
• Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
• Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
• Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
• Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.",इन्दौर,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Newell Brands,Cloud Data Engineer,"Job Title: Cloud Data Engineer

Report To: Sr. Manager, Data Engineering

Job Location: Guindy, Chennai, India

Job Duties
• Participates in the full lifecycle of cloud data architecture (Preferably Azure cloud) from gathering, understanding end-user analytics and reporting needs.
• Migrate On-Prem applications and build CI/CD pipeline in Cloud platform.
• Design, develop, test, and implement on Cloud platform (Ingestion, Transformation and export pipelines that are reliable and performant) .
• Ensures best practices are followed and business objectives are achieved by focusing on process improvements.
• Quickly adapt by learning and recommending new technologies and trends.
• Develop and Test Data engineering related activities on Cloud Data Platform.
• Work with dynamic tools within a BI/reporting environment.

Job Requirements
• B.E/B.Tech, M.Sc/MCA.
• 5+ years experience in Rapid development environment, preferably within an analytics environment.
• 3+ years experience with Cloud experience (Preferably Azure cloud but not mandatory).
• DB : T-SQL, SQL Scripts, Queries, Stored Procedures, Functions and Triggers
• Language : Python / C# or Scala
• Cloud: Azure / AWS / Google cloud
• Frameworks: Cloud ETL/ELT framework

Preferred
• Azure Data Factory, Azure Synapse Analytics, Azure SQL, Azure Data lakes, Azure Data bricks, Airflow and Power BI
• Data warehousing principles and frameworks.
• Knowledge in Cloud DevOps and CI/CD pipelines would be an added advantage.

Newell Brands (NASDAQ: NWL) is a leading global consumer goods company with a strong portfolio of well-known brands, including Rubbermaid, FoodSaver, Calphalon, Sistema, Sharpie, Paper Mate, Dymo, EXPO, Elmer's, Yankee Candle, Graco, NUK, Rubbermaid Commercial Products, Spontex, Coleman, Campingaz, Oster, Sunbeam and Mr. Coffee. Newell Brands' beloved, planet friendly brands enhance and brighten consumers lives at home and outside by creating moments of joy, building confidence and providing peace of mind.",Chennai,True,False,True,False,False,False,False,False,True,False,False,False,False,False,True,False
Zupee,Lead Data Engineer,"About Zupee

Zupee is India’s fastest growing Technology backed Behavioral Science company. We are innovating Skill-Based Gaming with a mission to become the most trusted and responsible entertainment company in the world. We have been constantly focusing on innovation of indigenous games to entertain the mass.

Our strategy is to invest in our people & user experience to drive profitable growth and become the market leader in our space. We have been experiencing phenomenal growth since inception and running profitable at EBT level since Q3, 2020. We have closed Series B funding at $102 million, at a valuation $600 million.

The company also announced a partnership with Reliance Jio Platforms, post which Zupee games will distribute its content across all customers using Jio phones. The partnership now gives Zupee the biggest reach of all gaming companies in India, transforming it from a fast-growing startup to a firm contender for the biggest gaming studio in India.

About The Job

Lead Data Engineer

We are looking for someone to develop the next generation of our Data platform

collaborating across functions like product, marketing design, growth, strategy, customer

experience and technology.

Core Responsibilities

●Understand, implement and automate ETL and data pipelines with up-to-date

industry standards

●Hands-on involvement in the design, development and implementation of optimal and

scalable AWS services

What are we looking for?

●S/he must have experience in Python

●S/he must have experience in Big Data – Spark, Hadoop, Hive, HBase and Presto

●S/he must have experience in Data Warehousing

●S/he must have experience in building reliable and scalable ETL pipelines

Qualifications and Skills

●6-12 years of professional experience in data engineering profile

●BS or MS in Computer Science or similar Engineering stream

●Hands-on experience in data warehousing tools

●Knowledge of distributed systems such as Hadoop, Hive, Spark and Kafka etc.

●Experience with AWS services (EC2, RDS, S3, Athena, data pipeline/glue, lambda, dynamodb etc.
•",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
UST Product Engineering,Data Engineer,"Job Description :

- 4-8 Years experience in data warehousing , ETL processes, and data analytics.

- Good experience in developing, maintaining, and testing infrastructures for data generation, verification and transformation.

- Good understanding database concepts (SQL, Cloud DBs)

- Strong SQL query, profiling and troubleshooting skills

- Good understanding AWS Data related concepts like big data, big query etc.

- Basic understanding AWS (or supported) ETL tools - Glue, Airflow etc etc. would be an added advantage

- Good Understanding of python programming

- Basic understanding of programming language like C# or similar

- Basic knowledge of working in scrum/agile teams and tools like JIRA, confluence etc.",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.

Apply for this job",Mumbai,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
ThousandEyes,Cloud Application and Data Engineer,"Cloud Application and Data Engineer

Who We Are

The name ThousandEyes was born from two big ideas: the power to see what’s not ordinarily possible, and the ability to collect intelligence from vantage points as diverse and global as the Internet. As organizations depend on cloud services, the Internet has become their defacto network connecting cloud applications to users. Our Internet and cloud intelligence platform is like a ‘Google maps of the Internet’, providing the only collectively powered view of digital experiences end-to-end. We enable our customers made up of the world’s largest and fastest-growing brands, to identify problems before they impact revenue, brand reputation, or employee productivity.

In August 2020, Cisco Systems completed the acquisition of ThousandEyes, which now forms the ThousandEyes Business Unit within Cisco’s Network Services Business Group,and is a foundational component of Cisco’s growing Observability business.About The Team

Digital experiences rely on a vast ecosystem of ISPs, cloud providers, SaaS applications, individual configurations, unique devices, and many other external services that are critically dependent on the Internet. Trying to identify the root cause of a problem or a deviation from normal is like finding a needle in a haystack. This leads to long downtimes and poor customer or employee experience.

The AI Analytics team at ThousandEyes is leveraging machine learning at scale, while working across several different business units, to our help customers answer tough questions like:
• What is normal in my network and how do I catch deviations from this normal?
• How do I understand the root cause of a problem in my network or application stack?
• How do I ensure that devices joining my network are who they say they are?
• How do I know when my networking gear is about to break?

The goal of the AI Analytics Team is to leverage different machine learning techniques to deliver actionable insights for our customers to solve real world problems in their complex environments.

What You’ll Do

You will be part of our data and platform team. A worldwide distributed team responsible for data collection, ingestion, processing, and quality. You will play an important role in helping to deliver new ML powered features to our customers as well as monitoring and improving the existing features for performance and quality. You will work in the AI Cloud hosted on AWS with Python, Go, Spark, Hive, Open Search, and other cutting-edge technologies.

Responsibilities
• Collaborating closely with ML engineer to bring new features to production.
• Create and maintain an optimal data pipeline architecture.
• Contribute and operate data quality tooling.
• Monitor and optimize compute and query performances.
• Troubleshoot and debug issues across our applications and services.

Who you are

Agile, pragmatic and hardworking. You also love to interact with data scientists, machine learning engineers and software engineers to develop pipelines that scale seamlessly at huge volumes of data. You love technology, innovation and building products at scale.

You hold a degree in computer science, or a related field and you can demonstrate a consistent track record in the following areas:
• At least 4 years of software development experience.
• 2 years of experience building and developing data-intensive systems at industrial scale.
• Dimensional data modeling and schema design for both SQL and NoSQL databases.
• Prior exposure to data science, machine learning or statistics is a plus.
• Previous experience developing applications running on a public cloud infrastructure is a plus.
• Strong Communication and documentation skills in English.
• Strong sense of ownership, drive, attention to detail and ability to work in a distributed team.

We are looking for candidates based in Bangalore to work hybrid

Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.

Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do –you can’t put us in a box! But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. So, you have colourful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us.

We recognize that diverse teams make the strongest teams, and we encourage people from all backgrounds to apply.

Cisco COVID-19 Vaccination Requirements

The health and safety of Cisco's employees, customers, and partners is a top priority. Our goal is to protect and mitigate the spread of COVID-19 infection for strong business resiliency during the pandemic. Therefore, Cisco may require new hires to be fully vaccinated against COVID-19 if the role requires business-related travel, meeting with customers/partners (including visiting third-party sites on behalf of Cisco), attending trade events, and Cisco office entry, unless otherwise prohibited by applicable law, and in countries where COVID-19 vaccination is legally required. The company will consider legally required accommodations/exceptions for medical, religious, and other reasons as per the requirements of the role and in accordance with applicable law. Additional information will be provided to candidates about the requirements and accommodation process at the offer time based on region.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Verizon,Principal Engineer - Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

You will be expected to architect solutions for business projects, work with enterprise architects to align application & system architecture to enterprise strategy and deliver individually and/or with the help of a team. You need to have passion to learn and educate fellow associates/subordinates and guide them to follow best practices. Principal consultant to the team that develops, maintains and enhances the service delivery and management for NS applications
• Architecting/Developing solutions for the application which deals with big data platforms.
• Engaging with Enterprise Architects on HLAs and defining new solutions that adhere to big data volume processing.
• Driving a Culture of Innovation: Champion a culture of innovation and drive as an example.
• Supporting customers with major platform issues and coordinating triage efforts to solve them.
• Identifying and aligning project requirements and conducting impact analysis.
• Working closely with the business team, and other internal IT teams to deliver projects on time.
• Preparing presentations and reports to internal and external customers, as well as internal Executives.
• Evaluating various new technical products based on changing business needs and making product recommendations to management keeping in mind the architecture of the entire list of applications supported.
• Providing technical leadership and business-related subject matter expertise on large, highly complex projects.
• Guiding the team on best practices for efficient and streamlined delivery of software to production. Guiding teams on maintaining security posture and code quality of applications keeping the tech debt in check.
• Identifying chronic production issues, pain points of customers by evaluating feedback and monitoring the NPS to maintain it above the required threshold.
• Working with Quality Assurance, UAT & Production Support teams to support releases, troubleshoot progression/regression issues, integration & E2E testing and implement deliverables as per the targeted timelines.
• Working with infrastructure teams to implement DevOps capabilities that help streamline the CICD process. Leverage innovative technologies to build proof-of-concepts that help build customer experiences, reduce pain points in the current experience, and provide a delight factor to customers.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You view technology through a lens of making things better and more effective. Understanding and building continual improvements to the digital value chain is something you flourish with. You enjoy the process of solving complex issues while empowering the team around you to do the same.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Experience in Hadoop, Hive, Pyspark , Spark Scala, PIG, Java, GCP, AWS, CICD (Jenkins/ Gitlab).
• Experience in Big query, Composer, Cloud Functions & Java script.
• Experience with any of RDBMS, Druid and MongoDB.
• Experience in Devops & automation.
• Experience in Docker/K8s & SRE Practice.
• Experience in Agile & SAFe methodologies.

Even better if you have one or more of the following:
• A Master's degree.
• Ability to design products which can scale up for large volumes of data.
• Knowledge in Wireless Domain.
• Knowledge of Security Vulnerabilities.
• Strong written and verbal communication skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False
lululemon India Tech Hub,Data Engineer - SQL & Python,"We are looking to hire dynamic Data Engineers for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.

Responsibilities
• Uses structured tools for analysis and presentation of concepts and models to enhance the BRD
• Develop, maintain and deliver training materials to the supply chain end-users
• Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs
• Support day to day reporting needs where required
• Support production issues as relate to application functionality and integrations
• Excellent spoken and written communication skills (verbal and non-verbal)
• Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)
• Solid scripting capability for analysis and reporting (ANSI SQL)
• Solid experience in RDBMS and NoSql technologies
• Strong analytical skills to support BAs.
• Strong problem-solving skills (Math skills required for data modeling)
• Ability to work as an integration / data engineer.
• Ability to manage and complete multiple tasks within tight deadlines
• Possess expert level understanding of software development practices and project life cycles.
• Working experience with Java batch spring boot/ python.
• Working Experience with cloud-native technologies
• Must have: Working experience in dealing with big data and data manipulation.
• Desired: Familiarity with Retail planning / merchandising systems/ supply chain.
• Desired: Familiarity with DevOps practices like CICD pipeline
• Desired: Retail experience is a plus. (fashion retail experience would be ideal)
• Must Have: Working experience with cloud platforms namely AWS
• Must Have: Working experience with large data sets (at least 80 – 100 GB data)

Requirements
• name : lululemon India Tech Hub
• location : Bengaluru, IN
• experience : 5 - 8 years
• Primary Skills: SQL or RDBMS or NoSQL,Python,AWS,Springboot,ETL",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Splunk,Senior Data Engineer - 27505,"The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will work as part of an evolving Enterprise Data Management (EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.

The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible for building and scaling the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineers and Infrastructure teams plus other data roles, including Data Governance, Quality, and Architecture Stewards). Strong technical experience within enterprise software is essential.

Responsibilities:
• Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
• Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
• Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
• Identify and document standard methodologies, standards, and architecture guidelines
• Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
• Identify ways to improve Data Reliability, Data Efficiency and Data Quality

Required Qualifications, Skills & Experience:
• 7+ years of data engineering related experience such as data analysis, data modeling, and data integration.
• Experience with Sales Operations, Partner Operations and customer success business processes and applications
• Experience in custom ETL design, implementation, and maintenance
• Strong knowledge of programming languages (e.g. Python and Object Oriented Programming)
• Hands-on experience with SQL database design
• Experience working on CI/CD processes and source control tools such as GitHub and GitLab
• Experience working in Snowflake and relational databases
• Extensive hands-on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake
• Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
• Experience with or exposure to data governance & quality principles and practices
• Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
• Passionate about data solutions, technologies, and frameworks
• Experience in Data Visualization tools such as Tableau

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Knowledge of enterprise systems such as Salesforce, Workday, SAP etc.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
Reverate,Senior Data Engineer - Remote,"Reverate Tech is a product and service-based start-up, working with International Clients. Our services include Data Engineering, Web Development, BI/Data Warehousing, Enterprise Application Implementation (ERP/CRM), and NetSuite. Our product portfolio has business apps in the domain of ERP, Auto Service, and Personal Safety.

This is an exciting opportunity to work as Senior Data Engineer for our client SellerX.

SellerX is the 3rd fastest growing company in the EU evaluated at more than 1 Billion Euros. It has an ambitious goal: to become a leading global acquirer and operator of a new generation of eCommerce businesses.

Your Job:
• You are responsible for all types of data management processes (collection, storage, cleansing, preparation, maintenance, accumulation, transfer to business reporting).
• You optimize and develop existing and new data warehouse applications using tools for data ingestion and data modeling
• You design and document new data models and best-practice solutions.
• You are responsible for prototyping and implementing new ETL jobs and modeling approaches.
• As a data engineer, you will deal with python programs and their configurations in order to create or improve automated data engineering tasks.
• In addition to technical project management, you advise our other tech teams in Data Management aspects.
• Ensuring data security (e.g. encryption) and improving the backup strategy.
• You work hand in hand with data architects, data analysts, and data scientists.
• You ensure that quality, stability, and robustness along the entire process chain meet our high standards.

Your Background:
• You have a bachelor's degree with a focus on software engineering.
• 5+ years of experience in data engineering.
• Strong with Algorithms and have worked on scaling pipelines/solutions
• Hands-on experience with data ingestion tools like Fivetran, Daton, Stitch, or Data Virtuality.
• Hands-on experience with ETL and orchestration tools like Apache Airflow or similar.
• Object-oriented Python programming is more than just a plus.
• Expert knowledge in the areas of data modeling and ETL processes on the SQL level (e.g. using DBT), as well as experience working with REST APIs, is beneficial.
• DevOps experience
• In addition to your ""hands-on"" mentality, you score points with a high technical affinity and a strong analytical mindset.
• Your working standards do not suffer in terms of quality, even in hectic times.

Benefits:
• Compensation: up to 40 LPA
• 100% remote-working;
• Flexible working hours;
• Development of your personal strengths in a dynamic environment;
• An attractive and varied job with a high level of personal responsibility;
• A collegial togetherness and a modern management style/startup;

Interested? Join us and start your learning and growth journey.

Reverate focuses on Software Engineering. Their company has offices in India. They have a small team that's between 11-50 employees.

You can view their website at https://reverate.tech/",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
"6221, Roche",Senior Data Engineer,"The Position

Roche sequencing solution is developing the next generation sequencing based on nanopore technology. This has the potential to make sequencing based diagnostics cheaper, faster and more accurate enabling precision medicine and early diagnosis of many diseases improving the health outcome.

As part of Data Science Automation group, you will get to work on key software technologies enabling research and development of sequencer. You will solve complex problems related to processing terabytes of data coming out sequencer and deriving useful insights from the data. This requires massively parallel computation locally on GPU as well as in the cloud. You will gain exposure to latest and greatest in data engineering and data pipeline tools and technologies. You will also work with advanced data visualization problems involving millions of data points.

You also will get to collaborate with multidisciplinary team of scientists and engineers working in fields ranging from protein engineering, bio chemistry, biophysics, stats modeling, bioinformatics and deep learning.

If you are excited to become part of the next generation sequencing research and development and revolutionize the healthcare, we have a rare opportunity for you to come and work with us.

We need an experienced Data/Workflow Engineer with a strong background in designing and developing highly scalable data management solutions and workflow pipelines. You will work across a variety of problems and application spaces involved in high availability systems, for data management and compute systems, at a very large scale. You will be working with a hardworking team of engineers and data-scientists who are passionate about building creative and novel solutions at the forefront of Sequencing research.

Required Qualifications:
• BS in CS or similar and 10+ years professional experience, or MS with 7+ years of experience in building highly scalable, performant software systems, in a Linux environment.
• Strong, hands on experience building and supporting Enterprise level Workflow management systems such as airflow, nexflow, kubeflow etc. Experience with building performant airflow pipelines with a large number of DAGs and dynamic DAGs is desirable.
• Working experience in deploying and managing airflow platforms, knowledge of Terraform, Kind, Helm etc. is a huge plus.
• At least 3+ years’ experience of developing solutions using container and cloud technologies. Preferably Kubernetes, Docker.
• Experience building with cloud native technologies (e.g. GCP, AWS), blob stores and knowledge of various data compression formats.
• Demonstrated skill with software development following current software engineering best practices using languages such as: Python, Java and BASH scripting.
• Have a strong understanding of modern software development practices and tools, including: version control systems (e.g., Git), issue trackers, and test frameworks.
• Experience building and using automation tools, CI/CD, unit testing.

You 'll go above and beyond our required requirements if you...
• Possess a PhD/MS in Computer Science, Computer Engineering, or another related, technical discipline.
• Have at least ten years of relevant experience in the development of software systems ideally in a Linux environment.
• Have experience using modern frontend and backend software frameworks for software applications.
• Knowledge of challenges involved in large-scale, high-availability data platforms. Experience with designing and implementing platforms providing secured access to large datasets.
• Have experience applying software expertise to full project lifecycles, including requirements analysis, design, implementation, and testing.

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,True,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False
Koch,Senior Data Engineer,"Description

Position Description/ Requirements

The Data Engineer will be a part of an international team that designs, develops and delivers Data Pipelines and Data Analytics Solutions for Koch Industries. Koch Industries is a privately held global organization with over 120,000 employees around the world, with subsidiaries involved in manufacturing, trading, and investments. Koch Global Solution India (KGSI) is being developed in India to extend its IT operations, as well as act as a hub for innovation in the IT function. As KSGI rapidly scales up its operations in India, it’s employees will get opportunities to carve out a career path for themselves within the organization. This role will have the opportunity to join on the ground floor and will play a critical part in helping build out the Koch Global Solution (KGS) over the next several years. Working closely with global colleagues would provide significant international exposure to the employees.

The Enterprise data and analytics team at Georgia Pacific is focused on creating an enterprise capability around Data Engineering Solutions for operational and commercial data as well as helping businesses develop, deploy, manage monitor Data Pipelines and Analytics solutions of manufacturing, operations, supply chain and other key areas.

A Day In The Life Could Include:

(job responsibilities)
• Partner/collaborate with Business stakeholders and build high-quality end-to-end data solutions.
• Build a data architecture for ingestion, processing, and surfacing of data for large-scale applications in the cloud (AWS/ Azure)
• Create and maintain optimal data pipeline architecture.
• Follow best practices of Agile and DevOps focusing on the delivering of high-quality products and providing the ongoing support to meet the customers' needs
• Implement processes for Continuous integration, Test automation and Deployment (CI/CD Pipelines)
• Provide quality documentation of your design (process and workflows) and implementation including experiment tracking / logs.
• Provide on-call support on an as-needed basis
• Handle support cases to ensure issues are recorded, tracked, resolved, and follow-ups finished in a timely manner.

What You Will Need To Bring With You:

(experience & education required)
• Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science). Master’s degrees preferred.
• 6+ years of IT experience.
• In depth knowledge of Data Engineering concepts and platforms - SQL based systems, Hadoop, Spark, Distributed computing, In-memory computing, real time processing, pub-sub, orchestration, etc.
• Expertise of building data pipelines using (Pyspark based) and Databricks utilising techniques in Azure or AWS.
• 4-5 years of experience in DevOps and CI/CD using tools like Git, Terraform, Jenkins, Ansible.
• 5+ year of experience in Data modeling, SQL, Data Warehouse skills are a MUST.
• A passion and fearlessness for learning new technologies and methods in the areas of Administration
• Ability to thrive in a team environment and juggle multiple priorities.
• Excellent written and verbal communication skills.

What Will Put You Ahead:

(experience & education preferred)
• In depth knowledge of entire suite of services in AWS/Azure Cloud Platform.
• Strong coding experience using Pyspark.
• Experience of designing and implementing ETL process using SSIS.
• Cloud Data Anaytics/Engineering certification.

Other Considerations:

(physical demands/ unusual working conditions)
• Some work may involve hours outside of normal KGS works hours.

Koch is proud to be an equal opportunity workplace",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Greetings from TCS !!!

TCS India presents excellent opportunities for IT professionals.

Role :- Data Engineer

Experience:- 7 to 10 years

Location- Bangalore / Mumbai / Chennai / Bhubaneswar

Required Technical Skill Set- Data Engineer – Big Data, Hadoop, Hive, Spark, Yarn

Must-Have:-

1. 4-8 Yrs of hands-on development experience

2. Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is mandatory.

3. Experience working with various data exchange formats (JSON, CSV, XML etc.).

4. Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred.

5. Excellent knowledge of SQL and Linux shell scripting.

6. Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)

Good-to-Have:-

1. Experience building real-time data pipelines using Kafka or spark streaming is preferred.

2. Exposure to Microsoft Azure (or other cloud) platforms is preferred.

3. Experience with Agile methodologies for project development.

4. Excellent diagnostic, analytical and problem-solving skills are preferred.

5. Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred

TCS Eligibility Criteria:
• BE/B.Tech/MCA/M.Sc/MS with minimum 3 years of relevant IT-experience post Qualification.
• B.Sc Graduates with minimum 4+ years of relevant IT-experience post qualification.
• Only Full Time courses would be considered.
• Consistent academic records class X onwards (Minimum 50%)
• Candidates who have attended TCS interview in the last 3 months need not apply.

Interested candidate can share their resumes with the mandatory details mentioned below.

Please update the details:

1. Total years of Exp:

2 Email ID :

3. Present Company:

4. Current & Preferred Location:

5. Mobile No.:

6. Current CTC:

7. Expected CTC:

8.Notice Period:

9: Working With TCS /CMC ( Direct Payroll) earlier (Yes/ NO):

10. No Of job change-

Interested candidate can share their resumes with hiba.fathima@tcs.com",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LTIMindtree,GCP Data Engineering POD Lead,"Primary Skill – GCP Data Engineering POD Lead

Total Exp – 3 to 14 Years

Notice Period – 0 to 30 Days

Job Location – Kolkata, Bangalore, Mumbai, Pune, Chennai, Hyderabad

Job Description:

Job Description:

TPrimary Skill – GCP

Secondary Skill – Python, Big query

Overall, more than 8+ Yrs of experience in Data Science Statistical Modeling and Projects to Develop and Deliver Data Science work Strong understanding of Machine Learning Statistics fundamentals Technology Skill Set Python R Pandas Scikit Learn R s

Desired Candidate Profile Technology & Engineering Expertise

• 5+ years of experience in implementing data solutions using GCP/SQL programming

• Proficient in dealing data access layer, RDBMS | NO-SQL.

• Experience in implementing and deploying Big data applications with GCP Big Data Services.

• Good to have SQL skills.

• Experience with different development methodologies (RUP | Scrum | XP) Soft skills

• Able to deal with diverse set of stakeholders

• Proficient in articulation, communication, and presentation

• High integrity

• Problem solving skills & learning attitude

• Team player Key Responsibilities

• Implement data solutions using GCP and need to be familiar in programming with SQL/python.

• Ensure clarity on NFR and implement these requirements.

• Work with Client Technical Manager by understanding customer’s landscape & their IT priorities

• Lead performance engineering and capacity planning exercises for databases",,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Arcadis,Azure Data Engineer,"ARCADIS is looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in “Everything Digital, Digital Everything”. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.

Technology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.

Role accountabilities:
• Possess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment
• You are excited about working with Azure Data Platform
• challenges while building the next wave of software engineering solutions
• Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform
• Leading the craftsmanship, security, availability, resilience, and scalability of your solutions
• Very strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.
• Strong experience in
• T-SQL, SSIS, SSAS, SSRS
• Azure Data Factory
• Azure Data Lake Store
• Azure Data Lake Analytics (Good to have, not mandatory)
• Azure SQL DB
• Azure SQL DW
• Azure Analysis Services, DAX
• Azure Data Bricks with Python/Scala
• Experience in building end to end solution using Azure data analytics platform.
• Experience in building generic framework solution which can be reused for upcoming similar use cases.
• Experience in building Azure data analytics solutions with DevOps (CI/CD) approach.
• Experience in using TFS, Azure Repos.
• Mentor peers to gain expertise on Azure data platform solutions skills.
• Experience in developing, maintaining, publishing, and supporting dashboards using Power BI.
• Strong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded

Qualifications & Experience:

Basic Qualifications:
• Bachelor in Engineering/Math/Statistics/Econometrics or related discipline
• Should have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.
• Preferred Qualifications:
• Master’s or Minor in Computer Science
• 3+ years of experience developing Data Engineering solutions
• Architecture, design experience with good knowledge of data model design & their implementation.

Why Become an Arcadian?

Our work with clients has a direct impact on people’s lives and on the planet. We make moving, living and belonging in cities safer, more resilient and more sustainable. By partnering with our clients as responsible custodians of our earth's resources, we can create a sustainable planet.

We continue to think of new ways to make positive impacts and create better experiences for people; data driven and digital solutions have become part of the Arcadis DNA. Working together with clients and using techniques like design thinking, we can get to the heart of our clients’ most pressing challenges and work together to solve them.

As a global business, we have committed to support five of the UN’s Sustainable Development Goals to ensure that our projects contribute to a better and more sustainable future for all. But it’s not just the work that we do on client projects that benefits communities and our planet. As a global business, we are committed to making a positive impact to society by supporting local communities where we operate.

To help protect our planet, we monitor and measure non-financial information to inform business decisions and reduce our own environmental impact as part of our commitment to be net zero carbon as a global company by 2030.

Our Commitment to Equality, Diversity, Inclusion & Belonging:

We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.

In accordance with the Colorado Equal Pay Transparency Rules:

Arcadis offers benefits for full time positions. These benefits include medical, dental, and vision coverage along with a 401K plan, STD and LTD, and Life Insurance as well as some additional optional benefits. Full time positions also come with annual PTO days and at certain levels a bonus program may apply. The Salary range for this role is $61,360 - $95,000 for Colorado based positions only. Other locations will vary in salary range

Transform Your World",Hyderabad,True,False,True,False,False,False,False,True,True,False,True,False,False,False,False,False
Dolby Laboratories,Data Engineer,"Join the leader in entertainment innovation and help us design the future. At Dolby, science meets art, and high tech means more than computer code. As a member of the Dolby team, you’ll see and hear the results of your work everywhere, from movie theaters to smartphones. We continue to revolutionize how people create, deliver, and enjoy entertainment worldwide. To do that, we need the absolute best talent. We’re big enough to give you all the resources you need, and small enough so you can make a real difference and earn recognition for your work. We offer a collegial culture, challenging projects, and excellent compensation and benefits, not to mention a Flex Work approach that is truly flexible to support where, when, and how you do your best work.

Play a key role as part of Dolby's new R+D Center in Bangalore as a Data Engineer in our Advanced Technology Group ""ATG"". ATG is the research and technology arm of Dolby Labs. It has multiple competencies that innovate technologies in audio, video, AR/VR, gaming, music, and movies. Many areas of expertise related to computer science and electrical engineering, such as AI/ML, computer vision, image processing, algorithms, digital signal processing, audio engineering, data science & analytics, distributed systems, cloud, edge & mobile computing, natural language processing, knowledge engineering and management, social network analysis, computer graphics, image & signal compression, computer networking, IoT are highly relevant to our research.

Responsibilities:

As a Data Engineer, you’ll be a part of a growing engineering team building and designing our core data infrastructure for our internal technology research and development efforts. You’ll have the chance to partner closely with our research and data science teams to understand data and functional requirements. We are looking for an experienced data professional who is a problem solver, logical thinker and passionate about everything relating to data and analytics. Your responsibilities include:
• Create and maintain optimal data pipeline architecture for data coming from different sources, in various formats and of different content type (text, audio, video etc.) allowing to standardize, clean and ingest data.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Design and develop solutions which are scalable, generic and reusable. Be responsible for collecting, storing, processing, and analyzing huge sets including, but not limited audio, video, and metadata.
• Develop techniques to analyze and enhance both structured/unstructured data and work with big data tools and frameworks.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Databricks, and AWS ‘big data’ technologies.
• Create data tools for research and data scientist teams.

What You Bring To The Role
• BsC/Msc degree in CS or EE. Work experienced desired, but not required.
• Experience building and optimizing streaming big data pipelines, architectures, and data sets.
• Deep understanding data pipeline frameworks including Databricks and Fivetran.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Experience or solid theoretical understanding of data workflows including:
• Ingestion
• Batch and stream processing
• Storage and archiving
• Visualization/Reporting and Dashboards
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Understanding of the current state of infrastructure automation, continuous integration/deployment - CI/CD, SQL/NoSQL, security, networking, and cloud-based delivery models.
• In-depth understanding of:
• NoSql databases (Kafka, HBase, Spark, Hadoop ,Cassandra, MongoDb etc). SQL development and any procedural extension language (T-SQL, PL/SQL, Pg/PLSQL etc.)
• Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Distributed data processing frameworks like Apache Spark, Apache Flink
• Scalable ML pipelines for image, video and audio modalities with tools such as Flyte, MLflow, Prefect, or AirFlow
• Data collection, labeling, cleaning, and generation tools such as LabelBox, SuperAnnontate, Scale Ai, or V7
• Scripting abilities with two or more general purpose programming languages including but not limited to Java, C/C++, C#, Objective C, Python, JavaScript.
• Data modeling and extraction of data from different sources
• Strong documentation skills, communication and client facing Experience
• Experience supporting and working with cross-functional teams in a dynamic environment.

Build your career profile, also within the Careers tab in Employee Central to open the possibility of new opportunities finding you. Express your interest. If you want to express your interest in a specific opportunity and be contacted by a recruiter, click the apply button associated with the relevant job description. The Recruiter is the only one who will see your application.

Please refer to the recruiting website for more information: https://jobs.dolby.com/careers

]]>",Bengaluru,True,False,True,True,False,True,True,True,False,False,False,True,False,False,True,False
Mercede,Positions for Data Engineer,"Technical Skills Competencies
• Deep hands-on expertise in Databricks (Scala or Python).
• Experience in Design and implementation of Big Data technologies (Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases) and familiarity with data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture).
• Experience in working as a Big Data Engineer: query tuning, performance tuning, troubleshooting, and debugging Spark and other big data solutions.
• Familiarity with a full range of data engineering approaches, covering theoretical best practices and the technical applications of these methods.
• Experience building and deploying a range of data engineering pipelines into production, including using automation best practices for CI/CD.
• Very good experience in writing SQL queries.
• Hands-on experience with any of the cloud providers such as AWS or Azure.
• Familiarity with databases and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Ability to juggle and prioritize multiple tasks within a collaborative team environment
• Desire to learn and grow both technical and functional skill sets, and drive team s potential
• Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience And Skills

Microsoft Azure and AWS Certifications
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Trained in Data Factory, Delta lake, Data bricks Notebooks
• Working experience in SAFe - Scaled agile framework
• Working experience in an international team environment
,

This job is provided by Shine.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
HuQuo,Interesting Job Opportunity: Azure Data Engineer - ETL/MDM,"Job Description
• To collaborate with various teams/regions in driving facilitating data design, identifying architectural risks and key areas of improvement in data landscape, and developing and refining data models and architecture frameworks
• Technical experience and knowledge in Cloud Data Warehousing, data migration and data transformation
• Develop and test ETL components to high standards of data quality and performance as a hands-on development lead
• Familiarity with Data Lakes, Data Warehouses, MDM, BI, Dashboards, AI, ML
• Design data architecture patterns and ecosystems including data stores (operational systems, data lakes, data warehouses, data marts), ingress patterns (API, streaming, ETL/ELT), and egress patterns (analytics/decision tools, BI tools). Lead, consult or oversee multiple architectural engagements
• Oversee and contribute to the creation and maintenance of relevant data artifacts (data lineages, source to target mappings, high level designs, interface agreements, etc.) in compliance with enterprise level architecture standards
• Experience in leading and delivering data centric projects with concentration on Data Quality and adherence to data standards and best practices.
• Experience in data modeling, metadata support, development and testing for enterprise wide data solutions
• Azure cloud experience is a must have with familiarity of the services: Azure Databricks, Azure Datafactory, Azure Datalake, Spark SQL, PySpark, Airflow, SQL server and Informatica MDM.
• Additional exposure to GCP and AWS is good to have.

Key Skill: Azure Databricks, ADF, ETL, Pipeline Dev, SQL, DWH, ADLS.

(ref:hirist.com)",Gurugram,False,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
AXA XL,Data Engineer,"Gurgaon, Haryana, India

The Application Developer plays a critical role within the Data and Analytics SDC as this person is responsible for designing and implementing data structures to support current and future analytical projects. We are looking for candidates that have experience working with data from a raw, unprocessed state and organizing it intuitively. Building this data pipeline enables our partners to analyze data better and faster – ultimately leading the organization in optimizing the decision-making process.

DISCOVER your opportunity

What will your essential responsibilities include?
• Candidates for this role should have experience developing data processes with source data in a variety of formats (structured / unstructured, databases, APIs) into a target state. This will involve building proper data pipelines to support initial exploration and real-time integration.
• Data development using appropriate tools and techniques to process data required for advanced analytics. A candidate would be expected to interact with Data Engineering Leads and Data Scientists to understand requirements and would be responsible for the development of the solution.
• Providing the right context of data required for a given analysis. This would require the candidate to work with data modelers/analysts to understand the business problems they are trying to solve and create data structures to feed into their analysis.
• Build upon learnings of internal and external data to become more proactive. This includes thinking ahead of what modelers will anticipate with their data needs and designing structures that are intuitive to use.
• Making sure quality and understanding of analytical data. This would require hands-on data experience to look into data issues and seek resolution or acceptance. Create the appropriate amount of documentation, leverage standards, and build upon them. Data should be reconciled and documented at various stages for integrity.
• Take part in developing governance and rigor of data management practice within the Data and Analytics SDC. This will also include partnering with enterprise IT groups and involvement in enterprise data-related functions.
• You will report to Data Manager/Principal Data Engineer.

SHARE your talent

We’re looking for someone who has these abilities and skills:
• Demonstrated ability to work through data complexities which include a variety of sources, formats, and structures. Robust preference for experience in the Insurance domain.
• Ability to see through ambiguous concepts and break down complex problems into manageable components.
• Detail-orientated, proven ability to recognize patterns in data.
• Demonstrated ability to incorporate data quality standards into data development.
• Possesses natural curiosity. Seek to understand the world around you, and question when appropriate.
• Robust SQL Skills required.
• 2-4 years of development experience using data development (visual ETL or coded) / analysis tools (ex. SAS, SPSS, R, Microsoft SSIS/SSAS, Informatica, DataStage, AbInitio).
• Experience in .NET, Python, or Java development is a plus.
• Experience in web extraction, unstructured data, advanced text parsing, machine learning, and NLP a plus.
• Familiarity with developer support tools (TFS/GIT, Jenkins) is a plus.
• College Degree in MIS, Information Technology, Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, or equivalent.

FIND your future

AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals, and even some inspirational individuals we don’t just provide re/insurance, we reinvent it.

How? By combining an effective and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business − property, casualty, professional, financial lines, and specialty.

With an innovative and flexible approach to risk answers, we partner with those who move the world forward.

Learn more at axaxl.com

Inclusion & Diversity

AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic.

At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That’s why we have made a strategic commitment to attract, develop, advance, and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It’s about helping one another — and our business — to move forward and succeed.
• Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability, and inclusion with 20 Chapters around the globe
• Robust support for Flexible Working Arrangements
• Enhanced family-friendly leave benefits
• Named to the Diversity Best Practices Index
• Signatory to the UK Women in Finance Charter

Learn more at axaxl.com/about-us/inclusion-and-diversity. AXA XL is an Equal Opportunity Employer.

Sustainability

At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called “Roots of resilience”, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations.

Our Pillars
• Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society – are essential to our future. We’re committed to protecting and restoring nature – from mangrove forests to the bees in our backyard – by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans.
• Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions.
• Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We’re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting.
• AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL’s “Hearts in Action” programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day – the Global Day of Giving.

For more information, please see axaxl.com/sustainability

Flexible Work Eligible

None

AXA XL is an Equal Opportunity Employer.

Location

IN-HR-Silokhera Gurgaon

Job Field

IT

Schedule

Full-time

Job Type

Standard",Gurugram,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,False
Inference Labs,Data Engineer,"Responsibilities for the job Key Responsibilities: - Data Model Designing, Developing and maintaining Data pipelines on cloud (AWS Platform) Translate business needs to technical specifications and framework Maintain and support data mart, data analytics platforms & application. Perform quality assurance to make sure the data correctness Develop sub-marts using SQL and OLAP function to fulfil immediate/ad-hoc need of the business users basis the comprehensive marts Monitoring of the performance of ETL and Mart Refresh processes, understand the problem areas and open a project to fix the performance bottlenecks. Other Responsibilities (If Any):- Availability during month-end Deck generation, may be sometime during week-end/holidays. Eligibility Criteria for the Job Education B.E/B.Tech in any specialization, BCA, M.Tech in any specialization, MCA Work Experience Data Engineer: 4+ years of experience in data engineering on cloud platforms like AWS, Azure, GCP Exposure with working on BFSI domain / big data warehouse project Exposure to manage multiple source of the information, both structured / unstructured data Manage data lake environment for point in time analysis (SCD Type 2), multiple refresh during the day, event based refresh Should have exposure on Managing environment having real time dashboard, data mart requirement. Primary Skill Must have orchestrated using any of the cloud platforms Expert in writing complex SQL Command using OLAP Working experience on BFSI Domain Technical Skills Must have orchestrated at least 3 projects using any of the cloud platforms (GCP, Azure, AWS etc.) is a must. Must have worked on any cloud PaaS/SaaS database/DWH such as AWS redshift/ Big Query/ Snowflake Python/Java Hands - on Exp from data engineering perspective is a must Experience with any of the object-oriented/object function scripting languages: Python, Java, Scala, Shell, .NET scripting, etc. is a must Experience in at least one of the major ETL tools (Talend + TAC, SSIS, Informatica) will be added advantage Management Skills Ability to handle given tasks and projects simultaneously in an organized and timely manner. Soft Skills Good communication skills, verbal and written. Attention to details. Positive attitude and confident.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
PwC,Data Engineer-Manager-P&T Labs,"Line of Service
Internal Firm Services

Industry/Sector
Not Applicable

Specialism
IFS - Internal Firm Services - Other

Management Level
Manager

Job Description & Summary
A career in National Special Functions, within Internal Firm Services, will provide you with the opportunity to support service, sector, and market leaders deliver the unique PwC client experience to our clients. You’ll play an important part in continuously innovating and improving Firm operations so that we can continue to provide the highest quality of services to our current and prospective clients.

Our team focuses on representing data as a strategic business asset to help serve our clients. You’ll focus on using data and information across PwC to drive change and improvements in data related operations to help enable the business as well as provide insights related to attendant risks.

Preferred Knowledge/Skills:

Demonstrates intimate knowledge and/or a proven record of success in the following areas:
• Understanding architectural design and data platform delivery in technologies that include, but are not limited to cloud, ETL, data streaming, data storage, data modeling, APIs/microservices, automation, continuous integration/continuous deployment;
• Showcasing work experience as a Data Engineer, Data Architect or similar role;
• Showcasing data engineering knowledge around complex efforts within established Software Development Lifecycles and methodologies including agile, scrum, iterative and waterfall;
• Showcasing technical knowledge that spans multiple platforms and portfolio of applications with demonstrated knowledge of the business strategic priorities in order to resolve complex problems;
• Utilizing IT processes and frameworks including, but not limited to, Identity Access Management (IdAM), Enterprise Application Integration, Data Warehousing, Business Intelligence, Reporting, Mobility, Master Data Management, and Search;
• Understanding of database structure principles;
• Showcasing advanced experience building and maintaining optimal data pipeline architecture and data streaming and integrations using tools such as ADF, SSIS, Informatica, API Management, Enterprise Service Bus (preferably Kafka);
• Showcasing advanced SQL knowledge and experience working with relational databases and performance optimization;
• Demonstrating data mining and segmentation techniques;
• Exhibiting knowledge in relational SQL, NoSQL and Big Data technologies;
• Understanding Data Federation/Virtualization technologies, such as PowerBI, Tableau, D3.js, and implementing Cloud based solutions;
• Assessing and analyzing system requirements;
• Showcasing analytical skills and a problem-solving attitude;
• Demonstrating virtual leadership and motivational skills;
• Recommending and participating in activities related to the design, development and maintenance of the Enterprise Data Architecture;
• developing internal relationships and PwC brand;
• Demonstrating time management skills with the ability to handle multiple projects simultaneously;
• Leveraging business knowledge and interpersonal skills to build, maintain, and influence relationships with leaders throughout the business and IT.

Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements
Not Specified

Available for Work Visa Sponsorship?
No

Government Clearance Required?
Yes

Job Posting End Date
May 10, 2023",Hyderabad,False,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Mindera,Data Engineer,"We are looking for an experienced Data Engineer to join our team.

Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.

As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.

This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.

National and international expected travelling time varies according to project/client and organisational needs: 0%-15% estimated

Requirements

You’re great at
• Python
• AWS like (Glue, S3, EMR, Athena and ECS/Fargate)
• SQL
• Airflow
• Data Modelling
• Pyspark

It also would be cool if you have
• Exposure to DBT would be preferable
• Experience working with modern data platforms such as redshift or snowflake would be preferable
• Experience working with Airflow, Docker, Terraform and CI/CD would be preferable
• Experience working with docker, Scala, and Kafka would be an added advantage

What You Will Be Doing
• Implement/support new data solutions in the data lake/warehouse built on the snowflake
• Develop and design data pipelines using python.
• Design and Implement Continuous Integration/Continuous Deployments pipelines.
• Perform Data Modelling using downstream requirements.
• Develop transformation scripts using advanced SQL and DBT.
• Write test cases/scenarios to ensure incident-free production release.
• Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
• Debug production and development issues and provide support to colleagues where necessary.
• Perform data quality checks to ensure the quality of the data exposed to the end users.
• Build strong relationships with team, peers and stakeholders.
• Contributes to overall data platform implementation.

Benefits

We offer
• Flexible working hours (self-managed)
• Competitive salary
• Annual bonus, subject to company performance
• Access to Udemy online training and opportunities to learn and grow within the role

At Mindera we use technology to build products we are proud of, with people we love.

Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.

We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world.

You get to work with a bunch of great people, and the whole team owns the project together.

Our culture reflects our lean and self-organisation attitude.

We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.

Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook

Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Rishabh Software,Big Data Engineer,"Job Description:

Roles and Responsibilities:

1) Work with BigData Practice Tech lead to Execute BigData Projects

2) Work with Techlead , helping him in Solutioning, Architecture and Technical Design

3) Analyze requirements and prepare low level design

4) Hands on implementation of Data ingestion, data processing and Data storage code and algorithms

5) Team management under Techlead guidance - including work distribution and delivery

6) Participate in potential client meetings and demos

Required Skills:

Any one programming Language - Java or Scala

Good Experience with Apache Spark

Any one Data integration platform - Kafka or similar

Any one No Sql data storage - S3 or No Sql Database

One live BigData project - Data ingestion , processing and Storage

Basic Cloud Exposure - AWS preferred

Excellent Analytical and problem solving skills.

Excellent Communication Skills",Vadodara,False,False,True,True,False,False,False,True,False,False,False,True,False,False,False,False
Cloud Software Group,Senior Data Engineer,"About Cloud Software Group

Cloud Software Group combines the capabilities of both Citrix and TIBCO, creating one of the world’s largest cloud software providers, serving more than 100 million users around the globe. When you join Cloud Software Group, you are making a difference for real people, each of whom count on our suite of cloud solutions to get work done – from anywhere. Members of our team will tell you that we value diverse lived experiences, varied perspectives, and having the courage to take risks. Our teams are encouraged to learn, dream, and build the future of work. We are on the brink of another Cambrian leap - a moment of immense evolution and growth. And we need your expertise and experience to do it. Now is the perfect time to move your skills to the cloud.

Position Summary

This is an individual contributor role with responsibility for supporting all data warehouse processes including technical analysis, design, development, implementation, and support of ETL solutions. The ideal candidate needs to have at least 5 years of experience developing with Microsoft SQL applications in an implementation and support role of a business intelligence organization.

Primary Duties / Responsibilities

Responsibilities will include, but are not limited to:

• Supporting the designs, tasks, and continuous improvements to maintain a scalable data warehouse

• Analyzing and validating data to ensure that business requirements are satisfied

• Creating data flow diagrams to depict business logic relating to data transformations

• Creating conceptual, logical, and physical data models for relational and dimensional solutions

• Breaking down, estimating, and executing increments of work

• Developing ETL packages of high complexity to fulfill all the business requirements

• Supporting deployment and delivery of defined technical solutions

• Communicating accurate and timely project status, issues, risks, and scope changes

• Performing root cause analysis of data discrepancies

• Creating data dictionaries and business glossaries to document data lineages, data definitions and metadata for all business-critical data domains

• Documenting all work (both technical and procedural) and ensuring that co-workers understand how to support system from an operational perspective

• Working in a highly collaborative team environment following the Agile Framework for planning and executing deliverables

Qualifications (include knowledge, skills, abilities, and related work experience)

• Bachelor’s degree in computer science or related field, or equivalent combination of education and recent, relevant work experience

• Minimum 5 years of experience in developing T-SQL Queries, Stored Procedures, and ETL packages with Microsoft SQL databases

• Strong understanding of data warehouse design and report development principles

• Experience in creating data flow diagrams and data models pertaining to business intelligence

• Experience in analyzing and developing reporting output such as Power BI, Tableau, or SSAS

• Strong interpersonal and problem resolution skills

• Strong teamwork and customer support focus

• Strong written (technical documentation) and verbal communication skills

• Ability to handle numerous conflicting priorities in a professional manner

Cloud Software Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all federal, state and local laws that prohibit employment discrimination on the basis of age, race, color, gender, sexual orientation, gender identity, ethnicity, national origin, citizenship, religion, genetic carrier status, disability, pregnancy, childbirth or related medical conditions, marital status, protected veteran status, and other protected classifications.",Bengaluru,False,False,True,False,False,False,False,False,True,True,True,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"Roles and responsibilities:
• Mandatory: Strong in Azure, ADF, Data Lake, Databricks, Pyspark
• Hands-on-experience in developing data lake solutions using Azure (Azure data factory for ingestion, Data Lake gen 2 and Azure SQL server for storage, Azure analysis service for transformations, Azure data bricks)
• Implement a robust data pipeline using Microsoft Stack.
• Create reusable and scalable data pipelines.
• Development and deployment of new data platforms.
• Leverage Azure BI services for development of Big Data Platforms.
• Work closely with the Product Owners and Architects to develop Azure Data Platforms.
• Work with the leadership to set the standards for software engineering practices within the team and support across other disciplines.
• Produce high-quality code that allows us to put solutions into production.
• Refactor code into reusable libraries, APIs, and tools.",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Affine,Data Engineer,"Company Description

About Company

http://www.affine.ai

""AFFINE"" cited by GARTNER as a SPECIALIST MIDSIZE CONSULTANCY in ANALYTICS and MACHINE LEARNING solutions and services. Click to Read More ""

Affine is a provider of high-end analytics services to solve complex business problems with offices in NJ, USA & Bangalore, India. We combine data driven algorithmic analysis with heuristic domain expertise to provide actionable solutions that empower organizations make better and informed decisions. Affine's value proposition is enabling clients to implement and realize ROI of the recommendations.

Affine has a group of people with significant experience in Analytics industry along with solid pedigree, deep business understanding and strong problem solving acumen. Our group primarily consists of Statisticians, Operations Researchers, Econometricians, MBAs and Engineers. Our employees have experience of working for many Fortune 500 companies.

Job Description

What the candidate will do:
• Contribute to adoption of cloud & cloud-based technologies and good design practices, while finding opportunities to simplify and scale
• Resolve problems and roadblocks as they occur with peers and help unblock junior members of the team. Follow through on details and drive issues to closure
• Define, develop, and maintain artifacts like technical design or partner documentation
• Drive for continuous improvement in Data engineering process within an agile development team
• Own and deliver assigned sprint tasks and help drive the team forward.
• Communicate and work effectively with geographically distributed cross functional teams

Experience

4 to 6 Years in Deploying models, Sage Maker or TensorFlow

Required skillset.
• Big Data: Spark, Kafka, Hadoop, Hive, SQL and NoSQL
• Cloud: AWS, EMR, Qubole/Databricks, VPC
• Devops: Docker containers and Jenkins. Spinnaker is preferred but not required.
• Programming languages: Scala and Pyspark is mandatory
• Agile and scrum experience and working with a remote team (nice to have, not required)

Must Have Skills
• Spark, AWS, Scala/Python, SQL, Java
• ML ops tools:Tensorflow or Sagemaker

Additional Information

Others
• Quick learner
• Excellent written and oral communication skills
• Excellent interpersonal & organizational skills
• Good listening and comprehension skills",Bengaluru,True,False,True,True,True,False,False,True,False,False,False,False,False,False,False,False
DISH Careers,Data Engineer,"About DISH:

DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world’s best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH’s largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market

Job Duties and Responsibilities:
• Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production
• Creation and optimization of data engineering pipelines for analytic projects
• Strong analytic capability and the ability to create innovative solutions
• Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues
• Provide technical guidance to L1 team members and help to resolve ETL related issues
• Need to work as on call-support

Skills, Experience and Requirements:
• Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment
• Experience in developing/debugging and fixing data ingestion pipelines both real time and batch
• Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb
• Experience in transforming/integrating data in Redshift/Snowflake
• Strong in writing complex SQLs to ingest data into cloud data warehouses
• Good hands on experience in shell scripting or python
• Experience with scheduling tools - ControlM, Airflow , StepFunction
• Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements
• Perform root cause analysis (RCA) for failures
• Good Communication skills – written and verbal with the ability to understand and interact with the diverse range of stakeholders
• Capable of working without much supervision",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
MPOWER Financing,"Data Engineer - Data and Analytics - Bangalore, India","THE COMPANY

MPOWER’s borderless loans and scholarships enable students from around the world to realize their full academic and career potential by attending top universities in the U.S and Canada.

As a mission-oriented fintech/edtech company, we move extremely quickly and leverage the latest technologies, global best practices, and heavy analytics to tackle one of the biggest challenges in financial inclusion. We’re backed by over $150 million in equity capital from top global investors, which enables fast growth and provides our company with financial stability and a clear path to an IPO over the coming years.

Our global team is composed of former management consultants, financial service and technology professionals, and other experts in their respective fields. We work hard, have fun, and believe strongly in our cause. For us, MPOWER’s mission is personal.

As a member of our team, you’ll be challenged to think quickly, act autonomously, and constantly grow creatively in an environment where fast change and exponential growth are the norm. Ideation and implementation happen very quickly. We value feedback and emphasize personal and professional development by providing the resources you need to further your skills and grow with the company. MPOWER is committed to cultivating your strengths and curiosity and helping you make an immediate impact.

MPOWER has been named one of the best fintechs to work for by American Banker for 2018, 2019, 2020, and 2021. We pride ourselves on being a “growth company for grown-ups,” where there are no pool tables but rather great health, education, and maternity/paternity benefits instead. Our team diversity has been recognized as well; we’re one of the most diverse workforces in the world in terms of nationality, gender, religion, age, sexual orientation, and educational background.

THIS IS A FULL-TIME POSITION, BASED IN OUR BANGALORE OFFICE

THE ROLE

You will be tasked with building and maintaining MPOWER’s data infrastructure. You’ll also play a key role in acquiring, organising and analysing data to provide insights that enable the company in making sound business decisions. This includes, but is not limited to:
• Maintaining MPOWER’s database and building on the existing database infrastructure
• Establishing the needs of different users and monitoring user access and security
• Capacity planning and refining the physical design of the database to meet system storage requirements
• Creating efficient queries and tools to obtain data for different business needs
• Building data models to identify, analyze and interpret trends or patterns in data sets that inform business decisions and strategy
• Working with various internal and external stakeholders to maintain and develop enhanced data collection systems
• Performing periodic data analyses, creating and presenting findings and insights
• Performing scheduled data audits in order to locate and correct code errors and maintain data integrity
• Collaborating with MPOWER’s global tech team to build data collection and data analysis tools

THE QUALIFICATIONS
• Undergraduate degree in computer science; advanced degree preferred
• 5+ years of experience in database programming, database administration and data analysis
• Must have prior experience in building high quality databases in accordance with end users information needs and views
• Proficiency in Big Data and Hadoop ecosystems.
• Deep familiarity with database design and documentation
• Hands-on expertise and exposure to at least one database technology (MySQL, PostgreSQL)
• Advanced knowledge of R/Python, PySpark, or Scala is a plus
• Prior experience building data pipelines and data orchestration is a plus.
• Superior analytical and problem solving skills
• Proven ability to create and present comprehensive reports
• Ability to multitask and own several key responsibilities at a given time
• Passion for excellence: constantly striving to improve professional skills and business operations

A passion for financial inclusion and access to higher education is a must, as well as comfortable working with a global team across multiple-time zones and sites!

In addition, you should be comfortable working in a fast growth environment, meaning a small agile team, fast-evolving roles and responsibilities, variable workload and tight deadlines, a high degree of autonomy, and 80-20 everything.

MPOWER Financing focuses on Financial Services, Finance, Finance Technology, Higher Education, and Education Technology. Their company has offices in New York City, Washington DC, and Washington. They have a small team that's between 11-50 employees. To date, MPOWER Financing has raised $7.291M of funding; their latest round was closed on October 2016.

You can view their website at http://www.mpowerfinancing.com/ or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,False,False,False
Cortex Consultants LLC,Data Engineer,"Hi,

Welcome to Cortex

Job Title: Data Engineer

Job Description

2+ years of Data Engineer experience in Snowflake (on Azure Cloud Preferred).

Strong knowledge of SQL to build queries and Optimization techniques.

Strong Knowledge of the ETL process using SSIS / ADF (Azure Data Factory) / Matillion

Experience of Python programming is an added advantage.

Location: Chennai

Work type-Hybrid

Immediate joiners

Interested candidates share your resume to

Deepak.g@cortexconsultants.com

Contact No: 9080100600",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Roche,Data Engineering Manager,"The Position
Engineering Manager is a critical leadership role in our Data Engineering team. This is a people management role that needs the ability to hire and grow top engineering talent and to manage multiple teams. It includes responsibility to deliver and operate high quality, scalable, and extensible products & solutions, including making appropriate design and technology choices. The role requires strong strategic thinking and making build/buy/partner decisions for technical capabilities. Effective Communication is critical, as you will be working closely with a variety of stakeholders to understand and address their needs. A healthcare background with experience in integrating healthcare IT systems would be good to have.

KEY RESPONSIBILITIES
• Manage team of Data Engineers working on multiple data analytics products.
• Work with different agile product teams, understand and fulfill their staffing needs.
• Work with business stakeholders to develop high level project plans and roles and responsibilities.
• Prepare training and development plans for the team.
• Understand and create a career path for the team members.
• Evolve and develop a long-term roadmap for team and projects.
• Apply data engineering best practices in terms of quality, security, scalability and maintainability.
• Participate in how the budget and staff is allocated for the projects.
• Maintain project time frames, budget estimates and status reports.
• Create management, communication plans and processes. Analyze and develop process for management and technical duties.
• Foster team bonding and trust within the team. Responsible for hiring, growing and motivating engineers on your team, ensuring you recruit and retain top talent.

REQUIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• BS degree in Computer Science, Computer Engineer or a related technical discipline with 10+ years of IT industry experience.
• At least 4-6 years of proven managerial experience developing a high-performing team.
• Experience in Agile Solution Delivery and Operations Management and people management.
• Quick learner with the ability to understand complex workflows and develop and validate innovative solutions to solve difficult problems.
• Strong communication, with the ability to explain complex technical problems to non-technical audiences and the ability to translate customer requirements to technical designs.
• Strong interpersonal skills, with proven ability to navigate complex corporate environments and influence stakeholders and partners.

DESIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• Proven work experience in AWS or other cloud related technologies.
• Experience of working in product based organization
• Proven work experience as an Engineering Manager or similar role
• Communication skills for overseeing staff and working with other management personnel
• Organizational skills for keeping track of various budgets, employees, and schedules simultaneously
• Leadership, team-building, and mentoring skills
• Personnel and project management skills
• Ability to work on multiple projects in various stages simultaneously
• Experience in the Healthcare Laboratory domain is a plus.

EDUCATION

Bachelor’s degree in Engineering

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Data.Ai,DNA Team - Data Engineer,"data.ai is the mobile standard and the trusted source for the digital economy. Our vision is to be the first Unified Data AI company that combines consumer and market data to provide insights powered by artificial intelligence. We passionately serve enterprise clients to create winning digital experiences for their customers.

We care deeply about our high-performance culture and operate as a global team. We put our customers at the center of every decision [Customer First], follow through with what we say we are going to do [Own It & Deliver] and propose solutions, not just issues [Challenge, Them Commit] to Win As A Team.

We are a remote-first company and we trust our people to get it done from the location that works for them.

What can you tell your friends when they ask you what you do?

As a DNA Team Data Engineer, I’ll be a key contributor to DNA team data services. I’ll help the DNA team to build and enhance internal processes of data production and transaction/transformation, as well as internal tools. And help colleagues from other teams and/or external clients to better experience the DNA team services.

You will be responsible for and take pride in…
• Exciting Projects using technical expertise across Python, SQL, Spark, DataBricks
• Build data pipeline across different data sources/databases such as AWS S3, PG database, and Snowflake
• Produce and maintain relevant documentation
• Support internal and external customers
• Becoming better at what you do every day

You should recognize yourself in the following…
• Bachelor’s degree in Computer Science, Engineering, or equivalent experience
• At least 5 years of related work experience in building data pipelines
• Strong skills in Python and PL/SQL
• Deep understanding and experience in building data pipelines across different data sources/databases such as AWS S3, PG database, and Snowflake
• Experience in data processing such as ETL
• Knowledge of machine learning and AI is preferred
• Familiarity with specific app markets (e.g.: Gaming, Entertainment, Finance, etc.) is a big plus
• Strong problem-solving, analytical, and troubleshooting skills
• A self-starter who identifies and solves problems before anyone has noticed
• Fluent in English, both written and oral

data.ai are in the process of establishing an entity in India, in the interim the employees will be on the rolls of Leap 29 our Global Employer of Record",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Danske Bank,Senior Data Engineer-ETL Datastage,"Experience 5-8Years

The ideal applicant should have the following skills:

- Strong technical experience in Data Warehousing and Experience in working with ETL tools (Datastage, Informatica etc) for the purpose of creating data marts for analytical purposes

- Strong understanding of relational database concepts & technology. Exposure to Big Data technologies is an added advantage.

- Strong analytical and problem solving skills with the ability to collect, organize, analyse and process large volumes of data in a complex environment

- Good written and verbal communication skills with the ability to communicate and articulate one's thought process clearly.

- Be self driven and work closely with business stakeholders, in a global environment, to gather enough context to translate the business
objective into an analytical solution.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Splunk,Data Engineer - 27516,"As a Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have a deep understanding of the architecture for enterprise-level data lake solutions using multiple platforms (RDBMS, AWS, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.

What you'll do: Yeah, I want to and can do that.
• As a Data Engineer, you will be responsible for engineering data pipelines for Splunk’s enterprise data platform, democratizing datasets, enabling advanced analytics capabilities, integrating data from various systems, and applications. You will work as part of an evolving Enterprise Data Management(EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. The role will collaborate closely with other specialists, Product Managers & key stakeholders across the company.
• Build large-scale batch and real-time data pipelines using the cloud data technologies, such as Snowflake, Matillion, Kubernetes, Python, Apache Airflow and Apache Kafka
• Serve as a resource for data management implementations on other technology teams and collaborate with data owners, business owners, and leaders.
• Supports the design and development of framework based data integration and interoperability across multiple Splunk Business applications.
• Advanced level skills in Python, SQL, data integration, data modeling and data architecture.

Requirements: I’ve already done that or have that!
• A minimum of 5 years of related experience
• 3+ years of experience as a Data Warehouse Architect or Data Engineer.
• 2+ years of experience driving adoption and building automation of data management services and tools.
• 2+ years of experience with API based ELT automation framework, data management, or interface design, development and maintenance.
• Large scale design, implementation and operations of Cloud data storage technologies such as AWS Redshift, Snowflake, Kubernetes, etc.
• 3+ years of experience with programming scripting and data science languages such as Python, SQL, etc.
• Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.
• Advanced knowledge of Big Data concepts in organising both structured and unstructured data

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Experience with Sales Operations, Partner Operations and customer success business processes and applications

Education: Got it!
• Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Verizon,Manager-Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

As a Manager for Data Engineering team, you will be managing data platforms and implementing new technologies and tools to further enhance and enable data science/analytics, focus to drive scalable data management and governance practices. Leading the team of data engineers & solutions architects to deliver solutions to business teams.
• Driving the vision with leadership team for data platforms enrichment covering the areas like Data Warehousing/Data Lake/BI across the portfolio.
• Defining and executing on a plan to achieve that vision.
• Building a high-quality Data engineering team and continue to drive to scale up.
• Ensuring the team adheres to the standard methodologies on data engineering practices.
• Building cross-functional relationships with Data Scientists, Data Analysts and Business teams to understand data needs and deliver data for insight solution.
• Driving the design, building, and launching of new data models and data pipelines.
• Driving data quality across all data pipelines and related business areas.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You are curious and passionate about Data and highly scalable data platforms. People count on you for your expertise in data management in all phases of the software development cycle. You create environments where teams thrive and feel valued, respected and supported. You enjoy the challenge of managing resources and competing priorities in a dynamic, complex and deadline-oriented environment. Building effective working relationships with other managers across the organization comes naturally to you.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Two or more years of experience in leading the team and tracking the end-to-end deliverables.
• Experience in end-to-end delivery of Data Platform Solutions and working on large scale data transformation.
• Knowledge of Spark, Hive, Scala, Pig, Kafka, Pulsar, Nifi, Python, Shell scripting.
• Knowledge of Google Cloud Platform/BigQuery.
• Knowledge of Teradata.
• Experience in working with DevOps tools like Bitbucket, Artifactory, Jenkins.
• Knowledge of Data Governance and Data Quality.
• Experience in building / mentoring the team.

Even better if you have one or more of the following:
• Master’s degree.
• Experience in data engineering, big data, hadoop and DevOps technologies.
• Certifications in any Data Warehousing/Analytical solutioning.
• Certification in program/project management.
• Experience in technical leadership in architecture, design, implementation and support of large-scale data and analytics solutions that are highly reliable, flexible, and scalable.
• Ability to meet tight deadlines, multi-task, and prioritize workload.
• Experience in collaborating with cross-functional teams and managing stakeholder expectations.
• Experience in working with globally distributed teams.
• Good Communication and Presentation skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
FairMoney,Senior Data Engineer,"About FairMoney

FairMoney is a credit-led mobile bank for emerging markets. The company was launched in 2017, operates in Nigeria & India, and raised close to €50m from global investors like Tiger Global, DST & Flourish Ventures. The company has offices in France, Nigeria, and India.

Role and responsibilities

At FairMoney, we are making a lot of data driven decisions in real time: risk scoring, fraud detection as examples.

Our data is mainly produced by our backend services, and is being used by data science team, BI team, and management team. We are building more and more real time data driven decision making processes, as well as a self serve data analytics layer.

As a senior data engineer at FairMoney, you will help building our Data Platform:

• Ensure data quality and availability for all data consumers, mainly data science and BI teams.
• Ingest raw data into our DataWarehouse (BigQuery / Snowflake)
• Make sure data is processed and stored efficiently:
• Work with backend teams to offload data from backend storage
• Work with data scientists to build a machine learning feature store
• Spread best practices in terms of data architecture across all tech teams
• Effectively form relationships with the business in order to help with the adoption of data-driven decision-making.

You will be part of the Datatech team, sitting right between data producers and data consumers. You will help building the central nervous system of our real time data processing layer by building an ecosystem around data contracts between producers and consumers.

Our current stack is made of

• Batch processing jobs (Apache Spark in Python or Scala)
• Streaming jobs (Apache Flink deployed on Kinesis Data Analytics - Apache Beam deployed on Google Dataflow)
• REST apis (Python FastApi)

Our tool stack

• Programming language: Python, SQL
• Streaming Applications: Flink, Kafka
• Databases: MySQL, DynamoDB
• DWH: BigQuery, Snowflake
• BI: Tableau, Metabase, dbt
• ETL: Hevo, Airflow
• Production Environment: Python API deployed on Amazon EKS (Docker, Kubernetes, Flask)
• ML: Scikit-Learn, LightGBM, XGBoost, shap
• Cloud: AWS, GCP

Requirements

You will work on a daily basis with the below tools, so you need working experience on

• Languages: Python and Scala.
• Big data processing frameworks: all or one of Apache Spark (batch/streaming) - Apache Flink (streaming) - Apache Beam.
• Streaming services: Apache Kafka / AWS Kinesis.
• Managed cloud services: one of AWS EMR / AWS Kinesis Data Analytics / Google Dataflow.
• Docker.
• Building REST APIs.

Ideally, you have experience with:

• deployment/management of stateful streaming jobs.
• the Kafka ecosystem: Kafka connects mainly.
• infrastructure as code frameworks (Terraform).
• architecture around data contracts: Avro Schemas management, schema registries (Confluent Kafka / AWS Glue).
• Kubernetes.

Overall experience required for this role: 6+ Years.

Benefits

• Training & Development
• Family Leave (Maternity, Paternity)
• Paid Time Off (Vacation, Sick & Public Holidays)
• Remote Work

Recruitment Process • A screening interview with one of the members of the Talent Acquisition team for 30 minutes.
• Takeaway assignment to be done at home.
• Technical design interview for 60-90 minutes.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,True,False,True,True,True
Boston Consulting Group,IT Senior Data Engineer,"WHAT YOU'LL DO
Under the general supervision of senior management and the Data Engineering Chapter Lead in the Enterprise Data Tribe, you will be working with key customers to deliver timely and accurate data engineering pipelines in a secure manner. You are expected to provide guidance on proper engineering design ensuring that our architectural guidelines are met, and the appropriate support model is in place for production deployments. This role will work in a multi-functional agile squad and support the product owner. You will also be supporting the Chapter Lead and other team members of the Data Engineering chapter in proof-of-concept activities and other Data Engineering chapter related work.
YOU'RE GOOD AT
You have experience in data warehousing, data modelling, and the building of data engineering pipelines. You are well versed in data engineering methods, such as ETL and ELT techniques through scripting and/or tooling. You are good in analysing performance bottlenecks and providing enhancement recommendations; you have a passion for customer service and a desire to learn and grow as a professional and a technologist.
• Viewed as subject matter expert for stakeholders, possessing in-depth knowledge and specialized technical skill set
• Able to work independently with minimal supervision
• Proactively identify and independently solve non-routine problems by applying expertise
• Perform research of viable technical and/or non-technical solutions
• Develop internal network with senior leaders within the chapter and key stakeholders in the tribe.
• Develop strategies for data engineering in Snowflake using DBT and Talend.
• Architect, design, and implement data pipelines to feed data models for subsequent consumption
• Actively monitor and resolve user support issues, working closely with your assigned squad and other squads as part of the chapter.
• Develop and maintain architectural standards, best practices, and measure compliance

YOU BRING (EXPERIENCE & QUALIFICATIONS)
You bring to us experience in data engineering technologies, database development, and data model design; both in IaaS and PaaS Cloud (AWS and/or Azure) environments.
• Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
• 5-7+ years of project experience, preferably as a Data Engineer/Developer and minimum of 3 years of agile project experience is a must (preferred tool - JIRA)
• Essential: Must have exposure to technologies such as DBT, Talend and Apache airflow
• Essential: SQL is heavily focused. An ideal candidate must have hands-on experience with SQL database design
• Essential: Extremely talented in applying SCD, CDC and DQ/DV framework
• Essential: Experience in data platforms: Snowflake, Oracle, SQL Server, PostgreSQL, and MySQL
• Essential: Lead R&D efforts to find solutions for data engineering requirements not addressed by existing technology standards
• Essential: Demonstrate ability to write new code i.e., well-documented and stored in a version control system (we use GitHub & Bitbucket)
• Essential: Develop metrics that illuminate the flow of data across the organization
• Essential: Experience in data modelling and relational database design
• Preferred: Experience in AWS and Azure data platforms.
• Preferred: Experience in Qlik Compose, Fivetran and HVR
• Preferred: Strong programming/ scripting skills (Python, Powershell, etc.)

YOU'LL WORK WITH
As part of the Enterprise Data Tribe, you don t have to fit into a mould at BCG. We seek people with strong drive, relentless curiosity, desire to create their own path, ability to work collaboratively, and the passion and leadership to make an impact. You ll collaborate on challenging projects with team members from many backgrounds and disciplines, increasing your understanding of complex business problems from diverse perspectives and developing new skills and experience to help you at every stage of your career. You ll be able to experience business on a genuinely global scale and learn how to bring together people from different cultures to uncover insights that challenge the status quo. As a member of the Product Engineering Group, you will work closely with a cross functional team that is collaborative, passionate and that holds themselves to a high standard.",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
General Mills,Data Engineer,":

India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.

Job Description:

Job Overview

The Enterprise Data Development team is responsible for designing & architecting solutions to integrate & transform business data into Data Lake to deliver data layer for the Enterprise using cutting edge technologies like Big Data - Hadoop. We design solutions to meet the expanding need for more and more internal/external information to be integrated with existing sources; research, implement and leverage new technologies to deliver more actionable insights to the enterprise. We integrate solutions that combine process, technology landscapes and business information from the core enterprise data sources that form our corporate information factory to provide end to end solutions for the business.

This position will develop solutions for the Enterprise Data Lake & Data Warehouse. You will be responsible for developing data lake solutions for business intelligence and data mining.

Job Responsibilities

70% of time Create, code, and support a variety of Hadoop, ETL & SQL solutions

Experience with agile techniques or methods

Work effectively in a distributed global team environment.

Works on pipelines of moderate scope & complexity

Effective technical & business communication with good influencing skills

Analyze existing processes and user development requirements to ensure maximum efficiency

Participates in the implementation and deployment of emerging tools and processes in the big data space

Turn information into insight by consulting with architects, solution managers, and analysts to understand the business needs & deliver solutions

20% of time Support existing Data warehouses & related jobs.

Job Scheduling experience (Tidal, Airflow, Linux)

10% of time Proactive research into up to date technology or techniques for development

Should have automation mindset to embrace a Continuous Improvement mentality to streamline & eliminate waste in all processes.

Desired Profile

Education:

Minimum Degree Requirements: Bachelors

Preferred Degree Requirements: Bachelors

Preferred Major Area of Study: Engineering

Experience:

Minimum years of Hadoop experience required: 2 years

Preferred years of Data Lake/Data warehouse experience: 2-4+ years

Total Experience required : 4-5 years

Specific Job Experience or Skills Needed

Skills Level: Beginner  Intermediate Expert  Advance

HDFS, Map reduce

Beginner

Hive, Impala & Kudu

Beginner

Python

Beginner

SQL, PLSQL

Proficient

Data Warehousing Concepts

Beginner

Other Competencies:
• Demonstrate learning agility & inquisitiveness towards latest technology
• Seeks to learn new skills via experienced team members, documented processes, and formal training
• Ability to deliver projects with minimal supervision
• Delivers assigned work within given parameter of time and quality
• Self-motivated team player and should have ability to overcome challenges and achieve desired results",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Fidelity India Careers,Lead - Software Engineering - Data Engineering,"Job Description:

Job Title – Lead Data Engineer [Data CoE]

The Purpose of This Role

At Fidelity, we use data and analytics to personalize incredible customer experiences and develop solutions that help our customers live the lives they want. As part of our digital transformation, we have significant investments to create innovative big data capabilities and platforms. One of them is to build various enterprise data lakes by gathering data across Business Units. We are looking for a hands-on data engineer who can help us design and develop our next generation, cloud enabled data capabilities.

The Value You Deliver
• You will be participating in end to end development which includes design, development, testing and deployment.
• You will be working closely with Technical Lead/Architects to ensure that solutions are consistent with IT Roadmap.
• You will be participating in technical life cycle processes, which include impact analysis, design review, code review, and peer testing.
• You will be participating in hands on development of application framework code in Oracle PL-SQL, pySpark, Python, NiFi, Informatica Power Center, along with Control-M and UNIX shell scripts.
• You will be troubleshooting and fixing any issues reported on data issues and performance.
• You will be presenting the findings and outcome to Senior Leadership teams and provide insights from the data to the business.
• You will be helping business teams optimize their current tasks and increase their productivity.

The Skills that are Key to this role

Technical / Behavioral
• You must be an expert in using SQL and PLSQL on Oracle or Netezza with UNIX shell scripting skills.
• You should be having working knowledge in Hadoop, HDFS, Hive, Spark, NoSQL DBs,
• Good knowledge on Python, JavaScript, Java and Scala
• You should have experience of using AWS services like RDS, EC2, S3, EMR and IAM to move data onto cloud platform
• Experience/Knowledge on Kubernetes, Containerization and building applications in Containers
• Knowledge of Logging, Telemetry and Data Security on AWS / Azure
• Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
• Experience of query tuning and optimization in one of the RBMS (oracle or DB2)
• You should be having experience in Control-M or similar scheduling tools.
• You should have proven analytical and problem-solving skills
• You should be strong in Database and Data Warehousing concepts.
• You must be able to work independently in a globally distributed environment
• You should have clear understanding of the business needs and incorporate these into technical solutions.

The Skills that is good to have for this role
• Experience in performance tuning and optimization techniques on SQL (Oracle and Netezza) and Informatica Power Center.
• Having strong inter-personal and communication skills including written, verbal, and technology illustrations.
• Having adequate knowledge on DevOps, JIRA and Agile practices.

How Your Work Impacts the Organization

Cloud Enablement and Data Model ready for Analytics.

The Expertise we’re looking for
• 3+[SE] / 7+ [Lead] years of experience in Data Warehousing, Big data, Analytics and Machine Learning
• Graduate / Post Graduate

Location: Bangalore , Chennai

Shift timings: 11:00 am - 8:00pm

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following:

For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2
For roles based in Ireland: Contact AccommodationsIreland@fmr.com
For roles based in Germany: Contact accommodationsgermany@fmr.com

Fidelity Privacy policy

Certifications:

Company Overview

At Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. Fidelity India has been the Global Inhouse Center of Fidelity Investments since 2003 with offices in Bangalore and Chennai. For information about working at Fidelity, visit India.Fidelity.com.

Fidelity Investments is an equal opportunity employer.",Bengaluru,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
EMERSON,Data Engineer - Sustainability,"JOB DESCRIPTION AS A PROFFESSIONALYOU WILL: Work closely with key stakeholders to understand business needs and translate them into technical requirements that would feed into developing effective data analytics solutions Design and implement end-to-end data solutions in collaboration with other technical and functional teams. Review and revise existing software development lifecycle andcode standards. Work closely with the data Architect onproduct roadmaps. Work on SharePoint and Power BI tools to manage, analyse and deduce data insights. Act as a point of escalation for complex operational issues to ensure optimal performance of analytics systems. WHO YOU ARE: You anticipate customer needs and provide services that are beyond customer expectations. You understand interpersonal and group dynamics and react in an effective manner. You encourage others to learn and adopt new technologies. You show a tremendous amount of initiative in tough situations and are exceptional at spotting and seizing opportunities. You promote high visibility of shared contributions to goals. REQUIRED EDUCATION, EXPERIENCE, & SKILLS: Bachelor's degree in Computer Science/Information Technology or equivalent Must have a minimum of 6+ years of experience in a Engineering role with experiences with: SharePoint Online and Power BI Experience in Visualization and Interpreting Data in various forms Technical expertise in data modelling, data mining, and segmentation techniques Experience with building new and troubleshooting existing data pipelines using Experience with batch and real-time data ingestion and processing frameworks Experience with languages such asPython andJava Knowledge of additional cloud-based analytics solutions Hands-on experience working on Linux and Windows systems Using Agile development methods Ability to work in a large, global corporate structure Ability to lead, manage and deliver large scale projects Advanced English level Demonstrated ability to clearly isolate and define problems, effectively evaluate alternative solutions, and make decisions in a timely manner Good decision-making ability, ability to operate in ambiguous situations, and high analytical ability to judge pros/cons of approaches against objectives PREFERRED EDUCATION, EXPERIENCE, & SKILLS: Expert level knowledge of data analytics and warehousing frameworks, including Snowflake and Cloud-based data integration solutions Experience with DevOps andCI/CD development practices Advanced level of software development knowledge",Chandigarh,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 24 2023

Come join us as we supercharge GSK’s data capability!

At GSK we are building a best-in-class data and prediction powered team that is ambitious for patients.

Scientific Digital and Tech’s goal is to power the discovery, development and supply of medicines and vaccines to patients. This means new tools to discover new medicines and vaccines, predictive capability for pre-clinical research, accelerated CMC and supply chain and an improved day-to-day laboratory experience for our scientists. Our Digital & Tech solutions will automate workflows and speed up decisions; freeing hands and releasing minds to focus on science.

As R&D enters a new era of data driven science, we are building a data engineering capability to ensure we have high quality data captured with context and aligned data models, so that the data is useable and reusable for a variety of use cases.

GSK R&D and Digital and Tech’s collective goal is to deliver business impact, including the acceleration of the discovery and development of medicines and vaccines to patients. The R&D Digital and Tech remit has expanded over the past 2 years, and to position GSK for the future, The change will strengthen R&D Tech, to provide more strategic impact, focus, accountability, and improved decision making in the use of Digital, Data and Analytics (DDA) to strengthen the pipeline.

Job Purpose

This role contributes to the construction of the development data fabric and data strategy. This role will interact with architects, engineers, data modelers, product owners as well as other team members in Clinical Solutions and R&D. This role will actively participate in creating technical solutions, designs, implementations & participate in the relentless improvement of R&D Tech systems in alignment with agile and DevOps principles.

The Data Engineer demonstrates both depth and breadth across key data engineering competencies e.g. Software Development, Testing, DevOps, Data Science/Analytics, and cloud. Can collaborate with experts from other subject domains. Primary responsibilities include using Azure cloud services and GSK data platform tools to ingest, egress, and transform data from multiple sources.

In addition, the role will demonstrate core engineering knowledge/experience of industry technologies, practices, and frameworks such as data fabric and scaling data platforms, containerization, cloud-based platforms, data analytics, machine learning, and data streaming. Examples of technologies include Java/C#/Python, Denodo, GIT, Azure Devops, Data Bricks, Presto, Spark, Azure Data Factory, ADLS V2, Kafka, Selenium, JUnit/NUnit, SAFe, Kanban, Docker, AI/ML, Azure/GCP Cloud Architecture including networking principles and scaling applications.

The Data Engineer, Clinical Solutions role is a senior technical role and will provide you the opportunity to lead key activities to progress your career. These responsibilities include the following:
• Working with other teams that are defining devops and data platform practices to meet the requirements of clinical solutions.
• Supporting engineering teams in the adoption and creation of data fabric best practices.
• Conducting PoCs of new technologies and helping to embed them in product teams
• Being part of a cutting-edge team creating the Development Data Fabric
• Ensures that technical delivery is fully compliant with GSK Security, Quality and Regulatory standards
• Ensures use of relevant R&D Tech / central services and collaborating with service partners in identification and delivery of service improvements
• Maintains best practices for engineering and architecture on our Confluence site. This requires hands on experience with cutting edge technology.
• Pro-actively engages in experimentation and innovation to drive relentless improvement
• Provides leadership, technical direction and GSK expertise to architecture and engineering teams composed of GSK FTEs, strategic partners and software vendors.

Why you?

Basic Qualifications:

Are you ready to work in an environment where you are continuously expected to work on projects with new technology and expected to use this technology to deliver real business value?

We are looking for professionals with these required skills to achieve our goals:
• Total 15+ years of experience and proficient with at least 3 of the below skills and can demonstrate knowledge and value with relevant experience in all the following competencies:
• Must have experience in Spark, Python and Databricks
• Software development, architecture design & technology platforms/frameworks
• Data Platforms and Domain-driven design
• Agile, DevOps & Automation [of testing, build, deployment, CI/CD, etc.]
• Data science (e.g. AI/ML), data analytics & data quality/integrity
• Testing strategies & frameworks
• Role requires:
• Demonstrated skill in delivering high-quality engineered data products
• Knowledge of industry standards and technology platforms aligned to GSK and R&D roadmaps
• Excellent communication, negotiation, influencing and stakeholder management skills
• Customer focus and excellent problem-solving skills
• Computer Science or related bachelor’s degree – MS in Computer Science is preferred
• Familiarity and use of various open-source ecosystems including JavaScript, Bigdata, java, python etc.
• Good understanding of various software paradigms: domain-driven, procedural, data-driven, object-oriented, functional
• Familiar with .Net Core (C#), Java, Python
• Demonstrable knowledge depth in more than one area of software engineering and technology

Preferred Qualifications:

If you have the following characteristics, it would be a plus:
• Experience in agile software development and DevOps, relevant technology platforms [e.g., Kubernetes] and frameworks [e.g. Docker] including cloud technologies & data structures (i.e. information management), data models or relational database design
• Subject matter expertise in clinical development
• R&D Tech requires Engineers with understanding of the relevant technical and scientific domains. Able to deliver continuous change to meet rapidly evolving R&D strategy and ambition.
• Experience with agile development methods, with security strategies and best practices, data integration mechanisms, architectural design tools, delivering and integrating COTS applications, areas of Service Oriented Architecture (SOA), Application Integration, Business Process Management and Data Quality.
• Experience in applying AI/ML, data curation, virtualization, predictive modelling, workflow, and advanced visualization techniques to enable decision support across multiple products and assets to drive results across R&D business operations.

At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.
#LI-GSK

GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in “gsk.com”, you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,True,False,False,True,False,False,True,True,False,False,False,False,False,False,False,False
Bloom Consulting Services,Data Engineer,"Data Engineer ( Job ID : 815310498 )

data engineer

NA

Contract

Experience

06.0 - 08.0 years

Offered Salary

10.00 - 14.00

Notice Period

Not Disclosed

Job Description

Total Experience6 to 8 years

Min Relevant Experience: 3 to 5 years

Location :Bangalore

JD: Data Engineer

Role Description:

In this role, you will be part of a growing, global team of data engineers, who collaborate in DevOps mode, in order to enable business with state-of-the-art technology to leverage data as an asset and to take better informed decisions.

The Life Science Data Engineering Team is responsible for designing, developing, testing, and supporting automated end-to-end data pipelines and applications on Life Science’s data management and analytics platform (Palantir Foundry, Hadoop and other components).

The Foundry platform comprises multiple different technology stacks, which are hosted on Amazon Web Services (AWS) infrastructure or own data centers. Developing pipelines and applications on Foundry requires:
• Proficiency in SQL / Java / Python (Python required; all 3 not necessary)
• Proficiency in PySpark for distributed computation
• Familiarity with Postgres and ElasticSearch
• Familiarity with HTML, CSS, and JavaScript and basic design/visual competency
• Familiarity with common databases (e.g. JDBC, mySQL, Microsoft SQL). Not all types required

This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.

Roles & Responsibilities:
• Develop data pipelines by ingesting various data sources – structured and un-structured – into Palantir Foundry
• Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application
• Acts as business analyst for developing requirements for Foundry pipelines
• Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline
• Document technical work in a professional and transparent way. Create high quality technical documentation
• Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)
• Deploy applications on Foundry platform infrastructure with clearly defined checks
• Implementation of changes and bug fixes via change management framework and according to system engineering practices (additional training will be provided)
• DevOps project setup following Agile principles (e.g. Scrum)
• Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java
• Work closely with business users, data scientists/analysts to design physical data models

Education
• Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields

Professional Experience
• 5+ years of experience in system engineering or software development
• 3+ years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.

Required Knowledge, Skills, and Abilities

Data engineer",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"• Experience with Azure Data Bricks, Data Factory
• Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics
• Experience in Python/Pyspark/Scala/Hive Programming.
• Experience with Azure Databricks/ADB
• Good understanding of SQL queries, joins, stored procedures, relational schemas
• Experience with NoSQL databases, such as HBase, Cassandra, MongoDB",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Genpact,Data Engineer,"With a startup spirit and 90,000+ curious and courageous minds, we have the expertise to go deep with the world's biggest brands--and we have fun doing it! We dream in digital, dare in reality, and reinvent the ways companies work to make an impact far bigger than just our bottom line. We're harnessing the power of technology and humanity to create meaningful transformation that moves us forward in our pursuit of a world that works better for people.

Now, we're calling upon the thinkers and doers, those with a natural curiosity and a hunger to keep learning, keep growing. People who thrive on fearlessly experimenting, seizing opportunities, and pushing boundaries to turn our vision into reality. And as you help us create a better world, we will help you build your own intellectual firepower.

Welcome to the relentless pursuit of better.

In this role, resource will be expert in designing, building and maintaining data infrastructure. Work will help people with unmet medical needs, including those who wish to quit smoking, those with major depression disorder, and those with schizophrenia--ultimately improving lives through engineering. Help design and build a data infrastructure using state-of-the-art technologies with data security at utmost importance and employ elegant solutions to help ensure Client's data products meet compliance needs (e.g., GDPR and HIPAA) in different regions of the world.

Responsibilities!
• Design, build and maintain analytical data infrastructure which includes both data processing and data reporting.
• Onboarding data from both internal and external systems.
• Collaborate with Product, Engineering, Science, Data analysts and Data scientists to implement rich and re-usable datasets/metrics.
• To make data infrastructure and applications scalable, reliable, and secure.
• Strong attitude towards automating routine tasks via coding/scripting.
• Research on security and privacy requirements and provide solutions.

Qualifications we seek in you!
• B Tech/M Tech/BCA/MCA
• Experience in building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience writing complex, highly optimized SQL queries.
• Experience with reporting to enable explanatory and exploratory analytics.
• Python development experience.
• Have experience with dbt, Airflow, Snowflake and AWS infrastructure.
• Have experience implementing APIs to share data with internal / external vendors.
• Experience implementing streams.
• Understanding of privacy and security regulations (e.g., GDPR, HiTrust, HIPA)",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
Vanderlande Careers,Lead Data Engineer,"Lead Data Engineer at DSF

Vanderlande provides baggage handling systems for 600 airports around the globe, capable of moving over 4 billion pieces of baggage around the world per year. For the parcel market our systems handle 52 million parcels per day. All these systems generate data. Do you see a challenge in building data-driven services for our customers using that data? Do you want to contribute to the fast growing Vanderlande Technology Department on its journey to become more data driven? If so, then join our Digital Service Factory team!

Your Position

As a lead data engineer you will be leading the data engineering efforts in a product team. You will work together with product/solution architecture to provide technical necessities to design and develop end-to-end data ingestion pipelines and well tested and monitored data services. You will assess the technical dependency between different functional components and define a resolution. You will also provide technical guidance and coaching to the junior/medior data engineers in the team, set technical standards and best practices.

Your responsibilities:
• You will be designing, developing, testing, and documenting the data collection framework. The data collection consists of (complex) data pipelines with data from (IoT) sensors and low/high level control components to our Digital Service and Data Science platform.
• You will build monitoring solutions for data pipelines which enable data quality improvement.
• You will develop scalable data pipelines to transform and aggregate data for business use, following software engineering and Data Mesh best practices. For these data pipelines you will make use of the best and most applicable frameworks available for data processing.
• You develop our data services and data products for customer sites towards a product, using (test & deployment) automation, componentization, templates, and standardization to reduce delivery time of our projects for customers. The product provides insights in the performance of our material handling systems at customers all around the globe.
• You design and build a CI/CD pipeline, including (integration) test automation for data pipelines. In this process you strive for an ever-increasing degree of automation and high levels of security.
• You will work with infrastructure engineers to extend storage capabilities and types of data collection (e.g. streaming)
• You have experience in developing APIs.
• You will coach and train the junior data engineer with the state of art big data technologies.
• You will lead the Data Engineering Guild where passionate members discuss current trends, short term development, and solutions for ongoing issues that span multiple teams.

Your Profile
• Total experience of 10+ years (with at least 7+ years of programming exp)
• Experience programming in Python and/or Scala (Java programming exp is a plus)
• You are familiar with DevOps practices and have relevant experience in automation (CI/CD), measurement, applying lean practices and what DevOps culture entails
• You know how to achieve high performing secure pipelines, maintain and test them
• You are familiar with different storage formats (e.g. Azure Blob, SQL, noSQL)​
• Experience with scalable data processing frameworks (e.g. Spark)​
• Experience with event processing tools like Splunk or the ELK stack​
• Deploying services as containers (e.g. Docker and Kubernetes)​
• You have experience with streaming data platforms (e.g. Kafka )​ and messaging formats (e.g. Apache AVRO)
• Strong experience with cloud services (preferably with Azure)

Diversity & Inclusion

Vanderlande is an equal opportunity employer. Qualified applicants will be considered without regards to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Pune,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Visa,Sr. Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.
Job Description

This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. You will be an integral part of the Payment Products Development team focusing on design and development of software solutions that leverage data to solve business problems. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and testing of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Responsible for the design, development, and implementation
• Work on development of new products iteratively by building quick POCs and converting ideas into real products
• Design and develop mission-critical systems, delivering high-availability and performance
• Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology
• Develop code to ensure deliverables are on time, within budget, and with good code quality
• Have a passion for delivering zero defect code and be responsible for ensuring the team's deliverables meet or exceed the prescribed defect SLA
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
• Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.
Qualifications

We are seeking team members that are passionate, visionary and insatiably inquisitive. Successful candidates frequently have a mix of the following qualifications:

• Bachelor’s Degree or an Advanced Degree (e.g. Masters) in Computer Science/ Engineering, Information Science or a related discipline
• Minimum of 3 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies
• Extensive experience with SQL and Big Data technologies (Hadoop, Java, Spark, Kafka, Hive, Python) for large scale data processing and data transformation
• Deep knowledge of Unix/Linux
• Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
• Familiar with software design patterns
• Experience working in an Agile and Test-Driven Development environment
• Strong knowledge of API development is highly desired
• Strategic thinker and good business acumen to orient data engineering to the business needs of internal and external clients
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
• Previous exposure to financial services is a plus, but not required
Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
Shell,"Senior Data Engineer- Azure (ADF, Data lake)","Join the number One Global Lubricants supplier in the world and be part of the team that helps in shaping up the digital and the IDT strategy which delights our customers in over 100 countries across every sector.

If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

Where you fit

Shell's Projects and Technology (P&T) business exists to make the delivery of our strategies and the growth of our company possible. Our team develops the advanced products and technologies Shell needs to meet customer demand. Our solutions help our partners grow the LNG, Gas and Power businesses, deepen the integration of Manufacturing, Chemicals and Trading, and maximise the competitiveness of our Upstream business.

What's the role?

As a Data Engineer in Shell, you will create and maintain optimal data pipeline architecture and also will a ssemble large, complex data sets that meet functional / non-functional business requirements.

You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

More specifically, your role will include:
• Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.
• Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

What we need from you

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree and at least have a Seniority level in their previous workplace.

They should also have experience using the following software/tools:
• Experience with Azure: ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.
• Experience with relational SQL/NoSQL databases, file handlings and API integrations
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODS

Skills - Nice to Have
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing data pipelines using ADF
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of transforming, processing and extracting value from large disconnected datasets
• Strong team player with organizational and communication skills
• Experience supporting and working with cross-functional teams in a dynamic environment",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fibe India,Data Engineer - SQL,"Responsibilities:
• The candidate is expected to lead one of the key analytics areas end-to-end. This is a pure hands-on role.
• Ensure the solutions are built to meet the required best practices and coding standards.
• Ability to adapt to any new technology if the situation demands.
• Requirement gathering with business and getting this prioritized in the sprint cycle.
• Should be able to take end-to-end responsibility for the assigned task
• Ensure quality and timely delivery.

Requirements:
• Experience: 3- 6 years.
• Strong at PySpark, Python, and Java fundamentals
• Good understanding of Data Structure
• Good at SQL query/optimization
• Strong fundamental of OOPs programming
• Good understanding of AWS Cloud, Big Data.
• Nice to have Data Lake, AWS Glue, Athena, S3 Kinesis, SQL/NoSQL DB",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer,"Role: Data Engineer Job Description
• Design, build, and maintain distributed batch and real-time data pipelines and data models.
• Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
• Be curious and eager to work across a variety of engineering specialties (i.e., Data Science, and Machine Learning to name a few).
• Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
• Enforce privacy and security standards by design.
• Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Qualifications
• +3 years experience building complex data pipelines and working with both technical and business stakeholders.
• Experience in at least one primary language (e.g., Java, Scala, Python) and SQL (any variant).
• Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
• Experience creating and maintaining ETL processes.
• Experience designing, building, and operating a DataLake or Data Warehouse.
• Experience with DBMS and SQL tuning.
• Strong fundamentals in big data and machine learning.

Preferred Qualifications
• Experience with RESTful APIs, Pub/Sub Systems, or Database Clients.
• Experience with analytics and defining metrics.
• Experience with measuring data quality.
• Experience productionalizing a machine learning workflow; MLOps
• Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
• Language ability in Japanese and English is a plus (We have a professional translator but it is nice to have language skills).
• Experience with AWS services.
• Experience with microservices.
• Knowledge of Data Security and Privacy.

experience

6",Hyderabad,True,False,True,True,False,False,False,False,False,False,False,False,True,True,False,False
deloitte,Consulting - BO - Cloud Engineering - Manger - Azure Data Engineer,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organisations, Deloitte is where you will find numerous opportunities to succeed and realise your full potential.

The team

Deloitte is working with global customers on cloud technologies to help unlock growth, stability, and sustainability by enabling them to spot unseen business trends through curation, transformation, and blending of data. In our endeavors for continued expansion, we’re searching for like-minded individuals to help us ‘take it to the next level’.

In this exciting opportunity for an experienced developer, you will join a team delivering a transformative cloud hosted data platform for some of the world’s biggest organizations. The candidate we seek, needs to have a proven track record in implementing data ingestion and transformation pipelines on Microsoft Azure. Deep technical skills and experience with working on Azure Databricks. Familiarity with data modelling concepts and exposure to Synapse.

You will also be required to participate in stakeholder management, highlight risks, propose deliver plans and estimate for time and team size based on requirements. Hence, adequate levels of communication skills and relevant experience in handling such situations is desired.

Scope of work

Your main responsibilities will be:
• Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
• Delivering and presenting proofs of concept of key technology components to project stakeholders.
• Developing scalable and re-usable frameworks for ingesting and enriching datasets
• Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
• Working with event based / streaming technologies to ingest and process data
• Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
• Evaluating the performance and applicability of multiple tools against customer requirements
• Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.

Qualifications
• Strong knowledge of Data Management principles
• 9+ years of total years of experience
• Experience in building ETL / data warehouse transformation processes
• Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably Databricks).
• Experience using Apache Spark and associated design and development patterns
• Microsoft Azure Big Data Architecture certification is an advantage.
• Hands-on experience designing and delivering solutions using Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
• Experience with Apache Kafka / Nifi for use with streaming data / event-based data (Nice to have but not mandatory)
• Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)
• Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
• Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Terraform etc.

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters.

In addition to living our purpose, managers across our organisation:
• Develop self by actively seeking opportunities for growth, share knowledge and experiences with others, and act as a strong brand ambassadors
• Understand objectives for clients and Deloitte, align own work to objectives and set personal priorities
• Seek opportunities to challenge self
• Collaborate with others across businesses and borders to deliver and take accountability for own and team results
• Identify and embrace our purpose and values and put these into practice in their professional life
• Build relationships and communicate effectively in order to positively influence peers and other stakeholders

Professional growth

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn.From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Our Purpose

Deloitte is led by a purpose: To make an impact that matters.

Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and work—always striving to be an organisation that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description

insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.

Job Description
• Develops and maintains scalable data pipelines for bulk data movement between systems of record and systems of reference
• Develops and maintains scalable application to application integrations
• Aligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
• Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes
• Writes appropriate unit or integration tests to implement test-driven development
• Continually contributes to and enhances data team documentation
• Performs data analysis required to troubleshoot and resolve data related issues
• Works closely with a team of frontend and backend engineers, product managers, and analysts
• Defines company data assets, artifacts and data models

Qualifications

Required qualifications:
• 5 years of Data Engineering and Data Integration
• 5 Years of Data Warehousing
• 3 Years of Data Architecture and Modeling
• 2 years of Cloud Data Engineering
• Agile Methodologies

Preferred skills:
• AWS or Azure Data Certifications
• Experience with databricks, spark, python
• Experience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)
• Experience with Salesforce

Additional Information

All your information will be kept confidential according to EEO guidelines.
• * At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. **

insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Northern Tool + Equipment, India",Senior Data Engineer,"Are you an individual who wants to play a game changing role and make an impact in a fast-growing organization? We at Northern are waiting for you. Join us and unleash your potential!!

We are hiring <>!!

Join the core group of founding members at the NTE India to build an organization from the ground up.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Role Objective

PRIMARY OBJECTIVE OF POSITION:

We are looking for an Experienced Data Engineer who will partner with a specific business function and understand the requirements, builds data model, creates data pipelines and stored procedures. Also work with Data Analysts/Modelers, Data Visualization Engineers to deliver high performing analytics.

MAJOR AREAS OF ACCOUNTABILITY:
• SME for data structures and data models for specific line of business.
• Analyze and understand various source systems and related data structures.
• Build and automate creation of ETL pipelines and stored procedures to move data from source system to consumption layer using variety of ETL methods.
• Collaborate with Data Analysts/Data Architect/Data Visualization Engineers to provide them with Data mapping documents and ensure adherence to a common data model.
• Responsible for administration and security of data and analytics assets in Azure.
• Works collaboratively and effectively communicates with others across departments in order to perform and complete necessary tasks and projects.
• Follows established Software Development Life Cycle (SDLC) to enable CI/CD in relevant areas.
• Follow established change control, release management and incident management processes.
• Responsible for performance and tuning, scaling of Azure resources to optimize costs
• Builds and maintains relationships cross-functionally in order to stay current with the needs and operations of the business functional areas supported.
• Supports the day-to-day operation of the reporting and analytic solutions by troubleshooting ETL and other errors encountered during data processing.
• Keeps manager informed of important developments, potential problems, and related information necessary for effective management. Coordinates and communicates plans and activities with others, as appropriate to ensure a coordinated work effort and team approach.

Job Description

Performs related work as apparent or assigned.

QUALIFICATIONS:
• To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
• Bachelor’s Degree in Computer Science, Statistics, Mathematics, Business or related field.
• At least 6 years relevant work experience in Data and Analytics field.
• In-depth understanding of Data warehousing concepts.
• Hands-on experience in writing complex, highly optimized SQL queries across large data sets
• Hands-on experience in building performance optimized data pipelines (ETL/ELT)
• Experience in configuring, deploying, and provisioning of IaaS, PaaS with Terraform and PowerShell using Azure DevOps and GIT.
• Specific familiarity with the Microsoft Azure Data Stack - ADF, Azure SQL DB, Azure Synapse (SQL Data Warehouse), Azure Data Lake, Azure Storage and Analysis Services.
• Azure Security & Identity: Azure Active Directory App Permissions, Key Vaults.
• Hands-on experience in creating user groups, creating security policy and implementation of Row-Level Security(RLS) to restrict the data access to the users.
• Hands-on experience with data cataloging and data profiling concepts.
• Diversity of perspective for various tools and technologies like Azure Stream Analytics, Azure Databricks, NoSQL databases, read or write optimized databases to advocate for their appropriate adoption at Northern Tool.
• Basic programming experience using .NET, Python, or any scripting language.
• Must be willing to work as a team and possess the skills to work independently.
• Demonstrated ability to take initiative and utilize creativity on assigned projects.
• Must possess strong analytical, problem-solving, and technical design skills.
• Demonstrates Northern Tool + Equipment’s 12 Core Competencies.
• Sounds interesting? Here’s your chance to join our family at Northern.

About the Company

Northern Tool + Equipment is a retailer and manufacturer that specializes in offering superior quality tools at great prices, along with the knowledge and support needed to help customers get the job done right.

They’ve been in business for over 40 years, recently reaching revenues over $1.5 billion. The company not only supplies over 100,000 tools from the top brands in the industry but also designs, manufactures, and tests an extensive lineup of premium private label products that customers can’t get anywhere else.

Northern Tool’s far-reaching customer base includes handy men and women, weekend hobbyists, serious do-it-yourselfers, full-fledged contractors, trade professionals, and more. The company’s products can be found in over 120 retail stores in the USA, on its comprehensive international website, and via numerous catalogs throughout the year. Recently Northern Tool has expanded operations to offices in India to serve its global distribution better.

We are recently named as one of the Top Workplaces for MidSize Employers by Forbes in the US.

We have also been recognized as the “Top GCC to work for in AI and analytics” and our India HR team as the “Top HR Professionals in AI and Analytics” by 3AI which is a professional firm associated with analytics within India.

About NTE India

Northern Tool is making a significant investment in business transformation. We are committed to providing our customers with an exceptional experience. The team in India will enable Northern Tool to expand its internal capabilities in Finance, Merchandising, Product Engineers, Manufacturing Ops, Marketing, Contact Center, and Information Technology.

Why Northern?

True Northern: We know that our strength is our people. The distinct abilities they bring into the system are the key to our success. We seek talented people who wish to share their initiative, ideas, and expertise; we develop and support our teams, and we put them in a position to succeed. We know our customer; we provide value, and we act with integrity. We are True Northern.

Build Lasting Relationships: At Northern Tool + Equipment, we’re far more interested in building relationships than we are in simply making transactions. Our purpose is building a long-lasting relation with our customers and employees.

We care for our customers, employees and society. Our customer base is exceptionally loyal because customers know that we will give them the right solution.

Accelerate Decision Making: by collaborating with the brightest minds, bring ideas to life across our value chain of business operations across our vast network of over 120 stores across the US.

Lead with Innovation: Join us to elevate our customer experience?with cutting-edge products, technology, and business processes and?drive our business forward.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Does this sound interesting?? Be an early applicant!!

Northern Tool is an Equal Opportunity Employer. We encourage and empower everyone and support diversity in experience, and point of view. We are pledged to a fair and a transparent hiring process with no discrimination of race, color, ancestry, religion, gender, national origin, age, citizenship, marital status, disability, or veteran status.

Requirements
• name : Northern Tool + Equipment, India
• location : Hyderabad, IN
• experience : 6 - 9 years
• employmentType : Full-Time
• Primary Skills: ETL or ELT,Python,Data Warehousing,Azure Data Lakes or Data Factory,SQL",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Comcast India Engineering Center I, LLP",Data Engineer 3,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary About Sky We’re Sky, Europe’s biggest entertainment brand. Think top-quality shows. Breaking news. Innovative tech. Must-have products. Careers here mean the freedom and support you need to make an impact – pushing boundaries, creating solutions, hitting targets. And as part of our close-knit team, you’ll enjoy plenty of benefits. Plus, experiences you’ll only find at Sky. We love telling the world we work at Comcast . We’re fans too. We move fast and embrace pace. We have the freedom to be brilliant. And we work collaboratively because together we can. This is how we work at Comcast and why we love it. Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Documents all development activity. Assists with training non-technical personnel. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as a resource for colleagues with less experience. Job Description Core Responsibilities Create and maintain an optimal data pipeline architecture focussed upon network data, including real-time and batch data sources. Assemble large, complex data sets that meet functional and non-functional business requirements. Build batch/streaming ELT/ETL solutions from a wide variety of data sources in varying formats (SQL, JSON, AVRO, HTTP, API, etc.) using the right blend of tools. Keep our data compliant, relevant and secured across multiple data centres and regions. Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery and evolving current solutions whilst ensuring continuity of service. Create data tools for data scientist team members that assist them in building and optimizing into an innovative industry leader. Guide and collaborate with data consumers on analytics, tooling and platform related queries. Employees at all levels are expected to: Graduate degree BSc in Computer Science, Electrical Engineering or similar. Strong communications skills. SQL knowledge and experience working with relational databases. Strong analytic skills related to working with structured and unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Hands-on experience in building scalable data platforms. Awareness of security practices and privacy concerns when working with data across both in-house and cloud platforms. Ideally an awareness of network technologies and concepts or an insatiable desire to learn. Key technologies Apache Airflow / NiFi / Kafka / ZooKeeper Confluent ecosystem (Connect / Schema Registry / ksqlDB) GCP (BigQuery, Dataflow, Pub/Sub, IAM) Linux / Terraform / Ansible Python / Docker (Nice to have). Experience: 5 Years to 7.5 Years Location: Chennai Disclaimer: This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. Comcast is an EOE/Veterans/Disabled/LGBT employer. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools that are personalized to meet the needs of your reality—to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the benefits summary on our careers site for more details. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Certifications (if applicable) Relative Work Experience 5-7 Years Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. At Comcast , you have the power to connect the world. Your career options are endless as you grow in your career. Explore your future with access to a variety of teams, locations, and resources in an expanding network. You can also explore additional opportunities at our company, NBCUniversal.",,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
Revolo Infotech,Data Engineer - SQL/Python,"Job Description :

- Design, develop, and maintain data pipelines and architecture for data storage, processing, and analysis

- Work with cross-functional teams to understand and implement data requirements

- Build and optimize data pipelines using various cloud-based technologies such as AWS, Azure, or Google Cloud Implement data visualization solutions using cloud-based tools such as Tableau, Power BI, or Looker Monitor and troubleshoot data pipeline issues, and implement solutions to improve performance and scalability

- Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis

- Stay up-to-date with the latest technologies and industry trends in data engineering and data visualization

Requirements :

- 2+ years of experience as a data engineer with a focus on cloud-based data pipelines and visualization

- Strong experience with cloud-based technologies such as AWS, Azure, or Google Cloud

- Experience with data visualization tools such as Tableau, Power BI, or Looker

- Strong knowledge of SQL and programming languages such as Python or Java

- Familiarity with big data technologies such as Hadoop, Spark, or Hive

- Strong problem-solving and analytical skills

- Experience working in an Agile development environment Bachelor's degree in Computer Science or related field.

Preferred Qualifications :

- Experience with data warehousing concepts and technologies

- Experience with data governance and data management best practices.

- Experience with machine learning and AI technologies Strong communication and teamwork skills.

Job Types : Full-time, Regular / Permanent, Contractual / Temporary

Salary : 1,000,000.00 - 1,200,000.00 per year

Benefits :

- Health insurance

- Internet reimbursement

- Paid sick time

- Paid time off

Schedule :

- Day shift

- Monday to Friday

Power BI: 2 years (Preferred)

Tableau: 2 years (Preferred)

AWS: 2 years (Preferred)
(ref:hirist.com)",Navi Mumbai,True,False,True,True,False,False,False,False,True,True,False,False,False,False,False,False
MediaMath,Data Engineer,"About Us

MediaMath is the leading technology pioneer on a mission to make advertising better. We deliver outstanding results through powerful ad tech, partnership and a curiosity for what’s next. We help more than 3,500 advertisers solve complex marketing problems so they can deepen their customer relationships across screens and around the world.

Key Responsibilities

MediaMath’s Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by Analytics to discover and deliver insights that drive value for our clients. The Analytics team fulfils customers’ advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. As the Data Engineer on the Analytics Engineering team within the Analytics team, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:
• Become an expert in MediaMath data flows and the Analytics data infrastructure.
• Build, maintain, and own scalable data pipelines to support client data integration.
• Become a team SME in data munging and automated ETL processes.
• Work with Analysts to understand and leverage big data to solve client problems and needs.
• Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
• Be a team player, and bring the team and company forward by solving team and company priorities.

You are:
• Experienced in writing readable, re-usable code SQL and Python (our entire team uses Jupyter Notebook and Pandas!)
• Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
• Experienced in implementing data pipeline health monitoring, alerting
• Experienced with data infrastructure troubleshooting and working with system logs
• Experienced developing data flow schematics/blueprints
• Advocate for automation and building efficient, scalable solutions
• Self-driven, with a hunger to learn and spread knowledge by teaching others
• Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:
• Bachelor’s Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
• 3 - 5 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
• Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
• Experience partnering with client-facing teams to understand client needs and translate them to technical requirements

Nice-to-have’s:
• Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)
• Experience working with REST APIs, web services, object-oriented technologies like Java, C++
• Public GitHub repos or notebooks that illustrate the way you think about data
• Exposure to ad-tech, digital marketing, or e-commerce industries

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are three values that provide a framework for how we approach our work and the world: Win Together, Obsess Over Growth, and Do Good, Better. These values inform how we energize one another and engage with our clients. They get us amped to come to work.

Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath is recognized as a Leader in the Gartner 2020 Magic Quadrant for Ad Tech and has won Best Account Support by a Technology Company for two years in a row in the AdExchanger Awards.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristic as established by law.

MediaMath focuses on Digital Media, Internet, Advertising, Software, and Marketing. Their company has offices in New York City, San Francisco, Chicago, Durham, and Singapore. They have a large team that's between 501-1000 employees. To date, MediaMath has raised $617.877M of funding; their latest round was closed on July 2018.

You can view their website at http://www.mediamath.com or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,False
"Atlassian, Inc.",Senior Data Engineer,"Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.",,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
Motilal oswal,Data Engineer,"Job Description : Strong AWS Data Engineering skills. Exposure to SSIS, SSRS, SSAS will be an advantage,Handson experience working with S3, Redshift, Glue, EMR, RDS, Athena, Aurora,Strong development skills and experience coding with SQL, Pyspark, Python,High on ownership and accountability,Comfortable with change, initial hiccups and small failures,Experience with understanding designs, creating low level designs, unit test cases, unit testing and assisting with Integration and User acceptance testing,Experience of 2-6yrs with AWS Data technologies.",,True,False,True,False,False,False,False,False,False,False,True,False,True,False,False,False
Fisker Inc.,Data Engineer,"Responsibilities
• Work with leaders, engineering and data scientists to understand data needs.
• Design, build and launch efficient and reliable data pipelines to best utilize connected vehicle data for real-time systems and within data warehouses.
• Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
• Help insure that best practices are followed when storing, retrieving and accessing data.

Qualifications
• 3+ years of Python development experience.
• 3+ years of SQL experience.
• 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
• 3+ years experience with Data Modeling.
• Experience in organizing queries, tables and pipelines with proper indexing, partition and sharding.
• 3+ years experience in custom ETL design, implementation and maintenance.
• Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Clickhouse, Spark, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience with more than one coding language, ideally Go or C++ and java.
• Experience with designing and implementing real-time pipelines.
• Experience with data quality and validation.
• Experience with SQL performance tuning and E2E process optimization.
• Experience with notebook-based Data Science workflow.
• Experience with Airflow.
• Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,True,True,False
Poshmark,"Software Developer, Data Engineering","The Big Data team is a central player in the Poshmark organization. Our mission is to build a world-class big data platform to bring value out of data for us and for our customers. Our goal is to democratize data, support exploding business, provide reporting and analytics self-service tools, and fuel existing and new business critical initiatives.

The Data Engineering team at Poshmark is looking for an experienced software engineer to take care of Poshmak’s growth data, ensuring real-time access to quality data for all the stakeholders. The role requires strong understanding of software engineering best practices and excellent software development skills to build and maintain real-time and batch data pipelines with a focus on scalability and optimizations. In addition, the role also requires collaborating with Data Science, Analytics and other Engineering teams to build newer ETLs analyzing terabytes of data.

The role also requires being able to write clean and scalable code to pull datasets from disparate sources involving External APIs, S3 transfers, Web Scraping. You will work with cutting edge technologies and frameworks like Scala, Ruby, Apache Spark, Airflow, Redshift, Databricks, Docker. You will also manage the growth data infrastructure comprising ETL pipelines, Hive tables, Redshift tables, BI tools. We are looking for a software engineer who can help us define the next phase of growth data systems in terms of scalability and stability.

Responsibilities
• Design, Develop & Maintain growth data pipelines and integrate paid media sources like Facebook and Google to drive insights for business.
• Build highly scalable, available, fault-tolerant data processing systems using AWS technologies, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 100s of terabytes of data ingested every day and a petabyte-sized data warehouse.
• Responsible for architecting/designing/developing critical data pipelines at Poshmark.
• Productionizing ML models in collaboration with the Data Science and Engineering teams.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.
• Participate and contribute to constantly improving best practices in development.

Desired Skills & Experience
• Excellent technical problem solving using data structures and algorithms, with emphasis on optimization and code quality.
• 1-3 years of relevant software engineering experience using object oriented programming languages like Scala / Java / Ruby / Python / C++ etc.
• Expertise in architecting and building large-scale data processing systems using Big Data technologies like Spark, Hadoop, EMR, Kafka/ Kinesis, Flink, Druid.
• Expertise in SQL with knowledge on any existing data warehouse technology like Redshift
• Expertise in Google Apps Script, Databricks or API Integrations is a plus.
• Be self-driven, take complete ownership of initiatives, make pragmatic technical decisions and collaborate with cross-functional teams.",Chennai,True,False,True,True,True,True,False,True,False,False,False,True,True,False,True,False
Confidential,Data Engineer - AWS/ETL,"Role and responsibilities :- The Data Engineer will be responsible for leading design, development, transformation, deployment, and maintenance of Data Warehousing stack on AWS- Work with BI and dev team to build data pipelines using AWS Glue and similar tools.- Develop custom data ingestion jobs and ETL scripts using Python/Spark scripts- Perform data modelling and schema design activities in Data Lake and Data Warehouse environments as per the standard practices- Advanced SQL knowledge and experience working with relational databases, able to write/debug complex SQL queriesYour profile must have :- 4+ years of experience in building data pipelines and data warehouse architectures on cloud platforms such as AWS- Exposure to agile methodology- Experience in developing Python or Spark jobs- Strong understanding of data modelling principles- Good communication and collaborative skillsExtra points if you have :- Built processes supporting data transformations, data structures and workload management in Database/Data Warehouse- Experience in performance tuning of Redshift databases and implement recommendations- Experience with sourcing data using APIs from external systems- Experience working with teams across the globe, in a fast-paced, high-tech and customer-obsessed environment- Exposure to Shopify, Amazon and Syndicated data (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
ANI Calls India Private Limited,Azure Data Engineer with Big Data,"Anicalls Industry:

IT
Total Positions: 3

Job Type:
Full Time/

Permanent Gender:
No Preference Salary: 900000 INR - 1400000 INR ( Annually )

Education:
Bachelor′s degree Experience: 8 -12

Years Location:
Hyderabad, India . Azure Data Factory . Azure Databricks . Python, Scala, PySpark, Spark . HIVE / HIVE LLAP / HBASE / CosmoDb . Azure Active Directory Domain Services . Apache Ranger / Apache Ambari . Azure Key Vault . Expertise in HDInsight ( Minimum 2 -3 years ' experience with multiple implementations ) . Expertise in Cloud Native and Open Cloud Architecture",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
DAZN,Senior Data Engineer,"Are you an engineer who loves to make things that just work better? Do you love to work with cutting edge technologies and think about how can this run faster, be deployed quicker or fail less and deliver killer streaming applications that add business value and stick with customers?

DAZN is a tech-first sport streaming platform that reaches millions of users every week. We are challenging a traditional industry and giving power back to the fans. Our new Hyderabad tech hub will be the engine that drives us forward to the future. We’re pushing boundaries and doing things no-one has done before. Here, you have the opportunity to make your mark and the power to make change happen - to make a difference for our customers. When you join DAZN you will work on projects that impact millions of lives thanks to your critical contributions to our global products

This is the perfect place to work if you are passionate about technology and want an opportunity to use your creativity to help grow and scale a global range of IT systems, Infrastructure and IT Services. Our cutting-edge technology allows us to stream sports content to millions of concurrent viewers globally across multiple platforms and devices. DAZN’s Cloud based architecture unifies a range of technologies in order to deliver a seamless user experience and support a global user base and company infrastructure.

Join us in India’s beautiful “City of Pearls” and bring your ambition to life.

Benefits will include access to DAZN, an annual performance related bonus, family friendly community, free access for you and one other to our workplace mental health platform app (Unmind), learning and development resources, opportunity for flexible working, and access to our internal speaker series and events.
As our new Data Engineer, you'll have the opportunity to:

• Support building real-time user-facing analytics and data driven operations applications
• Be responsible with the rest of the team for the availability, performance, monitoring, emergency response, and capacity planning
• Use your love of big data systems, thinking about how to make them run as smoothly and securely as possible, support operational endpoints
• Have a strong sense of teamwork and put team’s / company’s interests first

You'll be set up for success if you have

• 5+ years’ experience writing clean, robust and testable code, preferably in Typescript
• Experience building high performant, low latency and high velocity data pipelines
• Working knowledge in AWS services, such as Kinesis, EventBridge, SQS, SNS Topic, S3, Lambda, Kinesis, EKS, Firehose
• Experience with infrastructure-as-code (preferably Terraform) and CI/CD processes
• Comfortable building & maintaining production level data pipelines; streaming or event driven.
• Strong analytical and communication skills.

Even better if you have:

• Exposure to streaming technologies such as Apache Kafka / Google PubSub, Apache Beam, Google Dataflow.
• Having worked in an agile environment with scrum / kanban delivery methodologies

At DAZN, we bring ambition to life. We are innovators, game-changers and pioneers. So if you want to push boundaries and make an impact, DAZN is the place to be.

As part of our team you'll have the opportunity to make your mark and the power to make change happen. We're doing things no-one has done before, giving fans and customers access to sport anytime, anywhere. We're using world-class technology to transform sports and revolutionise the industry and we're not going to stop.

If you're ambitious, inventive, brave and supportive, then you're the kind of person who's going to enjoy life at DAZN.

We are committed to fostering an inclusive environment, both inside and outside of our walls, that values equality and diversity and where everyone can contribute at the highest level and have their voices heard. For us, this means hiring and developing talent across all races, ethnicities, religions, age groups, sexual orientations, gender identities and abilities. We are supported by our talented Employee Resource Group communities: proud@DAZN, women@DAZN, disability@DAZN and ParentZONE.

If you’d like to include a cover letter with your application, please feel free to. Please do not feel you need to apply with a photo or disclose any other information that is not related to your professional experience.

Our aim is to make our hiring processes as accessible for everyone as possible, including providing adjustments for interviews where we can.

We look forward to hearing from you.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer: Enterprise Content Management,"Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As Enterprise Content Management, you will be working as an application developer on projects in OpenText Process suite BPM. Your role would also involve in playing a critical role in design of a new system

Responsibilities:
• As a Business Process Management (BPM) Developer, you will manage asset services and application development while collaborating with global team in harmonizing the development of asset management applications.
• You will focus on improving corporate performance by managing business processes.
• Identification and driving of related service quality improvements and engineering deliverables.
• Management and progression of Action items on time with prompt response
• Automation and process improvement, if applicable
• Client communication

Required Technical and Professional Expertise
• Minimum 4 years of core development experience as OpenText Process Suite Developer
• Proficient in OpenText Process Suite BPM and having knowledge to design and develop the workflow
• Experience in Xform, HTML5, Angular JS. Javascript, & SQL
• Working knowledge of Core Java, Web Services & Ws APP integration is an added advantage
• Knowledge on Rest API's and SOAP' API's

Preferred Technical and Professional Expertise
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications

About Business UnitIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients' businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBMIn a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you'll be able to learn and develop yourself and your career, you'll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBMIBM's greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we're also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it's time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location StatementWhen applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBMIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Bengaluru,False,False,True,True,False,False,True,False,False,False,True,False,False,False,False,False
Digital Mapout Solutions India Private Limited,Azure Data Engineer,"Role : Azure Data Engineer

Location : Bangalore / Hyderabad

Experience : 4+

M.O.H : Full Time

M.O.W : Work From Office

NP Immediate / 15 days

Education : ÂBE / BTech, ME / MTech / MCA.

Skillsets : Azure Data Factory+Azure Data Lake + Azure SQL+ Azure Synapse+PowerBI

JD : -

Azure data / Lead Engineer :

Mandatory Skill sets : T SQL, Data Warehousing (DW) , ADF, Synapse Analytics

Optional : Power BI-DAX,Data Bricks, Python,PySpark

experience : 3 to 15 years

Senior developer to leads
• Candidate must have a strong experience background in database, Data warehousing & ETL / ELT design and development
• Exposure to complex & large scale enterprise development environment
• Excellent communication & collaboration skills required. Ability to work with internal and external stakeholders is must.
• Good business acumen
• Good problem solving and analytical ability
• Ability & willingness to learn new skills

Core technical skills
• Azure data lake Gen 2
• Synapse Analytics
• Python / scala programming
• Synapse pipeline / Azure data factory
• Azure SQL
• T-SQL programming
• Power BI-DAX",Bengaluru,True,False,True,False,True,False,False,True,True,False,False,False,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.Job DescriptionDevelops and maintains scalable data pipelines for bulk data movement between systems of record and systems of referenceDevelops and maintains scalable application to application integrationsAligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organizationImplements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processesWrites appropriate unit or integration tests to implement test-driven developmentContinually contributes to and enhances data team documentationPerforms data analysis required to troubleshoot and resolve data related issuesWorks closely with a team of frontend and backend engineers, product managers, and analystsDefines company data assets, artifacts and data modelsQualificationsRequired qualifications:5 years of Data Engineering and Data Integration5 Years of Data Warehousing3 Years of Data Architecture and Modeling2 years of Cloud Data EngineeringAgile MethodologiesPreferred skills:AWS or Azure Data CertificationsExperience with databricks, spark, pythonExperience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)Experience with SalesforceAdditional InformationAll your information will be kept confidential according to EEO guidelines.** At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. ** insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Factspan,Factspan Analytics - Azure Data Engineer - Big Data/Hadoop,"Job Description :- 7+ Years of deep experience with complex data systems and good instincts around data modelling and usage.- Knowledge of data engineering technologies, architecture, and processes. Specifically, Azure Data Lake, Hadoop ecosystem, Kafka, and common third-party integration and orchestration tools.- Good knowledge of multi-cloud data ecosystem and build scalable solutions on cloud (Azure)- Good knowledge of Big Data Ecosystem-Spark, Hadoop, Databricks- Work across 3-4 teams to develop practices which lead to the highest quality products and contribute transformation change within the cloud- Experience building large scale data processing ecosystems with real time and batch style data as input using big data technologies- Experience in any programming language like Scala or Python.- Exposure to agile methodology and proven ability to technically lead a team of engineers across geographies.- Implement Data Quality, Data Governance on Azure Cloud ecosystem- Good instincts around technical architecture, including metadata, Rest API Integrations, Data API and Solution design of NoSQL and File systems.- Willingness and ability to invest in engineering growth.- Strong communication skills and ability to coordinate across a diverse group of technical and non-technical stakeholders.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Big Data Engineer,"DATA ENGINEER - JD

The role will be part of the Data and Analytics Team responsible for expanding and optimizing AECOM’s data and data pipeline architecture, data flow, and collection for cross functional teams. The role will support software developers, database architects, data analysts, and data scientists on data initiatives and will ensure consistent optimal data delivery architecture throughout ongoing projects.

Responsibilities & Duties
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep AECOM’s data separated and secure across national boundaries through multiple data centres and regions.
• Create data tools for analytics and data scientist team members -to assist them in building and optimizing our product into an innovative industry leader.
• Collaborate with data and analytics experts to strive for greater functionality in our data systems.
• Escalate issues and recommend resolutions to the Team Lead for timely. May support junior members of the team in addressing routine issues within the assigned processes.
• Maintain the SOP/DTP of current processes and incorporate documentation updates as required.
• Perform moderately complex tasks in compliance with service level agreement, process, policies, and procedures.
• Propose alternatives in identified issues and assist in investigating and in resolving common and unusual issues.
• Contribute in various and simultaneous process improvement initiatives to streamline processes, improve customer experience, and increase productivity. This includes automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Contribute specialized expertise to different assigned projects and may provide key updates to Team Lead and Manager.

Qualifications & Requirements

Minimum Requirements:
• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or relevant discipline in the quantitative field
• 6-10years
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Advanced working SQL/nosql, ADLS, Databricks, ADF, Azure DevOps
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency,and workload management.
• Demonstrated ability to manipulate, process, and extract value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.

Preferred Qualifications
• Experience with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Attributes
• Demonstrated ability to champion and drive ideas/programs/solutions
• Excellent organizational and time management skills, able to work under pressure and prioritize effectively
• Able to demonstrate passion, energy and drive, especially in the face of resistance
• Ability to effectively communicate and collaborate within a varied audience and internal and external customers. (Communication)
• Ability to maintain good customer relationship with the ability to suggest ways to improve customer support customer experience (Customer Service)
• Ability to be thorough and meticulous in completing assigned tasks and identifying errors, duplicates, & discrepancies through defined methods. (Attention to Detail)
• Ability to identify and resolve simple to moderate with the ability to provide resolution alternatives by following defined policies and procedures. (Problem Solving)

experience

10",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,True,False,True,False
Tiger Analytics India Consulting Private Limited,Senior Data Engineer - Denodo,"Job Title: Senior Data Engineer – Denodo

Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 2800+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Curious about the role? What your typical day would look like?
· Engage with clients to understand their business context.
· Translate business needs to technical specifications.
· Define Data Virtualization architecture, deployments, and standards.
· Support the development of data architecture principles, standards, and processes and applies these to deliverables
· Involving in data exploitation and the development of (advanced) analytical data models with multiple data sources using Denodo/Tibco or AtScale semantic layer.
· Developing integrated data solutions, modernizing, consolidating, and coordinating business needs across several applications.
· Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Senior Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Senior Data Engineer

Senior Data Engineer, Delivery Engineering Platform
Delivery Engineering Platforms is part of MasterCard Data & Services group and one of the most rapidly growing organization in the space. Platform Teams provides cloud-based analytic software tools that enable large, consumer-focused businesses to seize the Big Data analytics opportunity by triangulating between business strategy, algorithmic math, and large databases to improve decisions.

100 of the largest corporations in the world uses these products. Test & Learn™ for Sites, Test & Learn™ for Customers, Test & Learn™ for Ads, and other similar products employ patented algorithms and workflow to design and interpret business experiments that evaluate, target, and refine proposed business programs

The Delivery Engineering Platform team is a core component to consulting services, managing the data acquisition, integration and transformation of client provided data within the Test & Learn platform for global engagements.

Role

The Senior Data Engineer will lead and participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace. A Senior Data Engineer will:
• Independently lead projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base
• Act as an expert technical resource within the team and region
• Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors
• Build trust-based working relationships with peers and clients across local and global teams
• Implement best practices and collaborate in the design of effective streamlined processes for a complex global solutions group
• Leverage industry best practices including proper use of source control, participation in code reviews, data validation and testing
• Plays a lead role where he/she oversees the activities of the data engineers and ensures the efficient execution of their duties
• Act as an advisor/mentor and helps in managing careers for junior team members
• Comply and uphold all MasterCard internal policies and external regulations

All about you:
• BE/BTech in a quantitative field (e.g., Computer Science, Statistics, Econometrics, Engineering, Mathematics, Operations Research). ME/MTech preferred
• Excellent English quantitative, technical, and communication (oral/written) skills; is an excellent listener
• Expertise with hands-on experience with RDMS technologies, preferably with Microsoft SQL Server, the SSIS Stack and .Net; Proficiency with at least one scripting language (VB Script, Perl, Python)
• Proven self-motivated leader with experience working in teams
• Demonstrate excellent skills in the ability to innovate, think critically and disaggregate problems. Able to provide oversight, validation and quality control to own and team work product
• Ability to easily move between business, data management, and technical teams; ability to quickly intuit the business use case and identify technical solutions to enable it
• Able to balance multiple projects and differing project priorities
• Flexible to work with global offices across several time zones

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LodgIQ,Data Engineer,"About LodgIQ

Headquartered in New York, LodgIQ delivers a revolutionary SaaS platform for Algorithmic Pricing and Revenue Management for the hospitality industry by incorporating machine learning and artificial intelligence. For more information, visit http://www.lodgiq.com.

Backed by Highgate Ventures and Trilantic Capital Partners, LodgIQ is a well-funded company, seeking for a motivated and entrepreneurial Developer to join its Product/ Engineering team. Qualified candidates will be offered an excellent compensation and benefit package.

Title: Data Engineer

Location: India

Requirements:
• In-depth knowledge of Python.
• Understanding of Django/Flask, Pandas.
• Familiarity with AWS Environment (EC2, S3, IAM, Athena).
• Working knowledge of NoSQL databases such as MongoDB.
• Proficiency in consuming and developing REST APIs with JSON data.
• Ability to perform data mining and data exploration with intuitive sense for problem solving and strong desire for craftsmanship.

Specific Job Knowledge, Skills & Abilities:
• Real world experience with large-scale data on AWS or similar platform.
• Must be a self-starter and an effective data wrangler.
• Intellectual curiosity and strong desire to learn new Big Data and Machine Learning technologies.
• Deadline driven, and capable of delivering projects on time under a fast paced, high growth environment.
• Willingness to work with unstructured and messy data.
• Bachelor’s degree or Masters degree in relevant quantitative fields (e.g. Computer Science, Statistics, Electrical Engineering, Applied Mathematics, etc).",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Edu Angels India Private Limited,Data Engineer (PySpark),"Responsibilities
• Develop process workflows for data preparations, modeling, and mining Manage configurations to build reliable datasets for analysis Troubleshooting services, system bottlenecks, and application integration.
• Designing, integrating, and documenting technical components, and dependencies of big data platform Ensuring best practices that can be adopted in the Big Data stack and shared across teams.
• Design and Development of Data pipeline on AWS Cloud
• Data Pipeline development using Pyspark, AWS, and Python.
• Developing Pyspark streaming applications

Eligibility
• Hands-on experience in Spark, Python, and Cloud
• Highly analytical and data-oriented
• Good to have - Databricks",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Zepto,Data Engineer III (Lead Data Engineer),"Responsibilities:
• Collaborate with Tech and Analytics team to build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
• Oversee and govern the expansion of the current data architecture as the business grows and ensure best practices are followed.
• Design and build best-in-class architecture for data tables to ensure optimal querying performance in relational databases.
• Create and maintain connectors that expose the data securely for consumption by downstream systems and services in near real-time.
• Create and maintain data architecture docs to communicate data requirements that are important to business stakeholders and work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that.
• Build data governance and security protocols and ensure adherence from analytics, tech, and business teams.
• Build and mentor the data engineering team, recognize their strengths, and lead them to take ownership of end-to-end data architecture.
• Stay on top of the latest developments in the tech stack and propose potential upgrades to existing systems.

Requirements:
• 6 to 10 years of experience in Data Engineering - Designing databases, building data pipelines, and maintaining data governance protocols in cloud platforms.
• A visionary in technical architecture, with experience building and maintaining Data.
• Engineering Products, along with the demonstrated ability to take accountability for achieving results.
• Hands-on working experience with Python, ETL pipelines, and advanced SQL.
• Strong understanding of AWS Services - Redshift, Lambda, Glue, Athena, and security protocols.
• Experience in any Cloud DW Redshift/Snowflake/BigQuery/Synapse.
• Strong data Modelling and database design experience with Redshift or other relational databases.
• Experience working with Agile methodologies, Test Driven Development, and implementing CI/CD pipelines using Gitlab and Docker.
• Good understanding of ETL/ELT technology and processes.
• Experience in gathering and processing raw data at scale including writing scripts, web scraping, and calling APIs.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,True,False,True
Confidential,Data Engineer 3 - AWS & Python (Contractual),"IntroductionThe Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed. At our heart is a 50 year forward look, a global forecast of the majority of the world's economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions. We're changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products. What will you experience At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to. Accountabilities How you will contribute: Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment. Work in an agile environment: within a collaborative agile product team using Kanban Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements. Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives. Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals. Experience, skills and professional attributesTo succeed in this role it would be an advantage if you possess: Experience with programing in Python, and Lambda functions Knowledge of building bespoke ETL solutions, and extracting data using Data APIs MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting Prior experience in design and developing microservice architecture Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. A combination of IT skills, data governance skills, analytics skills and economics knowledge An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience. Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. This employer is a corporate member of myGwork - LGBTQ+ professionals, the business community for LGBTQ+ professionals, students, inclusive employers & anyone who believes in workplace equality. PRB",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Axtria - Ingenious Insights,Data Engineer,"• 5-8 years of experience in data engineering, consulting, and/or technology implementation roles
• Expertise in the design, data modeling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation and delivery
• Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena
• Strong proficiency in SQL and database design, development and maintenance
• Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence
• Good understanding of modern architecture patterns like serverless and microservices
• Expertise with analytics and business intelligence solutions (e.g. 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation and Agile Methodology
• Experience working with some of the following marketing data sources
• Traditional > TV / Print / Email
• Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data
• Experience leading project teams with members with different roles and skills
• Experience working in hybrid onshore/offshore team models
• Strong communication skills",New Delhi,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Valiance Solutions,Big Data Engineer,"About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc. Our team comprises 150+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire a Senior Data Engineer to help our customers create scalable data engineering pipelines and infrastructure for downstream analytics workloads. You should be good at understanding client data needs, the landscape of various heterogeneous data sources, identifying a set of services for data ingestion & transformation workloads, and timely execution of projects.

Roles & Responsibilities:
• As a data engineer with Pyspark & SQL skills you will be required to highly scalable, robust, and resilient data engineering pipelines .
• You will be working closely with business stakeholders & the data science team to understand their data requirements and underlying business logic.
• Deploy and monitor pyspark jobs on cloud infrastructure.
• Troubleshoot job failures and ensure system recovery at earliest.
• Attending regular client calls, communicating work status and pro-actively highlighting any delays to the product release.

Technical Skills :
• Hands on experience on pyspark for at least 3 years
• Solid programming experience in Python & SQL is required.
• Working experience of any one cloud platform; AWS, GCP or Azure
• Intermediate plus proficiency in shell scripting
• Experience deploying ML algorithms in production is preferred

Personal Skills :
• Excellent communication skills, both written & oral.
• Ability to learn new skills quickly, adjust to the changing needs of the project.
• You are highly enthusiastic about your work
• Ability to multi-task, manage high-pressure release scenarios occasionally.

Valiance Solutions focuses on Financial Services, Cloud Computing, Artificial Intelligence, Internet of Things, and Big Data Analytics. Their company has offices in Noida and Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://valiancesolutions.com or find them on Twitter and LinkedIn.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
DarioHealth,Data Engineer - Hybrid,"About The Position

At Dario, Every Day is a New Opportunity to Make a Difference.

﻿We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career.

DarioHealth is looking for an experienced Data Engineer who will join our team and create new data solutions, maintain existing solutions and be a focal point of all technical aspects of our data activity. As part of this position, you will develop advanced data and analytics solutions to support our analysts and production units with validated and reliable data.

Responsibilities
• Develop and maintain DarioHealth data infrastructure.
• Develop in-house applications for providing self-service tools.
• Develop real-time data applications for production.
• Provide analysts and data scientists technical support related to data infrastructure.
• Design, build and launch new data models and visualizations in production, leveraging common development toolkits.

Requirements
• At least 4 years of proven experience with Python - a must.
• Very high level of SQL and data warehouse modeling.
• Experience with 24/7 systems and real-time analytics.
• Experience developing data pipelines with Airflow or similar - a must
• Experience with big data solutions like Kinesis/Sparks - an advantage
• Experience with NoSQL databases like MongoDB/Redis.
• Experience with web development using Django/javascript/react - an advantage.
• Experience in the online industry.
• B.A./B.Sc. in industrial/information systems engineering, computer science, statistics, or equivalent.
• **DarioHealth promotes diversity of thought, culture and background, which connects the entire Dario team. We believe that every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions. We are passionate about building and sustaining an inclusive and equitable working and learning environments for all people, and do not discriminate against any employee or job candidate.***",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False
AlphaGrep Securities,Data Engineer,"About the Company

AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes (equities, commodities, currencies, fixed income) that trade on global exchanges..

We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.

Roles & Responsibilities
• Build infrastructure tools and applications to support trading teams across the firm.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Coordinate with global teams to understand their requirements and work alongside them.
• Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
• Set up practices on data reporting and continuous monitoring
• Write a highly efficient and optimized code that is easily scalable.
• Adherence to coding and quality standards.

Required Skills
• Strong working knowledge in Python.
• Strong working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have
• Experience with web crawling and scraping, text parsing
• Experience working in Linux Environment
• Experience with Stock Market Data

Why You Should Join Us
• Great People. We’re curious engineers, mathematicians, statisticians and like to have fun while achieving our goals
• Transparent Structure. Our employees know that we value their ideas and contributions
• Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.
• Health & Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Robert Bosch,Azure Data Engineer,"Job Description

Location : Bengaluru
Experience : 6 to 8 years
Requirements
• Overall 6+ IT experience out of which 3+ years of working experience in Azure, architecting data soultions with good experience on Azure SQL, Azure Data Lake, ADF, Azure DataBricks/Synapse etc.
• 5+ years experience with data modelling,implementing backends and data optimization.
• Good experience working with with Automotive domain usecases.
• Experience implementing compliances like GDPR,HIPAA etc.
• Enforcing data security at rest and in transit.
• Good experience implemeting data security in Azure storage systems.
• Ability to thrive in a fast-paced, dynamic, client-facing role where delivering solid work products to exceed high expectations is a measure of success
• Excellent leadership and interpersonal skills
• Eager to contribute in a team-oriented environment
• Ability to be creative and analytical in a problem-solving environment
• Effective verbal and written communication skills

Skills
• Must have skills : SQL,ADF,other Azure storage services.
• Good to have:Synapse,spark or any big data framework or data warehouse.
• Key Responsibilities : conceptualizaing ,modelling Optimizing databases.Designing data flows
• Knowledge or basic experience with Nosql,parquet,Predictive modelling etc
• Working knowledge, creating ETL packages and deploying them

Qualifications

BE,MCA,MSC,MS,MTech
Experience : 6 to 8 years

Additional Information

Additional information
• Nice to have : Power BI experience, Azure DevOps, Cost Monitoring, Azure AD, Azure Synapse.
• Ability to quickly ramp up on new Azure Components which comes in Azure Roadmap.
• Excellent communication skills (English)

Experience: 6.00-8.00 Years",,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
CX Customer Experience,Salesforce - Data Engineer,"Come create the technology that helps the world act together

Nokia is committed to innovation and technology leadership across mobile, fixed and cloud networks. Your career here will have a positive impact on people’s lives and will help us build the capabilities needed for a more productive, sustainable, and inclusive world.

We challenge ourselves to create an inclusive way of working where we are open to new ideas, empowered to take risks and fearless to bring our authentic selves to work.

The team you'll be part of

You will work as part of the CX Global Sales Operation team. You will work making our Data strategy come alive across CX. You will be involved with integrating data across multiple platforms and especially the integration of the Advanced Analytic platform and the use cases being developed on it. The Advance Analytic projects will enable the CX organization to increase speed and efficiency, and assure the right things are done in the right way to maximize Nokia business.

What you will learn and contribute to

Be responsible for the data engineering of the Advanced Analytics Platform and CX AI use cases

Data integration from different source data platform to the advanced analytics platform

Data integration from the advanced analytic platform to different platform with UI

Ongoing support for the integration

Potentially doing data cleaning and wrangling on the advanced analytics platform

Potential involvement in dashboard and UI development

The type of use case you will work on will center around one or more of the following:
• Machine learning prediction models,
• The use of machine learning to automate the currently manual business planning process
• Knowledge mining and recommendation systems to improve the likelihood to win new business.

Your skills and experience
• Deep experience end data engineering including but not limited to experience using SQL and other types of databases and database languages
• Proficiency developing software probably in python using jupyter notebooks
• Proven competency in agile and lean software development
• Competency in SCM (Git), Automation tools, infrastructure automation,
• Good knowledge about Azure cloud infrastructure, security and application development.
• Experience with Python / Spark and Delta Lake. Familiar with big data patterns like lake house.
• Experience with Azure DevOps, CI/CD pipelines, version control tools like GIT / VSTS. Familiar with IDE’s like Visual Studio
• Nokia Business and process understanding
• Bachelor’s degree or higher in information technology, data science or related disciplines
• Effective communication in English (written and verbal)

Nice to have:
• Experience in app development

What we offer

Nokia offers flexible and hybrid working schemes, continuous learning opportunities, well-being programs to support you mentally and physically, opportunities to join and get supported by employee resource groups, mentoring programs and highly diverse teams with an inclusive culture where people thrive and are empowered.

Nokia is committed to inclusion and is an equal opportunity employer

Nokia has received the following recognitions for its commitment to inclusion & equality:
• One of the World’s Most Ethical Companies by Ethisphere
• Gender-Equality Index by Bloomberg
• Workplace Pride Global Benchmark
• LGBT+ equality & best place to work by HRC Foundation

At Nokia, we act inclusively and respect the uniqueness of people.

Nokia’s employment decisions are made regardless of race, color, national or ethnic origin, religion, gender, sexual orientation, gender identity or expression, age, marital status, disability, protected veteran status or other characteristics protected by law.

We are committed to a culture of inclusion built upon our core value of respect.

Join us and be part of a company where you will feel included and empowered to succeed.

Additional Information",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 20 2023

Ready to help shape the future of healthcare?

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organization where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to impact the health of 2.5 billion people around the world in the next 10 years.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves - feeling welcome, valued and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.

The Senior Principal Data Engineer is a vital technical role in the successful design and delivery of Data and Analytics (D&A) initiatives for the GSK's Pharmaceutical and Vaccines Supply Chains. The primary purpose of this role is to ensure that D&A Products have an optimal solution design and that the technical development work to then deliver them into production and support is smooth and successful. This requires deep expertise in data and analytics platforms and technologies as well as domain understanding of Pharmaceutical & Vaccines manufacturing and quality processes. This also requires close collaboration with D&A Product Managers, D&A Development Squads and the D&A Platform & Architecture team as well as with business stakeholders and other Digital and Tech teams.

The MSAT & Quality D&A team currently has a portfolio of around 15 D&A products across 4 product groups with over 100 people (GSK employees plus contractors) working in agile squads to deliver these. The Sr Principal Engineer will oversee and be accountable for the technical success of all of these products.

Key Responsibilities:
• Accountable for optimal solution designs for D&A Products that facilitates an agile, product management approach, can be rapidly and cost-effectively delivered to meet the true business requirements and are robust, sustainable and supportable throughout their lifecycle
• Work closely with D&A Product Mangers, using deep technical expertise and domain understanding to effectively influence (and when needed challenge) business and architectural stakeholders to arrive at the right design
• Steer solution design through D&A Architecture Review process, aligning with enterprise platforms and architectural patterns by first intent
• Oversee technical work of development teams ensuring it is remains aligned with agreed design, is of high quality, complies with relevant standards and policies and will meet agreed business objectives
• Provide hands-on technical problem-solving expertise to address technical challenges during development and, where needed, during lifecycle support
• Act as mentor for more junior technical roles, supporting their development and promoting adoption of best practice across development teams
• Lead discovery / proof-of-concept activities to establish early technical feasibility of new Products or Product Features
• Input to, review and approve key technical documents (e.g. design spec, validation plan)
• Drive adoption by development teams of existing and future best-practice approaches from D&A Platform team (e.g. implementation of DevOps CI/CD pipelines, automated testing)
Why you?

Basic Qualifications:
• Computer Science or related Bachelor's degree
• 16+ years of experience and track record of engineering and delivery of flexible, scalable, and supportable data and analytics applications for large complex, global organizations
• End-to-end / 'full stack' D&A experience from data ingestion through transformation to user interaction (visualisation, analytics, etc.)
• Track record of designing and delivering solutions in a cloud environment using modern data architectures and engineering technologies
• Experience designing with DataOps and FinOps in mind to ensure solutions are flexible/future-proof and can scale to handle growing demand, while remaining cost effective
• Experience in Agile development
• Track record of designing and delivering solutions compliant with industry regulations and legislation
• Ability to oversee and matrix manage GSK and 3rd party technical resources
• Excellent communication, negotiation, influencing and stakeholder management skills.
• Customer focus and excellent problem-solving skills.
Preferred Qualifications:
• Computer Science or related Master's degree
• Microsoft Azure accreditation and experience
• SAFe (Scaled Agile) accreditation experience
• Experience developing and delivering GxP-validated solutions for the Pharma/Vaccines industry
• Experience with specific technologies in GSK stack: Talend, Databricks/DeltaLake, Azure Synapse, Snowflake, PowerBI, Azure Functions, Azure App Services,
At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.

#LI-GSK

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We're committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in ""gsk.com"", you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Confidential,Data Engineer,"Job purpose We are hiring a Data Engineer to join our Enterprise Analytics team. As a Data Engineer, you will be responsible for building, maintaining, and optimizing the data infrastructure needed to support our company's data-driven initiatives. You will work closely with our data analysts, data scientists, and business intelligence developers to ensure that our data is accurate, complete, and secure.Job Responsibilities:Design, build, and maintain the data infrastructure needed to support our company's data-driven initiatives.Develop and maintain data pipelines and ETL processes that move data from source systems to our data warehouse.Implement data quality checks to ensure that our data is accurate, complete, and consistent.Work closely with our data analysts, data scientists, and business intelligence developers to understand their data needs and ensure that our data infrastructure meets those needs.Optimize our data infrastructure to ensure that it can handle large amounts of data and support complex queries.Develop and maintain documentation for our data infrastructure and processes.Stay up to date with the latest technologies and trends in data engineering and recommend new tools and techniques as appropriate.Collaborate with other members of the BI and Data team to ensure that our data infrastructure is aligned with our company's strategic goals.Background and experience:Bachelor's degree in Computer Science, Engineering, or a related field.3+ years of experience in data engineering or a related field.Competencies and skills:· Strong knowledge of SQL and experience working with relational databases.· Experience with data modeling and schema design.· Experience building and maintaining data pipelines and ETL processes.· Experience with cloud-based data warehousing technologies, such as Azure Data Lake, Data Factory, Synapse Analytics.· Strong problem-solving skills and attention to detail.Excellent communication and collaboration skills. Transportation, Logistics and Storage,IT Services and IT Consulting,Truck Transportation",,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Loop Health,Data Engineer - Remote,"About Loop

Looking for a great mission? Help build a customer focused healthcare company.

Loop wants to create an inspirational healthcare and insurance company. We believe in the transformative nature of empathetic primary care, proactive financial coverage and want to bring that to our members. We want to fundamentally change how healthcare assurance is designed and delivered. We believe in the power of incentives. We are successful when we deliver health outcomes — when our members and their families get healthier.

“Why exactly are we building a new revolutionary healthcare system? The obvious answer is India deserves better care for its people. Not enough of it around, and what exists can be tough to navigate.

Imagine if doctors were paid to actually make you better. What a concept! What if they didn't have to worry about finishing consults in 10 minutes to meet their daily quota. What if they could take their time, really understand the symptoms, the family history & the lifestyle to come up with a plan, rather than just a prescription.

Imagine if hospital admissions, treatments, billing and insurance were as easy as ordering food home and your care doesn't end when they send you home from the hospital. It goes till you are back on your feet. And further, now imagine if your family had access to this great care anytime they wanted. From serious conditions to the smallest questions. So that they live longer. Wouldn't you worry less?

At the end of it. It's not why you would build this system... Why wouldn't you?”

Here’s how we are going about it:

- We built a high quality concierge and primary care program that allows members and their families to access unlimited care when they need it.
- We use technology to deliver this through highly engaging care.
- We work with insurers to bring financial protection to our members so that they do not worry about their families’ well being.
- As a healthcare insurance broker, we provide companies with the best coverage and claims service for their employees and dependents.

Doing this will mean that we create great products and services that work on changing behaviors and mindsets. This will need a deep understanding of design of products and services through an empathetic lens of what members need for their health and technology will play a very pivotal role in enabling our members to use our programs . We are looking for folks in our ‘EngineeringTeams’ to work with us to take Loop to this future.

If you'd like to learn more about what we are building at Loop, there are tons of resources. Here are some of our favorites:
https://yourstory.com/2021/06/loop-health-insurance-plans-improve-
healthcare/amphttps://yourstory.com/2022/04/loop-health-raises-25m-elevation-capital-general-catalyst/amp

Join us in making healthcare simple, reliable, and human.

Roles and Responsibilities

• Work in collaboration with engineers and stakeholders to build a platform for enabling data-driven decisions.
• Build reliable, scalable, CI/CD driven streaming and batch data engineering pipelines.
• Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.
• Create a conceptual data model to identify key business entities and visualize their relationships.
• Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and relationships
• Storage (cloud data warehouse, S3 data lake), orchestration (Airflow), processing (Spark, Flink), streaming services (Kafka), BI tools, graph database, and real-time large scale event aggregation store are all examples of data architecture to design and maintain.
• Work on cloud data warehouses, data as a service, business intelligence, and machine learning solutions.
• Data wrangling in a diverse environment.
• Ability to provide data and analytics solutions that are cutting-edge.
• Identify strategic and Operational KPIs for the team and drive the team to deliver the committed targets.

Qualifications

• SQL knowledge, as well as programming skills in Scala or Python.5+ years of applicable data warehousing, data engineering, or data architecture experience
• Experience with the GCP stack (BigQuery, GCP Databricks) is a plus
• Ability to design data analytics solutions to meet performance and scaling requirements.
• Demonstrated analytical and problem-solving abilities, particularly in the context of large data.
• Data warehousing concepts and modern data warehouse/Lambda architecture are well-understood.
• Good understanding of the Machine Learning and Artificial Intelligence (AI) solution space.
• Communication and interpersonal skills at all levels of management
• You are a detail-oriented person with excellent communication skills and a strong sense of teamwork.

What you can expect from us

  ‍  ‍   Loop Family Healthcare Health insurance for you and your family for all medical emergencies.

   High agency You'll always have the agency to shape projects, processes and outcomes independently.

  Learning Budget If there's a workshop, book or event you think will help you learn, we'll cover your bill.

   Work from home setup We'll help you set up your office the way you want to with the best equipment around.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,True,True,False
Visa,Sr. Data Engineer - Big Data Testing,"This position is ideal for an engineer who is passionate about solving challenging business problems. You will be an integral part of the Payment Products Development team focusing on test automation. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, and testing of new and existing functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Develop systems and processes to refine efficiency of automated testing solutions
• Design and execute tests for applications and services
• Develop and maintain tools for automation tracking and reporting
• Review product requirements and specifications and recommend improvements to ensure product testability
• Recommend areas of applications and services where automation would be beneficial
• Present technical solutions, capabilities, considerations, and features in business terms
• Effectively communicate status, issues, and risks in a precise and timely manner
• Perform other tasks on data governance, system infrastructure, and other cross team functions on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
PayPal,"MTS 1, Data Engineer","Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
The MTS 1 ? Data Engineer will directly report to and support Sr. Manger of Finance Technologies in the development and execution of strategic transformation programs & initiatives, strategic engineering architecture design, resource allocation, and platform performance monitoring. Ideal candidate is a technologist who believes that use of technology is in its infancy and the best is yet to come. The Regulatory Reporting Hadoop product owner (Business System Analyst) will be part of the Global Regulatory Reporting, and Merger & Acquisition Integration support. The nature of role is strategic, analytical and highly collaborative, working with team members across World and also as a liaison for Global projects.
• Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
• Undergraduate/ Master degree in Computer Engineering or equivalent from a leading university.
• 11+ years of post-college working experience as a Business System Analyst and leading large scale projects end to end.?
• Minimum 4+ years? experience working with large data sets, experience working with distributed computing a plus (Map/Reduce, Hadoop, Hive, Spark, etc.)
• Experience in Data Analysis, Data Validation.
• Strong knowledge in writing complex queries for validation of ETL process.
• Preferred/Basic understanding of Payments/Finance/Accounting Industry Background.
• Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations.
• Experience with databases, systems integration, application development and reporting.?
• Works independently and able to make decisions quickly when necessary.
• Quick Learner with an ability to ramp up in technologies and modules to meet business needs.
• Works in an Agile environment and continuously reviews the business needs, refines priorities, outlines milestones and deliverables, and identifies opportunities and risks.
• Maintain, track and collaborate with dev teams to ensure project estimation for delivery.
• Experience using JIRA and Confluence, or similar User Story workflow and management tool is a must.
• Highlight the bugs and blockers and coordinate with the development and operations team to come up with the best solutions/fixes and document them.
• Work across internal team in various geo-locations across the world
• ?Drive For Results? - Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.
• ?Priority Setting? - Spends his/her time and the time of others on what’s important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.
• Weekly and Monthly status reporting to leadership.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Software Engineer II | Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Software Engineer II | Data Engineer

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Overview
The Enterprise Data Solutions team is looking for a Big Data Engineer to drive our mission to unlock potential of data assets by consistently innovating, eliminating friction in how users access data from its Big Data repositories and enforce standards and principles in the Big Data space. The candidate will be part of an exciting, fast paced environment developing Data Engineering solutions in the data and analytics domain.

Role
• Develop high quality, secure and scalable data pipelines using spark, Scala/ python on Hadoop or object storage.
• Leverage new technologies and approaches to innovate with increasingly large data sets.
• Drive automation and efficiency in Data ingestion, data movement and data access workflows by innovation and collaboration.
• Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency.
• Perform assigned tasks and support production incidents.

All About You
• 4+ years of experience in Data Warehouse related projects in product or service-based organization
• Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
• Experience of building data pipelines through Spark with Scala/Python/Java on Hadoop or Object storage
• Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
• Experience of working on Nifi will be an added advantage
• Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
• Strong communication skills - both verbal and written
• Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
• Flexibility to work as a member of a matrix based diverse and geographically distributed project teams

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Plume Design,Senior Data Engineer,"Plume’s Cloud Platform team is looking for engineers to build and operate data pipelines that power the gamut of Plume products and analytics. Due to the massive scale and performance requirements of many of our use cases, you will be solving challenging problems on a daily basis using a variety of cutting edge technologies.

What you will do:
• Interact with stakeholders to gather and understand data requirements
• Design and implement data pipelines with high data quality goals
• Maintain up-to-date documentation of data warehouse schemas
• Write clean, maintainable code, and perform peer code-reviews
• Refactor code as needed to improve performance and simplify operations
• Provide production support in triaging and fixing issues relating to data quality and availability
• Mentor and assist junior team members and new hires to become successful and productive
• Adhere to data protection requirements including data access, retention, residency and de-identification
• Play an integral role in driving the technology roadmap and enhancing best practices

What You’ll Bring
• Education Requirements: BS/MS/PhD in Computer Science, Electrical Engineering or related technical field
• 5+ years of software development experience with a proven track record of building, scaling, and supporting production data pipelines
• High proficiency in writing idiomatic code, preferably in Java or Scala
• High proficiency in writing SQL in data warehousing technologies
• Strong understanding of large-scale data processing technologies, e.g. Apache Spark (preferred) or Apache Flink
• Strong understanding of data warehousing concepts
• Strong analytical and problem-solving skills
• Strong oral and written communication skills

Plume Design focuses on Internet Service Providers and Cloud Data Services. Their company has offices in Palo Alto. They have a mid-size team that's between 51-200 employees. To date, Plume Design has raised $37.5M of funding; their latest round was closed on June 2017.

You can view their website at https://platform.plume.com or find them on Twitter, Facebook, and LinkedIn.",Hyderabad,False,False,True,True,False,False,False,False,False,False,False,True,False,False,False,False
Lilly,Data Engineer - (DT) Business Insights & Analytics,"At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 35,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world.

Business Insights and Analytics: Data Engineer

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease. We’re looking for people who are determined to make life better for people around the world.

The LCCI (Lilly Capability Center India), BI&A (Business Insights & Analytics) team was started in 2017 with the objective of supporting business decisions for the commercial and marketing functions in the US and ex-US affiliates. This team is part of the LCCI - Commercial Services organization and works very closely with business analytics team based in Indianapolis (HQ). The team currently comprises of more than 100 staff members, with varied backgrounds and skills across data management, analytics and data sciences, business and commercial operations etc.

To better meet the evolving analytics needs, the LCCI BI&A team is ramping up the data engineering pillar. We are looking for data engineers who can be play integral role in developing, maintaining, and testing infrastructures for data generation, processing and storage; work closely with data scientists and help architecting solutions with the objective of driving right KPIs for the business.

Core Responsibilities:
• Create and maintain optimal data pipeline architecture ETL/ ELT into Structured data
• Assemble large, complex data sets that meet functional / non-functional business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.
• Expert level experience creating Fact tables, Dimensional tables and ingest datasets into Cloud based tools. Job Scheduling, automation experience is must.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Setup and maintain data ingestion, streaming, scheduling, and job monitoring automation. Connectivity between Lambda, Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and “big data” technologies like AWS and Google
• Build analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics
• Work with cross-functional teams including external consultants and IT teams to assist with data-related technical issues and support their data infrastructure needs
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

Experience Required
• 4-8 years of in-depth hands-on experience in data warehousing (Redshift or any OLAP) to support business/data analytics, business intelligence (BI)
• Advanced knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouses
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience building and optimizing “big data” data pipelines, architectures and data sets
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores
• Strong analytical and problem-solving skills to be able to structure and solve open ended business problems (pharma experience is highly preferred)

Education
• Bachelor’s/ Master’s degree in Technology OR Computer Sciences

Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response.

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,True,False,False,True
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications

• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
GE,Senior Data Engineer,"Job Description Summary
GE HealthCare is on a transformational journey leveraging Data and Analytics to drive business growth. GE HealthCare is looking for Senior Data Engineer who will be responsible for building and implementing the data ETL pipelines for Finance function data (from data ingestion to consumption).The Data Engineering team helps solve our customers' toughest challenges leveraging data and analytics. The Senior Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE HealthCare to drive business analytics to a new level of predictive analytics while leveraging On-prem, Cloud Platform, Big data tools and technologies.

GE HealthCare is a leading global medical technology and digital solutions innovator. Our purpose is to create a world where healthcare has no limits. Unlock your ambition, turn ideas into world-changing realities, and join an organization where every voice makes a difference, and every difference builds a healthier world.

Job Description

In this role you will:
• Responsible for building data and analytical engineering solutions with standard end to end design & ETL patterns, implementing data pipelines, data modelling and overseeing overall data quality.
• Responsible to work with cross functional teams in GEHC to make the data usable for functional users, data scientists and application users to enable delivery of business values to customers.
• Responsible to enable access of data in AWS storage layers and transformations in AWS Datawarehouse and further transporting in respective databases, consumers, data marts etc.
• As a Senior Data Engineer, you will be part of a data engineering or cross-disciplinary team on Finance facing development projects, typically involving large, complex data sets. These teams typically include data engineers, data visualization engineers, architects, data scientists, product managers, and end users, working in cohorts with partners in GE business units.
• Implement Data warehouse entities with common re-usable data model designs with automation and data quality capabilities.
• Demonstrate proficiency at industry standard data modeling tools (e.g., Erwin, ER Studio, etc.).
• Integrate domain data knowledge into development of data requirements.
• Develop processing codebase using pySpark and implement medium to complex transformations, business logics.
• Look across multiple systems, understands the purpose of each system and defines data requirements by systems.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Lead other horizontal improvement initiatives to benefit technology and leap further on a problem area or Hackathon etc
• Establish and maintain as a trusted advisor relationship within GE Healthcare Data & Analytics (Finance Function)
• Establish and maintain close working relationships with teams responsible for delivering solutions to the businesses and functions
• Engage collaboratively with project teams to support project objectives through the application of sound data engineering principles
• Identify risks and assumptions for the in scope Data & Analytics solutions
• Work with the contract/vendor resources to deliver the solution and manage the technical resources work

Qualifications
• Bachelor's Degree in Computer Science, Information Technology or equivalent (STEM)
• A minimum of 6 year of similar experience working on Database(s), SQL, Python, Datawarehouse, Java, ETL and AWS cloud platform is required. AWS certifications would be added advantage
• Experienced in Deployment process on-prem and on-cloud using Kubernetes, Dockers, Jenkins
• Ability to drive projects in big data (structured/unstructured/machine/logs/streaming data types)
• 3+ Year of Data modelling & Data warehousing experience with MPP systems (Teradata, Netezza, Greenplum etc.)
• 3+ years in AWS Services Like Redshift, RDS, S3, Glue, Step Function, Lambda etc.
• Hands on experience in delivering analytics in modern data architecture (Massively Parallel Processing Database Platforms and Semantic Modelling)
• Demonstrable knowledge of ETL and ELT patterns and when to use either one; experience selecting among different tools that could be leveraged to accomplish this. (i.e. Informatica, HVR, Talend etc)
• Demonstrable knowledge of and experience with different scripting languages (python, shell)
• Understands data quality and solves for application-level needs
• Understanding of DaaS, Data management tools / solutions
• Strong verbal & written communication
• Experience working with solutions delivery teams using Agile/Scrum ore similar methodologies
• Added advantage if experienced in working on Finance data

Desired skills:
• Delivers results when working on shorter-term (weeks-months), outcome-focused service engagements
• Proactively learning new technology, predicts trends, and identifies new opportunities based on trends
• Leverages knowledge about technology trends, and changing business needs across the broad environment to bring new ideas to the team
• Articulates the value proposition of existing technology capabilities and maps them to customer requirements to minimize incremental cost of development
• Experienced in working with On-prem (Teradata) data warehouse – Dimensional and data modelling. Experienced in one of the ETL like Informatica.
• Identifies the customer’s business and strategic needs, concerns, and desires for the value delivery capabilities of the Product
• Functional understanding of finance - Close Book, Treasury, Cash, Controllership, Credit, Account Payables, Account Receivables, Cash Forecasting, Balance sheet exposure, Debt, Forex etc.

Inclusion and Diversity

GE Healthcare is an Equal Opportunity Employer where inclusion matters. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

We expect all employees to live and breathe our behaviors: to act with humility and build trust; lead with transparency; deliver with focus, and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.
#LI-Hybrid
#LI-GM2

Additional Information

Relocation Assistance Provided: Yes",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,False
Concinnity Media Technologies,Senior Data Engineer,"Preferred Experience:

• 8+ years’ experience building mobile, web and/or API-based applications

• Follow engineering standards and best practices

• Knowledge of databases: MySQL, PostgreSQL, SQL, etc...

• 4+ years of Python server development experience

• Django, Flask, Bottle, or similar framework experience

• 2+ years industry experience

• Knowledge of cloud deployment strategies using AWS, Azure, Rackspace, etc.

• Expertise working with and building RESTful APIs

• Ability to operate in Agile / Scrum development environments

• Understanding of OOP and Data Structures and know when to apply them in daily coding scenarios

• Knowledge in the following web-technologies:
• JavaScript
• HTML & HTML5
• CSS3
• JavaScript frameworks (React, Angular, Next.js, etc.)

• Understand the development of the following:
• Responsive Web Development
• Accessibility
• Secure web applications

• Message queue implementations (RabbitMQ, ZeroMQ, Kafka, etc.)

• Background task processing (Celery, etc.)

• Experience configuring container like systems (Vagrant, Docker, etc.)

• Container orchestration with Kubernetes

• Ability to self-organize with minimal guidance/competing priorities and work effectively within a team

• Ability to provide innovative, creative solutions to tasks/problems

• Ability to complete work following engineering standards and best practices

• Experience with GIT and Gitlab is a plus

• Experience with JSON is a plus",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Hewlett Packard Careers,Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.
We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

The Data Engineer will develop, test, and maintain Big Data solutions for a company. Gather large amounts of data from multiple sources and ensure that downstream users can access the data quickly and efficiently. Essentially, the company’s data pipelines are scalable, secure, and able to serve multiple users.

Job description
• Meeting with managers to determine the company’s Big Data needs.
• Developing Hadoop systems.
• Loading disparate data sets and conducting pre-processing services using Spark, Hive or Pig.
• Finalizing the scope of the system and delivering Big Data solutions.
• Managing the communications between the internal system and the vendor.
• Collaborating with the software research and development teams.
• Building cloud platforms for the development of company applications.
• Maintaining production systems.
• Training staff on data management.

Big Data Engineer Requirements:
• Bachelor’s degree in computer engineering or computer science.
• Previous experience as a big data engineer.
• In-depth knowledge of Hadoop, Spark, and similar frameworks.
• Knowledge of scripting languages is preferred .
• Knowledge of NoSQL and RDBMS databases including Redis and MongoDB.
• Familiarity with Mesos, AWS, and Docker tools.
• Excellent project management skills.
• Good communication skills.
• Ability to solve complex data, and software issues.

Education and Experience Required:
• Typically, 6+ years of progressive professional experience as a big data engineer.
• Bachelor’s degree in computer engineering or computer science.

We love our work environment. We think you will too:
• It’s a friendly atmosphere with supportive leaders to bring your creativity to the max.
• Work-life balance support including flex-time arrangements and work from home opportunities.
• Corporate Social Responsibility initiatives to help you make an impact to communities at large.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Omnivio,Data Engineer - Full Time,"Job Profile

Omnivio is a startup in the Supply Chain and Logistics domain. We help retailers deliver an 'Amazon like' shopping experience to their customers. We optimize and manage delivery times and cost, inventory, geo-distributed stores etc. One of the core pieces of our infrastructure is the data engineering required to get data from various upstream systems into a data model that we use for intelligence. If you've worked in data engineering before, you might imagine that this has a good number of challenging engineering problems. We are looking for junior to mid level data engineers to join our team.

Experience / Skills required

We are looking for previous experience in data engineering for this role. Here's a list of tools and technologies that you can expect to be working with in this job. The listed examples are not necessarily all a part of our stack, but they are solid indicators of your skills being a good fit for the job.
• Relational Databases, both OLTP and OLAP, such as MySQL, Postgres, Redshift, BigQuery, CLickhouse etc
• Solid software engineering fundamentals, and experience with one or more general purpose programming languages such as Python, Typescript
• Data engineering programming libraries such as Pandas, NumPy etc
• Building data engineering pipelines using orchestration tools such as Airflow, Airbyte, Temporal, or other commercial offerings
• Transformations using dbt, or a similar alternative
• Experience with AWS's data engineering stack is definitely a plus
• Deployment/operating experience with any of these tools would be really interesting too

Work culture
• No ego anywhere in the team, including higher management.
• Remote, asynchronous, flexible work timings.
• Collaboration and team-thinking. No single person owns the failure.
• Ample time and attention to help developers level up.

Work Ex - 3+ years

Omnivio focuses on Supply Chain Management, Logistics, Cloud Infrastructure, Logistics Software, and Logistics / Transportation / Shipping. Their company has offices in Noida. They have a small team that's between 11-50 employees. To date, Omnivio has raised $400k of funding; their latest round was closed on July 2022 at a valuation of $5M.

You can view their website at https://omnivio.io or find them on LinkedIn.",,True,False,True,False,False,False,False,False,False,False,False,False,True,True,True,False
Mercedes-Benz Research and Development India Private Limited,Big Data Engineer,"AufgabenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team playerQualifikationenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team player",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Confidential,Data Engineer - SQL/Data Pipeline,"Job Description : : - Hands on working knowledge on building and optimizing 'big data' data pipelines, architectures, and data sets- Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases- Strong know-how on data ingestion tools and APIs to prioritize data sources, validate them, and dispatch data to ensure an effective ingestion process. Knowledge on data ingestion tools such as Apache Kafka/ Apache Storm/Apache Flume/Apache Sqoop/Wavefront, and more.- Working knowledge on data mining tools such as Apache Mahout/KNIME/Rapid Miner/Weka,- Strong know-how on ETL tools such as Talend/Informatica PowerCenter/AWS Glue/Stitch,- Ability to handle various types of data in the form of text, speech, image, video, or live stream from IoT/ Sensors/ Web- Ability to manipulate, process and extract value from large, disconnected datasets and articulate the same in business contextPreferred : - Skilled in the use of business intelligence and visualization tools, such as PowerBI, Tableau - Experience with stream-processing systems such as Storm, Spark-Streaming, etc.- Ability to leverage MLOps Platforms such as Teraform, Ansible, Kubeflow, Google AI Platform- Know-how on software development languages such as Python, Java, C++, Scala, etc (ref:hirist.com) IT",,True,False,True,True,False,True,False,False,False,True,False,False,False,False,False,False
NIRA,Sr Data Engineer/Architect (3y-7y),"About the job

Starting with credit, NIRA aspires to be the pre-eminent financial brand for the mass market or ""Middle India"". We already have customers in over 5,000 towns and cities, and we're growing quickly (15% MoM for the last 20 months!). It's a very exciting time to join us. We have over 200+ employees.

Currently, only 10% of Indians can use banks when they need credit: banks typically require a high credit score or collateral, something most people don't have. It need not be this way. Using a combination of traditional data and the vast amount of digital data available, it is now possible to score the unscored.

Today, we receive 15000 new loan applications daily from across 4000 cities in India, and we are growing 15-20% MoM. People reach us at their time of need, and we offer them credit via our app. Money reaches their bank account within 24 hrs of application.

We are addressing head-on a big challenge. It's also a great opportunity from both a commercial and societal impact perspective. We can improve lives for millions. It is no exaggeration to say that our addressable market will be 400mm within 5 years. It's pretty exciting, we think.

If our mission resonates with you, and you are a talented and hardworking individual that wants to commit yourself to an incredible challenge, then we want to hear from you.

As one of the senior data engineers on the team, you’ll be working on our core data platform and infrastructure powering business decisions and data science workloads.

Job Overview

We are looking for an experienced Data Engineer to join our engineering team. The hire will be responsible for building our data and data pipeline architecture. You will optimise our data flow starting with the collection of data for cross functional teams and purposes. You will support our key data science initiatives while maintaining consistency of data delivery architecture throughout ongoing projects. Ideal candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities

• Create and maintain optimal data pipeline architecture

• Assemble large, complex data sets that meet performance and business requirements

• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies

• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, credit risk, operational efficiency and other business KPIs.

• Create data tools for analytics and data scientist team members that assist them in building and scaling our core products

• Work with cross domain data and analytics experts to strive for stronger data driven outcomes for the business.

Qualification

• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science. They should also have experience using the following software/tools:

• Experience with big data tools: Hadoop, Spark / PySpark, Kafka.

• Experience with relational SQL and NoSQL databases.

• Experience with object-oriented/object function scripting languages: Python.

• Expertise in data modelling and buiding data driven systems.

• Well versed with Shell Scripting.

• Hands on experience on various AWS services like EMR,EC2,Glue.

• Deploy existing data projects using CICD pieplines.

• Knowledge on Docker is a plus.

What we offer:

• Competitive salary

• Medical Insurance

You can learn more about NIRA here:

Press:

https://yourstory.com/2019/05/startup-fintech-nira-entrepreneur-loans/

https://www.livemint.com/companies/news/muthoot-finance-partners-with-nira-to-provide-personal-loans-11620309871295.html

https://www.financialexpress.com/money/personal-loan-collection-rates-return-to-pre-covid-levels-data-from-nira-reveals/2313170/

NIRA focuses on Consumer Lending and Fin Tech. Their company has offices in Bengaluru. They have a large team that's between 201-500 employees. To date, NIRA has raised $3.1M of funding; their latest round was closed on April 2020.

You can view their website at https://www.nirafinance.com or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Referrals Only,Consultant-Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.
Job responsibilities• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will collaborate with Data Scientists in order to design scalable implementations of their models
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Assure effective collaboration between Thoughtworks' and the client's teams, encouraging open communication and advocating for shared outcomes
Job qualificationsTechnical skills• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Professional skills• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more
Other things to knowL&DThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.
About ThoughtworksThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 28+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.

Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
ANI Calls India Private Limited,Senior Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Bengaluru, India
Candidate should have:
Worked collaboratively with cross-functional teams and stakeholders to achieve an organizational goal.
Worked in an agile environment and are comfortable running an agile process for the data and analytics team.
Strong experience in data pipelines, ETL design (both implementation and maintenance), data warehousing, and data modeling (preferably in dbt).
Implementation and tuning experience in the Big Data Ecosystem,
(such as Data Analytics (Dataproc, Airflow, Hadoop, Spark, Hive),
Google Cloud Platform AI and ML Services and Data Warehousing (such as BigQuery, schema design,
query tuning and optimization) and data migration and integration.
End to end hands-on to carry out complex POC, Pilot, Limited production rollout, assignments requiring the development of new or improved techniques and procedures.
Participated in deep architectural discussions to build confidence and ensure customer, success when building new, or migrating existing, applications, software, and services on the Google Cloud Platform.
advanced skills in SQL, data modeling, ETL/ELT development, and data warehousing.
Strong skills in Optimization - performance, pipeline, spark.
Experience on Pyspark.
5+ years of design & implementation experience with distributed applications.
5+ years of experience architecting/operating solutions built on Google Cloud Platform.
. Bachelor's degree.

Experience: 5.00-10.00 Years",,False,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
deloitte,Consulting- SAMA- A&C-Azure Data Engineer- AD,"JD:
• Location - Mumbai OR Pune
• Experience range:
• 12-15 yrs for AD
• Strong experience in Python programming and related skills like PySpark
• Strong SQL skills
• Strong experience with any of the data engineering platforms like Hadoop, Spark, Synapse, Databricks, Apache Airflow, etc.
• Preferred: Knowledge of any cloud platform like Azure/AWS/Google
• For AD level: Need technology leadership experience in terms of architecture/solutioning and team leading",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Comcast,"Data Engineer, Data Products Engineering","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary INTRODUCTION: At Comcast, we believe in the talent of our people. It's our passion and commitment to excellence that drives Comcast's vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It's what makes us uniquely Comcast. Here you can create the extraordinary. Join us. ABOUT THE ROLE: Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered Job Description Responsibilities Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc) 2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas. Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications. Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data. Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time. REQUIREMENTS: Bachelor's degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience. Strong Computer Science/Engineering/Information Systems background 3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies. Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas Strong problem-solving skills. Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences. PREFERRED QUALIFICATIONS: 1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills Ability to design concise and visually appealing reports, user interfaces, mockups and documentation Ability to read external API documentation and write pipelines to extract data from our partners systems Ability to write and stand up internal API endpoints to share data with other internal teams. Strong analytical focus, results-oriented and execution driven. Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures. Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed. Flexibility to adjust to changing requirements, schedules and priorities. Ability to work independently under minimum supervision and proactive in solving issues Energetic, committed and solution focused with the ability to perform under pressure and meeting targets Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Relevant Work Experience 2-5 Years Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,False,True
Versor Investments,Data Engineer,"India

Versor Investments (“Versor”) is a quantitative investment boutique headquartered in Midtown Manhattan. The Firm currently has an AUM of $1.8 billion*. Versor creates diversified sources of absolute returns across multiple asset classes. Within a scientific, hypothesis-driven framework, Versor leverages modern statistical methods and vast datasets to drive every step of the investment process. Alpha forecast models, portfolio construction, and the trading process rely on the ingenuity and mathematical expertise of 60+ investment professionals. Versor offers two categories of investment products – Hedge Funds and Alternative Risk Premia. Both are designed to provide superior risk-adjusted returns while exhibiting low correlation to traditional and alternative asset classes. Each invests in liquid, scalable markets. On average, Versor’s partners have spent over 20 years researching, investing and trading systematic alternative investment strategies.

Role Summary

The Data Engineer position will be based in Mumbai and be part of the Portfolio Analytics team. They will collaborate closely with senior researchers to design and develop a large-scale data lake. We are seeking candidates who have excelled in engineering (specifically computer science). Prior experience in investments and finance is beneficial but not mandatory.

This role is ideal for candidates who are passionate about technology and excited about building a data platform.

Responsibilities
• Design architecture for a data platform.
• Design data pipelines based on business and functional requirements.
• Extract, transform, and load logic to automate data collection and manage data processes/pipelines. This includes data quality and monitoring.
• Develop data access tools to allow researchers to access data seamlessly.
• Develop integration tools and analytical reports for the databases and data warehouse.
• Write and review technical documents. This includes requirements and design documents for existing and future data systems, as well as data standards and policies.
• Collaborate with analysts, support/system engineers, and business stakeholders to ensure data infrastructure meets constantly evolving requirements.

Requirements
• E., B.Tech., M.Tech., or M.Sc. in Computer Science, Computer Engineering or similar discipline from a top tier institute.
• 2+ years direct experience working as a data engineer.
• Experience in design, architecture and implementation of data lake, data pipelines and flows.
• Experience with developing software code and APIs in one or more languages such as Python and C#.
• Experience designing and deploying large scale distributed data processing systems with one or more technologies such as MS SQL Server, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, Hive, Teradata, or MicroStrategy.
• A high-level understanding of automation in a cloud environment (AWS experience preferred).
• Excellent communication, presentation, and problem-solving skills.
• Data as of December 31, 2022. AUM reflects regulatory AUM as per SEC definition for the purposes of Item 5.F on the Form ADV Part 1a.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tredence Inc.,Senior Data Engineer,"Associate Manager – Data Engineering (8-11 Years)

This position requires someone with good problem solving, business understanding and client presence.

Overall professional experience of the candidate should be above 8 years. A minimum of 4 years of experience in Data Engineering space. Should have good understanding of business operations, challenges faced, and business technology used across business functions.

The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.

Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.

Role Description:
• Engages with Leadership of Tredence' s clients to identify critical business problems, define the need for data engineering solutions and build strategy and roadmap
• S/he possesses a wide exposure to complete lifecycle of data starting from creation to consumption
• S/he has in the past built repeatable tools / data-models to solve specific business problems
• S/he should have hand-on experience of having worked on projects (either as a consultant or with in a company) that needed them to –
• Provide consultation to senior client personnel
• Implement and enhance data warehouses or data lakes.
• Worked with business teams or was a part of the team that implemented process re-engineering driven by data analytics / insights
• Should have deep appreciation of how data can be used in decision making
• Should have perspective on newer ways of solving business problems. E.g. external data, innovative techniques, newer technology
• S/he must have a solution creation mindset. Ability to design and enhance scalable data platforms to address the business need
• Working experience on data engineering tool for one or more cloud platforms -Snowflake, AWS/Azure/GCP
• Engage with technology teams from Tredence and Clients to create last mile connectivity of the solutions -
• Should have experience of working with technology teams
• Demonstrated ability in thought leadership – Articles/White Papers/Interviews

Mandatory Skills

Program Management, Data Warehouse, Data Lake, Analytics, Cloud Platform

Job Location - Bangalore , Chennai , Pune , Gurugram.

Experience Level - 8-11 yrs.

Expected Joining Time - Immediate to Max 30 days.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
Mercedes-Benz Research and Development India Private Limited,Big Data CoE - Engineering.IT Data Engineer,"TasksOverview
A highly motivated and technically proficient professional, capable of delivering solution on Data Engineering in Cloud platform. He or she must be skilled in developing all rituals for Data Engineering especially using pyspark.
Job Responsibilities
Data Engineer:

· Development, Enhancement, testing and release of existing application.
· Develop and enhance end to end data pipeline.
· Interpret data to analyze results to a specific business problem or bottleneck that needs to be solved using statistical techniques.
· Ensure data efficiency and reliability.
Qualification
Mandatory:
· Bachelor's /post graduate degree in Computer Science, Computer Engineering or Data Science, Data Engineering with strong knowledge in below technologies,
· Pyspark
· Databricks,
· ADLS
· SQL
Desired:
· Work experience on Agile/SDLC process.
· Experience in building Knowledge Base (KB)
· Experience in taking over Knowledge Transfer
· Open to work/explore new technology areas.
· Automotive and Aviation domain is plus.Qualifications",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
D2C Ecommerce India Pvt Ltd,D2C Ecommerce - Data Engineer,"What is D2cecommerce :D2C Ecommerce is India's first multi-D2C brand online platform that sells its own homegrown brands across multiple home and lifestyle categories, including - apparel, cosmetics, beauty, jewelry, accessories, fitness, sports, shoes, bags, books, kitchen, food, auto accessories, electronics, kids and travel packages. Along with selling these products on its own portal, D2CEcommerce also has these items listed on leading e-commerce sites.Job Summary : We are seeking a highly motivated and skilled Data Engineer to join our team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our databases and reports. You will work closely with our team of developers, data scientists, and analysts to ensure that our data systems are accurate, efficient, and scalable.What would be your responsibilities :- Design and develop databases that are scalable, efficient, and accurate- Develop and maintain ETL pipelines to move data from various sources into our databases- Create and manage database reports to provide insights to our team of data scientists and analysts- Collaborate with our team of developers to integrate our databases into our applications and services- Continuously monitor and optimize our databases for performance and security- Ensure that our data systems adhere to industry best practices and compliance regulations- Stay up-to-date with the latest database technologies and trendsWhat are we looking for in the candidate :- Ability to build things from scratch.- Self-starter and motivated individuals who can drive processes on their own- A bachelor's or associate degree in management information systems, computer science, or a related field- 0-1 years of experience in database management or a similar role- 0-1 years of experience designing, developing, and producing database reports- Proficiency in SQL and experience with relational databases such as MySQL, Postgre SQL, or Oracle- Experience with ETL tools and techniques- Understanding of data modelling concepts- Familiarity with database administration and management tools- Strong analytical and problem-solving skills- Excellent verbal and written communication skillsWhy you should join D2cecommerce :In addition to an attractive compensation package, you own a piece of the company through ESOPs. Also you will have the opportunity to take up a role in a rapidly growing, highly disruptive organization, and shape the face of ecommerce for the future.Interested? What to do next :If reading the details above excited you and made you feel you can help us take D2Cecommerece to the next level, all you have to do is let us know!",Gurugram,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Thoughtworks Inc.,Senior Consultant - Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Aryng,Sr. Data Engineer,"Welcome You made it to the job description page

Aryng is looking for a cloud data engineer with experience in developing
enterprise-class distributed data engineering solutions on the cloud. We are seeking
an entrepreneurial and technology-proficient Data Engineer who is an expert in the
implementation of a large-scale, highly efficient data platform, batch, and real-time
pipelines and tools for Aryng clients. This role is based out of India. You will work
closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength
is creative and effective problem-solving.

Key Responsibilities:

● Should have implemented asynchronous data ingestion, high volume stream data
processing, and real-time data analytics using various Data Engineering
Techniques.
● Implement application components using Cloud technologies
and infrastructure.
● Assist in defining the data pipelines and able to identify bottlenecks to enable
the adoption of data management methodologies.
● Implementing cutting edge cloud platform solutions using the latest tools and
platforms offered by GCP, AWS, and Azure.

Requirements
• Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
5+ years of data engineering experience is a must.
• 2+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers
• 2+ years' experience in Python.
• Must be strong in SQL and its concepts.
• Experience in Big Query, Snowflake, Redshift, DBT.
• Strong understanding of data warehousing, data lake, and cloud concepts.
• Excellent communication and presentation skills
• Excellent problem-solving skills, highly proactive and self-driven
• Consulting background is a big plus.
• Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study
Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
This role requires mandatory overlap hours with clients in the US from 8 am - 1
pm PST.

Benefits
• Direct Client Access
• Flexible work hours
• Rapidly Growing Company
• Awesome work culture
• Learn From Experts
• Work-life Balance
• Competitive Salary
• Executive Presence
• End to End Problem Solving
• 50%+ Tax Benefit
• 100% Remote company
• Flat Hierarchy
• Opportunity to become a thought leader

Why Join Aryng: Click on the Youtube link",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,True,True
Emerson,Data Engineer - Sustainability,"AS AN Data Engineer, YOU WILL:

· Architect and design platform solutions to meet and exceed expectations of Projects.

· Proactively evolve and apply DevSecOps methodologies, standards and leading practices

· Apply architectural standards/principles, security standards, usability design standards, as approprioate.

· Lead a project from delivery perspective, including giving periodic updates to all stakeholders.

Skills Requirements:

· Must be able to communicate fluently in English, both written and verbal

· Excellent interpersonal communication and organizational skills

· Able to distil complex technical challenges to actionable and explainable decisions

· Inspire DevSecOps teams by building consensus and mediating compromises when necessary

· Demonstrate excellent technical & architecture skills, service management and product lifecycle management

· Demonstrate ability to rapidly learn new and emerging technologies

· Operational abilities including early life support and driving root cause analysis and remediation

·Any Azure/Microsoft Big Data related certifications is highly preferred.

REQUIRED EXPERIENCE :

· Bachelor’s Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline)

· 10+ years overall IT industry experience

· 3+ years in a solution design role using service and hosting solutions such as private/public cloud IaaS, PaaS and SaaS platforms.

· Large scale design, implementation and operations of OLTP, OLAP, DW and NoSQL data storage technologies such as SQL Server, Azure SQL, Azure SQL DW, PostgreSQL, CosmosDB, RedisCache, Azure Data Lake Store, Hadoop, Hive, MongoDB, MySQL, Neo4j, Cassandra, HBase

· Creation of descriptive, predictive and prescriptive analytics solutions using Azure Stream Analytics, Azure Analysis Services, Data Lake Analytics, HDInsight, HDP, Spark, Databricks, MapReduce, Pig, Hive, Tez, SSAS, Watson Analytics, SPSS

· Design and configuration of data movement, streaming and transformation (ETL) technologies such as Azure Data Factory, HDF, Nifi, Kafka, Storm, Sqoop, SSIS, LogicApps, Signiant, Aspera, MoveIT, Alteryx, Pentaho, IDQ,

· Enablement of data reuse through Data Catalog/Marketplace, Metadata, Search and Governance technologies such as including Azure Data Catalog, Waterline, Apache Atlas, Apache Solr, Azure Search, Alteryx Connect, Datawatch Monarch Swarm, Collibra Catalog, Enigma Councourse, Adaptive, Cambridge Semantics, Data Advantage Group (DAG), Global IDs, Alation

· Experience with any of the following: Azure, O365, Azure Stack, Azure AD

· Delivery using modern methodologies especially SAFe Agile.

Requisition ID : 23000590

Emerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.",Pune,False,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Confidential,Fractal.ai - Azure Data Engineer - SQL/PySpark,"Mandatory Skills :- Azure Databricks (ADB)- Azure DataFactory (ADF)- Python or Pyspark- SQLResponsibilities :- Be an integral part of large scale client business development and delivery engagements- Develop the software and systems needed for end-to-end execution on large projects- Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions- Build the knowledge base required to deliver increasingly complex technology projectQualifications & Experience :- A bachelor's degree in Computer Science or related field with 3-12 years of technology experience- Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space- Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks- Database programming using any flavours of SQL- Expertise in relational and dimensional modelling, including big data technologies Exposure across all the SDLC process, including testing and deployment- Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.- Good knowledge of Python and Spark are required- Good understanding of how to enable analytics using cloud technology and ML Ops- Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus- Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)- Characteristics of a forward thinker and self-starter - Ability to work with a global team of consulting professionals across multiple projects- Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems- Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to success. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LatentView,Principal Data Engineer,"About LatentView:
• LatentView Analytics is a leading global analytics and decision sciences provider, delivering solutions that help companies drive digital transformation and use data to gain a competitive advantage. With analytics solutions that provide 360-degree view of the digital consumer, fuel machine learning capabilities and support artificial intelligence initiatives., LatentView Analytics enables leading global brands to predict new revenue streams, anticipate product trends and popularity, improve customer retention rates, optimize investment decisions and turn unstructured data into a valuable business asset.
• We specialize in Predictive Modelling, Marketing Analytics, Big Data Analytics, Advanced Analytics, Web Analytics, Data Science, Data Engineering, Artificial Intelligence and Machine Learning Applications.
• LatentView Analytics is a trusted partner to enterprises worldwide, including more than two dozen Fortune 500 companies in the retail, CPG, financial, technology and healthcare sectors.

Job Description:
As a Manager - data engineer, they should build and maintain scalable, rock solid self-serve data pipelines for data analysts and data Scientists and support them by understanding the content and context of data and collaborating with them to figure out the best way to Extract, Transform, Load, and access it.
• Be an SME in DE and should know how to set up the process, requirement gathering for any movement or data ingestion and data platform creation
• Has to be the end-to-end project manager, allocating work, monitoring progress, providing feedback, and taking necessary steps to ensure agreed timelines are met
• Scripting skills: SQL and Python or PySpark
• Excellent understanding of Data Warehouse and its architecture
• Excellent understanding of Issue Tracking System like JIRA/ Dev-Ops
• Excellent experience in Version control like Github or Gitlab
• Ideal to have knowledge of other DE platforms like Azure databricks, Snowflake etc.

Education: UGEmployment Type: CONTRACTOR",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
CIEL HR Services,Data engineer,CTC-6LPA EXP-Freshers Area of Expertise - A) Data Hygiene B) Application of ML DL Tools If interested please call 9000338173 or forward cv to [Confidential Information],Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer - ETL,"Design, build, and maintain the data infrastructure that supports our data-
driven applications and services.• Develop and maintain ETL (Extract, Transform, Load) processes to ensure
data accuracy and completeness.
• Optimize data pipeline performance and scalability.
• Collaborate with data scientists, analysts, and developers to ensure that our
data pipeline meets their needs.• Implement data governance policies and procedures to ensure data quality
and consistency.
• Design and develop data models that support data-driven applications and
services.
• Troubleshoot and resolve issues related to the data pipeline.
• Participate in the evaluation and selection of data management and analytics
tools and technologies.
• Keep up to date with emerging trends and technologies in data engineering
and big data.

experience

8",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Swift,Data Engineer,"About us:

Swift is building a next generation checkout stack for India - a platform rolling up payments and logistics solution for all fulfillment needs. We give businesses the opportunity to provide a customer experience at par with the likes of Amazon and Flipkart, all the while saving money and time.

Its basically Amazon without the website listing - we let our sellers design their own sales channel :-)

We believe there are many things a seller or small business has to worry about when selling online, logistics/payments/etc shouldn't be one of them. With our solution, SMBs and D2C brands get access to technologies and services like next day delivery, same day delivery, live package tracking, Card/Cash on delivery, scheduled delivery etc, making parcel delivery just as simple as collecting payment.

We also provide robust APIs which makes it easy for developers to add shipping capabilities to their multichannel online store.

We want to be the #1 checkout platform that’s reliable, easy to use and affordable.

About you:

You have experience in working with data pipelines and ETL sets (programmatically and using tools) – say MongoDB, Spark streaming, Python/Java, Apache Beam, PARQR, Delta Lake, Airflow, etc. You are looking for challenges in growing a data backed company to deal with from hundreds to millions of visitors data points per month.

You like working with streaming/reactive architectures and have experience/interest in setting up data pipelines on cloud infra from scratch. You generally prefer to use a minimal set of simple tools to a diverse range of complex ones. We are looking to build a back-end cloud infrastructure (Google Cloud Platform preferably) which will be a fault-tolerant real-time stream processing system on the cloud - Our system will need to meet liveliness guarantees from a big data/ETL perspective.

You like to work on a variety of projects - at this job, you’ll be developing a complex ETL infra, a reactive streaming architecture and a cloud-native, highly available API for our customers.

You are someone who is:
• Experienced in any JVM based language or Python.
• Have worked on NoSQL (MongoDB)/ SQL databases.
• Have worked on creating data pipelines (both programmatically, say using spark streaming) or using tooling like Airflow, dataflow)
• Strong verbal and written communication skills and the ability to work well cross-functionally.
• We offer: *
• You to be a part of a small, but a super capable team.
• The opportunity to work closely with founders to define, scope, estimate and plan various aspects of the product.
• Being one of the first hires at Swift, you will be involved in both high and low-level decision making. This means a lot of ownership, which we cultivate by having a flat structure.

Swift focuses on E-Commerce, B2B, Small and Medium Businesses, Logistics, and D2C. Their company has offices in Bengaluru. They have a mid-size team that's between 51-200 employees. To date, Swift has raised $2.34M of funding; their latest round was closed on July 2021.

You can view their website at https://goswift.in or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
CarbyneTech India Pvt Ltd,CarbyneTech - Azure Data Engineer - IoT,"- Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions - Azure Synapse/Azure SQL DWH, Azure Data Lake, Azure Blob Storage, Spark, HDInsights, Databricks, CosmosDB, EventHub/IOTHub.- Experience in migrating on-premise data warehouses to data platforms on AZURE cloud.- Designing and implementing data engineering, ingestion, and transformation functions- Azure Synapse or Azure SQL data warehouse- Spark on Azure is available in HD insights and data bricks- Good customer communication.- Good Analytical skill",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Brillio,GCP Data Engineer - R01523647,"About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

GCP Data Engineer
Primary Skills

• BigQuery, Cloud Logging, Cloud Storage, Cloud Trace, Composer, Data Catalog, Data Modelling Fundamentals, Data Warehousing, Dataflow, Datafusion, Dataproc, ETL Fundamentals, Modern Data Platform Fundamentals, PLSQL, T-SQL, Stored Procedures, Python, SQL, SQL (Basic + Advanced)

Specialization

• GCP Data Engineering Basic: Senior Data Engineer

Job requirements

• Job description below. • 6 years of experience in software design and development • 5 years of experience in the data engineering field is preferred • 3 years of Hands-on experience in GCP cloud data implementation suite such as Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage, • Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms. • Mandatory 1 year of software development skills using Python • Extensive hands-on experience working with data using SQL and Python • Cloud Functions. Comparable skills in AWS and other cloud Big Data Engineering space is considered. • Experience in DevOps(CI/CD) pipeline facilitating automated deployment and testing • Experience with agile development methodologies • Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions • Bachelor's Degree in Computer Science, Information Technology, or closely related discipline

Know what it’s like to work and grow at Brillio: Click here",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,True,False
Latent View Analytics Private Limited,Data Engineer,"Job Title :
Data Engineer Experience : 2.5-5 Location : INDIA
• Chennai Job Description: Experience working with Cloud Data Platforms, especially AWS and its services, must be strongly experienced in building data pipelines.
Experience with big data tools like Python, Pyspark, and Spark SQL. Focus on scalability, performance, service robustness, and cost trade-offs.

A continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs.
Attention to detail, coupled with the ability to think abstractly. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product. Keen to learn new technologies and apply the knowledge in production systems.

AWS skills:
AWS Glue AWS EMR (any two AWS services)

Data Engineer Skills:
Python Pyspark Spark SQL HIVE HQL Scala Good to have: Snowflake querying Databricks AWS API Gateway AWS Lambda",Chennai,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,True
Whiteforce,Data Engineer - Java,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20246

LocationIndia

Job Descriptions

Job Purpose and Primary Objectives : Develop and Deploy data and analytics-led solutions on GCP Key responsibilities (please specify if the position is an individual one or part of a team): Data engineering solution on GCP using Cloud Bigquery, Cloud Dataflow, Pu-Sub, Cloud BigTable and AI/Ml solutions Key Skills/Knowledge : - Good Experience in GCP. - Python/Java, PySpark/Spark Java. - GCP BigQuery. - GCP Pub-Sub. - Secondary Skill - DataFlow, Compute Engine, Cloud Fusion. (ref:hirist.com)

Skills",,True,False,False,True,False,False,False,False,False,False,False,False,False,True,False,False
Confidential,Big Data Engineer - Python/AWS,"JOB_DESCRIPTION :- Has experience in the following #Python, #AWS_Athena, #Glue #Pyspark, #EMR, #DynamoDB, #Redshift, #Kinesis, #Lambda, #Snowflake.- Proficient in #AWS_Redshift, #S3, #Glue, #Athena, #DynamoDB.- Design, build and operationalize large-scale enterprise data solutions and applications using one or more of #AWS_data and analytics services in combination with 3rd parties - #PySpark, #EMR, #DynamoDB, #RedShift, #Kinesis, #Lambda, #Glue, #Snowflake. - Analyze, re-architect, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services. - Design and build production data pipelines from ingestion to consumption within a big data architecture, using Python/PySpark. - Design and implement data engineering, ingestion, crawling, manipulation and curation functions on AWS cloud using AWS native or custom programming. - Must have strong knowledge of databases like Postgres, MySQL, MongoDB, Cassandra, etc. - Must have experience in using AWS SDKs and libraries for interacting with different AWS services. - Experience in building REST APIs. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Varite India Private Limited,Data Engineer,"snowflake- Data Engineer Data engineering, integration, and data modeling experience . Can write scalable/performant pipelines, queries, and summaries of data . Has worked with various data systems and tools . Understands analytics and data science workflows and common use cases that leverage their work . Python . SQL . Datawarehouse experience . AWS experience . Data QA / validation skills (to check their work) . Snowflake experience (MUST) . Matillion, DBT, or other Data tech experience (ideal) . Marketing technology experience (ideal)",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
deloitte,Consulting-SAMA- A&C-Data Engineer/Architect-Associate Director,"Location: No Preference

Years of Exp: 10-15 Years

Hybrid: Yes

Mandatory Skill: MS Azure, Analytics, Delivery Management & Operations, People Management

Responsibilities:
• 10 - 15 years of deep delivery experience in cloud data engagements, with proven experience to lead teams sizes of 10+ resources
• Experience with platforms such as Azure Cloud Services, Solution Design & Review, Data Management, Data Visualization Tools
• Deep experience handling large volumes of data across multiple data sources like csv, relational data, SAP, json, parquet, flat files, streaming data, etc.
• Strong conceptual understanding of data warehousing, data modeling principles
• Strong Experience with visualization / reporting solutions
• Good understanding of data quality, data management and data governance principles
• Depth in data transformation / data modernization / ETL and ELT based data pipeline development
• Hands on with Agile/Scrum Methodology based implementations and end to end software delivery lifecycle
• Exposure working with international clients / geographies
• Excellent client handling, team handling and interpersonal skills
• Excellent written and spoken communication skills

Ability to understand and appreciate business / domain context and develop data and analytics solutions",Hyderabad,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Wipro Technologies,Data Engineer,"Share resume to akshara.raju@wipro.com

Location - Bangalore , Chennai , Pune , Hyd

JD :
• Azure data factory
• Azure data bricks with PySpark coding experience
• Experience in Snowflake
• Good to have knowledge in data visualization tool Tableau or PowerBI
• Good to have knowledge in data warehousing & data analytics

Data Analyst / Data Engineer
• 5 yrs. experience in working with large, complex data sets
• Create reports for internal teams and/or external clients
• Hands on Experience on Azure Data Factory and Azure
• Need to be able to code the data pipelines from ADF standpoint / Data Ingestion.
• Collaborate with team members to collect and analyze data
• Knowledge of Snowflake will be a big plus
• Need to ensure they can work on Data Mapping / Data Enrichment and Data Transformations.
• Use graphs and other methods to visualize data
• Establish KPIs to measure the effectiveness of business decisions
• Structure large data sets to find usable information
• Reporting and Data Visualization skills
• Experience in Data Mapping, data cleansing.",Bengaluru,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
ANI Calls India Private Limited,Lead Snowflake Data Engineer,"Anicalls

Industry : IT

Total Positions : 3

Job Type : Full Time / Permanent

Gender : No Preference

Salary : 900000 INR - 1800000 INR (Annually)

Education : Bachelor s degree

Experience : 5-10 Years

Location : Noida, India

Candidate should have :

Knowledge and experience in Big data and Data Vault methodology

Experience in power shell, shell scripting, and python

Exposure to data modeling for Snowflake

Experience in working with agile / scrum methodologies.

Experience in building data pipelines for large volumes of data across disparate data sources

Experience in DBT for Snowflake

Good experience on Azure Databricks

experience in Confluent cloud platform

Experience in building pipelines through Confluent Kafka and Knowledge of Azure Kubernetes Service

Good communication and presentation skills

Expertise in building data pipelines for Snowflake using Snowpipe, Snowpark, SnowSQL's and stored procedures.

Azure experience must be focused on Azure Data Factory, Azure storage solutions (such as Blob and Azure Data lake Gen2), and Azure data pipelines

Good experience on Snowflake and Snowflake architecture

7+ years of total experience in data projects with a focus on data integration and ingestion

3+ years of experience working primarily on Snowflake",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Impetus Technologies India Pvt. Ltd,GCP Data Engineer,"Role : GCP Data Engineer

Job Description :

- The candidate should have extensive production experience in GCP, Other cloud experience would be a strong bonus.

- Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.

- Exposure to enterprise application development is a must.

Roles & Responsibilities :

- 6-10 years of IT experience range is preferred.

- Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.

- Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOps.

- Good hands on expertise on either Python or Java programming.

- Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.

- Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.

- Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.

- Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.

- Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.

- Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.

- Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.

,",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Stantec Technology International,Senior Data Engineer,"Description
Grow with the best. Join a smart, creative, and inspired team that works behind the scenes to support operational excellence. As part of the Innovation Office, the Digital Technology & Innovation team is composed of digital experts who conduct research and development to keep our teams and our client's projects ahead of the technological curve. They implement established technologies and find emerging solutions for all business lines (Buildings, Energy & Resources, Environmental Services, Infrastructure, and Water), bridging existing knowledge domains and facilitating the integration of powerful tools and methods. The team's goal is to make projects more efficient and help provide higher-quality results to our clients. The ideal candidate will be a self-starter, a critical thinker, and highly interested in the application of new technologies and methods. The candidate will become a member of the Innovation Office, however, he or she will also be accessible to Stantec's project teams to support project work as needed.

Your Opportunity
The Innovation Office's Digital Technology & Innovation (DTI) team has an opportunity for a Senior Data Engineer. This position requires a person who is technically savvy, experienced in data engineering, and enjoys working with data to solve business problems, shaping & creating solutions, and helping to champion implementation. As a member of the Innovation Office, the Digital Technology Development group, as part of the DTI team, also engages in research & development and provides guidance and oversight as a center of excellence for the business. This group also engages in new product research and testing and the incubation of new ideas. The candidate will be responsible for the delivery of professional services and will recommend solutions to achieve complex strategic objectives across our large global team spanning Stantec IT, Business Lines, and the Office of Innovation.

Your Key Responsibilities
Serve at the direction of the Digital Technology Development Leader to:
- Take ownership of the project, work independently in a team environment, and mentor others as needed.
- A passion for solving problems and providing workable solutions, flexible to learn new technologies to meet the business needs.
- Translate complex functional and technical requirements into detailed designs.
- Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining.
- Implement access governance of production data systems to ensure compliance with our privacy and security policies.
- Oversee, design, and develop algorithms for real-time data processing within the business units and to create the frameworks that enable quick and efficient data acquisition.
- Build and maintain best practices to support the Continuous Integration and Delivery (CI/CD) of data engineering solutions.
- Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets (batch and stream data) into a readable and accessible format for end-user facing reports, data science, and ad-hoc analyses.
- Build and maintain reliable and scalable ETL on big data platforms as well as work with varied forms of data infrastructure inclusive of relational databases and NoSQL databases.
- Work collaboratively with DTI's Data & Analytics group, Stantec's internal business units, and clients to define problem statements, collect data and define solution approaches.
- Deliver highly reliable software and data pipelines using Software Engineering best practices like automation, version control, continuous integration/continuous delivery, testing, security, etc.
- Possess excellent time-management skills, a thorough understanding of task assignments and schedules, and efficient use of time and available resources.
- Perform other miscellaneous tasks associated with being a member of the Digital Technology & Innovation team and those typical of a data engineer.",Pune,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
TensorGo Technologies,TensorGo Technologies - Senior Data Engineer - Python,"Skillset : Python, PySpark, Kafka, Airflow, Sql, NoSql, API Integration,Data pipeline, Big Data, AWS/ GCP/ OCI/ AzureRequirements :Understanding our data sets and how to bring them together.Working with our engineering team to support custom solutions offered to the product development.Filling the gap between development, engineering and data ops.Creating, maintaining and documenting scripts to support ongoing custom solutions.Excellent organizational skills, including attention to precise detailsStrong multitasking skills and ability to work in a fast-paced environment3+ years experience with Python to develop scripts.Know your way around RESTFUL APIs.[Able to integrate not necessary to publish]You are familiar with pulling and pushing files from SFTP and AWS S3.Experience with any Cloud solutions including GCP / AWS / OCI / Azure.Familiarity with SQL programming to query and transform data from relational Databases.Familiarity to work with Linux (and Linux work environment).Excellent written and verbal communication skillsExtracting, transforming, and loading data into internal databases and HadoopOptimizing our new and existing data pipelines for speed and reliabilityDeploying product build and product improvementsDocumenting and managing multiple repositories of codeExperience with SQL and NoSQL databases (Casendra, MySQL)Hands-on experience in data pipelining and ETL. (Any of these frameworks/tools: Hadoop, BigQuery, RedShift, Athena)Hands-on experience in AirFlowUnderstanding of best practices, common coding patterns and good practices aroundstoring, partitioning, warehousing and indexing of dataExperience in reading the data from Kafka topic (both live stream and offline)Experience in PySpark and Data framesResponsibilities :You'll :Collaborating across an agile team to continuously design, iterate, and develop big data systems.Extracting, transforming, and loading data into internal databases.Optimizing our new and existing data pipelines for speed and reliability.Deploying new products and product improvements.Documenting and managing multiple repositories of code.",,True,False,True,False,False,False,False,True,False,False,False,False,True,True,True,False
Confidential,Data Engineer (Data Bricks) Manager,"EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL's proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients' decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries. Please visit www.exlservice.com for more information about EXL Analytics. Location - Bangalore/ Gurgaon Note- Currently Remote & expecting candidates to relocate to either Bangalore/Gurgaon once office reopens.Requirements: 7+ years of Data engineering exp with 3+ years hands on Databricks (DB) experience. Should be able to create New Clusters, Cluster Pools and attach existing clusters to pool in DB. Should have some pool management experience. Should be good in Datalakehouse concepts. Should have good experience in Data Engineering in Databricks Batch process, Streaming is good to have. Should have good experience in creating Workflows & scheduling the pipelines. Should have good exposure on how to make packages or libraries available in DB. Should have good experience in Databricks default runtimes, Photon & Light is good to have. Some experience in Databricks SQL / DW in Databricks. Delta Live Tables experience is good to have. IT Services and IT Consulting",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Streamline Digital,Sr. Data Engineer- Azure/ Snowflake/ Databricks,"Sr. Data Engineer

Who We Are

At Streamline, we are experts in Enterprise Mobility, Product Engineering, and IT Transformation. We help organizations navigate the constantly evolving landscape of IT. Our sole focus is ensuring that our client’s organization is armed with the strategies, products and solutions that are transformative to their business. Streamline works closely with our clients, takes pride in developing genuine relationships and embraces open communication and collaboration with our clients. We become a part of our client’s team, working together to achieve short-term goals and enable long-term success. Our team is comprised of world-class strategists, architects, engineers, and developers.

In our new flagship product, iEnterprise, we are taking things to the next level, using our collective experience and customer input to create new enterprise mobility management products that reduce operational costs, prevent issues before they happen, and resolve issues faster than with traditional tools and approaches.
Role Summary

This position is full time remote position. The Data Engineer will work with a team of other software developers to define, design, develop, integrate, and re-engineer the Enterprise Data warehouse. Data Marts, Data Virtualization and Data Visualization components in different environments which meet customers’ analytical and business intelligence requirements, scales easily and supports deployment in highly available environments. The Senior engineer acts as the development and technical lead and individual contributor on complex projects, contributing to strategic vision and technical decisions, participating in vendor analysis and selection projects, introducing process improvements, completing proof-of-concept projects for the introduction of changes to our architecture, and providing oversight on the work of other technical staff.

Role Responsibilities
• Build data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective
• Create and maintain highly scalable data pipelines using Databricks, Azure, AWS, Kafka.
• Design and build ETL/ELT data pipelines to extract and process data from a variety of external/internal data sources.
• Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery
• Build data insights, data analytics, ML models, fraud and anomaly detection using Snowflake
• Build and deploy modern data solutions.
• Define, implement and build jobs to populate data models.
• Relational and NoSQL Database management
• DevOps building CI/CD pipelines

Technical System Expertise: Understands data warehouse development practices and processes including ETL design, Dimensional models, slowly changing dimensions, Data Security components, Data quality methods, how MPP databases operate, and data flows. Aware of current technology benefits. Expected to independently develop full stack ETL solution from source to staging to presentation layers. Understands the building blocks, interactions, dependencies, and tools required to complete BI Data warehousing software and automation work. An Independent study of current technology is expected. Interact with system engineers to define continuous delivery and/or DevOps solutions, data security, and/or necessary requirements for automation.

Technical Engineering Services: Supports BI Data Warehousing and Enterprise Data Solutions projects by analyzing, designing, and developing ETL solutions; conducting tests and inspections; creating BI reports and virtualization components. Create interface jobs/APIs for data sharing with internal and external stakeholders Ensure we use accurate and secure methods to extract data. Analyze Big Data and MPP Databases to discover trends and patterns. Participates in reviews (walkthroughs) of technical specifications and program code with other members of the DevOps team. Expected to supervise associate engineers on occasion.

Innovation: Presents new ideas which improve existing BI Data Warehousing systems/processes/services. Presents new ideas which utilize new frameworks and reusable components to improve existing ETL jobs/processes/services. Express new perspectives based on an independent study of the industry. Review current company processes to highlight questions that may drive process refinement and optimization.

Technical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the engineer level. Develop application support documentation as required by the application support teams for acceptance of systems changes into production.

Technical Leadership: Collaborates with other engineering, development teams and utilizes data engineering/analyst expertise to deliver technical solutions. Continuously learn and mentor on new technologies.
Qualifications & Skills
• Bachelor’s degree in CS, Statistics, Information systems or equivalent experience.
• Strong expertise working with relational databases. Exceptional ability to author and optimize complex SQL queries/workflows Strong analytical skills to work with unstructured data.
• Experience building and optimizing highly scalable data pipelines, architectures, and data sets.
• A successful history of manipulating, processing, and extracting value from large, disconnected datasets and provide valuable insights.
• Proficiency in workflow orchestration (Databricks, Azure data factory).
• Experience working with streaming data solutions such as Kafka, IoT hub and Spark Streaming.
• Working experience building insights, ML models, fraud and anomaly detection using Snowflake/Databricks.
• Experience working with NoSQL databases like MongoDB, Cassandra, Cosmos DB
• Proficiency in Java/Python/Scala, pandas, pySpark, NumPy
• 8+ years of ETL Development experience using Azure Databricks, Azure data factory, SSIS, Talend.
• 8+ years Professional experience in designing and building cost-effective large scale data marts, data warehousing, distributed big data processing MPP Systems (Databricks, Spark, Snowflake)
• 5+ years of Expertise in Logical and Physical Data Model design using various modelling Tools like Erwin 7.3 and Power Designer.
• 5+ years of experience with Azure and AWS cloud stack
• 3+ years BI Visualization experience developing reports using Snowflake, PowerBI, Grafana, Qlik and analytic cubes utilizing SQL Server Analysis Services (SSAS).

Preferred Skills
• Certifications
• DevOps experience
• Bash, Golang scripting experience
• Docker/Kubernetes experience
• CI/CD pipelines

Powered by JazzHR",Hyderabad,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,True
Whiteforce,Data Engineer - ETL/,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20453

LocationIndia

Job Descriptions

Key Responsibilities - ETL pipeline design and implementation - Streaming and batch data processingStorage optimization - Storage optimization - DataOps / MLOps - Preparing, cleaning, structuring data for statistical modeling / machine learning - Deployment, packaging, versioning of Machine Learning models that operate on data Educational Qualification & Experience - BE/BTech in Computer Science or Information Systems - (Preferred) ME/MTech in Computer Science or Information Systems - Minimum 5 years total experience working in the industry - Minimum 3-5 years experience in Data Engineering Knowledge and Skills Required - Experience with Big Data technologies: Presto / Trino / Hive / Hadop - Cloud data storage, query and analytics technologies, esp. on AWS - S3 / Athena / Glue or Elastic Stack (specifically for data/ML workflows) - Message passing / data broker systems: Kafka / RabbitMQ

Skills",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Apple,Cloud Data Engineer,"Summary

The people here at Apple don’t just build products — they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Imagine what you could do here. Are you passionate about handling large & complex data problems, want to make an impact and have the desire to work on groundbreaking big data technologies? Then we are looking for you. At Apple, phenomenal ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Business Intelligence team is looking for passionate, technical savvy, energetic leader who like to think creatively. Someone who is self motivated and ready to lead team of most hardworking engineers building high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse team deals with Petabytes of data catering to a wide variety of real- time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We leverage a diverse technology stacks such as Snowflake, AWS, Teradata, HANA, Vertica, Single Store, Dremio, Hadoop, Kafka, Spark, Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.

Key Qualifications

High expertise in modern cloud data lakes and implementation experience on any of the cloud platforms like AWS/GCP/Azure - preferably AWS.

Good Experience in cloud based data warehouse - Snowflake.

Hands on Experience in developing and building data pipelines on Cloud & Hybrid infrastructure for analytical needs- Preferably having Cloud certifications.

Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.

Database development experience with Relational or MPP/distributed systems such as Teradata/ SingleStore/ Hadoop

Experience working with data at scale (peta bytes) with big data tech stack and sophisticated programming languages is a plus e:g Python, Scala.

Description

As a Cloud Development Engineer you will design, develop and implement modern cloud data warehouse/ datalakes and influence overall data strategy for the organization

Translate sophisticated business requirements into scalable technical solutions meeting data warehousing/analytics design standards

Strong understanding of analytics needs and proactive-ness to build solutions to improve the efficiency along with that help execute leading data practices & standards

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business

Ability to communicate effectively, both written and verbal, with technical and non- technical multi-functional teams

You will engage with many other group’s & internal/external teams to deliver best-in-class products in an exciting constantly evolving environment

Education & Experience

Bachelors or Masters Degree in Computer Science or equivalent in Engineering.

Role Number: 200154652",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
SecureKloud,Data Engineer,"The candidate will work in a cloud development team using Informatics Data Architecture and Data Engineering concepts to deliver foundational data capabilities such as Data Lakes and Master Data Management. The candidate will be part of the DevOps Team and responsible for building data platforms, test cases, deployment documentation, and support documentation in accordance with internal and industry standards. This is a highly technical and hands-on role.

Responsibilities
• Defining database design, data flows, and data integration techniques
• Design and build data solutions ensuring data quality, reliability, and availability
• Create data processing routines for managing enterprise master data throughout the data lifecycle (capture, processing, and consumption)
• Maximize business outcomes using Mobile Device Management via improved data integrity, visibility, and accuracy
• Manage and evolve global data lakes platforms
• Develop data ingestion, data enrichment, data deidentification routines, and RESTful APIs for consumption of data lakes data",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Varite India Private Limited,Data Engineer II (India - Contract),"Description: Location: Gurgaon / Bangalore Contract Duration: 8 months (extension and conversion based on project and performance basis) Shift: Early US (over lapping with India) Please provide 2-3 values or traits that are important to this role: Strong data warehouse and modeling skills You are a self-starter who is highly organized and communicative Deals well with ambiguity Please list 3-4 functional activities the resource should be capable of: Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) What technical skills will successful candidates possess Bachelor's or Master's degree in Computer Science or Engineering with 6+ years of proven experience in related field Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Experience building low-latency data product APIs You value strong pull request reviews, understand when to stand your ground and when to let go You are a self-starter who is highly organized, communicative, quick learner, and team-oriented Successful background as a technical leader driving cross-organizational data initiatives to completion Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Deep expertise in data access patterns, data validation, data modeling, database performance, and cost optimization Hands-on knowledge with BI tools, modern OLAP engines such as Presto, Clickhouse, Druid, Pinot, and data processing frameworks such as Spark and Flink Experience establishing and applying standards for operational excellence, code quality, and software engineering best-practices Effective verbal and written communication skills with the ability to think strategically about technology decisions and manage relationships with stakeholders and senior leadership Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage Experience with BI tools like Tableau, DataDog Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) Experience working with SQL & NoSql databases along with programming experience in Python and/or JAVA Strong business acumen with an understanding of business drivers and of how to drive value by supporting data discoverability across the organization Experience building low-latency data product APIs Experience working in an agile environment Tell us about your team: Clientis seeking a data engineer to join the Business Enablement Technology team. As we challenge the status quo and take the CTO's agenda to the next level, your focus will be on building a data platform to enable analytics and reporting capabilities across Product & Technology. This is an exciting, high-impact, and cross-functional role. This role requires a highly inventive individual with strong technical and analytical skills, execution drive, and self-motivation. Is there any industry specific experience that would separate one candidate from another Experience migrating data/apps from on-prem to cloud, engineering degree, tech industry experience",Gurugram,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
ITC Infotech India Ltd,Azure Data Engineer,"• Azure Data Engineer
• Ability to understand the functional & technical specification to develop the Notebooks
• Perform Data Loads and Transformations
• Schedule the Jobs with the Azure Data Factory and monitoring the jobs
• Strong in write complex SQL queries",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
bp,Senior Data Engineer - dataWorx,"Job Profile Summary
Role Synopsis:
As part of bp “reinvent”, we have created a major new business line called “Innovation & Engineering” (I&E). One key remit of this group is to drive the transformation of the company through its use of digital and data. A major digital sub-team within I&E is Digital Production & Business Services (DP&BS). DP&BS are responsible for all digital and data initiatives and operations across the following areas of the bp business:
• Production & Projects including Health, Safety, Environment & Carbon
• Refining & Operations
• Wells & Subsurface
• Business Services including Finance, Procurement, People & Culture, Performance Management
• Strategy & Sustainability
• “DataWorx” is the name of the data team that is responsible for all data within these areas and we are developing deep data capabilities to transform the access, supply, control and quality to our vast and ever growing data reserves that are measured in Petabytes. The DataWorx team covers many data sub-disciplines, including data science, data analytics, data engineering and data management as well as specialist areas such as geospatial, remote sensing, knowledge management and digital twin. The DataWorx team works with a wide variety of data from structured data to unstructured data & we also work on Real-time streaming data processing along with Batch data processing. Key Responsibilities :
• Architects, designs, implements and maintains reliable and scalable data infrastructure
• Leads the team to write, deploy and maintain software to build, integrate, manage, maintain, and quality-assure data
• Architects, designs, develops, and delivers large-scale data ingestion, data processing, and data transformation projects on the Azure cloud
• Mentors and shares knowledge with the team to provide design reviews, discussions and prototypes
• Leads customer discussions from a technical standpoint to deploy, manage, and audit best practices for cloud products
• Leads the team to follow software & data engineering best practices (e.g. technical design and review, unit testing, monitoring, alerting, source control, code review & documentation)
• Leads the team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline
• Leads the team in following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Actively contributes to improve developer velocity
• Part of a cross-disciplinary team working closely with other data engineers, software engineers, data scientists, data managers and business partners in a Scrum/Agile setup
• responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Work closely with other data engineers, software engineers, data scientists, data managers and business partners

Job Advert
Job Requirements :
Education :
Bachelor or higher degree in computer science, Engineering, Information Systems or other quantitative fields

Experience :
• Years of experience: 8 to 12 years with minimum of 5 to 7 years relevant experience
• Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
• Hands on experience with:
• Databricks and using Spark for data processing (batch and/or real-time)
• Configuring Delta Lake on Azure Databricks
• Languages : Python, Scala, SQL
• Cloud platforms : Azure (ideally) or AWS
• Azure Data Factory
• Azure Data Lake, Azure SQL DB, Synapse, and Cosmos DB
• Data Management Gateway, Azure Storage Options, Stream Analytics and Event Hubs
• Designing data solutions in Azure incl. data distributions and partitions, scalability, disaster recovery and high availability
• Data modeling with relational or data-warehouse systems
• Advanced hand-on experience with different query languages
• Azure Devops (or similar tools) for source control & building CI/CD pipelines
• Understanding Data Structures & Algorithms & their performance
• Experience designing and implementing large-scale distributed systems
• Deep knowledge and hands-on experience in technologies across all data lifecycle stages
• Stakeholder management and ability to lead large organizations through influence

Desirable Criteria :
• Strong stakeholder management
• Continuous learning and improvement mindset
• Boy Scout mindset to leave the system better than you found it

Key Behaviours :
• Empathetic: Cares about our people, our community and our planet
• Curious: Seeks to explore and excel
• Creative: Imagines the extraordinary
• Inclusive: Brings out the best in each other

Entity
Innovation & Engineering

Job Family Group
IT&S Group

Relocation available
Yes - Domestic (In country) only

Travel Required
Yes - up to 10%

Time Type
Full time

Country
India

About BP
INNOVATION & ENGINEERING
Join us in creating, growing, and delivering innovation at pace, enabling us to thrive while transitioning to a net zero ‎world. All without compromising our operational risk management.

Working with us, you can do this by:
• deploying our integrated capability and standards in service of our net zero and ‎safety ambitions
• driving our digital transformation and pioneering new business models
• collaborating to deliver competitive customer-focused energy solutions
• originating, scaling and commercialising innovative ideas, and creating ground-breaking new ‎businesses from them
• protecting us by assuring management of our greatest physical and digital risks

Because together we are:
• Originators, builders, guardians and disruptors
• Engineers, technologists, scientists and entrepreneurs‎
• Empathetic, curious, creative and inclusive

Experience Level
Intermediate",Pune,True,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Domnic Lewis Private Limited,Big Data Engineer,"we are hiring for Big Data Engineer with the experience in spark , python , SQL , AWS glue Big Data Engineer: Spark, Python, SQL, AWS Cloud (Glue, Lambda, Athena) Hyderabad Location A big data engineer is an information technology (IT) professional who is responsible for designing, building, testing and maintaining complex data processing systems that work with large data sets.",Secunderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Invsto,Data Engineer Intern (2024/2025 graduates),"You are
• A self-starter who is fueled by a desire to improve customer experiences in the moments that matter most, approaching your work with a bias toward accountability, decision-making and action
• Inquisitive and creative, with an ability to listen to identify customer needs and dig deeper to understand the reasons behind those needs
• Able to transform conceptual thinking into deliverables that generate excitement, feedback and alignment among stakeholders
• Thrive in a collaborative environment, partnering across the company with business experts, software developers, data engineers, and marketers

You have
• Enthusiasm to take the initiative to tackle problems and work with others to expand on your experience and expertise
• Experience with Python, AWS and databases such as Postgres
• Know-how to build data engineering pipelines using services such as Airflow

You will

Work closely with and learn from a team of engineers to contribute to a build a variety of features and infra.

Must-Have skills
• Python, Database experience
• Django (in lieu of Django, an equivalent tech stack)

Qualification and Experience:
• 0-2 Years software engineering
• Education: BE/BTech or equivalent (in lieu of academics, equivalent software developer experience required)

Benefits
• Fully remote, forever
• Annual retreats

About Us

Invsto is building the future of financial engineering.

We believe in hiring the best and providing complete autonomy to our employees to build stuff that they think would make a difference to the world

How to Apply

Does this role sound like a good fit? Email us at [hello@invsto.com].
• Include the role's title in your subject line.
• Send along links that best showcase the relevant things you've built and done (Github, Behance, Dribbble etc)

Invsto focuses on Hedge Funds and Stock Exchanges. Their company has offices in Bangalore Urban. They have a small team that's between 11-50 employees.

You can view their website at https://invsto.com/ or find them on LinkedIn.",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False
Everyday Health Group,Data Engineer,"Description

Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.

Health eCareers, a property of Everyday Health Group, is looking for a Data Engineer to support our growing business.

Key Responsibilities
• Understand overall data collection strategies and implement them in data tables and connections.
• Program Python-based APIs and web services per implementation schema, such as creating applications to access Facebook, Twitter, Salesforce, and other data sources.
• Use SQL Server, MySQL, HDFS and AWS to design, develop and deploy data processing.
• Analyze and organize raw data from various ETL tools.
• Monitor/Forecast computing resources usage (data lakes, AWS, EC2, etc.).
• Collaborate with ETL developers located at various offices (US and India)
• Implement coding standards, procedures and techniques, concluding writing technical code base.
• Detect, repair, prevent data pipeline failures; identify systematic weaknesses and provide pre-emptive remedies with programming or processes.
• Build and optimize data storage systems.
• Prepare data for prescriptive and predictive modeling.
• Recommending and implement emerging database technologies.
• Create automation for repeating database tasks.

Job Qualifications
• 4+ years of programming experience with an emphasis on data processing.
• Coding skills: Python, SQL.
• Ability to create object-oriented programming and data architectures
• Knowledge of new, leading technology strategy in data engineering and management
• Understanding of industry technologies in scalability, performance, delivery pipeline and maintenance of these tools and systems.
• Hands-on experience with SQL database design, AWS or other cloud systems.
• 2+ year’s experience in Linux environments.
• Experience with SQL Server, MySQL or other popular database management tools.
• Good/Expert knowledge of API services
• Familiarity with Agile process
• Clear documentation requirements and specifications
• Bachelors or Advanced Degree in Information Management, Computer Science or related field.

Desirables:
• Experience in AWS, Tableau
• Experience in developing custom data pipelines in HDFS
• Experience working with social, Double Click, Adobe APIs
• Working with teams across multiple locations
• Skills: Pig; Hive; Spark; Impala; EMR

Our Culture and Values

We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.

Life at Everyday Health

At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work.

#HealtheCareers",,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Unusual Hire,Data Engineer,"Job type: Partial Onsite

Expert Level

Project detail

General:

– Ability to architect Data Science solutions

– Ability to lead a team

– Ability to gather requirements

Must have :

– 6-8 Years Data Science Experience

– At least 4 Data Science solutions

– At least 2 NLP / ML Solutions

– At least 1 solution with Deep learning framework Deep

– Python / PySpark

– Knowledge in Statistical Algorithms like classification, Regression , recommendation and Clustering , Neural Networks

– Azure ML , Databricks , Delta Lake , Data science workspace

Industry Categories

Data Scientist

Languages required

English

₹200 - ₹500

Cost

Location

India

Project ID: 00002514",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Autodesk,Data Engineer,"Job Requisition ID # 22WD66509 Job Description Position Overview In this role, you will be part of a highly energetic team of engineers working on Autodesk Enterprise Integration Platform to assemble, enrich and enhance the data as per the business needs. You would be part of the team working on providing advisory services and support to business in deriving sales strategies. you will be working to build robust and scalable self-service platform for Autodesk while using innovative technologies in big data space. Responsibilities Design and develop components of end-to-end Enterprise Integration Platform Work with the team to make the Enterprise Integration Platform an efficient, robust and scalable platform Enthusiastic and passionate member of a highly skilled and motivated agile development team Contribute to a team culture that values quality, robustness, and scalability while fostering initiatives and innovation Minimum Qualifications BS or MS in computer science or a related field with 5+ years of experience. 3+ years of experience with cloud ETL/ELTs Working experience in Agile methodologies Strong knowledge and experience in AWS technologies Experience in Matillion and Redshift Strong programming skills in either Java, Scala or Python Experience with Hive and one relational DB Strong problem-solving skills Strong communication skills Ability to learn new technical skills Preferred Qualifications Experience with Matillion Experience with Python, Scala Experience with SQL (Redshift) Experience with Kanban methodology At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. Are you an existing contractor or consultant with Autodesk Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact .",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Finxera,Data Engineer,"Company Description

Finxera, Powered by Priority Technology Holdings, Inc. (NASDAQ: PRTH), is headquartered in Alpharetta, Georgia USA. Our India office is located in Chandigarh, where our dynamic team builds state of the art, sophisticated Fintech products & solutions.

We are an emerging payments powerhouse that offer a single unified platform for Banking & Payments powering modern commerce.

Finxera offers a unique family of products which integrate into SMB Payments, B2B Payments and Enterprise Payments to help businesses thrive. We are on a mission to offer an industry agnostic platform that enables businesses to collect, store and send money using various new age payment methods.

Finxera is an employee-first organisation and we continually strive to ensure their professional and personal success supported by employee friendly policies and a positive work environment built on mutual respect and professionalism. We offer a dynamic work environment, with continuous growth & learning opportunities. We believe in growing together and our people are the driving force behind our success.

Summary Requirement

We are looking for a Data Engineer for a position in the Data and Analytics team.We need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively.

Responsibilities

1. Data Warehousing experience

2. Develop ETL pipelines against traditional databases and distributed systems to allow our team to remain agile in data requirements, and to flexibly produce data back to the business and analytics teams for analysis.

3.Dimensional Modelling experience

4.Database Skills - SQL / PLSQL

5.Python / Spark / Hive / Nifi Knowledge

6.Elasticsearch / Kafka knowledge would be plus

7.Data visualization tools knowledge

8.Participate in design reviews and code reviews

9.Trouble shoot and resolve production issues

10.Resolve performance related issues

11.Knowledge on Data Science / ML / Analytics would be plus

12.Build processes that analyze and monitor data to help maintain data accuracy and completeness.

13.Ability to work with colleagues across global locations remotely

14.Independent and able to work with little supervision

15.Able to mentor and supervise junior team members

16.Agile Methodology Experience

Required Skills & Qualifications

l **Should have scored 70% & above in 10th and 12th grade.

l SQL/PLSQL

l ETL/ETL

l Python

l Pyspark (Optional)

l Any BI Tool like Si Sense, Looker, Quick Sight, Tableau, Power BI, etc (Preferred LOOKER)

l Dimension Modelling (Kimbal) / Data warehousing

l Redshift (Good to have)

l OLAP Cubes",Chandigarh,True,False,True,False,False,False,False,False,True,True,False,False,True,False,False,False
Confidential,Associate - Data Engineer,"JOB DESCRIPTIONRole and responsibilitiesResponsible for overall data analysis (e.g wrangling, cleansing), tools, and technology implementationBuild data systems, pipelines, and workflowsAbility to organize structure/unstructured raw data sources of many types with the ability to combine into useable dataDevelop data wrangling and analytical applicationsCollaborate with data scientists and analyst to management data pipelinesExplore opportunities to enhance quality of data and processesManage a team of data analysts, MIS, and operations resourcesProactive communication with project manager, ensuring all client requirements are met and reports are submitted on timeOversee implementation of tools and technology to build efficiency and consistencyResolve and escalate issues with tools and applicationsDesired candidate profile3-5 years of experience as a data engineer and/or developerStrong background within the role of a data engineer for capabilities such as knowledge of various programming language with a strong emphasis on Python, SQL, Power Query and VBA/MacrosExcellent knowledge in Microsoft Excel and knowledge of advance functionsExperience with SQL set-up and advanced queries developmentStrong technical knowledge in data mining, wrangling, and structuringManaging large volumes of data with the ability to scale as neededAbility to develop and design end-to-end solutions with operations/analyst groups for deliverablesCreating custom scripts and applications to perform wrangling, cleansing, and storingTrouble shoot data issues at any level of project/structure pipeline(s)Presenting data/reports as neededExcellent people management skillsPassionate to drive business metrics - Productivity, Quality, and other key deliverablesAbility to prioritize between multiple complex projects/timelinesExcellent written and Verbal communicationHigh level of positive attitude with good listening skillsAbility to priorities between multiple complex projects/timelinesStrong attention to detail and the ability to conduct root cause analysisCandidates with demonstrated experience in Data Breach Response, or Incident Response will be preferredKnowledge and hands-on experience in breach notification and privacy laws around data breach scenarios is desirable but not must.UnitedLex is committed to preserving the confidentiality, integrity, and availability of all the physical and electronic information assets throughout the organization. Consistent with the UnitedLex ISMS policy and the ISO 27001 standard, every employee is responsible for complying with UnitedLex information security policies and reporting all security concerns, weaknesses, and breaches. Legal Services",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Oil Field Instrumentation (India),DATA ENGINEER,"Experience :

4+ years of Mud Logging experience as Data Engineer in India and abroad, both onshore and offshore rig operations. Deep Water experience will be an advantage. Proficient with real-time data monitoring and data management, WITSML transmission, gas detection and analysis, MLU maintenance.

Qualification :

Graduate or above in Geology, Applied Geology, Geoscience, Petroleum Technology or related, or B.E. in Petroleum Engineering.",Mumbai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
News Corp,Senior Data Engineer,"One of the most innovative and high-profile teams in News Corp is looking for a seasoned data engineer to help accelerate its vision. The role will involve combining all of News Corp’s data across categories, countries and organizations into a singular view of a customer enabling engagement, measurement and targeting. The Dow Jones Senior Data Engineer will partner closely with product, data & engineering teams to help make this data available and accessible to our businesses, teams and partners in order to drive revenue and improve the customer experience.

Responsibilities
• Design, implement and maintain high performance big data infrastructure/systems & big data processing pipelines scaling to billions of structured and unstructured events daily
• Design, implement and maintain deep integration with up-stream systems
• Monitor performance of the data platform and optimize as needed
• Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
• Support products with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
• Data analysis, understanding of business requirements and translation into logical pipelines & processes
• Conduct timely and effective research in response to specific requests (e.g. data collection, summarization, analysis, and synthesis of relevant data and information)
• Evaluate and prototype new technologies in the area of data processing
• Think quickly, communicate clearly and work collaboratively with product, data, engineering, QA and operations teams
• High energy level, strong team player and good work ethic

Technologies we use
• AWS (emr,ec2,glue,athena,redshift,lambda,s3,dynamodb,sns)
• Python, Spark (PySpark)
• Kubernetes, Docker
• Airflow
• Git for source code management
• Jira

Qualifications
• BS in Computer Science or other technical discipline
• 5+ years of experience designing and developing big data processing systems using distributed computing
• Fluency in Python and Spark
• Expert knowledge in optimizing complex SQL queries
• Experience with job orchestration tools like Airflow
• An affinity for automation
• Experience working with cloud platforms such as AWS & GCP
• Familiarity with networking and network application programming, including HTTP/HTTPS, JSON, and REST APIs
• Experience with at least one object oriented language (ex: Java)
• Strong OO design, data structure, and algorithm design skills
• Strong interest in emerging technologies: Hadoop, Hive

Nice to Have
• Experience in the digital advertising and marketing industry
• Contribution to Open Source projects",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,True,False
Saasvaap,Sr Data Engineer,"JOB DESCRIPTION
• Job Description (Data Engineer) ROLE AND RESPONSIBILITIES You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
• You will support our product, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
• You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
• You will be excited by the prospect of optimizing or even re-designing Peer Data architecture pipeline to support our next generation of products and data initiatives.
• Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implergent internal process improvements: automating manual processes, optimizing data delivery, re-designing hastructure for greater scalability, etc.
• Buld the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep our data separated and secure across national boundaries through multiple data centers and cloud regions.
• Create data tools for analytics assist in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.
• QUALIFICATIONS AND EDUCATION REQUIREMENTS • 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field PREFERRED SKILLS Experience with Big Data platforms such as Apache Hadoop and Apache Spark Deep understanding of REST, good API design, and OOP principles Experience with object-oriented/object function scripting languages: Python, C#, Scala, etc.
• Experience with relational SQL and NoSQL databases, including Postgres, Cosmos and Cassandra.
• Experience with data pipeline and workflow management tools: Keboola, Stitch, Azkaban, Luigi, Airflow, etc.

SKILLS REQUIRED

My SQL, AWS

EXPERIENCE

4-10

LOCATION

Kochi, Trivandrum

WORK TYPE

FullTime

TIME SHIFT

Day",,True,False,True,False,False,False,False,False,False,False,False,True,False,False,True,False
Confidential,Chistats - Senior Data Engineer - ETL/Data Pipeline,"Day-to-Day Responsibilities :- Create and manage ETL pipelines and job schedulers.- Handle unstructured data and work to automate data ingestion and validation.- Develop APIs with Flask and Python for data mining, transformation and ingestion to AWS.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Write clean, performant code to develop functional applications; build reusable code and libraries- Collaborate with team to evaluate technologies we can leverage, including open-source frameworks, libraries, and tools.- Support Business and application teams with respect to data-related requirements.- Build proactive data validation automation to catch data integrity issues.- Troubleshoot and resolve data issues using critical thinking.Required Qualifications :- Minimum of 3 years of relevant data platform experience (structured/non-structured database, data extraction, data ingestion)- A Degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines.- Experience in at least one of these databases: MS SQL, mySQL, PostGreSQL.- Experience in AWS would be ideal and basic cloud infrastructure knowledge.- Experience in Python is required for Web Scraping.- Strong fundamentals in data mining & data processing methodologies- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.- Build processes supporting data transformation, data structures, metadata, dependency and workload management.Good to have Critical Skills :- Ability to thrive in challenging situations and solve complex problems- Strong bias for action, and see yourself as an 'initiator' and 'problem solver'- Analytical and problem-solving skills- Customer centricity and Good communication (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Calix,Senior Data Engineer - Snowflake,"This position is based in Bangalore, India.

Calix is undergoing a growth transformation, and we are looking for the best and brightest engineers for our Data Engineering team. Our team is facilitating Calix’s transformation into a more data centric enterprise with our business operational leaders. We partner with our operational teams to identify the key points of decision. We create decision support tools that enable optimal data driven selection of business actions and we build out & maintain these decision support tools on a modern data technology stack using DataOps processes. We are building out the data foundations for the next phase of our growth journey. This is a great opportunity to join a rapidly scaling enterprise with a lot of opportunity for personal growth.

The Data Engineering team is seeking a Lead Data Engineer who will be an extraordinary addition to our growing team. You will build and maintain our cloud-based enterprise data platform. Key areas that you will own include data architecture, data modeling, data pipeline flow, data warehousing, security and governance protocols, data integrity processes, and data QA best practices. You will lead buildout of the end-to-end ETL/ELT data environment, integrating with new technologies, and the development of new processes to support the creation and deployment of trusted, accurate and secure decision support tools to the Calix operational business units.

The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

Responsibilities and Duties:
• Lead data modeling, data ingestion, ELT/ETL, and data integration development using our cloud-based tooling including Snowflake, AWS, Fivetran, dbt, Airflow and GitHub.
• Establish and maintain a DataOps approach for our data pipeline infrastructure and processes.
• Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
• Deploy production machine learning pipelines into business operations analytic tooling.
• Ensure data quality throughout all stages of acquisition and processing.
• Create and maintain secure and governed access to the enterprise data warehouse and reporting tools.
• Evaluate new technologies for continuous improvement in data engineering.
• Participate in project meetings, providing input to project plans and providing status updates.
• A desire to work in a collaborative, intellectually curious environment.
• Highly motivated self-starter with a bias to action and a passion for delivering high-quality data solutions.

Qualifications
• 5+ years of experience in related field; preferably experience building and delivering data pipelines, data lakes and ELT solutions at scale.
• Expert knowledge of data architecture, data engineering, data modeling, data warehousing, and data platforms.
• Experience with Snowflake, BigQuery, Redshift, AWS, and pipeline orchestration tools (Fivetran, Stitch, Airflow, etc.).
• Coding proficiency in at least one modern programming language (Python, Java, Ruby, Scala, etc.).
• Deep SQL expertise.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, operations, and technical documentation.
• Excellent verbal and written communication skills and technical writing skills.
• Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership to gain alignment, and drive progress.
• Bachelor’s degree or equivalent experience in Computer Science, Engineering, Management Information Systems (MIS), or related field.

Preferred Qualifications
• Experience with dbt SQL development environment.
• Experience developing and deploying machine learning models in a production environment.
• Experience with Power BI and/or SFDC Einstein/Tableau.
• Experience with Oracle ERP and Oracle Data Cloud tools.

Location:
• Bangalore, India",Bengaluru,True,False,True,True,False,False,False,True,True,True,False,False,True,True,True,True
Paradise Placement Consultancy,Azure Data Engineer (Mercedes-Benz)(BSL),"Job Description : Job Description: Job Description, Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, ETL,Big Data. Responsibilities: Touch base with customers to collect the requirements and analyze them Design and build end-to-end data pipelines to get the data for customers Unit testing of the pipelines and UAT support Deployment and post production support Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents. Adapt changes to the existing scripts, codes and pipelines. Reviewing design, code and other deliverables created by your team to guarantee high-quality results Capable enough to own the PoCs and deliver the results in reasonable time Accommodate and accomplish any ad-hoc assignments Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users. Job Qualifications Qualifications: Bachelor's or Master's degree in Computer Science, Information Technology or equivalent work experience 2+ years of full time data engineering experience 2+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks Skilled in ETL/ELT process. Good working knowledge with ETL tools. Experienced with Hadoop/Big data eco systems Data migration experience from on premise to cloud Expertise in structured query language and PL/SQL Exposure to log analytics and debugging Good to have DevOps, Continuous Deployment and testing techniques Agile development experience Fluent English in spoken and written Mercedes-Benz Research and Development India Private Limited. Preferred Qualifications: Microsoft Azure Certifications Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics Hands-on experience with Databricks notebooks Hands-on experience with Kafka/eventhubs Working experience in an international team or abroad.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Kaplan,Senior Data Engineer (Hybrid),"Job Title

Senior Data Engineer (Hybrid)

Job Description

For more than 80 years, Kaplan has been a trailblazer in education and professional advancement. We are a global company at the intersection of education and technology, focused on collaboration, innovation, and creativity to deliver a best-in-class educational experience and make Kaplan a great place to work.

Our offices in India opened in Bengaluru in 2018. Since then, our team has fueled growth and innovation across the organization, impacting students worldwide. We are eager to grow and expand with skilled professionals like you who use their talent to build solutions, enable effective learning, and improve students’ lives.

The future of education is here and we are eager to work alongside those who want to make a positive impact and inspire change in the world around them.

Job Impact and Scope Summary

The Senior Data Engineer at Kaplan North America (KNA) within the Analytics division will work with world class psychometricians, data scientists and business analysts to forever change the face of education. This role is a hands-on technical expert who will own the design and implementation of an Enterprise Data Warehouse powered by AWS RA3 as a key feature of our Lake House architecture.

The perfect candidate is an expert in data warehousing technical components (e.g. data modeling, ETL, reporting). You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be able to work with business customers in a fast-paced environment understanding the business requirements and implementing data & reporting solutions. Above all you should be passionate about working with big data and someone who loves to bring datasets together to answer business questions and drive change.

Responsibilities
• Hands-on technical leader. Continually raises the bar for the data engineering function.
• Leads the design, implementation, and successful delivery of large-scale, critical, or difficult data solutions. These efforts can be either a new data solution or a refactor of an existing solution and include writing a significant portion of the “critical-path” code.
• Sets an example through their code, designs and decisions. Provides insightful code reviews and take ownership of the outcome. (You ship it, you own it.)
• Proactively works to improve data quality and consistency by considering the architecture, not just the code for their solutions.
• Makes insightful contributions to team priorities and overall data approach, influencing the team’s technical and business strategy. Takes the lead in identifying and solving ambiguous problems, architecture deficiencies, or areas where their team bottlenecks the innovations of other teams. Makes data solutions simpler.
• Leads design reviews for their team and actively participates in design reviews of related development projects.
• Communicates ideas effectively to achieve the right outcome for their team and customer. Harmonizes discordant views and leads the resolution of contentious issues.
• Demonstrates technical influence over 1-2 teams, either via a collaborative development effort or by increasing their productivity and effectiveness by driving data engineering best practices (e.g. Code Quality, Data Quality, Logical and Physical Data Modelling, Operational Excellence, Security, etc.).
• Actively participates in the hiring process and is a mentor to others - improving their skills, their knowledge, and their ability to get things done.
• Hybrid Schedule: 3 days remote / 2 days in the office
• 30-day notification period preferred

Requirement:
• In-depth knowledge of the AWS stack (RA3, Redshift, Lambda, Glue, SnS).
• Expertise in data modeling, ETL development and data warehousing.
• 3+ years’ experience with Python, or Java, Scala
• Effective troubleshooting and problem-solving skills
• Strong customer focus, ownership, urgency and drive.
• Excellent verbal and written communication skills and the ability to work well in a team.

Preferred Qualification:
• 3+ years’ experience with AWS services including S3, RA3.
• Ability to distill ambiguous customer requirements into a technical design.
• Experience providing technical leadership and educating other engineers for best practices on data engineering.
• Familiarity with Airflow, Tableau & SSRS.

#LI-Remote

#LI-AK1

Location

Bangalore, KA, India

Additional Locations

Employee Type

Employee

Job Functional Area

Data Analytics/Business Intelligence

Business Unit

00092 Kaplan Health

Kaplan is an Equal Opportunity Employer. All positions with Kaplan are paid at least $15 per hour or$31,200 per year for full-time positions. Compensation for specific positions are based on job level, skills, years of experience, and education, among other factors. Additionally, certain positions are bonus or commission eligible. Information regarding benefits can be found here
.

Diversity & Inclusion Statement:

Diversity inspires innovation and growth in the Kaplan community. Kaplan strives to be a model employer for inclusiveness. Not only does Kaplan value its employees for their professionalism and skills, but also for the unique viewpoints they bring to the Organization. Kaplan's employees bring diverse perspectives, ideas, and backgrounds that give Kaplan a competitive edge in anticipating and exceeding our students' needs in today's global market. Learn more about our culture
.",Bengaluru,True,False,False,True,False,False,False,True,False,True,False,False,True,False,True,False
"Planview, Inc.",Data Engineer,"Overview

Planview is looking for a passionate data engineer to join our team innovating tools for connected work. You will work closely with other data engineers, data scientists and individual product teams to specify, validate, prototype, scale, and deploy features with a consistent customer experience across the Planview product suite.

Responsibilities
• Ability to work in a fast paced start up mindset. Should be able to manage all aspects of AI/ML activities
• Develop platforms that make data across applications/application deployments available for AI/ML-driven feature prototyping, proofs-of-concept, and general availability
• Refine data pipelines for analysis, while refining, automating, and scaling as needed for the use-case at hand
• Work on various aspects of the AI/ML ecosystem – data modelling, data pipelines, data observability, data documentation, scaling, deployment, monitoring and maintenance etc.
• Work closely with Data scientists and MLOps Engineers to come up with scalable system and model architectures for enabling real-time ML/AI services

Qualifications

Required qualifications
• Masters or equivalent experience in Informatics, CS, Data Science or a related field
• 2+ years of experience as a data engineer or data scientist with a focus on data engineering for ML applications
• Strong Python and SQL coding skills
• Familiar with AWS Data and ML Technologies (AWS Sagemaker, Data Pipeline, Glue, Athena, Redshift etc)

Preferred Qualifications
• Demonstrated experience building data and analytics pipelines/services that efficiently scale for cloud application usage, meeting a product team’s SLA for performance and resilience
• Exposure to database queries and strong in SQL
• Exposure to any of the libraries and frameworks in data science (Pandas, Numpy, Dask, PySpark etc)
• Exposure to data version control (DVC) and orchestration tools (Airflow, etc)
• AWS Certification is a plus
• Skilled at working as part of a global, diverse workforce of high-performing individuals
• AWS Certification is a plus",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Full Potential Solutions,Lead Data Engineering,"Overview:

About The Company

Full Potential Solutions (FPS Perch) is a global product company headquartered in the US. We help contact centers around the world better engage with their customers by combining our deep domain expertise with cutting-edge technology. Our AI enabled, cloud-scalable analytics products enhance the end-to-end customer journey via omnichannel engagement (voice, email, SMS, chat and conversational AI) and optimize agent performance. Our offices are spread across the US, India and Philippines.

Some exciting initiatives they are working on are:
• Designing and development amazing user experiences across data visualization and analytics in the customer engagement / contact center industry
• Salesforce based solutions for omni-channel customer engagement leveraging AI to enhance contact strategy and routing, guide agents on next best action (NBA) enable dynamic scripting, capture voice of customer (VOC)
• Developing standardized AI/ML models to optimize customer engagement and agent performance using various Python frameworks and AWS services. (Transcribe, Comprehend, Sagemaker, etc.)
• Developing a comprehensive Data, Analytics and Reporting platform to integrate, measure and analyze millions of daily customer interactions/metrics across all our clients.
• Building web/mobile apps to improve agent/manager performance across thousands of agents (metrics, gamification, collaboration).
• Deploying cutting-edge, tech-enablement services using the AWS ecosystem, including microservices and serverless architecture, CI/CD pipelines, event notification & search services, multi-tenant architecture ec.,

Our team has successfully built and scaled analytics-focused products at companies such as Salesforce, Demandware, and IBM. We have strong backgrounds in Computer Science, Math and Engineering from top universities such as IIT, Harvard and Yale. We strongly believe that empowered people are the key to building a great company, and our development process focuses on iteratively improving our products as well as ourselves as individuals and as a team. Our mission as a company is to create an environment where the people THRIVE.

Explore more at: FPS Website

Leadership Team: Explore here

About The Team

Our analytics team helps our clients leverage data to optimize their performance and results. Our Data Analysts possess a unique skill of understanding the data, making a clear sense of it, and telling a story to the business. We work closely with the Analytics Leaders and the rest of the Analytics product team to analyze and understand customer needs, explore granular data to identify key drivers that influence each KPI, measure the level of expression of attributes that move the KPI, and execute in a way that brings the best solutions to life. Most importantly our team exhibits our core values: Integrity, Excellence, Accountability and Grace.

Responsibilities:

Functional Requirements
• Data exploration skills to unravel data sources and identify the granularity, attributes & measure
• Discover data sources and determine the best EL approach and tools to bring data into the D
• Determine the data cleansing rules based on source data and ensure data replication accurac
• Coordinate & collaborate with data architects and BI engineers to ensure timely project deliver
• Understanding of data warehouse concepts and implementation methods of warehouse object
• Translate the data models from the ER diagram into physical models coded into data warehous
• Innovate reusable and configurable frameworks for data replication to build the bronze laye
• Well-versed with the data warehouse modeling concepts (star/snowflake/denormalized structures
• Define data quality rules, modeling standards, business metrics, standard processes and test case
• Understand the domain, business cases, objectives, and KPIs that need to be reported and analyse
• Participate in data discovery phase and capture the data sources, required KPIs with the formulae

Qualifications:

Technical Requirements
• Bachelor’s degree in a Technical/Quantitative subject such as Computer Science (B.E/B.Tech/MCA
• 7 - 9 years of relevant experience in the field of Data Engineering and Data Warehouse/BI solution
• Proficient in SQL, query optimization, coding stored procedures, and ad-hoc data analysis using SQ
• Develop data pipelines using some combination of ETL/ELT tools and data processing framework
• Develop DBT models as per the data model and implement the data transformations & test case
• Orchestrate data integration & transformation processing from sources to ssots in data warehous
• Design, develop, monitor and re-engineer database objects and processes/pipelines/schedules
• Use AWS services for data storage, access and retrieval and application setup and monitorin
• Design & document the data processing workflows and setup systems as per solution architecture
• Design the multi-dimensional data model in data warehouse in order to meet BI reporting needs
• Setup software engineering best practices for data pipelines with data-associated documentation
• CI/CD implementation with source code version control, QA checks and deployment automation
• Hands-on with languages like Spark, Python, Scala, R, Bash/Shell Scripting or stored procedures
• Must have worked with RDS or data lakes (MySQL, PostgreSQL, Oracle, Redshift, Snowflake, etc.)
• Hands-on experience in using ETL/ELT tools like Talend, Informatica, SSIS, Airbyte, Alteryx, Stitch etc.,
• Experience working with orchestration tools such as Airflow, Astronomer, Control-M, Prefect etc.,
• Experience working with DevOps tools such as Bitbucket, Github, Gitlab, Jenkins, CircleCI etc.,
• Experience working with AWS Cloud services such as S3, EC2, ECS, EFS, Lambda, Docker, Kubernetes
• Hands-on experience in using data-modeling tools like Erwin, DBSchema, MySQL Workbench, ER Studio

Leadership Responsibilities
• Lead a squad of data engineers to implement data warehouse solution across various projects
• Ensure adherence to the best practices of data engineering and testing across all projects
• Drive sprint planning/review and lead daily Kanban meetings in alignment with project plan
• Allocate work smartly and efficiently manage/ coach the team while monitoring the progress
• Develop systematic training plan for onboarding new joiners and upskilling existing resources",,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Kyndryl,Azure Data Engineer,"Why Kyndryl Kyndryl is a market leader that thinks and acts like a start-up. We design, build, manage, and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward - always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers, and our communities. We invest heavily in you - not only through learning, training, and career development, but also through the flexible working practices and stellar benefits that help you grow and progress long-term. And we give back - from planting 90,000 trees in our first 3 months as part of our One Tree Planted initiative to the Corporate Social Responsibility and Environment, Social and Governance practices embedded within everything we do, we are committed to powering human progress in an ethical, sustainable way. Your Role and Responsibilities Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Create and maintain clear and complete pipeline documentation, e.g., architecture diagrams and data transformations. Integrates data solutions with business processes. Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Provide actionable insights via compelling storytelling to drive business outcomes. Microsoft Certified: Azure Data Engineer Associate Required Technical and Professional Expertise Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Preferred Technical and Professional Experience Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Required Education Associate's Degree/College Diploma",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Role: AWS Data Engineer

Ex- 4 to 8 YRS

Locations- Delhi/NCR, Gurgaon, Bangalore, Ahmedabad, Pune, Indore, Mumbai, Kolkata

Must Have –
• Working on EMR, good knowledge of CDK and setting up ETL and Data pipeline
• Coding - Python
• AWS EMR, Athina, Supergule, Sagemaker, Sagemaker Studio
• Data security & encryption
• ML / AI
• Pipeline
• Redshift
• AWS Lambda

Expectations/Responsibility
• Industry experience in Data Engineering on AWS cloud with glue, redshift , Athena experience.
• Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python.
• 3+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
• Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.
• Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
• AWS engineer provides comprehensive systems administration functions on Amazon Web Services (AWS) infrastructure to include support of AWS products such as: AWS Console root user administration, Key Management, EC2 Compute, S3 Storage, Relational Database Service (RDS), AWS Networking & Content delivery (VPC, Route 53, ELB, etc.) Identity & Access Management, CloudWatch, CloudTrail, Cloud Formation, Auto Scaling, Cost and Usage Reports, and more.
• Train and guide the company’s HR engineering team on developing with aforementioned AWS tools, while also executing on specific deliverables (ingestion, Storage, integration, warehouse, visualization)
• Coach and mentor other technical resources on the team on AWS technologies
• Create ETL piplelines that are highly optimized with very large data sets
• Solve issues with data models and come up with solutions
• Developing and directing software system testing and validation procedures, programming and documentation
• Analyzing user needs and requirements to determine feasibility of design within time and cost constraints
• Provide technology expertise, direction, coordination, and consultation, in the development, integration, launch, scaling, and maintenance of new and existing products and solutions
• Establishes infrastructure technology architectures, standards, test plans, design templates and governance
• Works with the team to define standards and frameworks with regards to coding, programming, and the general development of applications for multiple platforms
• Work with business teams to understand customer issues and to investigate, prototype and deliver new and innovative solutions

FRESHERS DO NOT APPLY.",New Delhi,True,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False
Mercede,Azure Data Engineer-Karnataka-Bangalore,"Azure Data Engineer

Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, Big Data, Azure DevOps, Azure Boards, VSTS Git

Hey, do you want to change the world? Build #TheNextBigThing? Which means implementing ideas that are said to be unachievable? Like it was stated at first about smart chatbots or artificial intelligence. At MBRDI you are dealing with questions, for which there are no answers. Not yet.

About Us

The Corporate Center of Excellence (CoE) for AI, Advanced Analytics and Big Data is working on #TheNextBigThing. Besides conducting cool use cases in the field of Data Analytics and AI, we are inventing, building and running eXtollo a data analytics cloud platform based on Microsoft Azure for Daimler teams around the world. Our users trust in the cross-divisional platform to create secure and scalable cloud applications based on Machine Learning, Artificial Intelligence Big Data technologies. In addition to the provisioning and utilization of the technology we are also responsible for Daimler s data treasure the eXtollo Data Lake.

Role

We are searching you as a Data Engineer to continuously improve eXtollo within an agile working environment. You are expected to work directly with customers to understand their data demands and build end to end pipelines to get the data as they wish. Also be able to understand the current codes, pipelines and scripts which are already in production in a short time and support operations/changes.

Experience of data migration into Azure from various cloud and on-prem is a plus. You are expected to have good exposure to structured query language preferably MS Sql Server. Should be able to do PoC for the new adaptions and work independently with minimal guidance. Data warehouse/data engineering experience is appreciated. Most importantly the candidate should have real hands-on experience rather bookish/training experience.

Responsibilities
• Touch base with customers to collect the requirements and analyze them
• Design and build end-to-end data pipelines to get the data for customers
• Unit testing of the pipelines and UAT support
• Deployment and post production support
• Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents.
• Adapt changes to the existing scripts, codes and pipelines.
• Reviewing design, code and other deliverables created by your team to guarantee high-quality results
• Capable enough to own the PoCs and deliver the results in reasonable time
• Accommodate and accomplish any ad-hoc assignments
• Building CI/CD pipelines in Azure DevOps to deliver our services into various data centers worldwide
• Analyzing and solving incidents in productive environment while avoiding data loss and minimizing service outages
• Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users
Job Qualifications

Qualifications
• Bachelor s or Master s degree in Computer Science, Information Technology or equivalent work experience
• 3+ years of full time data engineering experience
• 3+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks
• Skilled in ETL/ELT process. Good working knowledge with ETL tools.
• Must be experienced with Hadoop/Big data eco systems
• Data migration experience from on premise to cloud
• Expertise in structured query language and PL/SQL
• Experienced in Powershell scripting for orchestration
• Exposure to log analytics and debugging
• Good knowledge in DevOps, Continuous Deployment and testing techniques
• Agile development experience
• Fluent English in spoken and written

Preferred Qualifications
• Microsoft Azure Certifications
• Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics
• Hands-on experience with Databricks notebooks
• Working experience in an international team or abroad
,

This job is provided by Shine.com",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
HARMAN International,Data Engineer,"What You Will Do :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis What You Need :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis",Bengaluru,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
GSPANN Technologies,Azure Data Engineer,"Should have experience in ADLS (Azure Data Lake storage) Experience implementing Azure Data Factory Pipelines using latest technologies and techniques Experience in working on Azure HDInsight Experience in working with Storage Strategy Azure developer should be able to ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements Expert in Azure Data Factory, Azure Data Lake Azure SQL Data Warehouse, Azure Functions, Databricks · Comfortable working with Spark, Python, and PowerShell Excellent problem solving, Critical and Analytical thinking skills Strong t-SQL skills with experience in Azure SQL DW DevOps, CI/CD, and Automation experience strongly preferred Able to interact with team members collaboratively Experience handling Structured and unstructured datasets Experience in Data Modelling and Advanced SQL techniques",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Uber,Data Engineer II,"What You'll Do
• Responsible for defining the Source of Truth (SOT), Dataset design for multiple Uber teams.
• Identify unified data models collaborating with Data Science teams
• Streamline data processing of the original event sources and consolidate them in source of truth event logs
• Build and maintain real-time/batch data pipelines that can consolidate and clean up usage analytics
• Build systems that monitor data losses from different sources and improve the data quality
• Owns the data quality and reliability of the Tier-1 & Tier-2 datasets including maintaining their SLAs, TTL and consumption
• Devise strategies to consolidate and compensate the data losses by correlating different sources
• Solve challenging data problems with cutting-edge design and algorithms.

What You'll Need
• 4+ years of extensive Data engineering experience working with large data volumes and different sources of data.
• Strong data modeling skills, domain knowledge, and domain mapping experience.
• Strong experience in using SQL language and writing complex queries.
• Experience with using other programming languages like Java, Scala, Python
• Good problem-solving and analytical skills
• Good communication, mentoring, and collaboration skills.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Cardinal Health,"Sr. Data Engineer, Data Engineering","Job function:

IT Quality Control is responsible for owning and implementing software testing and certification strategies for the enterprise. Debugs problems with software through standard tests and recommends solutions. Conducts defect trend analysis and continuous process improvement. Demonstrates knowledge of requirement and risk based testing principles, theories, concepts and techniques. Establishes internal IT service quality control standards, policies and procedures.

Job duties:

Create Test Strategy, define quality standards for Google Cloud Platform and define metrics to measure the efficiency and testing for applications on Google BigQuery, Dataflow and Airflow. Develop and maintain test automation frameworks, build regression test strategy and continuous testing process.

Skills:
• Ability to create test strategy balancing manual and automated testing
• 8+ years of experience with designing and implementing test frameworks in cloud
• Technical skills – SQL, Java/Python
• Good communication and collaboration skills across teams and business SMEs
• Should exhibit continuous testing mindset
• Knowledge on Devops and CI/CD process
• Develop automated test cases to be used in performance testing or as part of testing
• Identify and implement performance metrics to be measured
• Collaborate with functional and technical teams to identify test data or create through UI and database
• Generate automation or performance testing reports from execution
• Maintain record of test discrepancies, using designated QA tools
• Provide feedback of test results to development and infrastructure teams for resolution
• Review Business Requirements Documents and Functional and Technical Specifications towards determining test data scope
• Oversee defects from initial identification through post-deployment analysis
• Coordinate with other QA engineers, leadership, system administrators, architects and developers

What is expected of you and others at this level:
• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
• Participates in the development of policies and procedures to achieve specific goals
• Recommends new practices, processes, metrics, or models
• Works on or may lead complex projects of large scope
• Projects may have significant and long-term impact
• Provides solutions which may set precedent
• Apply design thinking mindset
• Independently determines method for completion of new projects
• Receives guidance on overall project objectives
• Acts as a mentor to less experienced colleagues

Cardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,True,True,False
Siemens Energy,Data Engineer (PMK Project),"A Snapshot ofYour Day

A Our team ofdata engineers supports several business teams to get needed data, prepare itproperly for the particular use cases and make it available in an efficientway. You are integrated in the business team and also work on business topics.

One strongexample is the Prescriptive Marketing team. They create forecasts for energymarkets about power prices, demand and other key factors. This is used to helpcustomers to improve their plants and thus their revenues. It is essential alsofor the data engineers to understand the big picture while fulfilling the datarequests.

You discussthe scope of the data and how to make it available, followed by implementingand maintaining data pipelines, normally Python based, with generic setups thatcan be re-used easily to scale up the market models. There is a wide range oftasks, from doing research about data sources up to presenting the data indashboards.

How You’ll Makean Impact
• As a data engineer you willsupport the team by using, building and maintaining ETL tools and pipelines forcollecting, transferring and preparing data for several use case like internalanalytics or consumer facing applications.
• You should be able to deliverrapidly in a reliable manner with the highest quality standards.
• You should be curious,passionate about problem solving, building and self-improving.

WhatYou Bring
• Master’s / bachelor’s degree in computer science or similar with a minimum 5 years of experience in the field of Software Development and Architecture.
• Deliver rapidly in a reliable manner with the highest quality standards
• Curious, passionate about problem solving, building and self-improving
• Experience in Agile development
• Fluent English Skills
• Experience working in and with international teams and stakeholders with different cultures

Technical Skills:
• Highly experienced in relational database setups and data warehouse environments
• Snowflake knowledge is a plus
• Solid experience with building ETL pipelines
• Strong SQL skills (aggregations, windows functions, pivoting etc.)
• Python knowledge
• Solid experience in software development, incl. design patterns is a plus
• Experience with OOP concepts (e. g. Java/C# background) is a plus

Working experience:
• version control systems (Gitlab) and tools like Confluence and Jira
• deployments with CI/CD pipelines
• AWS cloud environments
• Time series data
• integrating and managing structured and unstructured data in cloud based data management systems for example with PostgreSQL or Redshift, Snowflake is a plus
• Tableau dashboards is a plus

About The Team

""Let’s make tomorrowdifferent today"" is our genuine commitment at Siemens Energy to allcustomers and employees on the way to a sustainable future.

Ourteam belongs to the Software Engineering andProduct Development Function within Siemens Energy. Our missionis to grow the digital software business and develop solutions and products forour internal and external customers. This covers from edge data acquisition,data lake and processing up to development of digital twins and apps aroundcustomer assets.

Who is Siemens Energy?

At SiemensEnergy, we are more than just an energy technology company. We meet thegrowing energy demand across 90+ countries while ensuring our climate isprotected. With more than 92,000 dedicated employees, we not only generateelectricity for over 16% of the global community, but we’re also using ourtechnology to help protect people and the environment.

Ourglobal team is committed to making sustainable, reliable, and affordable energya reality by pushing the boundaries of what is possible. We uphold a 150-yearlegacy of innovation that encourages our search for people who will support ourfocus on decarbonization, new technologies, and energy transformation.

Our Commitment to Diversity

Lucky for us, we are not all the same.Through diversity we generate power. We run on inclusion and our combinedcreative energy is fueled by over 130 nationalities. Siemens Energy celebratescharacter – no matter what ethnic background, gender, age, religion, identity,or disability. We energize society, all of society, and we do not discriminatebased on our differences.

Rewards/Benefits
• The opportunity to engage inan exciting environment on challenging projects
• Strong professional supportand working with colleagues around the world
• Professional developmentopportunities within the company
• To be part of a growingfunction with a dynamic, informal, and inspiring working environment in aposition that entails a large responsibility
• Medical benefits
• Remote/Flexible work
• Time off/Paid holidays
• Parental leave
• Continual learning throughthe Learn@Siemens-Energy platform

https://jobs.siemens-energy.com/jobs",Gurugram,True,False,True,True,False,False,False,False,False,True,False,False,True,False,False,True
Snowflake,Data Engineer,"Build the future of data. Join the Snowflake team.

Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Because traditional on-premises and cloud solutions struggle with this, Snowflake developed an innovative product with a new built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.

In addition, Snowflake’s culture was built on the following values that are even more important to us today:
• Put Customers First. We only succeed when our customers succeed
• Integrity Always. Be open, honest, and respectful
• Think Big. Be ambitious and have big goals
• Be Excellent. Quality and excellence count in everything we do
• Get It Done. Results matter!
• Own It
• Make Each Other the Best
• Embrace each others’ Differences

Job Description
• Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data infrastructure that scales across the company
• Drive the design, building, and launching of new data models and data pipelines in production
• Build data expertise and own data quality for allocated areas of ownership
• Align to Product roadmap in building tools for data platform users
• Mature requirement and follow design develop and communicate model using Agile methodology for data ingestion and data tooling.

MINIMUM QUALIFICATION
• Expertise in SQL statements and modeling concepts.
• Must be aware of the cloud environment from data ingestion and modeling perspective.
• Must be strong in python
• Experience with Apache Airflow is highly desirable.
• schema design and dimensional data modeling.
• custom ETL design, implementation and maintenance.
• object-oriented programming languages.
• Understanding of API and connectors is highly desirable
• analyzing data to identify deliverables, gaps and inconsistencies.
• Experience in data warehouse space.

PREFERRED QUALIFICATION :
• BE/BTECH in Computer Science, IT or other technical field.
• Experience with data ingestions and data analytics.
• 4 years experience using Python and SQL, .",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,True
PepsiCo,Data Engineer,"Overview

This role is with the Global business services arm of Media Center of Excellence (CoE) at PepsiCo.

In this role, you will play a key role in shaping the future of media measurement strategy for PepsiCo, esp. with focus on building an understanding role of media as key Growth Driver thru ROI and related analytics.

You will be laying down strong data foundation through implementation of process & technology.

This role is the backbone of media measurement agenda at Pepsico and is responsible for building data systems and pipelines to feed into prescriptive and predictive modeling by establishing and enhancing process around data capture, storage, quality and reliability.

You will work with Media, Data engineering, Data Science and IT teams globally and within AMESA sector to identify best practices in the industry and across all PepsiCo’s brands, providing support to codify and scale best in-class methods that inspire continuous improvement in marketing effectiveness and ROIs.

Responsibilities
• Collect, structure, analyze, organize and maintain RAW data from various data sources needed for creating predictive models in structured databases in order to ensure faster model building
• Partner with PepsiCo functional teams, agencies and third parties to build seamless process for acquiring, tagging, cataloging and managing all media, Nielsen and internal data periodically in structured format as needed for measurement statistical models
• Design, build and codify data structures in efficient way to periodically feed in raw data from various internal and external sources and also manage and house model outputs for quick input to businesses;
• Build data systems and pipelines as per business needs and objectives, in this case prepare data to feed specifically to MMM and media measurement models or any descriptive or prescriptive analysis
• Promote data consistency globally to support common standards and analytics
• Establish periodic data verification processes to ensure data accuracy
• Build new technologies and algorithms to optimize any business process around creation and maintenance of databases/data lakes running of batch processes for data updation

Qualifications
• 3-6 years of experience in the field of data structures, building and managing data lakes
• Hands on experience in building database/Data Management Solutions
• Mandatory experience Python, Data modelling, data pipeline set up and meta data management
• Experience in relational databases as well as unstructured data streams
• Experience with schema design and dimensional data modeling (for ex: using data vault/snowflake/star schema)
• Hands on experience, Airflow (or any other Orchestration tools)
• Hands on experience in AWS (or any other cloud operator)
• Good to have experience on technologies like, DBT
• Good to have experience in data engineering teams in consulting or ecom/telecom sector
• Desirable - Experience optimizing larger applications to increase speed, scalability, and extensibility
• Educational Background- BE/B T ECH/ MS in computer science or related technical field",Peeramcheru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,True,True
HTC Global Services,Data Engineer-GCP,"Greetings from HTC Global Services

We are hiring Data Engineer- GCP

Skills Required:

Data Engineer- GCP

Experience with Big query, Terraform ,Hive

Experience:

3+ Years

Location:

Chennai

Notice:

Looking for candidates who can join within 15 days.

Interested candidates please drop your resume to sunitha.manohar@htcinc.com

Regards

Sunitha",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
HP,Senior Data Engineer,"The Company

HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations and dreams. We apply new thinking and ideas to create simpler, more valuable and trusted experiences with technology, continuously improving the way our customers live and work.

Position background

In the GTM analytics COE our mission is to deliver impact by building machine learning products to optimize pricing and marketing investments and provide guidance to our sales organization.

As a Big Data Engineer, you will be in a unique position to support the development one of our internal assets. You will work together with the project and asset team to understand the end state in which the data must be delivered and you will model the data using Big Data technologies like Spark.

We offer an international experience, collaborative culture, top rate experience in AI and ML and opportunity to create significant real-world impact.

What You Will Do

Create / Maintain ETL pipelines.

Ensure that processes are optimized.

Use Spark to model big volumes of data.

Contribute to the database architecture, design and implementation.

What You Will Need

Bachelor’s in computer engineering, Computer Science, Electrical Engineering, Robotics or a related field

2+ years on a similar role.Ability to work independently under a fast-paced environment, comfortable to deliver results under pressure.

You Have Strong Problem-solving Skills.Agile Experience.

Experience with modern application lifecycle management tools (Git, Visual Studio, Intellij, Code Reviews).

Proficient in at least one of the following languages: Python, PySpark, Scala, Spark, SparkR.

Experience with SQL & NoSQL databases is preferred (PostgreSQL, MongoDB, Elastic Search).

Experience in working with DataIku DSS software.

Strong analytical skills with demonstrated problem solving ability.

Who We Are

At HP, we believe in the power of ideas. We use ideas to put technology to work for everyone. And we believe that ideas thrive best in a culture of teamwork. That is why everyone – at every level in every function, is encouraged to think big, have original ideas and express and share them. We trust anything can be achieved if you really believe in it, and we will invest in your ideas to change lives and the way people work. This vision is what sets us apart as a company. At HP, we work across borders and without limits. Global virtual teams share resources, pool their big ideas to solve our biggest business opportunities. Everyone is valued for the unique skills, experiences and perspective they bring. That’s how we work at HP. And this is how ideas and people grow.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
DataTheta,Azure Data Engineer,"Experience Required: 5-10 Years

Location: Noida/Chennai

Azure Data Engineer | Job Description
• Dev / Architect
• 2+ years of experience in Azure cloud data stack such as Synapse/DW, Azure SQL DB, Azure Blob Storage
• 1+ years of experience in Logic Apps
• 3+ years of experience in Python
• 5+ years of experience in SQL
• 3+ years of experience in Databricks
• 2+ years of experience in Azure/AWS Lambda Functions
• 2+ years of experience in Microservices (REST) architecture
• Ability to project manage and work within an agile, flexible environment.
• Performs peer reviews for other data engineers’ work.
• Ensuring adherence to programming and documentation policies, software development, testing and release.
• Develop modeling, design, and coding practices.
• Experience with Lean / Agile development methodologies
• Positive attitude with great collaboration and communication skills",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Tech Mahindra,GCP Data Engineer,"Job Role - GCP Data Engineer
• Looking only GCP Data Engineers. (GCP Certification is not mandatory)
• Experience should be 4-8 Years Max.
• Candidates should be aware about BigQuery, Data Flow, Composures.
• GCP Services, SQL, Migration Process
• Migration Tools ( Plate spin, Cloud Physics, Stratozone)
• Work Location Pune/Bangalore/Noida/Chennai",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False,False
MOURI Tech,Sr. Data Engineer,"Hi Folks,

Greetings from Mouritech!!

We are hiring for Sr. Data Engineer

Mandatory Skill: Data Engineer, Python, SQL, DWH & GCP

Location: Gurgaon (Hybrid)

Exp: 4+ Yrs to 12 Yrs

Notice Period: Immediate to 30 Days Serving.

If interested pls share your profile on below mail id

surabhim.in@mouritech.com.",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
NAB,Senior Data Engineer [T500-7047],"Essential Skills:
• Advanced SQL skills (or equivalent database querying language), Database SQL skills (MySQL preferably), Able to write complex queries (subquery, window function, CTE, removing duplication), Able to analyse query bottleneck by “Explain” command.
• Construct RMDB database-Database modelling skills, Able to operate DDL (stored procedure, indexing, trigger, table, materialised view, view), Understand database modelling (normalisation)
• Develop, maintain and Implement Power BI dashboards,
• Understand batch job process, able to modify it, and solve batch job issue- Python (ODBC DB connection, Pandas, XML handling), PowerShell (General PS command), ETL experience.
• Extract meaningful insights out of the data.
• AWS experience -AWS EC2, RDS monitoring, parameter store
• Aware of GitHub skills- Merge/pull request/resolving conflicts, Pull/push.
• Translate business information requirements into a meaningful set of SQL queries.
• Develop and maintain key reporting metrics that drive business performance.
• Creating Views, Stored Procedures and Materialised Views in MySQL.
• Ability to parse XML tags into SQL human readable tables.

Job Requirements:
• Advanced SQL skills (or equivalent database querying language)
• 6+ years’ experience working in a similar role.
• 6+ years of experience working with BI tools such as Power BI
• Proficient in Python language
• Data Science enthusiast
• Understand Jenkin pipeline, Jira & Confluence
• Advanced MS Office skills, including Excel, PowerPoint, Access etc.
• Ability to deal with ambiguity, solve complex problems, and navigate large, global organisations.
• Self-motivated, assertive, analytical, and comfortable working in a fast-paced environment
• Stakeholder management

Department : Group Security

Sub Department : Cyber Security

Job code : AAZA01",Gurugram,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
SID Global Solutions,Senior Data Engineer(8+yrs),"Skillset: SQL, AWS Stack, Python, Redshift/MYSQL

Roles & Responsibilities:

Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift

Hands on Python programming.

Working knowledge on S3

AWS certification is a nice to have

Must be punctual and follow deadlines and deliver on time.

Must be able to clearly communicate with stakeholders and team",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
InVisions Ltd.,Data Engineer,"Hello people,

We are happy to assist the product and service company “Saras Analytics” in welcoming new Data Engineers to the team.

Now, a little bit about the company and the product:

Saras Analytics is a rapidly growing data management and analytics advisory firm with offices in Austin, USA and Hyderabad, India. We are a group of engineers and analysts focused on accelerating growth for e-commerce and digital businesses by setting up or transforming their data (analytics & BI) ecosystems and providing further analytics services. We are laser focused on providing the best ROI for our clients and leave no stone unturned in our quest to provide the best results for our customers.

We are an employee-centric organization and, to meet the ever-growing demand for our services, are looking for individuals who share our passion to make a difference and would be great additions to our analytics and growth consulting practice.

How Saras Analytics describes your role:

As a Data Engineer at Saras Analytics, you will be responsible for building and maintaining large-scale data pipelines as well as create and data pipelines that deal with large volumes of data.

You will deal with:
• Database programming using multiple flavors of SQL and Python.
• Understand and translate data, analytic requirements and functional needs into technical requirements.
• Build and maintain data pipelines to support large scale data management projects.
• Ensure alignment with data strategy and standards of data processing.
• Deploy scalable data pipelines for analytical needs.
• Big Data ecosystem - on-prem (Hortonworks/MapR) or Cloud (Dataproc/EMR/HDInsight).
• Work with Hadoop, Pig, SQL, Hive, Sqoop and SparkSQL.
• Experience in any orchestration/workflow tool such as Airflow/Oozie for scheduling pipelines.
• Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow.
• Understand and execute IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, writing optimized queries in Spark.
• Hands-on experience in using Spark Streaming, Kafka and Hbase.

What you bring with you:
• 4 to 6 years of experience in building data processing applications using Hadoop, Spark and NoSQL DB and Hadoop streaming. Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow is a plus.
• Expertise in data structures, distributed computing, manipulating and analyzing complex high-volume data from variety of internal and external sources.
• Experience in building structured and unstructured data pipelines.
• Proficient in programming language such as Python/Scala.
• Good understanding of data analysis techniques.
• Solid hands-on working knowledge of SQL and scripting.
• Good understanding of in relational/dimensional modelling and ETL concepts.
• Understanding of any reporting tools such as Looker, Tableau, Qlikview or PowerBI.
• Degree: Bachelor of Engineering - BE, Bachelor of Science - BS, Master of Engineering - MEng, Master of Science – MS or equivalent work experience.

Eligibility:
• Significant technical academic course work or equivalent work experience.
• Excellent communication and interpersonal skills.
• Willingness to work under labor contract, B2B contract is an option too.
• Dedicate 40 hours/weekly to Saras Analytics.

Let’s connect and check if we match!

You can state your interest by sending your CV and we will get in touch with the short-listed candidates.

We treat your personal information with respect and confidentiality, guaranteed and protected by the professional ethics, the Bulgarian and European law.

“InVisions” agency license № 2420 from 19.12.2017.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,False
LTIMindtree,Specialist Data Engineer- AZURE-ADF/ADB,"• Job Title- Specialist Data Engineer
• Primary skill- Azure+Databricks (ADF+ADB+Pyspark)
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 12 Years
• Notice Period- 0 to 30 Days
• Job Description-

Primary Skills

• 5+ years of experience in Python and Databricks.

• Deep understanding of data modelling techniques for analytical data (i.e. facts, dimensions, measures)

• Experience developing and managing reporting solutions, dashboards, etc. Design and architecture experience in data transformation.

• Should have experience with data platforms and in data transformation and extraction: some combination of ETL/ELT, table and database design, query design, performance analysis and optimization

• Worked as a data engineer or related specialty (Software Engineer/Developer, BI Engineer/Developer, DBA)

Secondary Skills

• Experience in Azure Data Factory and Azure Storage

• Hands on experience with handling of large amount of data using SQL, Azure Data Factory, Spark, Azure Cloud architecture

• Knowledge of cloud architecture and data solutions

• Proficiency in Snowflake would be added advantage.

• Excellent written and verbal communication skills",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
Rently,Data Engineer,"Must have:
• Overall experience of 4+ years
• Experience in AWS Cloud services & solutions
• Experience working with enterprise data warehouse
• Experience as an ETL/ELT Developer using various ETL/ELT tools
• Experience in SQL/NoSQL/DWH databases across SQL DB, Managed instance & Data warehouse
• Experience in AWS platform services such as S3, EMR, RedShift, Glue, Kinesis, OpenSearch, Athena, QuickSight
• Working on SnowFlake and pipeline tools like Fivetran or Matillion.
• Experience in Apache Spark, Databricks
• Experience in creating data structures optimized for storage and various query patterns like Parquet
• Experience in building secured visualization reports and dashboards with access controls
• Experience in working in an Agile SDLC methodology
• Experience in DevOps Services using Git Repos, deployment artifacts and release packages for Test & production environment
• Experience in building end-end scalable data solutions, from sourcing raw data, and transforming data to producing analytics reports
• Should have experience in developing a complete DWH ETL lifecycle
• Experience in Data Analysis, Data Modelling and Data Mart design
• Should have experience in developing ETL processes - ETL control tables, error logging, auditing, data quality, etc. - using ETL tools.
• Experience in Data Integrator Scripts, workflows, Dataflow, Data stores, Transforms, and Functions.
• Should have worked on at least 2 end-to-end implementations
• Worked on Change Data Capture on both SOURCE and TARGET levels and a good understanding of Slowly changing Dimension (SCD)
• Should be able to implement reusability, parameterization, workflow design
• Should have experience in interacting with customers in understanding business requirement documents and translating them into ETL specifications and Low/High-level design documents
• Strong database development skills like complex SQL queries, complex stored procedures
• Able to work in Agile Framework Should have exposure to Scrum meetings.

Good to have:
• Exposure to other ETL/ELT, DWT technologies
• Hands-on with Data visualization tools like Power BI, Tableau, Qlik, QuickSight etc.
• Exposure to Python on ETL and Data Visualization libraries

Additional Skills:
• Good Communication Skills.
• Able to deliver independently.
• Team player.

Professional Commitment:

Being a product based company we heavily invest in developing functional/ technology/ leadership skill sets in our team members. So candidates who are willing to commit to a minimum of 2 years need to apply.",Coimbatore,True,False,True,False,False,False,False,False,True,True,False,True,True,False,False,True
Embibe,Data Engineer,"Requirements
• Should have knowledge in Coding: Preferred Java.
• Good to have - ( Python / Scala).
• Should have Knowledge in Technologies: Spark, spark streaming, scala spark/py spark.
• Good to have Knowledge of Messaging buses like Apache Kafka/ Rabbit MQ.
• Good to have Knowledge of NoSQL databases like - MongoDB, ElasticSearch, Cassandra, Hive, Impala, ADX, Synapse, Redshift, Athena, etc.
• Should have Knowledge in building Microservices with Spring boot/ Fast-Api.",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Concentrix,Data Engineer Big Data,"Job Title:

Data Engineer Big Data

Job Description

Data Engineer Big Data

Keywords: RDBMS SQL & Spark/Hive SQL, Performance tuning, Modeling Design

Job Description

Develops and maintains scalable data pipelines

Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.

Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.

Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.

Designs data integrations and data quality framework.

Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.

Qualification:

Bachelor's Degree in Computer Science or related field

3+ years of work experience

Strong experience in SQL ( include complex SQL query , SQL performance tuning , Index , Lock )

Experience with schema design and dimensional data modeling

Experience with Hive SQL , Spark(Spark SQL, DataFrame)

Experience with near-realtime data warehouse (10-30 mins level)

Experience with data quality check

Experience in Java or Python or Shell Script

#CSS

Location:

India Bangalore - Divyashree

Language Requirements:

Time Type:

Full time

If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents

R1357932",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
CirrusLabs,AWS Data Engineer / Data Engineer / Lead AWS Data Engineer,"Job Role: Aws Data Engineer

Location: Bangalore / Hyderabad

Type: Fulltime

JOB DESCRIPTION

Data Engineer/Operational Support with Snowflake

Must-Have:
• 7+ years of experience in an Oracle/Informatica environment with knowledge of views, packages, stored procedures, functions, constraints, cursors, indexes, and table partitions.
• 7+ years of experience with an ETL tool such as SSIS, Azure Data Factory, or AWS Glue
• Strong background in a data warehouse, data management, and data analytics
• Monitor ETL production batch schedules to meet predefined SLAs.
• Resolve functional and system errors as identified by Business Partners
• Coordinate activity between Business Units and EIS to drive open action items to closure.
• Work with other technical teams to resolve infrastructure-related problems.
• Maintain a good relationship with other technology teams within the client enterprise.
• Generate, Control, and Resolve incident tickets relating to Production batch and Data availability issues.
• Serve as senior contact for production support issues and escalations.
• Enterprise L3 support to resolve production support issues in a timely manner.
• Candidate is expected to exude a take-charge attitude toward problems and thrive for excellence. This is a hands-on, delivery-focused role.
• Attempt to isolate, reproduce, and resolve problems using available systems and tools, and investigate potential workarounds for verified defects.
• Participate in the creation of Knowledge Base articles, solutions, and other related support collateral.
• To interface with Subject Matter Experts, where the problem cannot be resolved at a frontline support level.
• Good to have:
• Excellent written and verbal communication skills
• Detail-oriented; Analytical with problem-solving abilities",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Tata Technologies,Data Engineer,"Job Title : Data Engineer

Job Location : Thane(Mumbai)

Domain Knowledge:

Should be capable of carrying out the following operations on the data with any application.

• Familiarity with data loading tools like Flume, Sqoop.

• Analytical and problem-solving skills applicable to Big Data domain

• Proven understanding with Hadoop, PySpark, Hive, Hadoop

• Good aptitude in multi-threading and concurrency concepts",Thane,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Trademo,Data Engineer,"Position : Data engineer - Full Time and Intern

Role: Python/ Data scraping with Algo and Data structures with Automation

Experience: 0-1 years

Location: Gurgaon (Work from office)

About Trademo

Trademo is a Global Supply Chain Intelligence SaaS Company, headquartered in Palo-Alto, CA. Trademo collects public and private data on global trade transactions, sanctioned parties, trade tariffs, ESG and other events using its proprietary algorithms. Trademo analyzes and performs advanced data processing on billions of data points using technologies like Graph databases, NLP and Machine Learning to build end-to-end visibility on Global Supply Chains. Trademo's vision is to build a single truth on global supply chains to help large and small businesses - discover new commerce opportunities, ensure compliance with trade regulations and build operational resilience. Trademo last closed its $12.5 mn Seed Round from marquee Silicon Valley VCs.

Trademo has been founded by Shalabh Singhal who is a third-time tech entrepreneur. Shalabh last co-founded ZipLoan. ZipLoan is a leading fintech lending startup in India. He earlier founded Credence, a Data-driven Digital Marketing, CRM Product and Sales Solutions company. Shalabh is an Alumni of Goldman Sachs, IIT BHU, CFA Institute USA and Stanford GSB SEED. Trademo has recently closed a $12.5 mn Seed round from some of the marquee investors in Silicon Valley.

Website

https://www.trademo.com

Location

Gurgaon (Work from office)

Technical Skills Required
• Python 3.6+ version, Pandas
• Scraping → Selenium, Beautiful Soap
• Knowledge NOSQL/MYSQL Database
• Knowledge how to tackle the problems with optimal Solution
• Individual contributor role with eagerness to learn new technologies - Elasticsearch , BigData etc.
• Knowledge of Basics DS and algorithms",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Pracemo Global Solutions,Data Engineer,"We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Responsibilities
• Analyze and organize raw data
• Build data systems and pipelines
• Evaluate business needs and objectives
• Interpret trends and patterns
• Conduct complex data analysis and report on results
• Prepare data for prescriptive and predictive modeling
• Build algorithms and prototypes
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Requirements And Skills
• Previous experience as a data engineer or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills
• Degree in Computer Science, IT, or similar field; a Master’s is a plus
• Data engineering certification (e.g IBM Certified Data Engineer) is a plus
• Self-motivated with a results-driven approach
• Aptitude in delivering attractive presentations
• High school degree
Skills: data warehousing,etl,sql,python,java,hadoop,hive,spark,nosql databases,cloud computing,aws,azure,gcp,data modeling,data mining,data visualization,communication,project management,data lake,data quality,data architecture",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Thompsons HR Consulting LLP,Lead Data Engineer,"We are looking for Lead Data Engineer

with Strong experience in Python, Development, Business Intelligence (BI tools), AWS, Mysql

Experience: 10+ years

It is a Remote opportunity.

If interested, please share your resume at deepika.ashok@thompsonshr.com",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Anonymous,Data Engineer - Partime / Freelance,"Required skills: Pyspark, AWS-cloud, Hive
Good to have: streamsets, CICD
Experience: 2 - 5yr
Timing : 8pm to 12am on weekdays",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
New Era India,Data Engineer/Sr. Data Engineer/Lead Data Engineer - Data Axle,"Data Engineer / Sr. Data Engineer / Lead Data Engineer (Pune)

About Data Axle

Data Axle Inc. has been an industry leader in data, marketing solutions, sales and research for over 45 years in the USA. Data Axle has set up a strategic global center of excellence in Pune. This center delivers mission critical data services to its global customers powered by its proprietary cloud-based technology platform and by leveraging proprietary business & consumer databases. Data Axle is headquartered in Dallas, TX, USA.

Roles And Responsibilities
• Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
• Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
• Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency.
• Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
• Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning.
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

Basic Qualifications
• 3 to 12 years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
• Demonstrated strength in data modeling, ETL development, and data warehousing.
• Experience using big data processing technology using Spark.
• Knowledge of data management fundamentals and data storage principles
• Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, Power BI etc.)

Preferred Qualifications
• Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
• Experience working with AWS big data technologies (Redshift, S3, EMR, Spark)
• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience working with distributed systems as it pertains to data storage and computing.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.",Pune,False,False,True,False,False,False,False,True,True,True,False,False,True,False,False,False
Quadratyx,Lead Data Engineer,"About Quadratyx

We are a product-centric insight & automation services company globally. We help the world’s organizations make better & faster decisions using the power of insight & intelligent automation. We build and operationalize their next-gen strategy, through Big Data, Artificial Intelligence, Machine Learning, Unstructured Data Processing and Advanced Analytics. Quadratyx can boast of more extensive experience in data sciences & analytics than most other companies in India. We firmly believe in Excellence Everywhere.

Purpose of the Job/ Role:

As a Lead Data Engineer, your work is a combination of hands-on contribution, customer engagement and technical team management. Overall, you’ll design, architect, deploy and maintain big data solutions.

Key Requisites:

• Expertise in Data structures and algorithms.

• Technical management across the full life cycle of big data (Hadoop) projects from requirement gathering and analysis to platform selection, design of the architecture and deployment.

• Scaling of cloud-based infrastructure.

• Collaborating with business consultants, data scientists, engineers and developers to develop data solutions.

• Leading and mentoring a team of data engineers.

• Hands-on experience on test-driven development (TDD).

• Expertise in No SQL like Mongo, Cassandra etc., preferred is Mongo and strong knowledge of relational database.

• Good knowledge of Kafka and Spark Streaming internal architecture.

• Good knowledge of any Application Servers.

• Extensive knowledge on big data platforms like Hadoop; Hortonworks etc.

• Knowledge of data ingestion and integration on cloud services such as AWS; Google Cloud; Azure etc.

Skills/ Competencies Required

Technical Skills

• Strong expertise (9 or more out of 10) in at least one modern programming language, like Python, Java.

• Clear end-to-end experience in designing, programming, implementing large software systems.

• Passion and analytical abilities to solve complex problems.

Soft Skills

• Always speaking your mind freely.

• Communicating ideas clearly in talking and writing, integrity to never copy or plagiarize intellectual property of others.

• Exercising discretion and independent judgment where needed in performing duties; not needing micro-management, maintaining high professional standards.

Academic Qualifications & Experience Required

Required Educational Qualification & Relevant Experience

• Bachelor’s or Master’s in Computer Science, Computer Engineering, or related discipline from a well-known institute.

• Minimum 7 - 10 years of work experience as a developer in an IT organization (preferably Analytics /

Big Data/ Data Science / AI background.

Quadratyx is an equal opportunity employer - we will never differentiate candidates based on religion, caste, gender, language, disabilities or ethnic group.",Hyderabad,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Persistent Systems,Data Engineer (Immediate joiner),"About Persistent

We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above.

We are experiencing tremendous growth, with $701.1 million in trailing 12-month revenue, representing 29.8% year-over-year growth. Along with that growth, we onboarded over 4,500 new employees in the past year, bringing our total employee count to over 16,500 people located in 18 countries across the globe.

At Persistent, our values are more than a list of ideals to improve our corporate image. We’re dedicated to building an inclusive culture that reflects what’s important to our employees and is based on what they value. As a result, 95% of our employees approve of the CEO and 83% recommend working at Persistent to a friend.

For more details please login to www.persistent.com

About Position
• 4+ years of strong technology experience in the field of transactional data and analytics systems
• Lead client conversations and data discovery sessions
• Should understand and be able to command architecture design for transactional and analytics systems.
• Strong SQL skills
• Hands on experience in building end to end data / orchestration pipelines using Python.
• Cloud Experience- Should have experience with any cloud data products (AWS, GCP, Azure)
• Experience in Agile Methodologies
• Familiarity with source repositories (Git, BitBucket etc.)
• Excellent communication skills",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Niftel Resources,Senior Data Engineer,"Responsibilities:

 Design and build reusable components, frameworks and libraries at scale to support analytics

products

 Design and implement product features in collaboration with business and Technology

stakeholders

 Anticipate, identify and solve issues concerning data management to improve data quality

 Clean, prepare and optimize data at scale for ingestion and consumption

 Drive the implementation of new data management projects and re-structure of the current data

architecture

 Implement complex automated workflows and routines using workflow scheduling tools

 Build continuous integration, test-driven development and production deployment frameworks

 Drive collaborative reviews of design, code, test plans and dataset implementation performed by

other data engineers in support of maintaining data engineering standards

 Analyze and profile data for the purpose of designing scalable solutions

 Troubleshoot complex data issues and perform root cause analysis to proactively resolve product

and operational issues

 Mentor and develop other data engineers in adopting best practices

Qualifications:

Primary skillset:

 Experience working with distributed technology tools for developing Batch and

Streaming pipelines using SQL, Spark, Python [3+ years], Airflow [2+ years], Scala [1+

years].

 Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc.

 Able to quickly pick up new programming languages, technologies, and frameworks.

 Strong skills building positive relationships across Product and Engineering.

 Able to influence and communicate effectively, both verbally and written, with team members and

business stakeholders

 Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed

Spark jobs, build Docker images, etc.

 Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture

Good to have:

 Experience working with Data platforms, including EMR, Airflow, Databricks (Data Engineering &

Delta Lake components, and Lakehouse Medallion architecture), etc.

 Experience working in Agile and Scrum development process

 Experience in EMR/ EC2, Databricks etc.

 Experience working with Data warehousing tools, including SQL database, Presto, and

Snowflake

 Experience architecting data product in Streaming, Server less and Microservices Architecture

and platform.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,True,True
"Giant Eagle, Inc.",Senior Data Engineer,"Job Summary

As a Sr Data Engineer on the Marketing Data Platforms team, you will be working on a team to bring customer-centric personalization to life. In this role, you will be empowered to develop data solutions in support of analytics, data science, and business partners to understand capability requirements and develop data solutions based on priorities. This leading technical and architecture role will collaborate with product managers, architects, technology teams, analysts, marketing operations specialists, and monetization business partners to understand capabilities and that will be brought to life for Giant Eagle’s 4M+ customers. The ideal candidate will have experience within multiple technology platforms (e. g. GCP, Engagement Platforms, Customer Data Platform, Ad-Tech, etc.) while providing the vision and design for integrating customer data. Additional key skills and qualifications below

Job Description
• Primary Job Responsibilities:
• 5+ years of relevant technical experience working with various data engineering methodologies such as data integration and data pipelines (ETL/ELT) to activate against data at scale.
• 3+ years of experience of data modeling for analytic projects activities that include design, curation, and management of large datasets
• 3+ years of experience adeveloping on big data technologies with Spark and Hive, preferably leveraging such as DataBricks, Juypter notebooks, or GCP, AWS, and Azure equivalent technology.
• 2+ years of experience data solution design for data engineering pipelines
• Strong Experience building event driven systems using cloud technology: storage, Pub/Sub, cloud functions, API’s, and DataProc
• Expertise with databases experience such as BigQuery, Snowflake, and Synapse designing schema and dimensional data modeling
• Experience leveraging RESTful web services to collect and publish data.
• Experience in software engineering development and testing life cycles using but not limited to Python, R, Linux, Java, JavaScript, Lambda, and SQL programming
• Bachelor's degree in Computer Science, Mathematics, or other technical field or equivalent work experience. Advanced degree a plus
• Experience with Retail Media Networks and Ad Tech preferred

Role Requirements:
• Architect, develop and implement end-to-end complex data projects and technical solutions through translating business requirements into technical solutions and data-flow architectures.
• Architect, build and automate data pipelines that clean, transform, and aggregate unorganized data into data sources that are ready for analysis.
• Use expertise to apply various analytic methods to discover and interpret information about customer behavior from multiple data sources to implement analytics solutions
• Use expertise in database design to implement, operate stable and scalable dataflows from multiple marketing platforms into a cloud data lake for Ad Tech
• Experience building data visualization tools Tableau, PowerBI, and Looker with data modeling and Looker ML preferred
• Design, implement and deploy data applications and mechanisms using big data technology
• Provides subject matter expertise for multiple projects concurrently through all phases of the development lifecycle.
• Develop, enhance, govern, and administer for data platform to: collect data, transform, enrich, unify, segment, and integrate data
• Strong adherence data ethics rules around PII data sets
• Work collaboratively with IT teams, Performance Marketing team, and business leaders to ensure actionable is provided key stakeholders
• Research and analyze customer behavior data to improve customer experience
• Experience with agile or other rapid application development methods a plus
• Retail industry experience a plus

About The Company

Since our founding in 1931, Giant Eagle, Inc. has evolved into one of the top 40 largest private corporations in the U. S. and one of the country’s largest food retailers and distributors. With more than 37,000 Team Members and $9.7 billion in revenue, we are committed to investing in people, technology, and data to elevate our customer’s experience across multiple touchpoints. It helps us follow on our commitment to serving others and improving our communities.

About Giant Eagle Bangalore

The Giant Eagle GCC in Bangalore is our global capability center. Our team of more than 370 members at the GCC enables us to expand internal capabilities in the areas such as data analytics, merchandising and eCommerce, quality engineering, and automation to generate insights for faster decision-making and helping us accelerate our business strategy. Our team in India plays a pivotal role in helping the company transition to new ways of working by redefining the food and grocery shopping experience for over 4.6 million customers.

About Us

At Giant Eagle Inc., we’re more than just food, fuel and convenience. We’re one giant family of diverse and talented Team Members. Our people are the heart and soul of our company. It’s why we strive to create a nurturing environment that offers countless career opportunities to grow. Deep caring and solid family values are what makes us one of the top work places for jobs in the Greater Pittsburgh, Cleveland, Columbus and Indianapolis Areas. From our Warehouses to our GetGo’s, our grocery Stores through our Corporate home office, we are working together to put food on shoppers' tables and smiles on their faces. We’re always searching for the best Team Members to welcome to our family. We invite you to join our Giant Eagle family. Come start a lasting career with us.",,True,False,True,True,False,False,True,False,False,True,False,False,False,True,False,True
TMRW House of Brands,Data Engineer-III,"Responsibilties:

Create, implement, and operate the strategy for robust and scalable data pipelines for business intelligence and machine learning.

Develop and maintain core data framework and key infrastructures

Create and support the ETL pipeline to get the data flowing correctly from the existing and new sources to our data warehouse.

Data Warehouse design and data modeling for efficient and cost-effective reporting

Collaborate with data analysts, data scientists, and other data consumers within the business to manage the data warehouse table structure and optimize it for reporting.

Constantly striving to improve software development process and team productivity

Define and implement Data Governance processes related to data discovery, lineage, access control, and quality assurance

Perform code reviews and QA data imported by various processes

Qualifications

6-10 years of experience.

At least 3+ years of experience in data engineering and data infrastructure space on any of the big data technologies: Hive, Spark, Pyspark(Batch and Streaming), Airflow, and Delta Lake.

Experience in product-based companies or startups.

Strong understanding of data warehousing concepts and the data ecosystem.

Strong Design/Architecture experience architecting, developing, and maintaining solutions in AWS.

Experience building data pipelines and managing the pipelines after they’re deployed.

Experience with building data pipelines from business applications using APIs.

Previous experience in Databricks is a big plus.

Understanding of Dev Ops would be preferable though not a must

Working knowledge of BI Tools like Metabase, and Power BI is plus

Experience in architecting systems for data access is a major plus.",Bengaluru,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False
Impetus,GCP Data Engineer,"Qualification
• The candidate should have extensive production experience (3-5 Years ) in GCP, Other cloud experience would be a strong bonus.
• Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
• Exposure to enterprise application development is a must

Role
• 6-10 years of IT experience range is preferred.
• Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.
• Strong experience in Big Data technologies – Hadoop, Sqoop, Hive and Spark including DevOPs.
• Good hands on expertise on either Python or Java programming.
• Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
• Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
• Ability to drive the deployment of the customers’ workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
• Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
• Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
• Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
• Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.",इन्दौर,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Newell Brands,Cloud Data Engineer,"Job Title: Cloud Data Engineer

Report To: Sr. Manager, Data Engineering

Job Location: Guindy, Chennai, India

Job Duties
• Participates in the full lifecycle of cloud data architecture (Preferably Azure cloud) from gathering, understanding end-user analytics and reporting needs.
• Migrate On-Prem applications and build CI/CD pipeline in Cloud platform.
• Design, develop, test, and implement on Cloud platform (Ingestion, Transformation and export pipelines that are reliable and performant) .
• Ensures best practices are followed and business objectives are achieved by focusing on process improvements.
• Quickly adapt by learning and recommending new technologies and trends.
• Develop and Test Data engineering related activities on Cloud Data Platform.
• Work with dynamic tools within a BI/reporting environment.

Job Requirements
• B.E/B.Tech, M.Sc/MCA.
• 5+ years experience in Rapid development environment, preferably within an analytics environment.
• 3+ years experience with Cloud experience (Preferably Azure cloud but not mandatory).
• DB : T-SQL, SQL Scripts, Queries, Stored Procedures, Functions and Triggers
• Language : Python / C# or Scala
• Cloud: Azure / AWS / Google cloud
• Frameworks: Cloud ETL/ELT framework

Preferred
• Azure Data Factory, Azure Synapse Analytics, Azure SQL, Azure Data lakes, Azure Data bricks, Airflow and Power BI
• Data warehousing principles and frameworks.
• Knowledge in Cloud DevOps and CI/CD pipelines would be an added advantage.

Newell Brands (NASDAQ: NWL) is a leading global consumer goods company with a strong portfolio of well-known brands, including Rubbermaid, FoodSaver, Calphalon, Sistema, Sharpie, Paper Mate, Dymo, EXPO, Elmer's, Yankee Candle, Graco, NUK, Rubbermaid Commercial Products, Spontex, Coleman, Campingaz, Oster, Sunbeam and Mr. Coffee. Newell Brands' beloved, planet friendly brands enhance and brighten consumers lives at home and outside by creating moments of joy, building confidence and providing peace of mind.",Chennai,True,False,True,False,False,False,False,False,True,False,False,False,False,False,True,False
Zupee,Lead Data Engineer,"About Zupee

Zupee is India’s fastest growing Technology backed Behavioral Science company. We are innovating Skill-Based Gaming with a mission to become the most trusted and responsible entertainment company in the world. We have been constantly focusing on innovation of indigenous games to entertain the mass.

Our strategy is to invest in our people & user experience to drive profitable growth and become the market leader in our space. We have been experiencing phenomenal growth since inception and running profitable at EBT level since Q3, 2020. We have closed Series B funding at $102 million, at a valuation $600 million.

The company also announced a partnership with Reliance Jio Platforms, post which Zupee games will distribute its content across all customers using Jio phones. The partnership now gives Zupee the biggest reach of all gaming companies in India, transforming it from a fast-growing startup to a firm contender for the biggest gaming studio in India.

About The Job

Lead Data Engineer

We are looking for someone to develop the next generation of our Data platform

collaborating across functions like product, marketing design, growth, strategy, customer

experience and technology.

Core Responsibilities

●Understand, implement and automate ETL and data pipelines with up-to-date

industry standards

●Hands-on involvement in the design, development and implementation of optimal and

scalable AWS services

What are we looking for?

●S/he must have experience in Python

●S/he must have experience in Big Data – Spark, Hadoop, Hive, HBase and Presto

●S/he must have experience in Data Warehousing

●S/he must have experience in building reliable and scalable ETL pipelines

Qualifications and Skills

●6-12 years of professional experience in data engineering profile

●BS or MS in Computer Science or similar Engineering stream

●Hands-on experience in data warehousing tools

●Knowledge of distributed systems such as Hadoop, Hive, Spark and Kafka etc.

●Experience with AWS services (EC2, RDS, S3, Athena, data pipeline/glue, lambda, dynamodb etc.
•",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
UST Product Engineering,Data Engineer,"Job Description :

- 4-8 Years experience in data warehousing , ETL processes, and data analytics.

- Good experience in developing, maintaining, and testing infrastructures for data generation, verification and transformation.

- Good understanding database concepts (SQL, Cloud DBs)

- Strong SQL query, profiling and troubleshooting skills

- Good understanding AWS Data related concepts like big data, big query etc.

- Basic understanding AWS (or supported) ETL tools - Glue, Airflow etc etc. would be an added advantage

- Good Understanding of python programming

- Basic understanding of programming language like C# or similar

- Basic knowledge of working in scrum/agile teams and tools like JIRA, confluence etc.",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.

Apply for this job",Mumbai,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
ThousandEyes,Cloud Application and Data Engineer,"Cloud Application and Data Engineer

Who We Are

The name ThousandEyes was born from two big ideas: the power to see what’s not ordinarily possible, and the ability to collect intelligence from vantage points as diverse and global as the Internet. As organizations depend on cloud services, the Internet has become their defacto network connecting cloud applications to users. Our Internet and cloud intelligence platform is like a ‘Google maps of the Internet’, providing the only collectively powered view of digital experiences end-to-end. We enable our customers made up of the world’s largest and fastest-growing brands, to identify problems before they impact revenue, brand reputation, or employee productivity.

In August 2020, Cisco Systems completed the acquisition of ThousandEyes, which now forms the ThousandEyes Business Unit within Cisco’s Network Services Business Group,and is a foundational component of Cisco’s growing Observability business.About The Team

Digital experiences rely on a vast ecosystem of ISPs, cloud providers, SaaS applications, individual configurations, unique devices, and many other external services that are critically dependent on the Internet. Trying to identify the root cause of a problem or a deviation from normal is like finding a needle in a haystack. This leads to long downtimes and poor customer or employee experience.

The AI Analytics team at ThousandEyes is leveraging machine learning at scale, while working across several different business units, to our help customers answer tough questions like:
• What is normal in my network and how do I catch deviations from this normal?
• How do I understand the root cause of a problem in my network or application stack?
• How do I ensure that devices joining my network are who they say they are?
• How do I know when my networking gear is about to break?

The goal of the AI Analytics Team is to leverage different machine learning techniques to deliver actionable insights for our customers to solve real world problems in their complex environments.

What You’ll Do

You will be part of our data and platform team. A worldwide distributed team responsible for data collection, ingestion, processing, and quality. You will play an important role in helping to deliver new ML powered features to our customers as well as monitoring and improving the existing features for performance and quality. You will work in the AI Cloud hosted on AWS with Python, Go, Spark, Hive, Open Search, and other cutting-edge technologies.

Responsibilities
• Collaborating closely with ML engineer to bring new features to production.
• Create and maintain an optimal data pipeline architecture.
• Contribute and operate data quality tooling.
• Monitor and optimize compute and query performances.
• Troubleshoot and debug issues across our applications and services.

Who you are

Agile, pragmatic and hardworking. You also love to interact with data scientists, machine learning engineers and software engineers to develop pipelines that scale seamlessly at huge volumes of data. You love technology, innovation and building products at scale.

You hold a degree in computer science, or a related field and you can demonstrate a consistent track record in the following areas:
• At least 4 years of software development experience.
• 2 years of experience building and developing data-intensive systems at industrial scale.
• Dimensional data modeling and schema design for both SQL and NoSQL databases.
• Prior exposure to data science, machine learning or statistics is a plus.
• Previous experience developing applications running on a public cloud infrastructure is a plus.
• Strong Communication and documentation skills in English.
• Strong sense of ownership, drive, attention to detail and ability to work in a distributed team.

We are looking for candidates based in Bangalore to work hybrid

Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.

Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do –you can’t put us in a box! But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. So, you have colourful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us.

We recognize that diverse teams make the strongest teams, and we encourage people from all backgrounds to apply.

Cisco COVID-19 Vaccination Requirements

The health and safety of Cisco's employees, customers, and partners is a top priority. Our goal is to protect and mitigate the spread of COVID-19 infection for strong business resiliency during the pandemic. Therefore, Cisco may require new hires to be fully vaccinated against COVID-19 if the role requires business-related travel, meeting with customers/partners (including visiting third-party sites on behalf of Cisco), attending trade events, and Cisco office entry, unless otherwise prohibited by applicable law, and in countries where COVID-19 vaccination is legally required. The company will consider legally required accommodations/exceptions for medical, religious, and other reasons as per the requirements of the role and in accordance with applicable law. Additional information will be provided to candidates about the requirements and accommodation process at the offer time based on region.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Verizon,Principal Engineer - Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

You will be expected to architect solutions for business projects, work with enterprise architects to align application & system architecture to enterprise strategy and deliver individually and/or with the help of a team. You need to have passion to learn and educate fellow associates/subordinates and guide them to follow best practices. Principal consultant to the team that develops, maintains and enhances the service delivery and management for NS applications
• Architecting/Developing solutions for the application which deals with big data platforms.
• Engaging with Enterprise Architects on HLAs and defining new solutions that adhere to big data volume processing.
• Driving a Culture of Innovation: Champion a culture of innovation and drive as an example.
• Supporting customers with major platform issues and coordinating triage efforts to solve them.
• Identifying and aligning project requirements and conducting impact analysis.
• Working closely with the business team, and other internal IT teams to deliver projects on time.
• Preparing presentations and reports to internal and external customers, as well as internal Executives.
• Evaluating various new technical products based on changing business needs and making product recommendations to management keeping in mind the architecture of the entire list of applications supported.
• Providing technical leadership and business-related subject matter expertise on large, highly complex projects.
• Guiding the team on best practices for efficient and streamlined delivery of software to production. Guiding teams on maintaining security posture and code quality of applications keeping the tech debt in check.
• Identifying chronic production issues, pain points of customers by evaluating feedback and monitoring the NPS to maintain it above the required threshold.
• Working with Quality Assurance, UAT & Production Support teams to support releases, troubleshoot progression/regression issues, integration & E2E testing and implement deliverables as per the targeted timelines.
• Working with infrastructure teams to implement DevOps capabilities that help streamline the CICD process. Leverage innovative technologies to build proof-of-concepts that help build customer experiences, reduce pain points in the current experience, and provide a delight factor to customers.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You view technology through a lens of making things better and more effective. Understanding and building continual improvements to the digital value chain is something you flourish with. You enjoy the process of solving complex issues while empowering the team around you to do the same.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Experience in Hadoop, Hive, Pyspark , Spark Scala, PIG, Java, GCP, AWS, CICD (Jenkins/ Gitlab).
• Experience in Big query, Composer, Cloud Functions & Java script.
• Experience with any of RDBMS, Druid and MongoDB.
• Experience in Devops & automation.
• Experience in Docker/K8s & SRE Practice.
• Experience in Agile & SAFe methodologies.

Even better if you have one or more of the following:
• A Master's degree.
• Ability to design products which can scale up for large volumes of data.
• Knowledge in Wireless Domain.
• Knowledge of Security Vulnerabilities.
• Strong written and verbal communication skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False
lululemon India Tech Hub,Data Engineer - SQL & Python,"We are looking to hire dynamic Data Engineers for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.

Responsibilities
• Uses structured tools for analysis and presentation of concepts and models to enhance the BRD
• Develop, maintain and deliver training materials to the supply chain end-users
• Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs
• Support day to day reporting needs where required
• Support production issues as relate to application functionality and integrations
• Excellent spoken and written communication skills (verbal and non-verbal)
• Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)
• Solid scripting capability for analysis and reporting (ANSI SQL)
• Solid experience in RDBMS and NoSql technologies
• Strong analytical skills to support BAs.
• Strong problem-solving skills (Math skills required for data modeling)
• Ability to work as an integration / data engineer.
• Ability to manage and complete multiple tasks within tight deadlines
• Possess expert level understanding of software development practices and project life cycles.
• Working experience with Java batch spring boot/ python.
• Working Experience with cloud-native technologies
• Must have: Working experience in dealing with big data and data manipulation.
• Desired: Familiarity with Retail planning / merchandising systems/ supply chain.
• Desired: Familiarity with DevOps practices like CICD pipeline
• Desired: Retail experience is a plus. (fashion retail experience would be ideal)
• Must Have: Working experience with cloud platforms namely AWS
• Must Have: Working experience with large data sets (at least 80 – 100 GB data)

Requirements
• name : lululemon India Tech Hub
• location : Bengaluru, IN
• experience : 5 - 8 years
• Primary Skills: SQL or RDBMS or NoSQL,Python,AWS,Springboot,ETL",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Splunk,Senior Data Engineer - 27505,"The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will work as part of an evolving Enterprise Data Management (EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.

The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible for building and scaling the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineers and Infrastructure teams plus other data roles, including Data Governance, Quality, and Architecture Stewards). Strong technical experience within enterprise software is essential.

Responsibilities:
• Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
• Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
• Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
• Identify and document standard methodologies, standards, and architecture guidelines
• Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
• Identify ways to improve Data Reliability, Data Efficiency and Data Quality

Required Qualifications, Skills & Experience:
• 7+ years of data engineering related experience such as data analysis, data modeling, and data integration.
• Experience with Sales Operations, Partner Operations and customer success business processes and applications
• Experience in custom ETL design, implementation, and maintenance
• Strong knowledge of programming languages (e.g. Python and Object Oriented Programming)
• Hands-on experience with SQL database design
• Experience working on CI/CD processes and source control tools such as GitHub and GitLab
• Experience working in Snowflake and relational databases
• Extensive hands-on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake
• Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
• Experience with or exposure to data governance & quality principles and practices
• Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
• Passionate about data solutions, technologies, and frameworks
• Experience in Data Visualization tools such as Tableau

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Knowledge of enterprise systems such as Salesforce, Workday, SAP etc.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
Reverate,Senior Data Engineer - Remote,"Reverate Tech is a product and service-based start-up, working with International Clients. Our services include Data Engineering, Web Development, BI/Data Warehousing, Enterprise Application Implementation (ERP/CRM), and NetSuite. Our product portfolio has business apps in the domain of ERP, Auto Service, and Personal Safety.

This is an exciting opportunity to work as Senior Data Engineer for our client SellerX.

SellerX is the 3rd fastest growing company in the EU evaluated at more than 1 Billion Euros. It has an ambitious goal: to become a leading global acquirer and operator of a new generation of eCommerce businesses.

Your Job:
• You are responsible for all types of data management processes (collection, storage, cleansing, preparation, maintenance, accumulation, transfer to business reporting).
• You optimize and develop existing and new data warehouse applications using tools for data ingestion and data modeling
• You design and document new data models and best-practice solutions.
• You are responsible for prototyping and implementing new ETL jobs and modeling approaches.
• As a data engineer, you will deal with python programs and their configurations in order to create or improve automated data engineering tasks.
• In addition to technical project management, you advise our other tech teams in Data Management aspects.
• Ensuring data security (e.g. encryption) and improving the backup strategy.
• You work hand in hand with data architects, data analysts, and data scientists.
• You ensure that quality, stability, and robustness along the entire process chain meet our high standards.

Your Background:
• You have a bachelor's degree with a focus on software engineering.
• 5+ years of experience in data engineering.
• Strong with Algorithms and have worked on scaling pipelines/solutions
• Hands-on experience with data ingestion tools like Fivetran, Daton, Stitch, or Data Virtuality.
• Hands-on experience with ETL and orchestration tools like Apache Airflow or similar.
• Object-oriented Python programming is more than just a plus.
• Expert knowledge in the areas of data modeling and ETL processes on the SQL level (e.g. using DBT), as well as experience working with REST APIs, is beneficial.
• DevOps experience
• In addition to your ""hands-on"" mentality, you score points with a high technical affinity and a strong analytical mindset.
• Your working standards do not suffer in terms of quality, even in hectic times.

Benefits:
• Compensation: up to 40 LPA
• 100% remote-working;
• Flexible working hours;
• Development of your personal strengths in a dynamic environment;
• An attractive and varied job with a high level of personal responsibility;
• A collegial togetherness and a modern management style/startup;

Interested? Join us and start your learning and growth journey.

Reverate focuses on Software Engineering. Their company has offices in India. They have a small team that's between 11-50 employees.

You can view their website at https://reverate.tech/",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
"6221, Roche",Senior Data Engineer,"The Position

Roche sequencing solution is developing the next generation sequencing based on nanopore technology. This has the potential to make sequencing based diagnostics cheaper, faster and more accurate enabling precision medicine and early diagnosis of many diseases improving the health outcome.

As part of Data Science Automation group, you will get to work on key software technologies enabling research and development of sequencer. You will solve complex problems related to processing terabytes of data coming out sequencer and deriving useful insights from the data. This requires massively parallel computation locally on GPU as well as in the cloud. You will gain exposure to latest and greatest in data engineering and data pipeline tools and technologies. You will also work with advanced data visualization problems involving millions of data points.

You also will get to collaborate with multidisciplinary team of scientists and engineers working in fields ranging from protein engineering, bio chemistry, biophysics, stats modeling, bioinformatics and deep learning.

If you are excited to become part of the next generation sequencing research and development and revolutionize the healthcare, we have a rare opportunity for you to come and work with us.

We need an experienced Data/Workflow Engineer with a strong background in designing and developing highly scalable data management solutions and workflow pipelines. You will work across a variety of problems and application spaces involved in high availability systems, for data management and compute systems, at a very large scale. You will be working with a hardworking team of engineers and data-scientists who are passionate about building creative and novel solutions at the forefront of Sequencing research.

Required Qualifications:
• BS in CS or similar and 10+ years professional experience, or MS with 7+ years of experience in building highly scalable, performant software systems, in a Linux environment.
• Strong, hands on experience building and supporting Enterprise level Workflow management systems such as airflow, nexflow, kubeflow etc. Experience with building performant airflow pipelines with a large number of DAGs and dynamic DAGs is desirable.
• Working experience in deploying and managing airflow platforms, knowledge of Terraform, Kind, Helm etc. is a huge plus.
• At least 3+ years’ experience of developing solutions using container and cloud technologies. Preferably Kubernetes, Docker.
• Experience building with cloud native technologies (e.g. GCP, AWS), blob stores and knowledge of various data compression formats.
• Demonstrated skill with software development following current software engineering best practices using languages such as: Python, Java and BASH scripting.
• Have a strong understanding of modern software development practices and tools, including: version control systems (e.g., Git), issue trackers, and test frameworks.
• Experience building and using automation tools, CI/CD, unit testing.

You 'll go above and beyond our required requirements if you...
• Possess a PhD/MS in Computer Science, Computer Engineering, or another related, technical discipline.
• Have at least ten years of relevant experience in the development of software systems ideally in a Linux environment.
• Have experience using modern frontend and backend software frameworks for software applications.
• Knowledge of challenges involved in large-scale, high-availability data platforms. Experience with designing and implementing platforms providing secured access to large datasets.
• Have experience applying software expertise to full project lifecycles, including requirements analysis, design, implementation, and testing.

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,True,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False
Koch,Senior Data Engineer,"Description

Position Description/ Requirements

The Data Engineer will be a part of an international team that designs, develops and delivers Data Pipelines and Data Analytics Solutions for Koch Industries. Koch Industries is a privately held global organization with over 120,000 employees around the world, with subsidiaries involved in manufacturing, trading, and investments. Koch Global Solution India (KGSI) is being developed in India to extend its IT operations, as well as act as a hub for innovation in the IT function. As KSGI rapidly scales up its operations in India, it’s employees will get opportunities to carve out a career path for themselves within the organization. This role will have the opportunity to join on the ground floor and will play a critical part in helping build out the Koch Global Solution (KGS) over the next several years. Working closely with global colleagues would provide significant international exposure to the employees.

The Enterprise data and analytics team at Georgia Pacific is focused on creating an enterprise capability around Data Engineering Solutions for operational and commercial data as well as helping businesses develop, deploy, manage monitor Data Pipelines and Analytics solutions of manufacturing, operations, supply chain and other key areas.

A Day In The Life Could Include:

(job responsibilities)
• Partner/collaborate with Business stakeholders and build high-quality end-to-end data solutions.
• Build a data architecture for ingestion, processing, and surfacing of data for large-scale applications in the cloud (AWS/ Azure)
• Create and maintain optimal data pipeline architecture.
• Follow best practices of Agile and DevOps focusing on the delivering of high-quality products and providing the ongoing support to meet the customers' needs
• Implement processes for Continuous integration, Test automation and Deployment (CI/CD Pipelines)
• Provide quality documentation of your design (process and workflows) and implementation including experiment tracking / logs.
• Provide on-call support on an as-needed basis
• Handle support cases to ensure issues are recorded, tracked, resolved, and follow-ups finished in a timely manner.

What You Will Need To Bring With You:

(experience & education required)
• Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science). Master’s degrees preferred.
• 6+ years of IT experience.
• In depth knowledge of Data Engineering concepts and platforms - SQL based systems, Hadoop, Spark, Distributed computing, In-memory computing, real time processing, pub-sub, orchestration, etc.
• Expertise of building data pipelines using (Pyspark based) and Databricks utilising techniques in Azure or AWS.
• 4-5 years of experience in DevOps and CI/CD using tools like Git, Terraform, Jenkins, Ansible.
• 5+ year of experience in Data modeling, SQL, Data Warehouse skills are a MUST.
• A passion and fearlessness for learning new technologies and methods in the areas of Administration
• Ability to thrive in a team environment and juggle multiple priorities.
• Excellent written and verbal communication skills.

What Will Put You Ahead:

(experience & education preferred)
• In depth knowledge of entire suite of services in AWS/Azure Cloud Platform.
• Strong coding experience using Pyspark.
• Experience of designing and implementing ETL process using SSIS.
• Cloud Data Anaytics/Engineering certification.

Other Considerations:

(physical demands/ unusual working conditions)
• Some work may involve hours outside of normal KGS works hours.

Koch is proud to be an equal opportunity workplace",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Greetings from TCS !!!

TCS India presents excellent opportunities for IT professionals.

Role :- Data Engineer

Experience:- 7 to 10 years

Location- Bangalore / Mumbai / Chennai / Bhubaneswar

Required Technical Skill Set- Data Engineer – Big Data, Hadoop, Hive, Spark, Yarn

Must-Have:-

1. 4-8 Yrs of hands-on development experience

2. Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is mandatory.

3. Experience working with various data exchange formats (JSON, CSV, XML etc.).

4. Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred.

5. Excellent knowledge of SQL and Linux shell scripting.

6. Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)

Good-to-Have:-

1. Experience building real-time data pipelines using Kafka or spark streaming is preferred.

2. Exposure to Microsoft Azure (or other cloud) platforms is preferred.

3. Experience with Agile methodologies for project development.

4. Excellent diagnostic, analytical and problem-solving skills are preferred.

5. Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred

TCS Eligibility Criteria:
• BE/B.Tech/MCA/M.Sc/MS with minimum 3 years of relevant IT-experience post Qualification.
• B.Sc Graduates with minimum 4+ years of relevant IT-experience post qualification.
• Only Full Time courses would be considered.
• Consistent academic records class X onwards (Minimum 50%)
• Candidates who have attended TCS interview in the last 3 months need not apply.

Interested candidate can share their resumes with the mandatory details mentioned below.

Please update the details:

1. Total years of Exp:

2 Email ID :

3. Present Company:

4. Current & Preferred Location:

5. Mobile No.:

6. Current CTC:

7. Expected CTC:

8.Notice Period:

9: Working With TCS /CMC ( Direct Payroll) earlier (Yes/ NO):

10. No Of job change-

Interested candidate can share their resumes with hiba.fathima@tcs.com",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LTIMindtree,GCP Data Engineering POD Lead,"Primary Skill – GCP Data Engineering POD Lead

Total Exp – 3 to 14 Years

Notice Period – 0 to 30 Days

Job Location – Kolkata, Bangalore, Mumbai, Pune, Chennai, Hyderabad

Job Description:

Job Description:

TPrimary Skill – GCP

Secondary Skill – Python, Big query

Overall, more than 8+ Yrs of experience in Data Science Statistical Modeling and Projects to Develop and Deliver Data Science work Strong understanding of Machine Learning Statistics fundamentals Technology Skill Set Python R Pandas Scikit Learn R s

Desired Candidate Profile Technology & Engineering Expertise

• 5+ years of experience in implementing data solutions using GCP/SQL programming

• Proficient in dealing data access layer, RDBMS | NO-SQL.

• Experience in implementing and deploying Big data applications with GCP Big Data Services.

• Good to have SQL skills.

• Experience with different development methodologies (RUP | Scrum | XP) Soft skills

• Able to deal with diverse set of stakeholders

• Proficient in articulation, communication, and presentation

• High integrity

• Problem solving skills & learning attitude

• Team player Key Responsibilities

• Implement data solutions using GCP and need to be familiar in programming with SQL/python.

• Ensure clarity on NFR and implement these requirements.

• Work with Client Technical Manager by understanding customer’s landscape & their IT priorities

• Lead performance engineering and capacity planning exercises for databases",,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Arcadis,Azure Data Engineer,"ARCADIS is looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in “Everything Digital, Digital Everything”. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.

Technology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.

Role accountabilities:
• Possess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment
• You are excited about working with Azure Data Platform
• challenges while building the next wave of software engineering solutions
• Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform
• Leading the craftsmanship, security, availability, resilience, and scalability of your solutions
• Very strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.
• Strong experience in
• T-SQL, SSIS, SSAS, SSRS
• Azure Data Factory
• Azure Data Lake Store
• Azure Data Lake Analytics (Good to have, not mandatory)
• Azure SQL DB
• Azure SQL DW
• Azure Analysis Services, DAX
• Azure Data Bricks with Python/Scala
• Experience in building end to end solution using Azure data analytics platform.
• Experience in building generic framework solution which can be reused for upcoming similar use cases.
• Experience in building Azure data analytics solutions with DevOps (CI/CD) approach.
• Experience in using TFS, Azure Repos.
• Mentor peers to gain expertise on Azure data platform solutions skills.
• Experience in developing, maintaining, publishing, and supporting dashboards using Power BI.
• Strong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded

Qualifications & Experience:

Basic Qualifications:
• Bachelor in Engineering/Math/Statistics/Econometrics or related discipline
• Should have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.
• Preferred Qualifications:
• Master’s or Minor in Computer Science
• 3+ years of experience developing Data Engineering solutions
• Architecture, design experience with good knowledge of data model design & their implementation.

Why Become an Arcadian?

Our work with clients has a direct impact on people’s lives and on the planet. We make moving, living and belonging in cities safer, more resilient and more sustainable. By partnering with our clients as responsible custodians of our earth's resources, we can create a sustainable planet.

We continue to think of new ways to make positive impacts and create better experiences for people; data driven and digital solutions have become part of the Arcadis DNA. Working together with clients and using techniques like design thinking, we can get to the heart of our clients’ most pressing challenges and work together to solve them.

As a global business, we have committed to support five of the UN’s Sustainable Development Goals to ensure that our projects contribute to a better and more sustainable future for all. But it’s not just the work that we do on client projects that benefits communities and our planet. As a global business, we are committed to making a positive impact to society by supporting local communities where we operate.

To help protect our planet, we monitor and measure non-financial information to inform business decisions and reduce our own environmental impact as part of our commitment to be net zero carbon as a global company by 2030.

Our Commitment to Equality, Diversity, Inclusion & Belonging:

We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.

In accordance with the Colorado Equal Pay Transparency Rules:

Arcadis offers benefits for full time positions. These benefits include medical, dental, and vision coverage along with a 401K plan, STD and LTD, and Life Insurance as well as some additional optional benefits. Full time positions also come with annual PTO days and at certain levels a bonus program may apply. The Salary range for this role is $61,360 - $95,000 for Colorado based positions only. Other locations will vary in salary range

Transform Your World",Hyderabad,True,False,True,False,False,False,False,True,True,False,True,False,False,False,False,False
Dolby Laboratories,Data Engineer,"Join the leader in entertainment innovation and help us design the future. At Dolby, science meets art, and high tech means more than computer code. As a member of the Dolby team, you’ll see and hear the results of your work everywhere, from movie theaters to smartphones. We continue to revolutionize how people create, deliver, and enjoy entertainment worldwide. To do that, we need the absolute best talent. We’re big enough to give you all the resources you need, and small enough so you can make a real difference and earn recognition for your work. We offer a collegial culture, challenging projects, and excellent compensation and benefits, not to mention a Flex Work approach that is truly flexible to support where, when, and how you do your best work.

Play a key role as part of Dolby's new R+D Center in Bangalore as a Data Engineer in our Advanced Technology Group ""ATG"". ATG is the research and technology arm of Dolby Labs. It has multiple competencies that innovate technologies in audio, video, AR/VR, gaming, music, and movies. Many areas of expertise related to computer science and electrical engineering, such as AI/ML, computer vision, image processing, algorithms, digital signal processing, audio engineering, data science & analytics, distributed systems, cloud, edge & mobile computing, natural language processing, knowledge engineering and management, social network analysis, computer graphics, image & signal compression, computer networking, IoT are highly relevant to our research.

Responsibilities:

As a Data Engineer, you’ll be a part of a growing engineering team building and designing our core data infrastructure for our internal technology research and development efforts. You’ll have the chance to partner closely with our research and data science teams to understand data and functional requirements. We are looking for an experienced data professional who is a problem solver, logical thinker and passionate about everything relating to data and analytics. Your responsibilities include:
• Create and maintain optimal data pipeline architecture for data coming from different sources, in various formats and of different content type (text, audio, video etc.) allowing to standardize, clean and ingest data.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Design and develop solutions which are scalable, generic and reusable. Be responsible for collecting, storing, processing, and analyzing huge sets including, but not limited audio, video, and metadata.
• Develop techniques to analyze and enhance both structured/unstructured data and work with big data tools and frameworks.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Databricks, and AWS ‘big data’ technologies.
• Create data tools for research and data scientist teams.

What You Bring To The Role
• BsC/Msc degree in CS or EE. Work experienced desired, but not required.
• Experience building and optimizing streaming big data pipelines, architectures, and data sets.
• Deep understanding data pipeline frameworks including Databricks and Fivetran.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Experience or solid theoretical understanding of data workflows including:
• Ingestion
• Batch and stream processing
• Storage and archiving
• Visualization/Reporting and Dashboards
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Understanding of the current state of infrastructure automation, continuous integration/deployment - CI/CD, SQL/NoSQL, security, networking, and cloud-based delivery models.
• In-depth understanding of:
• NoSql databases (Kafka, HBase, Spark, Hadoop ,Cassandra, MongoDb etc). SQL development and any procedural extension language (T-SQL, PL/SQL, Pg/PLSQL etc.)
• Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Distributed data processing frameworks like Apache Spark, Apache Flink
• Scalable ML pipelines for image, video and audio modalities with tools such as Flyte, MLflow, Prefect, or AirFlow
• Data collection, labeling, cleaning, and generation tools such as LabelBox, SuperAnnontate, Scale Ai, or V7
• Scripting abilities with two or more general purpose programming languages including but not limited to Java, C/C++, C#, Objective C, Python, JavaScript.
• Data modeling and extraction of data from different sources
• Strong documentation skills, communication and client facing Experience
• Experience supporting and working with cross-functional teams in a dynamic environment.

Build your career profile, also within the Careers tab in Employee Central to open the possibility of new opportunities finding you. Express your interest. If you want to express your interest in a specific opportunity and be contacted by a recruiter, click the apply button associated with the relevant job description. The Recruiter is the only one who will see your application.

Please refer to the recruiting website for more information: https://jobs.dolby.com/careers

]]>",Bengaluru,True,False,True,True,False,True,True,True,False,False,False,True,False,False,True,False
Mercede,Positions for Data Engineer,"Technical Skills Competencies
• Deep hands-on expertise in Databricks (Scala or Python).
• Experience in Design and implementation of Big Data technologies (Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases) and familiarity with data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture).
• Experience in working as a Big Data Engineer: query tuning, performance tuning, troubleshooting, and debugging Spark and other big data solutions.
• Familiarity with a full range of data engineering approaches, covering theoretical best practices and the technical applications of these methods.
• Experience building and deploying a range of data engineering pipelines into production, including using automation best practices for CI/CD.
• Very good experience in writing SQL queries.
• Hands-on experience with any of the cloud providers such as AWS or Azure.
• Familiarity with databases and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Ability to juggle and prioritize multiple tasks within a collaborative team environment
• Desire to learn and grow both technical and functional skill sets, and drive team s potential
• Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience And Skills

Microsoft Azure and AWS Certifications
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Trained in Data Factory, Delta lake, Data bricks Notebooks
• Working experience in SAFe - Scaled agile framework
• Working experience in an international team environment
,

This job is provided by Shine.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
HuQuo,Interesting Job Opportunity: Azure Data Engineer - ETL/MDM,"Job Description
• To collaborate with various teams/regions in driving facilitating data design, identifying architectural risks and key areas of improvement in data landscape, and developing and refining data models and architecture frameworks
• Technical experience and knowledge in Cloud Data Warehousing, data migration and data transformation
• Develop and test ETL components to high standards of data quality and performance as a hands-on development lead
• Familiarity with Data Lakes, Data Warehouses, MDM, BI, Dashboards, AI, ML
• Design data architecture patterns and ecosystems including data stores (operational systems, data lakes, data warehouses, data marts), ingress patterns (API, streaming, ETL/ELT), and egress patterns (analytics/decision tools, BI tools). Lead, consult or oversee multiple architectural engagements
• Oversee and contribute to the creation and maintenance of relevant data artifacts (data lineages, source to target mappings, high level designs, interface agreements, etc.) in compliance with enterprise level architecture standards
• Experience in leading and delivering data centric projects with concentration on Data Quality and adherence to data standards and best practices.
• Experience in data modeling, metadata support, development and testing for enterprise wide data solutions
• Azure cloud experience is a must have with familiarity of the services: Azure Databricks, Azure Datafactory, Azure Datalake, Spark SQL, PySpark, Airflow, SQL server and Informatica MDM.
• Additional exposure to GCP and AWS is good to have.

Key Skill: Azure Databricks, ADF, ETL, Pipeline Dev, SQL, DWH, ADLS.

(ref:hirist.com)",Gurugram,False,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
AXA XL,Data Engineer,"Gurgaon, Haryana, India

The Application Developer plays a critical role within the Data and Analytics SDC as this person is responsible for designing and implementing data structures to support current and future analytical projects. We are looking for candidates that have experience working with data from a raw, unprocessed state and organizing it intuitively. Building this data pipeline enables our partners to analyze data better and faster – ultimately leading the organization in optimizing the decision-making process.

DISCOVER your opportunity

What will your essential responsibilities include?
• Candidates for this role should have experience developing data processes with source data in a variety of formats (structured / unstructured, databases, APIs) into a target state. This will involve building proper data pipelines to support initial exploration and real-time integration.
• Data development using appropriate tools and techniques to process data required for advanced analytics. A candidate would be expected to interact with Data Engineering Leads and Data Scientists to understand requirements and would be responsible for the development of the solution.
• Providing the right context of data required for a given analysis. This would require the candidate to work with data modelers/analysts to understand the business problems they are trying to solve and create data structures to feed into their analysis.
• Build upon learnings of internal and external data to become more proactive. This includes thinking ahead of what modelers will anticipate with their data needs and designing structures that are intuitive to use.
• Making sure quality and understanding of analytical data. This would require hands-on data experience to look into data issues and seek resolution or acceptance. Create the appropriate amount of documentation, leverage standards, and build upon them. Data should be reconciled and documented at various stages for integrity.
• Take part in developing governance and rigor of data management practice within the Data and Analytics SDC. This will also include partnering with enterprise IT groups and involvement in enterprise data-related functions.
• You will report to Data Manager/Principal Data Engineer.

SHARE your talent

We’re looking for someone who has these abilities and skills:
• Demonstrated ability to work through data complexities which include a variety of sources, formats, and structures. Robust preference for experience in the Insurance domain.
• Ability to see through ambiguous concepts and break down complex problems into manageable components.
• Detail-orientated, proven ability to recognize patterns in data.
• Demonstrated ability to incorporate data quality standards into data development.
• Possesses natural curiosity. Seek to understand the world around you, and question when appropriate.
• Robust SQL Skills required.
• 2-4 years of development experience using data development (visual ETL or coded) / analysis tools (ex. SAS, SPSS, R, Microsoft SSIS/SSAS, Informatica, DataStage, AbInitio).
• Experience in .NET, Python, or Java development is a plus.
• Experience in web extraction, unstructured data, advanced text parsing, machine learning, and NLP a plus.
• Familiarity with developer support tools (TFS/GIT, Jenkins) is a plus.
• College Degree in MIS, Information Technology, Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, or equivalent.

FIND your future

AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals, and even some inspirational individuals we don’t just provide re/insurance, we reinvent it.

How? By combining an effective and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business − property, casualty, professional, financial lines, and specialty.

With an innovative and flexible approach to risk answers, we partner with those who move the world forward.

Learn more at axaxl.com

Inclusion & Diversity

AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic.

At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That’s why we have made a strategic commitment to attract, develop, advance, and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It’s about helping one another — and our business — to move forward and succeed.
• Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability, and inclusion with 20 Chapters around the globe
• Robust support for Flexible Working Arrangements
• Enhanced family-friendly leave benefits
• Named to the Diversity Best Practices Index
• Signatory to the UK Women in Finance Charter

Learn more at axaxl.com/about-us/inclusion-and-diversity. AXA XL is an Equal Opportunity Employer.

Sustainability

At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called “Roots of resilience”, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations.

Our Pillars
• Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society – are essential to our future. We’re committed to protecting and restoring nature – from mangrove forests to the bees in our backyard – by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans.
• Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions.
• Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We’re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting.
• AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL’s “Hearts in Action” programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day – the Global Day of Giving.

For more information, please see axaxl.com/sustainability

Flexible Work Eligible

None

AXA XL is an Equal Opportunity Employer.

Location

IN-HR-Silokhera Gurgaon

Job Field

IT

Schedule

Full-time

Job Type

Standard",Gurugram,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,False
Inference Labs,Data Engineer,"Responsibilities for the job Key Responsibilities: - Data Model Designing, Developing and maintaining Data pipelines on cloud (AWS Platform) Translate business needs to technical specifications and framework Maintain and support data mart, data analytics platforms & application. Perform quality assurance to make sure the data correctness Develop sub-marts using SQL and OLAP function to fulfil immediate/ad-hoc need of the business users basis the comprehensive marts Monitoring of the performance of ETL and Mart Refresh processes, understand the problem areas and open a project to fix the performance bottlenecks. Other Responsibilities (If Any):- Availability during month-end Deck generation, may be sometime during week-end/holidays. Eligibility Criteria for the Job Education B.E/B.Tech in any specialization, BCA, M.Tech in any specialization, MCA Work Experience Data Engineer: 4+ years of experience in data engineering on cloud platforms like AWS, Azure, GCP Exposure with working on BFSI domain / big data warehouse project Exposure to manage multiple source of the information, both structured / unstructured data Manage data lake environment for point in time analysis (SCD Type 2), multiple refresh during the day, event based refresh Should have exposure on Managing environment having real time dashboard, data mart requirement. Primary Skill Must have orchestrated using any of the cloud platforms Expert in writing complex SQL Command using OLAP Working experience on BFSI Domain Technical Skills Must have orchestrated at least 3 projects using any of the cloud platforms (GCP, Azure, AWS etc.) is a must. Must have worked on any cloud PaaS/SaaS database/DWH such as AWS redshift/ Big Query/ Snowflake Python/Java Hands - on Exp from data engineering perspective is a must Experience with any of the object-oriented/object function scripting languages: Python, Java, Scala, Shell, .NET scripting, etc. is a must Experience in at least one of the major ETL tools (Talend + TAC, SSIS, Informatica) will be added advantage Management Skills Ability to handle given tasks and projects simultaneously in an organized and timely manner. Soft Skills Good communication skills, verbal and written. Attention to details. Positive attitude and confident.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
PwC,Data Engineer-Manager-P&T Labs,"Line of Service
Internal Firm Services

Industry/Sector
Not Applicable

Specialism
IFS - Internal Firm Services - Other

Management Level
Manager

Job Description & Summary
A career in National Special Functions, within Internal Firm Services, will provide you with the opportunity to support service, sector, and market leaders deliver the unique PwC client experience to our clients. You’ll play an important part in continuously innovating and improving Firm operations so that we can continue to provide the highest quality of services to our current and prospective clients.

Our team focuses on representing data as a strategic business asset to help serve our clients. You’ll focus on using data and information across PwC to drive change and improvements in data related operations to help enable the business as well as provide insights related to attendant risks.

Preferred Knowledge/Skills:

Demonstrates intimate knowledge and/or a proven record of success in the following areas:
• Understanding architectural design and data platform delivery in technologies that include, but are not limited to cloud, ETL, data streaming, data storage, data modeling, APIs/microservices, automation, continuous integration/continuous deployment;
• Showcasing work experience as a Data Engineer, Data Architect or similar role;
• Showcasing data engineering knowledge around complex efforts within established Software Development Lifecycles and methodologies including agile, scrum, iterative and waterfall;
• Showcasing technical knowledge that spans multiple platforms and portfolio of applications with demonstrated knowledge of the business strategic priorities in order to resolve complex problems;
• Utilizing IT processes and frameworks including, but not limited to, Identity Access Management (IdAM), Enterprise Application Integration, Data Warehousing, Business Intelligence, Reporting, Mobility, Master Data Management, and Search;
• Understanding of database structure principles;
• Showcasing advanced experience building and maintaining optimal data pipeline architecture and data streaming and integrations using tools such as ADF, SSIS, Informatica, API Management, Enterprise Service Bus (preferably Kafka);
• Showcasing advanced SQL knowledge and experience working with relational databases and performance optimization;
• Demonstrating data mining and segmentation techniques;
• Exhibiting knowledge in relational SQL, NoSQL and Big Data technologies;
• Understanding Data Federation/Virtualization technologies, such as PowerBI, Tableau, D3.js, and implementing Cloud based solutions;
• Assessing and analyzing system requirements;
• Showcasing analytical skills and a problem-solving attitude;
• Demonstrating virtual leadership and motivational skills;
• Recommending and participating in activities related to the design, development and maintenance of the Enterprise Data Architecture;
• developing internal relationships and PwC brand;
• Demonstrating time management skills with the ability to handle multiple projects simultaneously;
• Leveraging business knowledge and interpersonal skills to build, maintain, and influence relationships with leaders throughout the business and IT.

Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements
Not Specified

Available for Work Visa Sponsorship?
No

Government Clearance Required?
Yes

Job Posting End Date
May 10, 2023",Hyderabad,False,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Mindera,Data Engineer,"We are looking for an experienced Data Engineer to join our team.

Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.

As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.

This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.

National and international expected travelling time varies according to project/client and organisational needs: 0%-15% estimated

Requirements

You’re great at
• Python
• AWS like (Glue, S3, EMR, Athena and ECS/Fargate)
• SQL
• Airflow
• Data Modelling
• Pyspark

It also would be cool if you have
• Exposure to DBT would be preferable
• Experience working with modern data platforms such as redshift or snowflake would be preferable
• Experience working with Airflow, Docker, Terraform and CI/CD would be preferable
• Experience working with docker, Scala, and Kafka would be an added advantage

What You Will Be Doing
• Implement/support new data solutions in the data lake/warehouse built on the snowflake
• Develop and design data pipelines using python.
• Design and Implement Continuous Integration/Continuous Deployments pipelines.
• Perform Data Modelling using downstream requirements.
• Develop transformation scripts using advanced SQL and DBT.
• Write test cases/scenarios to ensure incident-free production release.
• Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
• Debug production and development issues and provide support to colleagues where necessary.
• Perform data quality checks to ensure the quality of the data exposed to the end users.
• Build strong relationships with team, peers and stakeholders.
• Contributes to overall data platform implementation.

Benefits

We offer
• Flexible working hours (self-managed)
• Competitive salary
• Annual bonus, subject to company performance
• Access to Udemy online training and opportunities to learn and grow within the role

At Mindera we use technology to build products we are proud of, with people we love.

Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.

We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world.

You get to work with a bunch of great people, and the whole team owns the project together.

Our culture reflects our lean and self-organisation attitude.

We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.

Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook

Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Rishabh Software,Big Data Engineer,"Job Description:

Roles and Responsibilities:

1) Work with BigData Practice Tech lead to Execute BigData Projects

2) Work with Techlead , helping him in Solutioning, Architecture and Technical Design

3) Analyze requirements and prepare low level design

4) Hands on implementation of Data ingestion, data processing and Data storage code and algorithms

5) Team management under Techlead guidance - including work distribution and delivery

6) Participate in potential client meetings and demos

Required Skills:

Any one programming Language - Java or Scala

Good Experience with Apache Spark

Any one Data integration platform - Kafka or similar

Any one No Sql data storage - S3 or No Sql Database

One live BigData project - Data ingestion , processing and Storage

Basic Cloud Exposure - AWS preferred

Excellent Analytical and problem solving skills.

Excellent Communication Skills",Vadodara,False,False,True,True,False,False,False,True,False,False,False,True,False,False,False,False
Cloud Software Group,Senior Data Engineer,"About Cloud Software Group

Cloud Software Group combines the capabilities of both Citrix and TIBCO, creating one of the world’s largest cloud software providers, serving more than 100 million users around the globe. When you join Cloud Software Group, you are making a difference for real people, each of whom count on our suite of cloud solutions to get work done – from anywhere. Members of our team will tell you that we value diverse lived experiences, varied perspectives, and having the courage to take risks. Our teams are encouraged to learn, dream, and build the future of work. We are on the brink of another Cambrian leap - a moment of immense evolution and growth. And we need your expertise and experience to do it. Now is the perfect time to move your skills to the cloud.

Position Summary

This is an individual contributor role with responsibility for supporting all data warehouse processes including technical analysis, design, development, implementation, and support of ETL solutions. The ideal candidate needs to have at least 5 years of experience developing with Microsoft SQL applications in an implementation and support role of a business intelligence organization.

Primary Duties / Responsibilities

Responsibilities will include, but are not limited to:

• Supporting the designs, tasks, and continuous improvements to maintain a scalable data warehouse

• Analyzing and validating data to ensure that business requirements are satisfied

• Creating data flow diagrams to depict business logic relating to data transformations

• Creating conceptual, logical, and physical data models for relational and dimensional solutions

• Breaking down, estimating, and executing increments of work

• Developing ETL packages of high complexity to fulfill all the business requirements

• Supporting deployment and delivery of defined technical solutions

• Communicating accurate and timely project status, issues, risks, and scope changes

• Performing root cause analysis of data discrepancies

• Creating data dictionaries and business glossaries to document data lineages, data definitions and metadata for all business-critical data domains

• Documenting all work (both technical and procedural) and ensuring that co-workers understand how to support system from an operational perspective

• Working in a highly collaborative team environment following the Agile Framework for planning and executing deliverables

Qualifications (include knowledge, skills, abilities, and related work experience)

• Bachelor’s degree in computer science or related field, or equivalent combination of education and recent, relevant work experience

• Minimum 5 years of experience in developing T-SQL Queries, Stored Procedures, and ETL packages with Microsoft SQL databases

• Strong understanding of data warehouse design and report development principles

• Experience in creating data flow diagrams and data models pertaining to business intelligence

• Experience in analyzing and developing reporting output such as Power BI, Tableau, or SSAS

• Strong interpersonal and problem resolution skills

• Strong teamwork and customer support focus

• Strong written (technical documentation) and verbal communication skills

• Ability to handle numerous conflicting priorities in a professional manner

Cloud Software Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all federal, state and local laws that prohibit employment discrimination on the basis of age, race, color, gender, sexual orientation, gender identity, ethnicity, national origin, citizenship, religion, genetic carrier status, disability, pregnancy, childbirth or related medical conditions, marital status, protected veteran status, and other protected classifications.",Bengaluru,False,False,True,False,False,False,False,False,True,True,True,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"Roles and responsibilities:
• Mandatory: Strong in Azure, ADF, Data Lake, Databricks, Pyspark
• Hands-on-experience in developing data lake solutions using Azure (Azure data factory for ingestion, Data Lake gen 2 and Azure SQL server for storage, Azure analysis service for transformations, Azure data bricks)
• Implement a robust data pipeline using Microsoft Stack.
• Create reusable and scalable data pipelines.
• Development and deployment of new data platforms.
• Leverage Azure BI services for development of Big Data Platforms.
• Work closely with the Product Owners and Architects to develop Azure Data Platforms.
• Work with the leadership to set the standards for software engineering practices within the team and support across other disciplines.
• Produce high-quality code that allows us to put solutions into production.
• Refactor code into reusable libraries, APIs, and tools.",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Affine,Data Engineer,"Company Description

About Company

http://www.affine.ai

""AFFINE"" cited by GARTNER as a SPECIALIST MIDSIZE CONSULTANCY in ANALYTICS and MACHINE LEARNING solutions and services. Click to Read More ""

Affine is a provider of high-end analytics services to solve complex business problems with offices in NJ, USA & Bangalore, India. We combine data driven algorithmic analysis with heuristic domain expertise to provide actionable solutions that empower organizations make better and informed decisions. Affine's value proposition is enabling clients to implement and realize ROI of the recommendations.

Affine has a group of people with significant experience in Analytics industry along with solid pedigree, deep business understanding and strong problem solving acumen. Our group primarily consists of Statisticians, Operations Researchers, Econometricians, MBAs and Engineers. Our employees have experience of working for many Fortune 500 companies.

Job Description

What the candidate will do:
• Contribute to adoption of cloud & cloud-based technologies and good design practices, while finding opportunities to simplify and scale
• Resolve problems and roadblocks as they occur with peers and help unblock junior members of the team. Follow through on details and drive issues to closure
• Define, develop, and maintain artifacts like technical design or partner documentation
• Drive for continuous improvement in Data engineering process within an agile development team
• Own and deliver assigned sprint tasks and help drive the team forward.
• Communicate and work effectively with geographically distributed cross functional teams

Experience

4 to 6 Years in Deploying models, Sage Maker or TensorFlow

Required skillset.
• Big Data: Spark, Kafka, Hadoop, Hive, SQL and NoSQL
• Cloud: AWS, EMR, Qubole/Databricks, VPC
• Devops: Docker containers and Jenkins. Spinnaker is preferred but not required.
• Programming languages: Scala and Pyspark is mandatory
• Agile and scrum experience and working with a remote team (nice to have, not required)

Must Have Skills
• Spark, AWS, Scala/Python, SQL, Java
• ML ops tools:Tensorflow or Sagemaker

Additional Information

Others
• Quick learner
• Excellent written and oral communication skills
• Excellent interpersonal & organizational skills
• Good listening and comprehension skills",Bengaluru,True,False,True,True,True,False,False,True,False,False,False,False,False,False,False,False
DISH Careers,Data Engineer,"About DISH:

DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world’s best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH’s largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market

Job Duties and Responsibilities:
• Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production
• Creation and optimization of data engineering pipelines for analytic projects
• Strong analytic capability and the ability to create innovative solutions
• Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues
• Provide technical guidance to L1 team members and help to resolve ETL related issues
• Need to work as on call-support

Skills, Experience and Requirements:
• Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment
• Experience in developing/debugging and fixing data ingestion pipelines both real time and batch
• Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb
• Experience in transforming/integrating data in Redshift/Snowflake
• Strong in writing complex SQLs to ingest data into cloud data warehouses
• Good hands on experience in shell scripting or python
• Experience with scheduling tools - ControlM, Airflow , StepFunction
• Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements
• Perform root cause analysis (RCA) for failures
• Good Communication skills – written and verbal with the ability to understand and interact with the diverse range of stakeholders
• Capable of working without much supervision",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
MPOWER Financing,"Data Engineer - Data and Analytics - Bangalore, India","THE COMPANY

MPOWER’s borderless loans and scholarships enable students from around the world to realize their full academic and career potential by attending top universities in the U.S and Canada.

As a mission-oriented fintech/edtech company, we move extremely quickly and leverage the latest technologies, global best practices, and heavy analytics to tackle one of the biggest challenges in financial inclusion. We’re backed by over $150 million in equity capital from top global investors, which enables fast growth and provides our company with financial stability and a clear path to an IPO over the coming years.

Our global team is composed of former management consultants, financial service and technology professionals, and other experts in their respective fields. We work hard, have fun, and believe strongly in our cause. For us, MPOWER’s mission is personal.

As a member of our team, you’ll be challenged to think quickly, act autonomously, and constantly grow creatively in an environment where fast change and exponential growth are the norm. Ideation and implementation happen very quickly. We value feedback and emphasize personal and professional development by providing the resources you need to further your skills and grow with the company. MPOWER is committed to cultivating your strengths and curiosity and helping you make an immediate impact.

MPOWER has been named one of the best fintechs to work for by American Banker for 2018, 2019, 2020, and 2021. We pride ourselves on being a “growth company for grown-ups,” where there are no pool tables but rather great health, education, and maternity/paternity benefits instead. Our team diversity has been recognized as well; we’re one of the most diverse workforces in the world in terms of nationality, gender, religion, age, sexual orientation, and educational background.

THIS IS A FULL-TIME POSITION, BASED IN OUR BANGALORE OFFICE

THE ROLE

You will be tasked with building and maintaining MPOWER’s data infrastructure. You’ll also play a key role in acquiring, organising and analysing data to provide insights that enable the company in making sound business decisions. This includes, but is not limited to:
• Maintaining MPOWER’s database and building on the existing database infrastructure
• Establishing the needs of different users and monitoring user access and security
• Capacity planning and refining the physical design of the database to meet system storage requirements
• Creating efficient queries and tools to obtain data for different business needs
• Building data models to identify, analyze and interpret trends or patterns in data sets that inform business decisions and strategy
• Working with various internal and external stakeholders to maintain and develop enhanced data collection systems
• Performing periodic data analyses, creating and presenting findings and insights
• Performing scheduled data audits in order to locate and correct code errors and maintain data integrity
• Collaborating with MPOWER’s global tech team to build data collection and data analysis tools

THE QUALIFICATIONS
• Undergraduate degree in computer science; advanced degree preferred
• 5+ years of experience in database programming, database administration and data analysis
• Must have prior experience in building high quality databases in accordance with end users information needs and views
• Proficiency in Big Data and Hadoop ecosystems.
• Deep familiarity with database design and documentation
• Hands-on expertise and exposure to at least one database technology (MySQL, PostgreSQL)
• Advanced knowledge of R/Python, PySpark, or Scala is a plus
• Prior experience building data pipelines and data orchestration is a plus.
• Superior analytical and problem solving skills
• Proven ability to create and present comprehensive reports
• Ability to multitask and own several key responsibilities at a given time
• Passion for excellence: constantly striving to improve professional skills and business operations

A passion for financial inclusion and access to higher education is a must, as well as comfortable working with a global team across multiple-time zones and sites!

In addition, you should be comfortable working in a fast growth environment, meaning a small agile team, fast-evolving roles and responsibilities, variable workload and tight deadlines, a high degree of autonomy, and 80-20 everything.

MPOWER Financing focuses on Financial Services, Finance, Finance Technology, Higher Education, and Education Technology. Their company has offices in New York City, Washington DC, and Washington. They have a small team that's between 11-50 employees. To date, MPOWER Financing has raised $7.291M of funding; their latest round was closed on October 2016.

You can view their website at http://www.mpowerfinancing.com/ or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,False,False,False
Cortex Consultants LLC,Data Engineer,"Hi,

Welcome to Cortex

Job Title: Data Engineer

Job Description

2+ years of Data Engineer experience in Snowflake (on Azure Cloud Preferred).

Strong knowledge of SQL to build queries and Optimization techniques.

Strong Knowledge of the ETL process using SSIS / ADF (Azure Data Factory) / Matillion

Experience of Python programming is an added advantage.

Location: Chennai

Work type-Hybrid

Immediate joiners

Interested candidates share your resume to

Deepak.g@cortexconsultants.com

Contact No: 9080100600",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Roche,Data Engineering Manager,"The Position
Engineering Manager is a critical leadership role in our Data Engineering team. This is a people management role that needs the ability to hire and grow top engineering talent and to manage multiple teams. It includes responsibility to deliver and operate high quality, scalable, and extensible products & solutions, including making appropriate design and technology choices. The role requires strong strategic thinking and making build/buy/partner decisions for technical capabilities. Effective Communication is critical, as you will be working closely with a variety of stakeholders to understand and address their needs. A healthcare background with experience in integrating healthcare IT systems would be good to have.

KEY RESPONSIBILITIES
• Manage team of Data Engineers working on multiple data analytics products.
• Work with different agile product teams, understand and fulfill their staffing needs.
• Work with business stakeholders to develop high level project plans and roles and responsibilities.
• Prepare training and development plans for the team.
• Understand and create a career path for the team members.
• Evolve and develop a long-term roadmap for team and projects.
• Apply data engineering best practices in terms of quality, security, scalability and maintainability.
• Participate in how the budget and staff is allocated for the projects.
• Maintain project time frames, budget estimates and status reports.
• Create management, communication plans and processes. Analyze and develop process for management and technical duties.
• Foster team bonding and trust within the team. Responsible for hiring, growing and motivating engineers on your team, ensuring you recruit and retain top talent.

REQUIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• BS degree in Computer Science, Computer Engineer or a related technical discipline with 10+ years of IT industry experience.
• At least 4-6 years of proven managerial experience developing a high-performing team.
• Experience in Agile Solution Delivery and Operations Management and people management.
• Quick learner with the ability to understand complex workflows and develop and validate innovative solutions to solve difficult problems.
• Strong communication, with the ability to explain complex technical problems to non-technical audiences and the ability to translate customer requirements to technical designs.
• Strong interpersonal skills, with proven ability to navigate complex corporate environments and influence stakeholders and partners.

DESIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• Proven work experience in AWS or other cloud related technologies.
• Experience of working in product based organization
• Proven work experience as an Engineering Manager or similar role
• Communication skills for overseeing staff and working with other management personnel
• Organizational skills for keeping track of various budgets, employees, and schedules simultaneously
• Leadership, team-building, and mentoring skills
• Personnel and project management skills
• Ability to work on multiple projects in various stages simultaneously
• Experience in the Healthcare Laboratory domain is a plus.

EDUCATION

Bachelor’s degree in Engineering

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Data.Ai,DNA Team - Data Engineer,"data.ai is the mobile standard and the trusted source for the digital economy. Our vision is to be the first Unified Data AI company that combines consumer and market data to provide insights powered by artificial intelligence. We passionately serve enterprise clients to create winning digital experiences for their customers.

We care deeply about our high-performance culture and operate as a global team. We put our customers at the center of every decision [Customer First], follow through with what we say we are going to do [Own It & Deliver] and propose solutions, not just issues [Challenge, Them Commit] to Win As A Team.

We are a remote-first company and we trust our people to get it done from the location that works for them.

What can you tell your friends when they ask you what you do?

As a DNA Team Data Engineer, I’ll be a key contributor to DNA team data services. I’ll help the DNA team to build and enhance internal processes of data production and transaction/transformation, as well as internal tools. And help colleagues from other teams and/or external clients to better experience the DNA team services.

You will be responsible for and take pride in…
• Exciting Projects using technical expertise across Python, SQL, Spark, DataBricks
• Build data pipeline across different data sources/databases such as AWS S3, PG database, and Snowflake
• Produce and maintain relevant documentation
• Support internal and external customers
• Becoming better at what you do every day

You should recognize yourself in the following…
• Bachelor’s degree in Computer Science, Engineering, or equivalent experience
• At least 5 years of related work experience in building data pipelines
• Strong skills in Python and PL/SQL
• Deep understanding and experience in building data pipelines across different data sources/databases such as AWS S3, PG database, and Snowflake
• Experience in data processing such as ETL
• Knowledge of machine learning and AI is preferred
• Familiarity with specific app markets (e.g.: Gaming, Entertainment, Finance, etc.) is a big plus
• Strong problem-solving, analytical, and troubleshooting skills
• A self-starter who identifies and solves problems before anyone has noticed
• Fluent in English, both written and oral

data.ai are in the process of establishing an entity in India, in the interim the employees will be on the rolls of Leap 29 our Global Employer of Record",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Danske Bank,Senior Data Engineer-ETL Datastage,"Experience 5-8Years

The ideal applicant should have the following skills:

- Strong technical experience in Data Warehousing and Experience in working with ETL tools (Datastage, Informatica etc) for the purpose of creating data marts for analytical purposes

- Strong understanding of relational database concepts & technology. Exposure to Big Data technologies is an added advantage.

- Strong analytical and problem solving skills with the ability to collect, organize, analyse and process large volumes of data in a complex environment

- Good written and verbal communication skills with the ability to communicate and articulate one's thought process clearly.

- Be self driven and work closely with business stakeholders, in a global environment, to gather enough context to translate the business
objective into an analytical solution.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Splunk,Data Engineer - 27516,"As a Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have a deep understanding of the architecture for enterprise-level data lake solutions using multiple platforms (RDBMS, AWS, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.

What you'll do: Yeah, I want to and can do that.
• As a Data Engineer, you will be responsible for engineering data pipelines for Splunk’s enterprise data platform, democratizing datasets, enabling advanced analytics capabilities, integrating data from various systems, and applications. You will work as part of an evolving Enterprise Data Management(EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. The role will collaborate closely with other specialists, Product Managers & key stakeholders across the company.
• Build large-scale batch and real-time data pipelines using the cloud data technologies, such as Snowflake, Matillion, Kubernetes, Python, Apache Airflow and Apache Kafka
• Serve as a resource for data management implementations on other technology teams and collaborate with data owners, business owners, and leaders.
• Supports the design and development of framework based data integration and interoperability across multiple Splunk Business applications.
• Advanced level skills in Python, SQL, data integration, data modeling and data architecture.

Requirements: I’ve already done that or have that!
• A minimum of 5 years of related experience
• 3+ years of experience as a Data Warehouse Architect or Data Engineer.
• 2+ years of experience driving adoption and building automation of data management services and tools.
• 2+ years of experience with API based ELT automation framework, data management, or interface design, development and maintenance.
• Large scale design, implementation and operations of Cloud data storage technologies such as AWS Redshift, Snowflake, Kubernetes, etc.
• 3+ years of experience with programming scripting and data science languages such as Python, SQL, etc.
• Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.
• Advanced knowledge of Big Data concepts in organising both structured and unstructured data

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Experience with Sales Operations, Partner Operations and customer success business processes and applications

Education: Got it!
• Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Verizon,Manager-Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

As a Manager for Data Engineering team, you will be managing data platforms and implementing new technologies and tools to further enhance and enable data science/analytics, focus to drive scalable data management and governance practices. Leading the team of data engineers & solutions architects to deliver solutions to business teams.
• Driving the vision with leadership team for data platforms enrichment covering the areas like Data Warehousing/Data Lake/BI across the portfolio.
• Defining and executing on a plan to achieve that vision.
• Building a high-quality Data engineering team and continue to drive to scale up.
• Ensuring the team adheres to the standard methodologies on data engineering practices.
• Building cross-functional relationships with Data Scientists, Data Analysts and Business teams to understand data needs and deliver data for insight solution.
• Driving the design, building, and launching of new data models and data pipelines.
• Driving data quality across all data pipelines and related business areas.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You are curious and passionate about Data and highly scalable data platforms. People count on you for your expertise in data management in all phases of the software development cycle. You create environments where teams thrive and feel valued, respected and supported. You enjoy the challenge of managing resources and competing priorities in a dynamic, complex and deadline-oriented environment. Building effective working relationships with other managers across the organization comes naturally to you.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Two or more years of experience in leading the team and tracking the end-to-end deliverables.
• Experience in end-to-end delivery of Data Platform Solutions and working on large scale data transformation.
• Knowledge of Spark, Hive, Scala, Pig, Kafka, Pulsar, Nifi, Python, Shell scripting.
• Knowledge of Google Cloud Platform/BigQuery.
• Knowledge of Teradata.
• Experience in working with DevOps tools like Bitbucket, Artifactory, Jenkins.
• Knowledge of Data Governance and Data Quality.
• Experience in building / mentoring the team.

Even better if you have one or more of the following:
• Master’s degree.
• Experience in data engineering, big data, hadoop and DevOps technologies.
• Certifications in any Data Warehousing/Analytical solutioning.
• Certification in program/project management.
• Experience in technical leadership in architecture, design, implementation and support of large-scale data and analytics solutions that are highly reliable, flexible, and scalable.
• Ability to meet tight deadlines, multi-task, and prioritize workload.
• Experience in collaborating with cross-functional teams and managing stakeholder expectations.
• Experience in working with globally distributed teams.
• Good Communication and Presentation skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
FairMoney,Senior Data Engineer,"About FairMoney

FairMoney is a credit-led mobile bank for emerging markets. The company was launched in 2017, operates in Nigeria & India, and raised close to €50m from global investors like Tiger Global, DST & Flourish Ventures. The company has offices in France, Nigeria, and India.

Role and responsibilities

At FairMoney, we are making a lot of data driven decisions in real time: risk scoring, fraud detection as examples.

Our data is mainly produced by our backend services, and is being used by data science team, BI team, and management team. We are building more and more real time data driven decision making processes, as well as a self serve data analytics layer.

As a senior data engineer at FairMoney, you will help building our Data Platform:

• Ensure data quality and availability for all data consumers, mainly data science and BI teams.
• Ingest raw data into our DataWarehouse (BigQuery / Snowflake)
• Make sure data is processed and stored efficiently:
• Work with backend teams to offload data from backend storage
• Work with data scientists to build a machine learning feature store
• Spread best practices in terms of data architecture across all tech teams
• Effectively form relationships with the business in order to help with the adoption of data-driven decision-making.

You will be part of the Datatech team, sitting right between data producers and data consumers. You will help building the central nervous system of our real time data processing layer by building an ecosystem around data contracts between producers and consumers.

Our current stack is made of

• Batch processing jobs (Apache Spark in Python or Scala)
• Streaming jobs (Apache Flink deployed on Kinesis Data Analytics - Apache Beam deployed on Google Dataflow)
• REST apis (Python FastApi)

Our tool stack

• Programming language: Python, SQL
• Streaming Applications: Flink, Kafka
• Databases: MySQL, DynamoDB
• DWH: BigQuery, Snowflake
• BI: Tableau, Metabase, dbt
• ETL: Hevo, Airflow
• Production Environment: Python API deployed on Amazon EKS (Docker, Kubernetes, Flask)
• ML: Scikit-Learn, LightGBM, XGBoost, shap
• Cloud: AWS, GCP

Requirements

You will work on a daily basis with the below tools, so you need working experience on

• Languages: Python and Scala.
• Big data processing frameworks: all or one of Apache Spark (batch/streaming) - Apache Flink (streaming) - Apache Beam.
• Streaming services: Apache Kafka / AWS Kinesis.
• Managed cloud services: one of AWS EMR / AWS Kinesis Data Analytics / Google Dataflow.
• Docker.
• Building REST APIs.

Ideally, you have experience with:

• deployment/management of stateful streaming jobs.
• the Kafka ecosystem: Kafka connects mainly.
• infrastructure as code frameworks (Terraform).
• architecture around data contracts: Avro Schemas management, schema registries (Confluent Kafka / AWS Glue).
• Kubernetes.

Overall experience required for this role: 6+ Years.

Benefits

• Training & Development
• Family Leave (Maternity, Paternity)
• Paid Time Off (Vacation, Sick & Public Holidays)
• Remote Work

Recruitment Process • A screening interview with one of the members of the Talent Acquisition team for 30 minutes.
• Takeaway assignment to be done at home.
• Technical design interview for 60-90 minutes.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,True,False,True,True,True
Boston Consulting Group,IT Senior Data Engineer,"WHAT YOU'LL DO
Under the general supervision of senior management and the Data Engineering Chapter Lead in the Enterprise Data Tribe, you will be working with key customers to deliver timely and accurate data engineering pipelines in a secure manner. You are expected to provide guidance on proper engineering design ensuring that our architectural guidelines are met, and the appropriate support model is in place for production deployments. This role will work in a multi-functional agile squad and support the product owner. You will also be supporting the Chapter Lead and other team members of the Data Engineering chapter in proof-of-concept activities and other Data Engineering chapter related work.
YOU'RE GOOD AT
You have experience in data warehousing, data modelling, and the building of data engineering pipelines. You are well versed in data engineering methods, such as ETL and ELT techniques through scripting and/or tooling. You are good in analysing performance bottlenecks and providing enhancement recommendations; you have a passion for customer service and a desire to learn and grow as a professional and a technologist.
• Viewed as subject matter expert for stakeholders, possessing in-depth knowledge and specialized technical skill set
• Able to work independently with minimal supervision
• Proactively identify and independently solve non-routine problems by applying expertise
• Perform research of viable technical and/or non-technical solutions
• Develop internal network with senior leaders within the chapter and key stakeholders in the tribe.
• Develop strategies for data engineering in Snowflake using DBT and Talend.
• Architect, design, and implement data pipelines to feed data models for subsequent consumption
• Actively monitor and resolve user support issues, working closely with your assigned squad and other squads as part of the chapter.
• Develop and maintain architectural standards, best practices, and measure compliance

YOU BRING (EXPERIENCE & QUALIFICATIONS)
You bring to us experience in data engineering technologies, database development, and data model design; both in IaaS and PaaS Cloud (AWS and/or Azure) environments.
• Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
• 5-7+ years of project experience, preferably as a Data Engineer/Developer and minimum of 3 years of agile project experience is a must (preferred tool - JIRA)
• Essential: Must have exposure to technologies such as DBT, Talend and Apache airflow
• Essential: SQL is heavily focused. An ideal candidate must have hands-on experience with SQL database design
• Essential: Extremely talented in applying SCD, CDC and DQ/DV framework
• Essential: Experience in data platforms: Snowflake, Oracle, SQL Server, PostgreSQL, and MySQL
• Essential: Lead R&D efforts to find solutions for data engineering requirements not addressed by existing technology standards
• Essential: Demonstrate ability to write new code i.e., well-documented and stored in a version control system (we use GitHub & Bitbucket)
• Essential: Develop metrics that illuminate the flow of data across the organization
• Essential: Experience in data modelling and relational database design
• Preferred: Experience in AWS and Azure data platforms.
• Preferred: Experience in Qlik Compose, Fivetran and HVR
• Preferred: Strong programming/ scripting skills (Python, Powershell, etc.)

YOU'LL WORK WITH
As part of the Enterprise Data Tribe, you don t have to fit into a mould at BCG. We seek people with strong drive, relentless curiosity, desire to create their own path, ability to work collaboratively, and the passion and leadership to make an impact. You ll collaborate on challenging projects with team members from many backgrounds and disciplines, increasing your understanding of complex business problems from diverse perspectives and developing new skills and experience to help you at every stage of your career. You ll be able to experience business on a genuinely global scale and learn how to bring together people from different cultures to uncover insights that challenge the status quo. As a member of the Product Engineering Group, you will work closely with a cross functional team that is collaborative, passionate and that holds themselves to a high standard.",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
General Mills,Data Engineer,":

India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.

Job Description:

Job Overview

The Enterprise Data Development team is responsible for designing & architecting solutions to integrate & transform business data into Data Lake to deliver data layer for the Enterprise using cutting edge technologies like Big Data - Hadoop. We design solutions to meet the expanding need for more and more internal/external information to be integrated with existing sources; research, implement and leverage new technologies to deliver more actionable insights to the enterprise. We integrate solutions that combine process, technology landscapes and business information from the core enterprise data sources that form our corporate information factory to provide end to end solutions for the business.

This position will develop solutions for the Enterprise Data Lake & Data Warehouse. You will be responsible for developing data lake solutions for business intelligence and data mining.

Job Responsibilities

70% of time Create, code, and support a variety of Hadoop, ETL & SQL solutions

Experience with agile techniques or methods

Work effectively in a distributed global team environment.

Works on pipelines of moderate scope & complexity

Effective technical & business communication with good influencing skills

Analyze existing processes and user development requirements to ensure maximum efficiency

Participates in the implementation and deployment of emerging tools and processes in the big data space

Turn information into insight by consulting with architects, solution managers, and analysts to understand the business needs & deliver solutions

20% of time Support existing Data warehouses & related jobs.

Job Scheduling experience (Tidal, Airflow, Linux)

10% of time Proactive research into up to date technology or techniques for development

Should have automation mindset to embrace a Continuous Improvement mentality to streamline & eliminate waste in all processes.

Desired Profile

Education:

Minimum Degree Requirements: Bachelors

Preferred Degree Requirements: Bachelors

Preferred Major Area of Study: Engineering

Experience:

Minimum years of Hadoop experience required: 2 years

Preferred years of Data Lake/Data warehouse experience: 2-4+ years

Total Experience required : 4-5 years

Specific Job Experience or Skills Needed

Skills Level: Beginner  Intermediate Expert  Advance

HDFS, Map reduce

Beginner

Hive, Impala & Kudu

Beginner

Python

Beginner

SQL, PLSQL

Proficient

Data Warehousing Concepts

Beginner

Other Competencies:
• Demonstrate learning agility & inquisitiveness towards latest technology
• Seeks to learn new skills via experienced team members, documented processes, and formal training
• Ability to deliver projects with minimal supervision
• Delivers assigned work within given parameter of time and quality
• Self-motivated team player and should have ability to overcome challenges and achieve desired results",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Fidelity India Careers,Lead - Software Engineering - Data Engineering,"Job Description:

Job Title – Lead Data Engineer [Data CoE]

The Purpose of This Role

At Fidelity, we use data and analytics to personalize incredible customer experiences and develop solutions that help our customers live the lives they want. As part of our digital transformation, we have significant investments to create innovative big data capabilities and platforms. One of them is to build various enterprise data lakes by gathering data across Business Units. We are looking for a hands-on data engineer who can help us design and develop our next generation, cloud enabled data capabilities.

The Value You Deliver
• You will be participating in end to end development which includes design, development, testing and deployment.
• You will be working closely with Technical Lead/Architects to ensure that solutions are consistent with IT Roadmap.
• You will be participating in technical life cycle processes, which include impact analysis, design review, code review, and peer testing.
• You will be participating in hands on development of application framework code in Oracle PL-SQL, pySpark, Python, NiFi, Informatica Power Center, along with Control-M and UNIX shell scripts.
• You will be troubleshooting and fixing any issues reported on data issues and performance.
• You will be presenting the findings and outcome to Senior Leadership teams and provide insights from the data to the business.
• You will be helping business teams optimize their current tasks and increase their productivity.

The Skills that are Key to this role

Technical / Behavioral
• You must be an expert in using SQL and PLSQL on Oracle or Netezza with UNIX shell scripting skills.
• You should be having working knowledge in Hadoop, HDFS, Hive, Spark, NoSQL DBs,
• Good knowledge on Python, JavaScript, Java and Scala
• You should have experience of using AWS services like RDS, EC2, S3, EMR and IAM to move data onto cloud platform
• Experience/Knowledge on Kubernetes, Containerization and building applications in Containers
• Knowledge of Logging, Telemetry and Data Security on AWS / Azure
• Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
• Experience of query tuning and optimization in one of the RBMS (oracle or DB2)
• You should be having experience in Control-M or similar scheduling tools.
• You should have proven analytical and problem-solving skills
• You should be strong in Database and Data Warehousing concepts.
• You must be able to work independently in a globally distributed environment
• You should have clear understanding of the business needs and incorporate these into technical solutions.

The Skills that is good to have for this role
• Experience in performance tuning and optimization techniques on SQL (Oracle and Netezza) and Informatica Power Center.
• Having strong inter-personal and communication skills including written, verbal, and technology illustrations.
• Having adequate knowledge on DevOps, JIRA and Agile practices.

How Your Work Impacts the Organization

Cloud Enablement and Data Model ready for Analytics.

The Expertise we’re looking for
• 3+[SE] / 7+ [Lead] years of experience in Data Warehousing, Big data, Analytics and Machine Learning
• Graduate / Post Graduate

Location: Bangalore , Chennai

Shift timings: 11:00 am - 8:00pm

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following:

For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2
For roles based in Ireland: Contact AccommodationsIreland@fmr.com
For roles based in Germany: Contact accommodationsgermany@fmr.com

Fidelity Privacy policy

Certifications:

Company Overview

At Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. Fidelity India has been the Global Inhouse Center of Fidelity Investments since 2003 with offices in Bangalore and Chennai. For information about working at Fidelity, visit India.Fidelity.com.

Fidelity Investments is an equal opportunity employer.",Bengaluru,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
EMERSON,Data Engineer - Sustainability,"JOB DESCRIPTION AS A PROFFESSIONALYOU WILL: Work closely with key stakeholders to understand business needs and translate them into technical requirements that would feed into developing effective data analytics solutions Design and implement end-to-end data solutions in collaboration with other technical and functional teams. Review and revise existing software development lifecycle andcode standards. Work closely with the data Architect onproduct roadmaps. Work on SharePoint and Power BI tools to manage, analyse and deduce data insights. Act as a point of escalation for complex operational issues to ensure optimal performance of analytics systems. WHO YOU ARE: You anticipate customer needs and provide services that are beyond customer expectations. You understand interpersonal and group dynamics and react in an effective manner. You encourage others to learn and adopt new technologies. You show a tremendous amount of initiative in tough situations and are exceptional at spotting and seizing opportunities. You promote high visibility of shared contributions to goals. REQUIRED EDUCATION, EXPERIENCE, & SKILLS: Bachelor's degree in Computer Science/Information Technology or equivalent Must have a minimum of 6+ years of experience in a Engineering role with experiences with: SharePoint Online and Power BI Experience in Visualization and Interpreting Data in various forms Technical expertise in data modelling, data mining, and segmentation techniques Experience with building new and troubleshooting existing data pipelines using Experience with batch and real-time data ingestion and processing frameworks Experience with languages such asPython andJava Knowledge of additional cloud-based analytics solutions Hands-on experience working on Linux and Windows systems Using Agile development methods Ability to work in a large, global corporate structure Ability to lead, manage and deliver large scale projects Advanced English level Demonstrated ability to clearly isolate and define problems, effectively evaluate alternative solutions, and make decisions in a timely manner Good decision-making ability, ability to operate in ambiguous situations, and high analytical ability to judge pros/cons of approaches against objectives PREFERRED EDUCATION, EXPERIENCE, & SKILLS: Expert level knowledge of data analytics and warehousing frameworks, including Snowflake and Cloud-based data integration solutions Experience with DevOps andCI/CD development practices Advanced level of software development knowledge",Chandigarh,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 24 2023

Come join us as we supercharge GSK’s data capability!

At GSK we are building a best-in-class data and prediction powered team that is ambitious for patients.

Scientific Digital and Tech’s goal is to power the discovery, development and supply of medicines and vaccines to patients. This means new tools to discover new medicines and vaccines, predictive capability for pre-clinical research, accelerated CMC and supply chain and an improved day-to-day laboratory experience for our scientists. Our Digital & Tech solutions will automate workflows and speed up decisions; freeing hands and releasing minds to focus on science.

As R&D enters a new era of data driven science, we are building a data engineering capability to ensure we have high quality data captured with context and aligned data models, so that the data is useable and reusable for a variety of use cases.

GSK R&D and Digital and Tech’s collective goal is to deliver business impact, including the acceleration of the discovery and development of medicines and vaccines to patients. The R&D Digital and Tech remit has expanded over the past 2 years, and to position GSK for the future, The change will strengthen R&D Tech, to provide more strategic impact, focus, accountability, and improved decision making in the use of Digital, Data and Analytics (DDA) to strengthen the pipeline.

Job Purpose

This role contributes to the construction of the development data fabric and data strategy. This role will interact with architects, engineers, data modelers, product owners as well as other team members in Clinical Solutions and R&D. This role will actively participate in creating technical solutions, designs, implementations & participate in the relentless improvement of R&D Tech systems in alignment with agile and DevOps principles.

The Data Engineer demonstrates both depth and breadth across key data engineering competencies e.g. Software Development, Testing, DevOps, Data Science/Analytics, and cloud. Can collaborate with experts from other subject domains. Primary responsibilities include using Azure cloud services and GSK data platform tools to ingest, egress, and transform data from multiple sources.

In addition, the role will demonstrate core engineering knowledge/experience of industry technologies, practices, and frameworks such as data fabric and scaling data platforms, containerization, cloud-based platforms, data analytics, machine learning, and data streaming. Examples of technologies include Java/C#/Python, Denodo, GIT, Azure Devops, Data Bricks, Presto, Spark, Azure Data Factory, ADLS V2, Kafka, Selenium, JUnit/NUnit, SAFe, Kanban, Docker, AI/ML, Azure/GCP Cloud Architecture including networking principles and scaling applications.

The Data Engineer, Clinical Solutions role is a senior technical role and will provide you the opportunity to lead key activities to progress your career. These responsibilities include the following:
• Working with other teams that are defining devops and data platform practices to meet the requirements of clinical solutions.
• Supporting engineering teams in the adoption and creation of data fabric best practices.
• Conducting PoCs of new technologies and helping to embed them in product teams
• Being part of a cutting-edge team creating the Development Data Fabric
• Ensures that technical delivery is fully compliant with GSK Security, Quality and Regulatory standards
• Ensures use of relevant R&D Tech / central services and collaborating with service partners in identification and delivery of service improvements
• Maintains best practices for engineering and architecture on our Confluence site. This requires hands on experience with cutting edge technology.
• Pro-actively engages in experimentation and innovation to drive relentless improvement
• Provides leadership, technical direction and GSK expertise to architecture and engineering teams composed of GSK FTEs, strategic partners and software vendors.

Why you?

Basic Qualifications:

Are you ready to work in an environment where you are continuously expected to work on projects with new technology and expected to use this technology to deliver real business value?

We are looking for professionals with these required skills to achieve our goals:
• Total 15+ years of experience and proficient with at least 3 of the below skills and can demonstrate knowledge and value with relevant experience in all the following competencies:
• Must have experience in Spark, Python and Databricks
• Software development, architecture design & technology platforms/frameworks
• Data Platforms and Domain-driven design
• Agile, DevOps & Automation [of testing, build, deployment, CI/CD, etc.]
• Data science (e.g. AI/ML), data analytics & data quality/integrity
• Testing strategies & frameworks
• Role requires:
• Demonstrated skill in delivering high-quality engineered data products
• Knowledge of industry standards and technology platforms aligned to GSK and R&D roadmaps
• Excellent communication, negotiation, influencing and stakeholder management skills
• Customer focus and excellent problem-solving skills
• Computer Science or related bachelor’s degree – MS in Computer Science is preferred
• Familiarity and use of various open-source ecosystems including JavaScript, Bigdata, java, python etc.
• Good understanding of various software paradigms: domain-driven, procedural, data-driven, object-oriented, functional
• Familiar with .Net Core (C#), Java, Python
• Demonstrable knowledge depth in more than one area of software engineering and technology

Preferred Qualifications:

If you have the following characteristics, it would be a plus:
• Experience in agile software development and DevOps, relevant technology platforms [e.g., Kubernetes] and frameworks [e.g. Docker] including cloud technologies & data structures (i.e. information management), data models or relational database design
• Subject matter expertise in clinical development
• R&D Tech requires Engineers with understanding of the relevant technical and scientific domains. Able to deliver continuous change to meet rapidly evolving R&D strategy and ambition.
• Experience with agile development methods, with security strategies and best practices, data integration mechanisms, architectural design tools, delivering and integrating COTS applications, areas of Service Oriented Architecture (SOA), Application Integration, Business Process Management and Data Quality.
• Experience in applying AI/ML, data curation, virtualization, predictive modelling, workflow, and advanced visualization techniques to enable decision support across multiple products and assets to drive results across R&D business operations.

At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.
#LI-GSK

GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in “gsk.com”, you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,True,False,False,True,False,False,True,True,False,False,False,False,False,False,False,False
Bloom Consulting Services,Data Engineer,"Data Engineer ( Job ID : 815310498 )

data engineer

NA

Contract

Experience

06.0 - 08.0 years

Offered Salary

10.00 - 14.00

Notice Period

Not Disclosed

Job Description

Total Experience6 to 8 years

Min Relevant Experience: 3 to 5 years

Location :Bangalore

JD: Data Engineer

Role Description:

In this role, you will be part of a growing, global team of data engineers, who collaborate in DevOps mode, in order to enable business with state-of-the-art technology to leverage data as an asset and to take better informed decisions.

The Life Science Data Engineering Team is responsible for designing, developing, testing, and supporting automated end-to-end data pipelines and applications on Life Science’s data management and analytics platform (Palantir Foundry, Hadoop and other components).

The Foundry platform comprises multiple different technology stacks, which are hosted on Amazon Web Services (AWS) infrastructure or own data centers. Developing pipelines and applications on Foundry requires:
• Proficiency in SQL / Java / Python (Python required; all 3 not necessary)
• Proficiency in PySpark for distributed computation
• Familiarity with Postgres and ElasticSearch
• Familiarity with HTML, CSS, and JavaScript and basic design/visual competency
• Familiarity with common databases (e.g. JDBC, mySQL, Microsoft SQL). Not all types required

This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.

Roles & Responsibilities:
• Develop data pipelines by ingesting various data sources – structured and un-structured – into Palantir Foundry
• Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application
• Acts as business analyst for developing requirements for Foundry pipelines
• Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline
• Document technical work in a professional and transparent way. Create high quality technical documentation
• Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)
• Deploy applications on Foundry platform infrastructure with clearly defined checks
• Implementation of changes and bug fixes via change management framework and according to system engineering practices (additional training will be provided)
• DevOps project setup following Agile principles (e.g. Scrum)
• Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java
• Work closely with business users, data scientists/analysts to design physical data models

Education
• Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields

Professional Experience
• 5+ years of experience in system engineering or software development
• 3+ years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.

Required Knowledge, Skills, and Abilities

Data engineer",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"• Experience with Azure Data Bricks, Data Factory
• Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics
• Experience in Python/Pyspark/Scala/Hive Programming.
• Experience with Azure Databricks/ADB
• Good understanding of SQL queries, joins, stored procedures, relational schemas
• Experience with NoSQL databases, such as HBase, Cassandra, MongoDB",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Genpact,Data Engineer,"With a startup spirit and 90,000+ curious and courageous minds, we have the expertise to go deep with the world's biggest brands--and we have fun doing it! We dream in digital, dare in reality, and reinvent the ways companies work to make an impact far bigger than just our bottom line. We're harnessing the power of technology and humanity to create meaningful transformation that moves us forward in our pursuit of a world that works better for people.

Now, we're calling upon the thinkers and doers, those with a natural curiosity and a hunger to keep learning, keep growing. People who thrive on fearlessly experimenting, seizing opportunities, and pushing boundaries to turn our vision into reality. And as you help us create a better world, we will help you build your own intellectual firepower.

Welcome to the relentless pursuit of better.

In this role, resource will be expert in designing, building and maintaining data infrastructure. Work will help people with unmet medical needs, including those who wish to quit smoking, those with major depression disorder, and those with schizophrenia--ultimately improving lives through engineering. Help design and build a data infrastructure using state-of-the-art technologies with data security at utmost importance and employ elegant solutions to help ensure Client's data products meet compliance needs (e.g., GDPR and HIPAA) in different regions of the world.

Responsibilities!
• Design, build and maintain analytical data infrastructure which includes both data processing and data reporting.
• Onboarding data from both internal and external systems.
• Collaborate with Product, Engineering, Science, Data analysts and Data scientists to implement rich and re-usable datasets/metrics.
• To make data infrastructure and applications scalable, reliable, and secure.
• Strong attitude towards automating routine tasks via coding/scripting.
• Research on security and privacy requirements and provide solutions.

Qualifications we seek in you!
• B Tech/M Tech/BCA/MCA
• Experience in building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience writing complex, highly optimized SQL queries.
• Experience with reporting to enable explanatory and exploratory analytics.
• Python development experience.
• Have experience with dbt, Airflow, Snowflake and AWS infrastructure.
• Have experience implementing APIs to share data with internal / external vendors.
• Experience implementing streams.
• Understanding of privacy and security regulations (e.g., GDPR, HiTrust, HIPA)",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
Vanderlande Careers,Lead Data Engineer,"Lead Data Engineer at DSF

Vanderlande provides baggage handling systems for 600 airports around the globe, capable of moving over 4 billion pieces of baggage around the world per year. For the parcel market our systems handle 52 million parcels per day. All these systems generate data. Do you see a challenge in building data-driven services for our customers using that data? Do you want to contribute to the fast growing Vanderlande Technology Department on its journey to become more data driven? If so, then join our Digital Service Factory team!

Your Position

As a lead data engineer you will be leading the data engineering efforts in a product team. You will work together with product/solution architecture to provide technical necessities to design and develop end-to-end data ingestion pipelines and well tested and monitored data services. You will assess the technical dependency between different functional components and define a resolution. You will also provide technical guidance and coaching to the junior/medior data engineers in the team, set technical standards and best practices.

Your responsibilities:
• You will be designing, developing, testing, and documenting the data collection framework. The data collection consists of (complex) data pipelines with data from (IoT) sensors and low/high level control components to our Digital Service and Data Science platform.
• You will build monitoring solutions for data pipelines which enable data quality improvement.
• You will develop scalable data pipelines to transform and aggregate data for business use, following software engineering and Data Mesh best practices. For these data pipelines you will make use of the best and most applicable frameworks available for data processing.
• You develop our data services and data products for customer sites towards a product, using (test & deployment) automation, componentization, templates, and standardization to reduce delivery time of our projects for customers. The product provides insights in the performance of our material handling systems at customers all around the globe.
• You design and build a CI/CD pipeline, including (integration) test automation for data pipelines. In this process you strive for an ever-increasing degree of automation and high levels of security.
• You will work with infrastructure engineers to extend storage capabilities and types of data collection (e.g. streaming)
• You have experience in developing APIs.
• You will coach and train the junior data engineer with the state of art big data technologies.
• You will lead the Data Engineering Guild where passionate members discuss current trends, short term development, and solutions for ongoing issues that span multiple teams.

Your Profile
• Total experience of 10+ years (with at least 7+ years of programming exp)
• Experience programming in Python and/or Scala (Java programming exp is a plus)
• You are familiar with DevOps practices and have relevant experience in automation (CI/CD), measurement, applying lean practices and what DevOps culture entails
• You know how to achieve high performing secure pipelines, maintain and test them
• You are familiar with different storage formats (e.g. Azure Blob, SQL, noSQL)​
• Experience with scalable data processing frameworks (e.g. Spark)​
• Experience with event processing tools like Splunk or the ELK stack​
• Deploying services as containers (e.g. Docker and Kubernetes)​
• You have experience with streaming data platforms (e.g. Kafka )​ and messaging formats (e.g. Apache AVRO)
• Strong experience with cloud services (preferably with Azure)

Diversity & Inclusion

Vanderlande is an equal opportunity employer. Qualified applicants will be considered without regards to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Pune,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Visa,Sr. Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.
Job Description

This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. You will be an integral part of the Payment Products Development team focusing on design and development of software solutions that leverage data to solve business problems. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and testing of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Responsible for the design, development, and implementation
• Work on development of new products iteratively by building quick POCs and converting ideas into real products
• Design and develop mission-critical systems, delivering high-availability and performance
• Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology
• Develop code to ensure deliverables are on time, within budget, and with good code quality
• Have a passion for delivering zero defect code and be responsible for ensuring the team's deliverables meet or exceed the prescribed defect SLA
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
• Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.
Qualifications

We are seeking team members that are passionate, visionary and insatiably inquisitive. Successful candidates frequently have a mix of the following qualifications:

• Bachelor’s Degree or an Advanced Degree (e.g. Masters) in Computer Science/ Engineering, Information Science or a related discipline
• Minimum of 3 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies
• Extensive experience with SQL and Big Data technologies (Hadoop, Java, Spark, Kafka, Hive, Python) for large scale data processing and data transformation
• Deep knowledge of Unix/Linux
• Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
• Familiar with software design patterns
• Experience working in an Agile and Test-Driven Development environment
• Strong knowledge of API development is highly desired
• Strategic thinker and good business acumen to orient data engineering to the business needs of internal and external clients
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
• Previous exposure to financial services is a plus, but not required
Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
Shell,"Senior Data Engineer- Azure (ADF, Data lake)","Join the number One Global Lubricants supplier in the world and be part of the team that helps in shaping up the digital and the IDT strategy which delights our customers in over 100 countries across every sector.

If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

Where you fit

Shell's Projects and Technology (P&T) business exists to make the delivery of our strategies and the growth of our company possible. Our team develops the advanced products and technologies Shell needs to meet customer demand. Our solutions help our partners grow the LNG, Gas and Power businesses, deepen the integration of Manufacturing, Chemicals and Trading, and maximise the competitiveness of our Upstream business.

What's the role?

As a Data Engineer in Shell, you will create and maintain optimal data pipeline architecture and also will a ssemble large, complex data sets that meet functional / non-functional business requirements.

You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

More specifically, your role will include:
• Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.
• Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

What we need from you

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree and at least have a Seniority level in their previous workplace.

They should also have experience using the following software/tools:
• Experience with Azure: ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.
• Experience with relational SQL/NoSQL databases, file handlings and API integrations
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODS

Skills - Nice to Have
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing data pipelines using ADF
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of transforming, processing and extracting value from large disconnected datasets
• Strong team player with organizational and communication skills
• Experience supporting and working with cross-functional teams in a dynamic environment",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fibe India,Data Engineer - SQL,"Responsibilities:
• The candidate is expected to lead one of the key analytics areas end-to-end. This is a pure hands-on role.
• Ensure the solutions are built to meet the required best practices and coding standards.
• Ability to adapt to any new technology if the situation demands.
• Requirement gathering with business and getting this prioritized in the sprint cycle.
• Should be able to take end-to-end responsibility for the assigned task
• Ensure quality and timely delivery.

Requirements:
• Experience: 3- 6 years.
• Strong at PySpark, Python, and Java fundamentals
• Good understanding of Data Structure
• Good at SQL query/optimization
• Strong fundamental of OOPs programming
• Good understanding of AWS Cloud, Big Data.
• Nice to have Data Lake, AWS Glue, Athena, S3 Kinesis, SQL/NoSQL DB",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer,"Role: Data Engineer Job Description
• Design, build, and maintain distributed batch and real-time data pipelines and data models.
• Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
• Be curious and eager to work across a variety of engineering specialties (i.e., Data Science, and Machine Learning to name a few).
• Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
• Enforce privacy and security standards by design.
• Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Qualifications
• +3 years experience building complex data pipelines and working with both technical and business stakeholders.
• Experience in at least one primary language (e.g., Java, Scala, Python) and SQL (any variant).
• Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
• Experience creating and maintaining ETL processes.
• Experience designing, building, and operating a DataLake or Data Warehouse.
• Experience with DBMS and SQL tuning.
• Strong fundamentals in big data and machine learning.

Preferred Qualifications
• Experience with RESTful APIs, Pub/Sub Systems, or Database Clients.
• Experience with analytics and defining metrics.
• Experience with measuring data quality.
• Experience productionalizing a machine learning workflow; MLOps
• Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
• Language ability in Japanese and English is a plus (We have a professional translator but it is nice to have language skills).
• Experience with AWS services.
• Experience with microservices.
• Knowledge of Data Security and Privacy.

experience

6",Hyderabad,True,False,True,True,False,False,False,False,False,False,False,False,True,True,False,False
deloitte,Consulting - BO - Cloud Engineering - Manger - Azure Data Engineer,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organisations, Deloitte is where you will find numerous opportunities to succeed and realise your full potential.

The team

Deloitte is working with global customers on cloud technologies to help unlock growth, stability, and sustainability by enabling them to spot unseen business trends through curation, transformation, and blending of data. In our endeavors for continued expansion, we’re searching for like-minded individuals to help us ‘take it to the next level’.

In this exciting opportunity for an experienced developer, you will join a team delivering a transformative cloud hosted data platform for some of the world’s biggest organizations. The candidate we seek, needs to have a proven track record in implementing data ingestion and transformation pipelines on Microsoft Azure. Deep technical skills and experience with working on Azure Databricks. Familiarity with data modelling concepts and exposure to Synapse.

You will also be required to participate in stakeholder management, highlight risks, propose deliver plans and estimate for time and team size based on requirements. Hence, adequate levels of communication skills and relevant experience in handling such situations is desired.

Scope of work

Your main responsibilities will be:
• Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
• Delivering and presenting proofs of concept of key technology components to project stakeholders.
• Developing scalable and re-usable frameworks for ingesting and enriching datasets
• Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
• Working with event based / streaming technologies to ingest and process data
• Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
• Evaluating the performance and applicability of multiple tools against customer requirements
• Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.

Qualifications
• Strong knowledge of Data Management principles
• 9+ years of total years of experience
• Experience in building ETL / data warehouse transformation processes
• Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably Databricks).
• Experience using Apache Spark and associated design and development patterns
• Microsoft Azure Big Data Architecture certification is an advantage.
• Hands-on experience designing and delivering solutions using Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
• Experience with Apache Kafka / Nifi for use with streaming data / event-based data (Nice to have but not mandatory)
• Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)
• Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
• Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Terraform etc.

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters.

In addition to living our purpose, managers across our organisation:
• Develop self by actively seeking opportunities for growth, share knowledge and experiences with others, and act as a strong brand ambassadors
• Understand objectives for clients and Deloitte, align own work to objectives and set personal priorities
• Seek opportunities to challenge self
• Collaborate with others across businesses and borders to deliver and take accountability for own and team results
• Identify and embrace our purpose and values and put these into practice in their professional life
• Build relationships and communicate effectively in order to positively influence peers and other stakeholders

Professional growth

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn.From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Our Purpose

Deloitte is led by a purpose: To make an impact that matters.

Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and work—always striving to be an organisation that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description

insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.

Job Description
• Develops and maintains scalable data pipelines for bulk data movement between systems of record and systems of reference
• Develops and maintains scalable application to application integrations
• Aligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
• Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes
• Writes appropriate unit or integration tests to implement test-driven development
• Continually contributes to and enhances data team documentation
• Performs data analysis required to troubleshoot and resolve data related issues
• Works closely with a team of frontend and backend engineers, product managers, and analysts
• Defines company data assets, artifacts and data models

Qualifications

Required qualifications:
• 5 years of Data Engineering and Data Integration
• 5 Years of Data Warehousing
• 3 Years of Data Architecture and Modeling
• 2 years of Cloud Data Engineering
• Agile Methodologies

Preferred skills:
• AWS or Azure Data Certifications
• Experience with databricks, spark, python
• Experience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)
• Experience with Salesforce

Additional Information

All your information will be kept confidential according to EEO guidelines.
• * At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. **

insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Northern Tool + Equipment, India",Senior Data Engineer,"Are you an individual who wants to play a game changing role and make an impact in a fast-growing organization? We at Northern are waiting for you. Join us and unleash your potential!!

We are hiring <>!!

Join the core group of founding members at the NTE India to build an organization from the ground up.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Role Objective

PRIMARY OBJECTIVE OF POSITION:

We are looking for an Experienced Data Engineer who will partner with a specific business function and understand the requirements, builds data model, creates data pipelines and stored procedures. Also work with Data Analysts/Modelers, Data Visualization Engineers to deliver high performing analytics.

MAJOR AREAS OF ACCOUNTABILITY:
• SME for data structures and data models for specific line of business.
• Analyze and understand various source systems and related data structures.
• Build and automate creation of ETL pipelines and stored procedures to move data from source system to consumption layer using variety of ETL methods.
• Collaborate with Data Analysts/Data Architect/Data Visualization Engineers to provide them with Data mapping documents and ensure adherence to a common data model.
• Responsible for administration and security of data and analytics assets in Azure.
• Works collaboratively and effectively communicates with others across departments in order to perform and complete necessary tasks and projects.
• Follows established Software Development Life Cycle (SDLC) to enable CI/CD in relevant areas.
• Follow established change control, release management and incident management processes.
• Responsible for performance and tuning, scaling of Azure resources to optimize costs
• Builds and maintains relationships cross-functionally in order to stay current with the needs and operations of the business functional areas supported.
• Supports the day-to-day operation of the reporting and analytic solutions by troubleshooting ETL and other errors encountered during data processing.
• Keeps manager informed of important developments, potential problems, and related information necessary for effective management. Coordinates and communicates plans and activities with others, as appropriate to ensure a coordinated work effort and team approach.

Job Description

Performs related work as apparent or assigned.

QUALIFICATIONS:
• To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
• Bachelor’s Degree in Computer Science, Statistics, Mathematics, Business or related field.
• At least 6 years relevant work experience in Data and Analytics field.
• In-depth understanding of Data warehousing concepts.
• Hands-on experience in writing complex, highly optimized SQL queries across large data sets
• Hands-on experience in building performance optimized data pipelines (ETL/ELT)
• Experience in configuring, deploying, and provisioning of IaaS, PaaS with Terraform and PowerShell using Azure DevOps and GIT.
• Specific familiarity with the Microsoft Azure Data Stack - ADF, Azure SQL DB, Azure Synapse (SQL Data Warehouse), Azure Data Lake, Azure Storage and Analysis Services.
• Azure Security & Identity: Azure Active Directory App Permissions, Key Vaults.
• Hands-on experience in creating user groups, creating security policy and implementation of Row-Level Security(RLS) to restrict the data access to the users.
• Hands-on experience with data cataloging and data profiling concepts.
• Diversity of perspective for various tools and technologies like Azure Stream Analytics, Azure Databricks, NoSQL databases, read or write optimized databases to advocate for their appropriate adoption at Northern Tool.
• Basic programming experience using .NET, Python, or any scripting language.
• Must be willing to work as a team and possess the skills to work independently.
• Demonstrated ability to take initiative and utilize creativity on assigned projects.
• Must possess strong analytical, problem-solving, and technical design skills.
• Demonstrates Northern Tool + Equipment’s 12 Core Competencies.
• Sounds interesting? Here’s your chance to join our family at Northern.

About the Company

Northern Tool + Equipment is a retailer and manufacturer that specializes in offering superior quality tools at great prices, along with the knowledge and support needed to help customers get the job done right.

They’ve been in business for over 40 years, recently reaching revenues over $1.5 billion. The company not only supplies over 100,000 tools from the top brands in the industry but also designs, manufactures, and tests an extensive lineup of premium private label products that customers can’t get anywhere else.

Northern Tool’s far-reaching customer base includes handy men and women, weekend hobbyists, serious do-it-yourselfers, full-fledged contractors, trade professionals, and more. The company’s products can be found in over 120 retail stores in the USA, on its comprehensive international website, and via numerous catalogs throughout the year. Recently Northern Tool has expanded operations to offices in India to serve its global distribution better.

We are recently named as one of the Top Workplaces for MidSize Employers by Forbes in the US.

We have also been recognized as the “Top GCC to work for in AI and analytics” and our India HR team as the “Top HR Professionals in AI and Analytics” by 3AI which is a professional firm associated with analytics within India.

About NTE India

Northern Tool is making a significant investment in business transformation. We are committed to providing our customers with an exceptional experience. The team in India will enable Northern Tool to expand its internal capabilities in Finance, Merchandising, Product Engineers, Manufacturing Ops, Marketing, Contact Center, and Information Technology.

Why Northern?

True Northern: We know that our strength is our people. The distinct abilities they bring into the system are the key to our success. We seek talented people who wish to share their initiative, ideas, and expertise; we develop and support our teams, and we put them in a position to succeed. We know our customer; we provide value, and we act with integrity. We are True Northern.

Build Lasting Relationships: At Northern Tool + Equipment, we’re far more interested in building relationships than we are in simply making transactions. Our purpose is building a long-lasting relation with our customers and employees.

We care for our customers, employees and society. Our customer base is exceptionally loyal because customers know that we will give them the right solution.

Accelerate Decision Making: by collaborating with the brightest minds, bring ideas to life across our value chain of business operations across our vast network of over 120 stores across the US.

Lead with Innovation: Join us to elevate our customer experience?with cutting-edge products, technology, and business processes and?drive our business forward.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Does this sound interesting?? Be an early applicant!!

Northern Tool is an Equal Opportunity Employer. We encourage and empower everyone and support diversity in experience, and point of view. We are pledged to a fair and a transparent hiring process with no discrimination of race, color, ancestry, religion, gender, national origin, age, citizenship, marital status, disability, or veteran status.

Requirements
• name : Northern Tool + Equipment, India
• location : Hyderabad, IN
• experience : 6 - 9 years
• employmentType : Full-Time
• Primary Skills: ETL or ELT,Python,Data Warehousing,Azure Data Lakes or Data Factory,SQL",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Comcast India Engineering Center I, LLP",Data Engineer 3,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary About Sky We’re Sky, Europe’s biggest entertainment brand. Think top-quality shows. Breaking news. Innovative tech. Must-have products. Careers here mean the freedom and support you need to make an impact – pushing boundaries, creating solutions, hitting targets. And as part of our close-knit team, you’ll enjoy plenty of benefits. Plus, experiences you’ll only find at Sky. We love telling the world we work at Comcast . We’re fans too. We move fast and embrace pace. We have the freedom to be brilliant. And we work collaboratively because together we can. This is how we work at Comcast and why we love it. Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Documents all development activity. Assists with training non-technical personnel. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as a resource for colleagues with less experience. Job Description Core Responsibilities Create and maintain an optimal data pipeline architecture focussed upon network data, including real-time and batch data sources. Assemble large, complex data sets that meet functional and non-functional business requirements. Build batch/streaming ELT/ETL solutions from a wide variety of data sources in varying formats (SQL, JSON, AVRO, HTTP, API, etc.) using the right blend of tools. Keep our data compliant, relevant and secured across multiple data centres and regions. Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery and evolving current solutions whilst ensuring continuity of service. Create data tools for data scientist team members that assist them in building and optimizing into an innovative industry leader. Guide and collaborate with data consumers on analytics, tooling and platform related queries. Employees at all levels are expected to: Graduate degree BSc in Computer Science, Electrical Engineering or similar. Strong communications skills. SQL knowledge and experience working with relational databases. Strong analytic skills related to working with structured and unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Hands-on experience in building scalable data platforms. Awareness of security practices and privacy concerns when working with data across both in-house and cloud platforms. Ideally an awareness of network technologies and concepts or an insatiable desire to learn. Key technologies Apache Airflow / NiFi / Kafka / ZooKeeper Confluent ecosystem (Connect / Schema Registry / ksqlDB) GCP (BigQuery, Dataflow, Pub/Sub, IAM) Linux / Terraform / Ansible Python / Docker (Nice to have). Experience: 5 Years to 7.5 Years Location: Chennai Disclaimer: This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. Comcast is an EOE/Veterans/Disabled/LGBT employer. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools that are personalized to meet the needs of your reality—to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the benefits summary on our careers site for more details. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Certifications (if applicable) Relative Work Experience 5-7 Years Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. At Comcast , you have the power to connect the world. Your career options are endless as you grow in your career. Explore your future with access to a variety of teams, locations, and resources in an expanding network. You can also explore additional opportunities at our company, NBCUniversal.",,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
Revolo Infotech,Data Engineer - SQL/Python,"Job Description :

- Design, develop, and maintain data pipelines and architecture for data storage, processing, and analysis

- Work with cross-functional teams to understand and implement data requirements

- Build and optimize data pipelines using various cloud-based technologies such as AWS, Azure, or Google Cloud Implement data visualization solutions using cloud-based tools such as Tableau, Power BI, or Looker Monitor and troubleshoot data pipeline issues, and implement solutions to improve performance and scalability

- Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis

- Stay up-to-date with the latest technologies and industry trends in data engineering and data visualization

Requirements :

- 2+ years of experience as a data engineer with a focus on cloud-based data pipelines and visualization

- Strong experience with cloud-based technologies such as AWS, Azure, or Google Cloud

- Experience with data visualization tools such as Tableau, Power BI, or Looker

- Strong knowledge of SQL and programming languages such as Python or Java

- Familiarity with big data technologies such as Hadoop, Spark, or Hive

- Strong problem-solving and analytical skills

- Experience working in an Agile development environment Bachelor's degree in Computer Science or related field.

Preferred Qualifications :

- Experience with data warehousing concepts and technologies

- Experience with data governance and data management best practices.

- Experience with machine learning and AI technologies Strong communication and teamwork skills.

Job Types : Full-time, Regular / Permanent, Contractual / Temporary

Salary : 1,000,000.00 - 1,200,000.00 per year

Benefits :

- Health insurance

- Internet reimbursement

- Paid sick time

- Paid time off

Schedule :

- Day shift

- Monday to Friday

Power BI: 2 years (Preferred)

Tableau: 2 years (Preferred)

AWS: 2 years (Preferred)
(ref:hirist.com)",Navi Mumbai,True,False,True,True,False,False,False,False,True,True,False,False,False,False,False,False
MediaMath,Data Engineer,"About Us

MediaMath is the leading technology pioneer on a mission to make advertising better. We deliver outstanding results through powerful ad tech, partnership and a curiosity for what’s next. We help more than 3,500 advertisers solve complex marketing problems so they can deepen their customer relationships across screens and around the world.

Key Responsibilities

MediaMath’s Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by Analytics to discover and deliver insights that drive value for our clients. The Analytics team fulfils customers’ advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. As the Data Engineer on the Analytics Engineering team within the Analytics team, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:
• Become an expert in MediaMath data flows and the Analytics data infrastructure.
• Build, maintain, and own scalable data pipelines to support client data integration.
• Become a team SME in data munging and automated ETL processes.
• Work with Analysts to understand and leverage big data to solve client problems and needs.
• Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
• Be a team player, and bring the team and company forward by solving team and company priorities.

You are:
• Experienced in writing readable, re-usable code SQL and Python (our entire team uses Jupyter Notebook and Pandas!)
• Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
• Experienced in implementing data pipeline health monitoring, alerting
• Experienced with data infrastructure troubleshooting and working with system logs
• Experienced developing data flow schematics/blueprints
• Advocate for automation and building efficient, scalable solutions
• Self-driven, with a hunger to learn and spread knowledge by teaching others
• Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:
• Bachelor’s Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
• 3 - 5 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
• Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
• Experience partnering with client-facing teams to understand client needs and translate them to technical requirements

Nice-to-have’s:
• Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)
• Experience working with REST APIs, web services, object-oriented technologies like Java, C++
• Public GitHub repos or notebooks that illustrate the way you think about data
• Exposure to ad-tech, digital marketing, or e-commerce industries

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are three values that provide a framework for how we approach our work and the world: Win Together, Obsess Over Growth, and Do Good, Better. These values inform how we energize one another and engage with our clients. They get us amped to come to work.

Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath is recognized as a Leader in the Gartner 2020 Magic Quadrant for Ad Tech and has won Best Account Support by a Technology Company for two years in a row in the AdExchanger Awards.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristic as established by law.

MediaMath focuses on Digital Media, Internet, Advertising, Software, and Marketing. Their company has offices in New York City, San Francisco, Chicago, Durham, and Singapore. They have a large team that's between 501-1000 employees. To date, MediaMath has raised $617.877M of funding; their latest round was closed on July 2018.

You can view their website at http://www.mediamath.com or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,False
"Atlassian, Inc.",Senior Data Engineer,"Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.",,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
Motilal oswal,Data Engineer,"Job Description : Strong AWS Data Engineering skills. Exposure to SSIS, SSRS, SSAS will be an advantage,Handson experience working with S3, Redshift, Glue, EMR, RDS, Athena, Aurora,Strong development skills and experience coding with SQL, Pyspark, Python,High on ownership and accountability,Comfortable with change, initial hiccups and small failures,Experience with understanding designs, creating low level designs, unit test cases, unit testing and assisting with Integration and User acceptance testing,Experience of 2-6yrs with AWS Data technologies.",,True,False,True,False,False,False,False,False,False,False,True,False,True,False,False,False
Fisker Inc.,Data Engineer,"Responsibilities
• Work with leaders, engineering and data scientists to understand data needs.
• Design, build and launch efficient and reliable data pipelines to best utilize connected vehicle data for real-time systems and within data warehouses.
• Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
• Help insure that best practices are followed when storing, retrieving and accessing data.

Qualifications
• 3+ years of Python development experience.
• 3+ years of SQL experience.
• 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
• 3+ years experience with Data Modeling.
• Experience in organizing queries, tables and pipelines with proper indexing, partition and sharding.
• 3+ years experience in custom ETL design, implementation and maintenance.
• Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Clickhouse, Spark, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience with more than one coding language, ideally Go or C++ and java.
• Experience with designing and implementing real-time pipelines.
• Experience with data quality and validation.
• Experience with SQL performance tuning and E2E process optimization.
• Experience with notebook-based Data Science workflow.
• Experience with Airflow.
• Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,True,True,False
Poshmark,"Software Developer, Data Engineering","The Big Data team is a central player in the Poshmark organization. Our mission is to build a world-class big data platform to bring value out of data for us and for our customers. Our goal is to democratize data, support exploding business, provide reporting and analytics self-service tools, and fuel existing and new business critical initiatives.

The Data Engineering team at Poshmark is looking for an experienced software engineer to take care of Poshmak’s growth data, ensuring real-time access to quality data for all the stakeholders. The role requires strong understanding of software engineering best practices and excellent software development skills to build and maintain real-time and batch data pipelines with a focus on scalability and optimizations. In addition, the role also requires collaborating with Data Science, Analytics and other Engineering teams to build newer ETLs analyzing terabytes of data.

The role also requires being able to write clean and scalable code to pull datasets from disparate sources involving External APIs, S3 transfers, Web Scraping. You will work with cutting edge technologies and frameworks like Scala, Ruby, Apache Spark, Airflow, Redshift, Databricks, Docker. You will also manage the growth data infrastructure comprising ETL pipelines, Hive tables, Redshift tables, BI tools. We are looking for a software engineer who can help us define the next phase of growth data systems in terms of scalability and stability.

Responsibilities
• Design, Develop & Maintain growth data pipelines and integrate paid media sources like Facebook and Google to drive insights for business.
• Build highly scalable, available, fault-tolerant data processing systems using AWS technologies, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 100s of terabytes of data ingested every day and a petabyte-sized data warehouse.
• Responsible for architecting/designing/developing critical data pipelines at Poshmark.
• Productionizing ML models in collaboration with the Data Science and Engineering teams.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.
• Participate and contribute to constantly improving best practices in development.

Desired Skills & Experience
• Excellent technical problem solving using data structures and algorithms, with emphasis on optimization and code quality.
• 1-3 years of relevant software engineering experience using object oriented programming languages like Scala / Java / Ruby / Python / C++ etc.
• Expertise in architecting and building large-scale data processing systems using Big Data technologies like Spark, Hadoop, EMR, Kafka/ Kinesis, Flink, Druid.
• Expertise in SQL with knowledge on any existing data warehouse technology like Redshift
• Expertise in Google Apps Script, Databricks or API Integrations is a plus.
• Be self-driven, take complete ownership of initiatives, make pragmatic technical decisions and collaborate with cross-functional teams.",Chennai,True,False,True,True,True,True,False,True,False,False,False,True,True,False,True,False
Confidential,Data Engineer - AWS/ETL,"Role and responsibilities :- The Data Engineer will be responsible for leading design, development, transformation, deployment, and maintenance of Data Warehousing stack on AWS- Work with BI and dev team to build data pipelines using AWS Glue and similar tools.- Develop custom data ingestion jobs and ETL scripts using Python/Spark scripts- Perform data modelling and schema design activities in Data Lake and Data Warehouse environments as per the standard practices- Advanced SQL knowledge and experience working with relational databases, able to write/debug complex SQL queriesYour profile must have :- 4+ years of experience in building data pipelines and data warehouse architectures on cloud platforms such as AWS- Exposure to agile methodology- Experience in developing Python or Spark jobs- Strong understanding of data modelling principles- Good communication and collaborative skillsExtra points if you have :- Built processes supporting data transformations, data structures and workload management in Database/Data Warehouse- Experience in performance tuning of Redshift databases and implement recommendations- Experience with sourcing data using APIs from external systems- Experience working with teams across the globe, in a fast-paced, high-tech and customer-obsessed environment- Exposure to Shopify, Amazon and Syndicated data (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
ANI Calls India Private Limited,Azure Data Engineer with Big Data,"Anicalls Industry:

IT
Total Positions: 3

Job Type:
Full Time/

Permanent Gender:
No Preference Salary: 900000 INR - 1400000 INR ( Annually )

Education:
Bachelor′s degree Experience: 8 -12

Years Location:
Hyderabad, India . Azure Data Factory . Azure Databricks . Python, Scala, PySpark, Spark . HIVE / HIVE LLAP / HBASE / CosmoDb . Azure Active Directory Domain Services . Apache Ranger / Apache Ambari . Azure Key Vault . Expertise in HDInsight ( Minimum 2 -3 years ' experience with multiple implementations ) . Expertise in Cloud Native and Open Cloud Architecture",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
DAZN,Senior Data Engineer,"Are you an engineer who loves to make things that just work better? Do you love to work with cutting edge technologies and think about how can this run faster, be deployed quicker or fail less and deliver killer streaming applications that add business value and stick with customers?

DAZN is a tech-first sport streaming platform that reaches millions of users every week. We are challenging a traditional industry and giving power back to the fans. Our new Hyderabad tech hub will be the engine that drives us forward to the future. We’re pushing boundaries and doing things no-one has done before. Here, you have the opportunity to make your mark and the power to make change happen - to make a difference for our customers. When you join DAZN you will work on projects that impact millions of lives thanks to your critical contributions to our global products

This is the perfect place to work if you are passionate about technology and want an opportunity to use your creativity to help grow and scale a global range of IT systems, Infrastructure and IT Services. Our cutting-edge technology allows us to stream sports content to millions of concurrent viewers globally across multiple platforms and devices. DAZN’s Cloud based architecture unifies a range of technologies in order to deliver a seamless user experience and support a global user base and company infrastructure.

Join us in India’s beautiful “City of Pearls” and bring your ambition to life.

Benefits will include access to DAZN, an annual performance related bonus, family friendly community, free access for you and one other to our workplace mental health platform app (Unmind), learning and development resources, opportunity for flexible working, and access to our internal speaker series and events.
As our new Data Engineer, you'll have the opportunity to:

• Support building real-time user-facing analytics and data driven operations applications
• Be responsible with the rest of the team for the availability, performance, monitoring, emergency response, and capacity planning
• Use your love of big data systems, thinking about how to make them run as smoothly and securely as possible, support operational endpoints
• Have a strong sense of teamwork and put team’s / company’s interests first

You'll be set up for success if you have

• 5+ years’ experience writing clean, robust and testable code, preferably in Typescript
• Experience building high performant, low latency and high velocity data pipelines
• Working knowledge in AWS services, such as Kinesis, EventBridge, SQS, SNS Topic, S3, Lambda, Kinesis, EKS, Firehose
• Experience with infrastructure-as-code (preferably Terraform) and CI/CD processes
• Comfortable building & maintaining production level data pipelines; streaming or event driven.
• Strong analytical and communication skills.

Even better if you have:

• Exposure to streaming technologies such as Apache Kafka / Google PubSub, Apache Beam, Google Dataflow.
• Having worked in an agile environment with scrum / kanban delivery methodologies

At DAZN, we bring ambition to life. We are innovators, game-changers and pioneers. So if you want to push boundaries and make an impact, DAZN is the place to be.

As part of our team you'll have the opportunity to make your mark and the power to make change happen. We're doing things no-one has done before, giving fans and customers access to sport anytime, anywhere. We're using world-class technology to transform sports and revolutionise the industry and we're not going to stop.

If you're ambitious, inventive, brave and supportive, then you're the kind of person who's going to enjoy life at DAZN.

We are committed to fostering an inclusive environment, both inside and outside of our walls, that values equality and diversity and where everyone can contribute at the highest level and have their voices heard. For us, this means hiring and developing talent across all races, ethnicities, religions, age groups, sexual orientations, gender identities and abilities. We are supported by our talented Employee Resource Group communities: proud@DAZN, women@DAZN, disability@DAZN and ParentZONE.

If you’d like to include a cover letter with your application, please feel free to. Please do not feel you need to apply with a photo or disclose any other information that is not related to your professional experience.

Our aim is to make our hiring processes as accessible for everyone as possible, including providing adjustments for interviews where we can.

We look forward to hearing from you.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer: Enterprise Content Management,"Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As Enterprise Content Management, you will be working as an application developer on projects in OpenText Process suite BPM. Your role would also involve in playing a critical role in design of a new system

Responsibilities:
• As a Business Process Management (BPM) Developer, you will manage asset services and application development while collaborating with global team in harmonizing the development of asset management applications.
• You will focus on improving corporate performance by managing business processes.
• Identification and driving of related service quality improvements and engineering deliverables.
• Management and progression of Action items on time with prompt response
• Automation and process improvement, if applicable
• Client communication

Required Technical and Professional Expertise
• Minimum 4 years of core development experience as OpenText Process Suite Developer
• Proficient in OpenText Process Suite BPM and having knowledge to design and develop the workflow
• Experience in Xform, HTML5, Angular JS. Javascript, & SQL
• Working knowledge of Core Java, Web Services & Ws APP integration is an added advantage
• Knowledge on Rest API's and SOAP' API's

Preferred Technical and Professional Expertise
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications

About Business UnitIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients' businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBMIn a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you'll be able to learn and develop yourself and your career, you'll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBMIBM's greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we're also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it's time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location StatementWhen applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBMIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Bengaluru,False,False,True,True,False,False,True,False,False,False,True,False,False,False,False,False
Digital Mapout Solutions India Private Limited,Azure Data Engineer,"Role : Azure Data Engineer

Location : Bangalore / Hyderabad

Experience : 4+

M.O.H : Full Time

M.O.W : Work From Office

NP Immediate / 15 days

Education : ÂBE / BTech, ME / MTech / MCA.

Skillsets : Azure Data Factory+Azure Data Lake + Azure SQL+ Azure Synapse+PowerBI

JD : -

Azure data / Lead Engineer :

Mandatory Skill sets : T SQL, Data Warehousing (DW) , ADF, Synapse Analytics

Optional : Power BI-DAX,Data Bricks, Python,PySpark

experience : 3 to 15 years

Senior developer to leads
• Candidate must have a strong experience background in database, Data warehousing & ETL / ELT design and development
• Exposure to complex & large scale enterprise development environment
• Excellent communication & collaboration skills required. Ability to work with internal and external stakeholders is must.
• Good business acumen
• Good problem solving and analytical ability
• Ability & willingness to learn new skills

Core technical skills
• Azure data lake Gen 2
• Synapse Analytics
• Python / scala programming
• Synapse pipeline / Azure data factory
• Azure SQL
• T-SQL programming
• Power BI-DAX",Bengaluru,True,False,True,False,True,False,False,True,True,False,False,False,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.Job DescriptionDevelops and maintains scalable data pipelines for bulk data movement between systems of record and systems of referenceDevelops and maintains scalable application to application integrationsAligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organizationImplements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processesWrites appropriate unit or integration tests to implement test-driven developmentContinually contributes to and enhances data team documentationPerforms data analysis required to troubleshoot and resolve data related issuesWorks closely with a team of frontend and backend engineers, product managers, and analystsDefines company data assets, artifacts and data modelsQualificationsRequired qualifications:5 years of Data Engineering and Data Integration5 Years of Data Warehousing3 Years of Data Architecture and Modeling2 years of Cloud Data EngineeringAgile MethodologiesPreferred skills:AWS or Azure Data CertificationsExperience with databricks, spark, pythonExperience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)Experience with SalesforceAdditional InformationAll your information will be kept confidential according to EEO guidelines.** At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. ** insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Factspan,Factspan Analytics - Azure Data Engineer - Big Data/Hadoop,"Job Description :- 7+ Years of deep experience with complex data systems and good instincts around data modelling and usage.- Knowledge of data engineering technologies, architecture, and processes. Specifically, Azure Data Lake, Hadoop ecosystem, Kafka, and common third-party integration and orchestration tools.- Good knowledge of multi-cloud data ecosystem and build scalable solutions on cloud (Azure)- Good knowledge of Big Data Ecosystem-Spark, Hadoop, Databricks- Work across 3-4 teams to develop practices which lead to the highest quality products and contribute transformation change within the cloud- Experience building large scale data processing ecosystems with real time and batch style data as input using big data technologies- Experience in any programming language like Scala or Python.- Exposure to agile methodology and proven ability to technically lead a team of engineers across geographies.- Implement Data Quality, Data Governance on Azure Cloud ecosystem- Good instincts around technical architecture, including metadata, Rest API Integrations, Data API and Solution design of NoSQL and File systems.- Willingness and ability to invest in engineering growth.- Strong communication skills and ability to coordinate across a diverse group of technical and non-technical stakeholders.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Big Data Engineer,"DATA ENGINEER - JD

The role will be part of the Data and Analytics Team responsible for expanding and optimizing AECOM’s data and data pipeline architecture, data flow, and collection for cross functional teams. The role will support software developers, database architects, data analysts, and data scientists on data initiatives and will ensure consistent optimal data delivery architecture throughout ongoing projects.

Responsibilities & Duties
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep AECOM’s data separated and secure across national boundaries through multiple data centres and regions.
• Create data tools for analytics and data scientist team members -to assist them in building and optimizing our product into an innovative industry leader.
• Collaborate with data and analytics experts to strive for greater functionality in our data systems.
• Escalate issues and recommend resolutions to the Team Lead for timely. May support junior members of the team in addressing routine issues within the assigned processes.
• Maintain the SOP/DTP of current processes and incorporate documentation updates as required.
• Perform moderately complex tasks in compliance with service level agreement, process, policies, and procedures.
• Propose alternatives in identified issues and assist in investigating and in resolving common and unusual issues.
• Contribute in various and simultaneous process improvement initiatives to streamline processes, improve customer experience, and increase productivity. This includes automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Contribute specialized expertise to different assigned projects and may provide key updates to Team Lead and Manager.

Qualifications & Requirements

Minimum Requirements:
• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or relevant discipline in the quantitative field
• 6-10years
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Advanced working SQL/nosql, ADLS, Databricks, ADF, Azure DevOps
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency,and workload management.
• Demonstrated ability to manipulate, process, and extract value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.

Preferred Qualifications
• Experience with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Attributes
• Demonstrated ability to champion and drive ideas/programs/solutions
• Excellent organizational and time management skills, able to work under pressure and prioritize effectively
• Able to demonstrate passion, energy and drive, especially in the face of resistance
• Ability to effectively communicate and collaborate within a varied audience and internal and external customers. (Communication)
• Ability to maintain good customer relationship with the ability to suggest ways to improve customer support customer experience (Customer Service)
• Ability to be thorough and meticulous in completing assigned tasks and identifying errors, duplicates, & discrepancies through defined methods. (Attention to Detail)
• Ability to identify and resolve simple to moderate with the ability to provide resolution alternatives by following defined policies and procedures. (Problem Solving)

experience

10",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,True,False,True,False
Tiger Analytics India Consulting Private Limited,Senior Data Engineer - Denodo,"Job Title: Senior Data Engineer – Denodo

Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 2800+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Curious about the role? What your typical day would look like?
· Engage with clients to understand their business context.
· Translate business needs to technical specifications.
· Define Data Virtualization architecture, deployments, and standards.
· Support the development of data architecture principles, standards, and processes and applies these to deliverables
· Involving in data exploitation and the development of (advanced) analytical data models with multiple data sources using Denodo/Tibco or AtScale semantic layer.
· Developing integrated data solutions, modernizing, consolidating, and coordinating business needs across several applications.
· Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Senior Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Senior Data Engineer

Senior Data Engineer, Delivery Engineering Platform
Delivery Engineering Platforms is part of MasterCard Data & Services group and one of the most rapidly growing organization in the space. Platform Teams provides cloud-based analytic software tools that enable large, consumer-focused businesses to seize the Big Data analytics opportunity by triangulating between business strategy, algorithmic math, and large databases to improve decisions.

100 of the largest corporations in the world uses these products. Test & Learn™ for Sites, Test & Learn™ for Customers, Test & Learn™ for Ads, and other similar products employ patented algorithms and workflow to design and interpret business experiments that evaluate, target, and refine proposed business programs

The Delivery Engineering Platform team is a core component to consulting services, managing the data acquisition, integration and transformation of client provided data within the Test & Learn platform for global engagements.

Role

The Senior Data Engineer will lead and participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace. A Senior Data Engineer will:
• Independently lead projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base
• Act as an expert technical resource within the team and region
• Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors
• Build trust-based working relationships with peers and clients across local and global teams
• Implement best practices and collaborate in the design of effective streamlined processes for a complex global solutions group
• Leverage industry best practices including proper use of source control, participation in code reviews, data validation and testing
• Plays a lead role where he/she oversees the activities of the data engineers and ensures the efficient execution of their duties
• Act as an advisor/mentor and helps in managing careers for junior team members
• Comply and uphold all MasterCard internal policies and external regulations

All about you:
• BE/BTech in a quantitative field (e.g., Computer Science, Statistics, Econometrics, Engineering, Mathematics, Operations Research). ME/MTech preferred
• Excellent English quantitative, technical, and communication (oral/written) skills; is an excellent listener
• Expertise with hands-on experience with RDMS technologies, preferably with Microsoft SQL Server, the SSIS Stack and .Net; Proficiency with at least one scripting language (VB Script, Perl, Python)
• Proven self-motivated leader with experience working in teams
• Demonstrate excellent skills in the ability to innovate, think critically and disaggregate problems. Able to provide oversight, validation and quality control to own and team work product
• Ability to easily move between business, data management, and technical teams; ability to quickly intuit the business use case and identify technical solutions to enable it
• Able to balance multiple projects and differing project priorities
• Flexible to work with global offices across several time zones

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LodgIQ,Data Engineer,"About LodgIQ

Headquartered in New York, LodgIQ delivers a revolutionary SaaS platform for Algorithmic Pricing and Revenue Management for the hospitality industry by incorporating machine learning and artificial intelligence. For more information, visit http://www.lodgiq.com.

Backed by Highgate Ventures and Trilantic Capital Partners, LodgIQ is a well-funded company, seeking for a motivated and entrepreneurial Developer to join its Product/ Engineering team. Qualified candidates will be offered an excellent compensation and benefit package.

Title: Data Engineer

Location: India

Requirements:
• In-depth knowledge of Python.
• Understanding of Django/Flask, Pandas.
• Familiarity with AWS Environment (EC2, S3, IAM, Athena).
• Working knowledge of NoSQL databases such as MongoDB.
• Proficiency in consuming and developing REST APIs with JSON data.
• Ability to perform data mining and data exploration with intuitive sense for problem solving and strong desire for craftsmanship.

Specific Job Knowledge, Skills & Abilities:
• Real world experience with large-scale data on AWS or similar platform.
• Must be a self-starter and an effective data wrangler.
• Intellectual curiosity and strong desire to learn new Big Data and Machine Learning technologies.
• Deadline driven, and capable of delivering projects on time under a fast paced, high growth environment.
• Willingness to work with unstructured and messy data.
• Bachelor’s degree or Masters degree in relevant quantitative fields (e.g. Computer Science, Statistics, Electrical Engineering, Applied Mathematics, etc).",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Edu Angels India Private Limited,Data Engineer (PySpark),"Responsibilities
• Develop process workflows for data preparations, modeling, and mining Manage configurations to build reliable datasets for analysis Troubleshooting services, system bottlenecks, and application integration.
• Designing, integrating, and documenting technical components, and dependencies of big data platform Ensuring best practices that can be adopted in the Big Data stack and shared across teams.
• Design and Development of Data pipeline on AWS Cloud
• Data Pipeline development using Pyspark, AWS, and Python.
• Developing Pyspark streaming applications

Eligibility
• Hands-on experience in Spark, Python, and Cloud
• Highly analytical and data-oriented
• Good to have - Databricks",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Zepto,Data Engineer III (Lead Data Engineer),"Responsibilities:
• Collaborate with Tech and Analytics team to build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
• Oversee and govern the expansion of the current data architecture as the business grows and ensure best practices are followed.
• Design and build best-in-class architecture for data tables to ensure optimal querying performance in relational databases.
• Create and maintain connectors that expose the data securely for consumption by downstream systems and services in near real-time.
• Create and maintain data architecture docs to communicate data requirements that are important to business stakeholders and work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that.
• Build data governance and security protocols and ensure adherence from analytics, tech, and business teams.
• Build and mentor the data engineering team, recognize their strengths, and lead them to take ownership of end-to-end data architecture.
• Stay on top of the latest developments in the tech stack and propose potential upgrades to existing systems.

Requirements:
• 6 to 10 years of experience in Data Engineering - Designing databases, building data pipelines, and maintaining data governance protocols in cloud platforms.
• A visionary in technical architecture, with experience building and maintaining Data.
• Engineering Products, along with the demonstrated ability to take accountability for achieving results.
• Hands-on working experience with Python, ETL pipelines, and advanced SQL.
• Strong understanding of AWS Services - Redshift, Lambda, Glue, Athena, and security protocols.
• Experience in any Cloud DW Redshift/Snowflake/BigQuery/Synapse.
• Strong data Modelling and database design experience with Redshift or other relational databases.
• Experience working with Agile methodologies, Test Driven Development, and implementing CI/CD pipelines using Gitlab and Docker.
• Good understanding of ETL/ELT technology and processes.
• Experience in gathering and processing raw data at scale including writing scripts, web scraping, and calling APIs.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,True,False,True
Confidential,Data Engineer 3 - AWS & Python (Contractual),"IntroductionThe Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed. At our heart is a 50 year forward look, a global forecast of the majority of the world's economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions. We're changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products. What will you experience At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to. Accountabilities How you will contribute: Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment. Work in an agile environment: within a collaborative agile product team using Kanban Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements. Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives. Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals. Experience, skills and professional attributesTo succeed in this role it would be an advantage if you possess: Experience with programing in Python, and Lambda functions Knowledge of building bespoke ETL solutions, and extracting data using Data APIs MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting Prior experience in design and developing microservice architecture Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. A combination of IT skills, data governance skills, analytics skills and economics knowledge An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience. Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. This employer is a corporate member of myGwork - LGBTQ+ professionals, the business community for LGBTQ+ professionals, students, inclusive employers & anyone who believes in workplace equality. PRB",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Axtria - Ingenious Insights,Data Engineer,"• 5-8 years of experience in data engineering, consulting, and/or technology implementation roles
• Expertise in the design, data modeling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation and delivery
• Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena
• Strong proficiency in SQL and database design, development and maintenance
• Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence
• Good understanding of modern architecture patterns like serverless and microservices
• Expertise with analytics and business intelligence solutions (e.g. 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation and Agile Methodology
• Experience working with some of the following marketing data sources
• Traditional > TV / Print / Email
• Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data
• Experience leading project teams with members with different roles and skills
• Experience working in hybrid onshore/offshore team models
• Strong communication skills",New Delhi,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Valiance Solutions,Big Data Engineer,"About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc. Our team comprises 150+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire a Senior Data Engineer to help our customers create scalable data engineering pipelines and infrastructure for downstream analytics workloads. You should be good at understanding client data needs, the landscape of various heterogeneous data sources, identifying a set of services for data ingestion & transformation workloads, and timely execution of projects.

Roles & Responsibilities:
• As a data engineer with Pyspark & SQL skills you will be required to highly scalable, robust, and resilient data engineering pipelines .
• You will be working closely with business stakeholders & the data science team to understand their data requirements and underlying business logic.
• Deploy and monitor pyspark jobs on cloud infrastructure.
• Troubleshoot job failures and ensure system recovery at earliest.
• Attending regular client calls, communicating work status and pro-actively highlighting any delays to the product release.

Technical Skills :
• Hands on experience on pyspark for at least 3 years
• Solid programming experience in Python & SQL is required.
• Working experience of any one cloud platform; AWS, GCP or Azure
• Intermediate plus proficiency in shell scripting
• Experience deploying ML algorithms in production is preferred

Personal Skills :
• Excellent communication skills, both written & oral.
• Ability to learn new skills quickly, adjust to the changing needs of the project.
• You are highly enthusiastic about your work
• Ability to multi-task, manage high-pressure release scenarios occasionally.

Valiance Solutions focuses on Financial Services, Cloud Computing, Artificial Intelligence, Internet of Things, and Big Data Analytics. Their company has offices in Noida and Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://valiancesolutions.com or find them on Twitter and LinkedIn.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
DarioHealth,Data Engineer - Hybrid,"About The Position

At Dario, Every Day is a New Opportunity to Make a Difference.

﻿We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career.

DarioHealth is looking for an experienced Data Engineer who will join our team and create new data solutions, maintain existing solutions and be a focal point of all technical aspects of our data activity. As part of this position, you will develop advanced data and analytics solutions to support our analysts and production units with validated and reliable data.

Responsibilities
• Develop and maintain DarioHealth data infrastructure.
• Develop in-house applications for providing self-service tools.
• Develop real-time data applications for production.
• Provide analysts and data scientists technical support related to data infrastructure.
• Design, build and launch new data models and visualizations in production, leveraging common development toolkits.

Requirements
• At least 4 years of proven experience with Python - a must.
• Very high level of SQL and data warehouse modeling.
• Experience with 24/7 systems and real-time analytics.
• Experience developing data pipelines with Airflow or similar - a must
• Experience with big data solutions like Kinesis/Sparks - an advantage
• Experience with NoSQL databases like MongoDB/Redis.
• Experience with web development using Django/javascript/react - an advantage.
• Experience in the online industry.
• B.A./B.Sc. in industrial/information systems engineering, computer science, statistics, or equivalent.
• **DarioHealth promotes diversity of thought, culture and background, which connects the entire Dario team. We believe that every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions. We are passionate about building and sustaining an inclusive and equitable working and learning environments for all people, and do not discriminate against any employee or job candidate.***",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False
AlphaGrep Securities,Data Engineer,"About the Company

AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes (equities, commodities, currencies, fixed income) that trade on global exchanges..

We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.

Roles & Responsibilities
• Build infrastructure tools and applications to support trading teams across the firm.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Coordinate with global teams to understand their requirements and work alongside them.
• Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
• Set up practices on data reporting and continuous monitoring
• Write a highly efficient and optimized code that is easily scalable.
• Adherence to coding and quality standards.

Required Skills
• Strong working knowledge in Python.
• Strong working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have
• Experience with web crawling and scraping, text parsing
• Experience working in Linux Environment
• Experience with Stock Market Data

Why You Should Join Us
• Great People. We’re curious engineers, mathematicians, statisticians and like to have fun while achieving our goals
• Transparent Structure. Our employees know that we value their ideas and contributions
• Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.
• Health & Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Robert Bosch,Azure Data Engineer,"Job Description

Location : Bengaluru
Experience : 6 to 8 years
Requirements
• Overall 6+ IT experience out of which 3+ years of working experience in Azure, architecting data soultions with good experience on Azure SQL, Azure Data Lake, ADF, Azure DataBricks/Synapse etc.
• 5+ years experience with data modelling,implementing backends and data optimization.
• Good experience working with with Automotive domain usecases.
• Experience implementing compliances like GDPR,HIPAA etc.
• Enforcing data security at rest and in transit.
• Good experience implemeting data security in Azure storage systems.
• Ability to thrive in a fast-paced, dynamic, client-facing role where delivering solid work products to exceed high expectations is a measure of success
• Excellent leadership and interpersonal skills
• Eager to contribute in a team-oriented environment
• Ability to be creative and analytical in a problem-solving environment
• Effective verbal and written communication skills

Skills
• Must have skills : SQL,ADF,other Azure storage services.
• Good to have:Synapse,spark or any big data framework or data warehouse.
• Key Responsibilities : conceptualizaing ,modelling Optimizing databases.Designing data flows
• Knowledge or basic experience with Nosql,parquet,Predictive modelling etc
• Working knowledge, creating ETL packages and deploying them

Qualifications

BE,MCA,MSC,MS,MTech
Experience : 6 to 8 years

Additional Information

Additional information
• Nice to have : Power BI experience, Azure DevOps, Cost Monitoring, Azure AD, Azure Synapse.
• Ability to quickly ramp up on new Azure Components which comes in Azure Roadmap.
• Excellent communication skills (English)

Experience: 6.00-8.00 Years",,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
CX Customer Experience,Salesforce - Data Engineer,"Come create the technology that helps the world act together

Nokia is committed to innovation and technology leadership across mobile, fixed and cloud networks. Your career here will have a positive impact on people’s lives and will help us build the capabilities needed for a more productive, sustainable, and inclusive world.

We challenge ourselves to create an inclusive way of working where we are open to new ideas, empowered to take risks and fearless to bring our authentic selves to work.

The team you'll be part of

You will work as part of the CX Global Sales Operation team. You will work making our Data strategy come alive across CX. You will be involved with integrating data across multiple platforms and especially the integration of the Advanced Analytic platform and the use cases being developed on it. The Advance Analytic projects will enable the CX organization to increase speed and efficiency, and assure the right things are done in the right way to maximize Nokia business.

What you will learn and contribute to

Be responsible for the data engineering of the Advanced Analytics Platform and CX AI use cases

Data integration from different source data platform to the advanced analytics platform

Data integration from the advanced analytic platform to different platform with UI

Ongoing support for the integration

Potentially doing data cleaning and wrangling on the advanced analytics platform

Potential involvement in dashboard and UI development

The type of use case you will work on will center around one or more of the following:
• Machine learning prediction models,
• The use of machine learning to automate the currently manual business planning process
• Knowledge mining and recommendation systems to improve the likelihood to win new business.

Your skills and experience
• Deep experience end data engineering including but not limited to experience using SQL and other types of databases and database languages
• Proficiency developing software probably in python using jupyter notebooks
• Proven competency in agile and lean software development
• Competency in SCM (Git), Automation tools, infrastructure automation,
• Good knowledge about Azure cloud infrastructure, security and application development.
• Experience with Python / Spark and Delta Lake. Familiar with big data patterns like lake house.
• Experience with Azure DevOps, CI/CD pipelines, version control tools like GIT / VSTS. Familiar with IDE’s like Visual Studio
• Nokia Business and process understanding
• Bachelor’s degree or higher in information technology, data science or related disciplines
• Effective communication in English (written and verbal)

Nice to have:
• Experience in app development

What we offer

Nokia offers flexible and hybrid working schemes, continuous learning opportunities, well-being programs to support you mentally and physically, opportunities to join and get supported by employee resource groups, mentoring programs and highly diverse teams with an inclusive culture where people thrive and are empowered.

Nokia is committed to inclusion and is an equal opportunity employer

Nokia has received the following recognitions for its commitment to inclusion & equality:
• One of the World’s Most Ethical Companies by Ethisphere
• Gender-Equality Index by Bloomberg
• Workplace Pride Global Benchmark
• LGBT+ equality & best place to work by HRC Foundation

At Nokia, we act inclusively and respect the uniqueness of people.

Nokia’s employment decisions are made regardless of race, color, national or ethnic origin, religion, gender, sexual orientation, gender identity or expression, age, marital status, disability, protected veteran status or other characteristics protected by law.

We are committed to a culture of inclusion built upon our core value of respect.

Join us and be part of a company where you will feel included and empowered to succeed.

Additional Information",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 20 2023

Ready to help shape the future of healthcare?

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organization where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to impact the health of 2.5 billion people around the world in the next 10 years.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves - feeling welcome, valued and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.

The Senior Principal Data Engineer is a vital technical role in the successful design and delivery of Data and Analytics (D&A) initiatives for the GSK's Pharmaceutical and Vaccines Supply Chains. The primary purpose of this role is to ensure that D&A Products have an optimal solution design and that the technical development work to then deliver them into production and support is smooth and successful. This requires deep expertise in data and analytics platforms and technologies as well as domain understanding of Pharmaceutical & Vaccines manufacturing and quality processes. This also requires close collaboration with D&A Product Managers, D&A Development Squads and the D&A Platform & Architecture team as well as with business stakeholders and other Digital and Tech teams.

The MSAT & Quality D&A team currently has a portfolio of around 15 D&A products across 4 product groups with over 100 people (GSK employees plus contractors) working in agile squads to deliver these. The Sr Principal Engineer will oversee and be accountable for the technical success of all of these products.

Key Responsibilities:
• Accountable for optimal solution designs for D&A Products that facilitates an agile, product management approach, can be rapidly and cost-effectively delivered to meet the true business requirements and are robust, sustainable and supportable throughout their lifecycle
• Work closely with D&A Product Mangers, using deep technical expertise and domain understanding to effectively influence (and when needed challenge) business and architectural stakeholders to arrive at the right design
• Steer solution design through D&A Architecture Review process, aligning with enterprise platforms and architectural patterns by first intent
• Oversee technical work of development teams ensuring it is remains aligned with agreed design, is of high quality, complies with relevant standards and policies and will meet agreed business objectives
• Provide hands-on technical problem-solving expertise to address technical challenges during development and, where needed, during lifecycle support
• Act as mentor for more junior technical roles, supporting their development and promoting adoption of best practice across development teams
• Lead discovery / proof-of-concept activities to establish early technical feasibility of new Products or Product Features
• Input to, review and approve key technical documents (e.g. design spec, validation plan)
• Drive adoption by development teams of existing and future best-practice approaches from D&A Platform team (e.g. implementation of DevOps CI/CD pipelines, automated testing)
Why you?

Basic Qualifications:
• Computer Science or related Bachelor's degree
• 16+ years of experience and track record of engineering and delivery of flexible, scalable, and supportable data and analytics applications for large complex, global organizations
• End-to-end / 'full stack' D&A experience from data ingestion through transformation to user interaction (visualisation, analytics, etc.)
• Track record of designing and delivering solutions in a cloud environment using modern data architectures and engineering technologies
• Experience designing with DataOps and FinOps in mind to ensure solutions are flexible/future-proof and can scale to handle growing demand, while remaining cost effective
• Experience in Agile development
• Track record of designing and delivering solutions compliant with industry regulations and legislation
• Ability to oversee and matrix manage GSK and 3rd party technical resources
• Excellent communication, negotiation, influencing and stakeholder management skills.
• Customer focus and excellent problem-solving skills.
Preferred Qualifications:
• Computer Science or related Master's degree
• Microsoft Azure accreditation and experience
• SAFe (Scaled Agile) accreditation experience
• Experience developing and delivering GxP-validated solutions for the Pharma/Vaccines industry
• Experience with specific technologies in GSK stack: Talend, Databricks/DeltaLake, Azure Synapse, Snowflake, PowerBI, Azure Functions, Azure App Services,
At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.

#LI-GSK

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We're committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in ""gsk.com"", you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Confidential,Data Engineer,"Job purpose We are hiring a Data Engineer to join our Enterprise Analytics team. As a Data Engineer, you will be responsible for building, maintaining, and optimizing the data infrastructure needed to support our company's data-driven initiatives. You will work closely with our data analysts, data scientists, and business intelligence developers to ensure that our data is accurate, complete, and secure.Job Responsibilities:Design, build, and maintain the data infrastructure needed to support our company's data-driven initiatives.Develop and maintain data pipelines and ETL processes that move data from source systems to our data warehouse.Implement data quality checks to ensure that our data is accurate, complete, and consistent.Work closely with our data analysts, data scientists, and business intelligence developers to understand their data needs and ensure that our data infrastructure meets those needs.Optimize our data infrastructure to ensure that it can handle large amounts of data and support complex queries.Develop and maintain documentation for our data infrastructure and processes.Stay up to date with the latest technologies and trends in data engineering and recommend new tools and techniques as appropriate.Collaborate with other members of the BI and Data team to ensure that our data infrastructure is aligned with our company's strategic goals.Background and experience:Bachelor's degree in Computer Science, Engineering, or a related field.3+ years of experience in data engineering or a related field.Competencies and skills:· Strong knowledge of SQL and experience working with relational databases.· Experience with data modeling and schema design.· Experience building and maintaining data pipelines and ETL processes.· Experience with cloud-based data warehousing technologies, such as Azure Data Lake, Data Factory, Synapse Analytics.· Strong problem-solving skills and attention to detail.Excellent communication and collaboration skills. Transportation, Logistics and Storage,IT Services and IT Consulting,Truck Transportation",,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Loop Health,Data Engineer - Remote,"About Loop

Looking for a great mission? Help build a customer focused healthcare company.

Loop wants to create an inspirational healthcare and insurance company. We believe in the transformative nature of empathetic primary care, proactive financial coverage and want to bring that to our members. We want to fundamentally change how healthcare assurance is designed and delivered. We believe in the power of incentives. We are successful when we deliver health outcomes — when our members and their families get healthier.

“Why exactly are we building a new revolutionary healthcare system? The obvious answer is India deserves better care for its people. Not enough of it around, and what exists can be tough to navigate.

Imagine if doctors were paid to actually make you better. What a concept! What if they didn't have to worry about finishing consults in 10 minutes to meet their daily quota. What if they could take their time, really understand the symptoms, the family history & the lifestyle to come up with a plan, rather than just a prescription.

Imagine if hospital admissions, treatments, billing and insurance were as easy as ordering food home and your care doesn't end when they send you home from the hospital. It goes till you are back on your feet. And further, now imagine if your family had access to this great care anytime they wanted. From serious conditions to the smallest questions. So that they live longer. Wouldn't you worry less?

At the end of it. It's not why you would build this system... Why wouldn't you?”

Here’s how we are going about it:

- We built a high quality concierge and primary care program that allows members and their families to access unlimited care when they need it.
- We use technology to deliver this through highly engaging care.
- We work with insurers to bring financial protection to our members so that they do not worry about their families’ well being.
- As a healthcare insurance broker, we provide companies with the best coverage and claims service for their employees and dependents.

Doing this will mean that we create great products and services that work on changing behaviors and mindsets. This will need a deep understanding of design of products and services through an empathetic lens of what members need for their health and technology will play a very pivotal role in enabling our members to use our programs . We are looking for folks in our ‘EngineeringTeams’ to work with us to take Loop to this future.

If you'd like to learn more about what we are building at Loop, there are tons of resources. Here are some of our favorites:
https://yourstory.com/2021/06/loop-health-insurance-plans-improve-
healthcare/amphttps://yourstory.com/2022/04/loop-health-raises-25m-elevation-capital-general-catalyst/amp

Join us in making healthcare simple, reliable, and human.

Roles and Responsibilities

• Work in collaboration with engineers and stakeholders to build a platform for enabling data-driven decisions.
• Build reliable, scalable, CI/CD driven streaming and batch data engineering pipelines.
• Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.
• Create a conceptual data model to identify key business entities and visualize their relationships.
• Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and relationships
• Storage (cloud data warehouse, S3 data lake), orchestration (Airflow), processing (Spark, Flink), streaming services (Kafka), BI tools, graph database, and real-time large scale event aggregation store are all examples of data architecture to design and maintain.
• Work on cloud data warehouses, data as a service, business intelligence, and machine learning solutions.
• Data wrangling in a diverse environment.
• Ability to provide data and analytics solutions that are cutting-edge.
• Identify strategic and Operational KPIs for the team and drive the team to deliver the committed targets.

Qualifications

• SQL knowledge, as well as programming skills in Scala or Python.5+ years of applicable data warehousing, data engineering, or data architecture experience
• Experience with the GCP stack (BigQuery, GCP Databricks) is a plus
• Ability to design data analytics solutions to meet performance and scaling requirements.
• Demonstrated analytical and problem-solving abilities, particularly in the context of large data.
• Data warehousing concepts and modern data warehouse/Lambda architecture are well-understood.
• Good understanding of the Machine Learning and Artificial Intelligence (AI) solution space.
• Communication and interpersonal skills at all levels of management
• You are a detail-oriented person with excellent communication skills and a strong sense of teamwork.

What you can expect from us

  ‍  ‍   Loop Family Healthcare Health insurance for you and your family for all medical emergencies.

   High agency You'll always have the agency to shape projects, processes and outcomes independently.

  Learning Budget If there's a workshop, book or event you think will help you learn, we'll cover your bill.

   Work from home setup We'll help you set up your office the way you want to with the best equipment around.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,True,True,False
Visa,Sr. Data Engineer - Big Data Testing,"This position is ideal for an engineer who is passionate about solving challenging business problems. You will be an integral part of the Payment Products Development team focusing on test automation. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, and testing of new and existing functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Develop systems and processes to refine efficiency of automated testing solutions
• Design and execute tests for applications and services
• Develop and maintain tools for automation tracking and reporting
• Review product requirements and specifications and recommend improvements to ensure product testability
• Recommend areas of applications and services where automation would be beneficial
• Present technical solutions, capabilities, considerations, and features in business terms
• Effectively communicate status, issues, and risks in a precise and timely manner
• Perform other tasks on data governance, system infrastructure, and other cross team functions on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
PayPal,"MTS 1, Data Engineer","Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
The MTS 1 ? Data Engineer will directly report to and support Sr. Manger of Finance Technologies in the development and execution of strategic transformation programs & initiatives, strategic engineering architecture design, resource allocation, and platform performance monitoring. Ideal candidate is a technologist who believes that use of technology is in its infancy and the best is yet to come. The Regulatory Reporting Hadoop product owner (Business System Analyst) will be part of the Global Regulatory Reporting, and Merger & Acquisition Integration support. The nature of role is strategic, analytical and highly collaborative, working with team members across World and also as a liaison for Global projects.
• Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
• Undergraduate/ Master degree in Computer Engineering or equivalent from a leading university.
• 11+ years of post-college working experience as a Business System Analyst and leading large scale projects end to end.?
• Minimum 4+ years? experience working with large data sets, experience working with distributed computing a plus (Map/Reduce, Hadoop, Hive, Spark, etc.)
• Experience in Data Analysis, Data Validation.
• Strong knowledge in writing complex queries for validation of ETL process.
• Preferred/Basic understanding of Payments/Finance/Accounting Industry Background.
• Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations.
• Experience with databases, systems integration, application development and reporting.?
• Works independently and able to make decisions quickly when necessary.
• Quick Learner with an ability to ramp up in technologies and modules to meet business needs.
• Works in an Agile environment and continuously reviews the business needs, refines priorities, outlines milestones and deliverables, and identifies opportunities and risks.
• Maintain, track and collaborate with dev teams to ensure project estimation for delivery.
• Experience using JIRA and Confluence, or similar User Story workflow and management tool is a must.
• Highlight the bugs and blockers and coordinate with the development and operations team to come up with the best solutions/fixes and document them.
• Work across internal team in various geo-locations across the world
• ?Drive For Results? - Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.
• ?Priority Setting? - Spends his/her time and the time of others on what’s important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.
• Weekly and Monthly status reporting to leadership.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Software Engineer II | Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Software Engineer II | Data Engineer

Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.

Overview
The Enterprise Data Solutions team is looking for a Big Data Engineer to drive our mission to unlock potential of data assets by consistently innovating, eliminating friction in how users access data from its Big Data repositories and enforce standards and principles in the Big Data space. The candidate will be part of an exciting, fast paced environment developing Data Engineering solutions in the data and analytics domain.

Role
• Develop high quality, secure and scalable data pipelines using spark, Scala/ python on Hadoop or object storage.
• Leverage new technologies and approaches to innovate with increasingly large data sets.
• Drive automation and efficiency in Data ingestion, data movement and data access workflows by innovation and collaboration.
• Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency.
• Perform assigned tasks and support production incidents.

All About You
• 4+ years of experience in Data Warehouse related projects in product or service-based organization
• Expertise in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
• Experience of building data pipelines through Spark with Scala/Python/Java on Hadoop or Object storage
• Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
• Experience of working on Nifi will be an added advantage
• Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
• Strong communication skills - both verbal and written
• Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
• Flexibility to work as a member of a matrix based diverse and geographically distributed project teams

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Narwal,Senior Data Engineer,"Hello There, Good Day!

I'm Gowtham from Narwalinc. We are a niche technology company with a specialization in the recruitment of IT professionals. One of our customers is looking for a Data Engineer

Job Description:

Developer/engineer who is experienced in data integration from source systems to target systems (like a data warehouse) leveraging ETL/ELT technologies as well as streaming technologies.

Required Skills:

• 5+ years of hands-on experience leveraging Snowflake platform and its ecosystem of tools

• 5+ years of hands-on experience leveraging Informatica Power Center in the context of ETL/ELT to take data from Oracle Data Warehouse to Snowflake

• 5+ years of experience with data integration from Data Lake/Data Warehouse to Snowflake

• Extremely comfortable with SQL.

• Very good communication and presentation skills

• Must be a self-starter, takes initiative, actively collaborates with team members to solve problems

• Ability to actively contribute and be productive with minimum supervision.

• Willing to work overlapping US EST hours - 2 PM to 11 PM IST (for India employees, should be available till noon EST.)

• Work remotely.

Preferred Skills:

• Experience with Matillion data integration platform

• Experience with Streamsets for data integration pipelines

• Experience with CI/CD processes.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Plume Design,Senior Data Engineer,"Plume’s Cloud Platform team is looking for engineers to build and operate data pipelines that power the gamut of Plume products and analytics. Due to the massive scale and performance requirements of many of our use cases, you will be solving challenging problems on a daily basis using a variety of cutting edge technologies.

What you will do:
• Interact with stakeholders to gather and understand data requirements
• Design and implement data pipelines with high data quality goals
• Maintain up-to-date documentation of data warehouse schemas
• Write clean, maintainable code, and perform peer code-reviews
• Refactor code as needed to improve performance and simplify operations
• Provide production support in triaging and fixing issues relating to data quality and availability
• Mentor and assist junior team members and new hires to become successful and productive
• Adhere to data protection requirements including data access, retention, residency and de-identification
• Play an integral role in driving the technology roadmap and enhancing best practices

What You’ll Bring
• Education Requirements: BS/MS/PhD in Computer Science, Electrical Engineering or related technical field
• 5+ years of software development experience with a proven track record of building, scaling, and supporting production data pipelines
• High proficiency in writing idiomatic code, preferably in Java or Scala
• High proficiency in writing SQL in data warehousing technologies
• Strong understanding of large-scale data processing technologies, e.g. Apache Spark (preferred) or Apache Flink
• Strong understanding of data warehousing concepts
• Strong analytical and problem-solving skills
• Strong oral and written communication skills

Plume Design focuses on Internet Service Providers and Cloud Data Services. Their company has offices in Palo Alto. They have a mid-size team that's between 51-200 employees. To date, Plume Design has raised $37.5M of funding; their latest round was closed on June 2017.

You can view their website at https://platform.plume.com or find them on Twitter, Facebook, and LinkedIn.",Hyderabad,False,False,True,True,False,False,False,False,False,False,False,True,False,False,False,False
Lilly,Data Engineer - (DT) Business Insights & Analytics,"At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 35,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease, and give back to our communities through philanthropy and volunteerism. We give our best effort to our work, and we put people first. We’re looking for people who are determined to make life better for people around the world.

Business Insights and Analytics: Data Engineer

At Lilly, we unite caring with discovery to make life better for people around the world. We are a global healthcare leader headquartered in Indianapolis, Indiana. Our 39,000 employees around the world work to discover and bring life-changing medicines to those who need them, improve the understanding and management of disease. We’re looking for people who are determined to make life better for people around the world.

The LCCI (Lilly Capability Center India), BI&A (Business Insights & Analytics) team was started in 2017 with the objective of supporting business decisions for the commercial and marketing functions in the US and ex-US affiliates. This team is part of the LCCI - Commercial Services organization and works very closely with business analytics team based in Indianapolis (HQ). The team currently comprises of more than 100 staff members, with varied backgrounds and skills across data management, analytics and data sciences, business and commercial operations etc.

To better meet the evolving analytics needs, the LCCI BI&A team is ramping up the data engineering pillar. We are looking for data engineers who can be play integral role in developing, maintaining, and testing infrastructures for data generation, processing and storage; work closely with data scientists and help architecting solutions with the objective of driving right KPIs for the business.

Core Responsibilities:
• Create and maintain optimal data pipeline architecture ETL/ ELT into Structured data
• Assemble large, complex data sets that meet functional / non-functional business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.
• Expert level experience creating Fact tables, Dimensional tables and ingest datasets into Cloud based tools. Job Scheduling, automation experience is must.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Setup and maintain data ingestion, streaming, scheduling, and job monitoring automation. Connectivity between Lambda, Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and “big data” technologies like AWS and Google
• Build analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics
• Work with cross-functional teams including external consultants and IT teams to assist with data-related technical issues and support their data infrastructure needs
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader

Experience Required
• 4-8 years of in-depth hands-on experience in data warehousing (Redshift or any OLAP) to support business/data analytics, business intelligence (BI)
• Advanced knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouses
• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation
• Data compression into PARQUET to improve processing and finetuning SQL programming skills required
• Experience building and optimizing “big data” data pipelines, architectures and data sets
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Experience with manipulating, processing, and extracting value from large unrelated datasets
• Working knowledge of message queuing, stream processing, and highly scalable “big data” stores
• Strong analytical and problem-solving skills to be able to structure and solve open ended business problems (pharma experience is highly preferred)

Education
• Bachelor’s/ Master’s degree in Technology OR Computer Sciences

Eli Lilly and Company, Lilly USA, LLC and our wholly owned subsidiaries (collectively “Lilly”) are committed to help individuals with disabilities to participate in the workforce and ensure equal opportunity to compete for jobs. If you require an accommodation to submit a resume for positions at Lilly, please email Lilly Human Resources ( Lilly_Recruiting_Compliance@lists.lilly.com ) for further assistance. Please note This email address is intended for use only to request an accommodation as part of the application process. Any other correspondence will not receive a response.

Lilly does not discriminate on the basis of age, race, color, religion, gender, sexual orientation, gender identity, gender expression, national origin, protected veteran status, disability or any other legally protected status.

#WeAreLilly",Bengaluru,False,False,True,False,False,False,False,False,True,False,False,False,True,False,False,True
Visa,Lead Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.

Job Description

New Payment Flows (NPF) division’s charter is to capture new sources of money movement through card and non-card flows, including Visa Business Solutions, Government Solutions and Visa Direct which presents an enormous growth opportunity. Our team brings payment solutions and associated services to clients around the globe. Our global clients and partners deploy our solutions to serve the needs of Small Businesses, Middle Market Clients, Large Corporate Clients, Multi Nationals and Governments.

The Visa Business Solutions (VBS) and Visa Government Solutions (VGS) team is a world-class technology organization experiencing tremendous, double-digit growth as we expand products into new payment flows and continue to grow our core card solutions. This is an incredibly exciting team to join as we expand globally.

Essential Functions
• Strong technology and leadership background building enterprise scale applications using Scala/Java, Spring, REST APIs, RDBMS, and Angular/React. Machine Learning, Data Engineering (Hadoop, Hive, Spark), NoSQL, Kafka, Streaming and Data Pipelines desirable.
• Design and deploy data and pipeline management frameworks built on top of open-source components, including Hadoop, Hive, Spark, HBase, Kafka streaming and other Big Data technologies.
• Champion Design and Coding best practices while technically leading a small team.
• Experience with Continuous Integration and Automated Test tools such as Jenkins, Artifactory, Git, Selenium, Chef desirable
• Familiarity or experience with data mining, data science, machine learning and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
• Responsible for the design and implementation of an innovative, scalable, and distributed systems that take advantage of technology to allow standardization, security, timeliness and quality of data.
• Work with and manage remote teams
• Work with product managers in developing a strategy and road map to provide compelling capabilities that helps them succeed in their business goals.
• Work closely with senior engineers to develop the best technical design and approach for new product development.
• Instill best practices for software development and documentation, assure designs meet requirements, and deliver high quality work on tight schedules.
• Project management: prioritization, planning of projects and features, stakeholder management and tracking of external commitments
• Operational Excellence: monitoring & operation of production services
• Identify opportunities for further enhancements and refinements to standards and processes.
• Mentor junior team members, develop departmental procedures and best practices standards.
• Hire and retain world class talents to deliver data platform projects.
• Strong Negotiation Skills: You will be a distinguished ambassador for product development, collaborating, negotiating, managing tradeoffs and evaluating opportunistic new ideas with business partners

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.

Qualifications

• Bachelor degree in a technical field such as computer science, computer engineering or related field required. Advanced degree preferred
• Requires 10+ years of experience, at least 3 of which were in leading engineering teams
• 6+ years of hands-on experience in Hadoop using Core Java Programming, Spark, Scala, Hive, PIG scripts, Sqoop, Streaming, Kafka any ETL tool exposure
• Strong knowledge of Database concepts and UNIX
• Strong knowledge on CI/CD and engineering efficiency tools including code coverage
• Experience in handling very large data volume in low latency and/or batch mode
• Proven experience delivering large scale, highly available production software
• Ability to handle multiple competing priorities in a fast-paced environment
• A deep understanding of end-to-end software development in a team, and a track record of shipping software on time
• Payment processing background desirable but not required
• Experience working in an Agile and Test-Driven Development environment.
• Strong business and technical vision
• Outstanding verbal, written, presentation, facilitation, and interaction skills, including ability to effectively communicate architectural issues and concepts to multiple organization levels and executive management
• Quick learner, self-starter, detailed and work with minimal supervision

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,False,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
GE,Senior Data Engineer,"Job Description Summary
GE HealthCare is on a transformational journey leveraging Data and Analytics to drive business growth. GE HealthCare is looking for Senior Data Engineer who will be responsible for building and implementing the data ETL pipelines for Finance function data (from data ingestion to consumption).The Data Engineering team helps solve our customers' toughest challenges leveraging data and analytics. The Senior Data Engineer will work with the team to create state-of-the-art data and analytics driven solutions, working across GE HealthCare to drive business analytics to a new level of predictive analytics while leveraging On-prem, Cloud Platform, Big data tools and technologies.

GE HealthCare is a leading global medical technology and digital solutions innovator. Our purpose is to create a world where healthcare has no limits. Unlock your ambition, turn ideas into world-changing realities, and join an organization where every voice makes a difference, and every difference builds a healthier world.

Job Description

In this role you will:
• Responsible for building data and analytical engineering solutions with standard end to end design & ETL patterns, implementing data pipelines, data modelling and overseeing overall data quality.
• Responsible to work with cross functional teams in GEHC to make the data usable for functional users, data scientists and application users to enable delivery of business values to customers.
• Responsible to enable access of data in AWS storage layers and transformations in AWS Datawarehouse and further transporting in respective databases, consumers, data marts etc.
• As a Senior Data Engineer, you will be part of a data engineering or cross-disciplinary team on Finance facing development projects, typically involving large, complex data sets. These teams typically include data engineers, data visualization engineers, architects, data scientists, product managers, and end users, working in cohorts with partners in GE business units.
• Implement Data warehouse entities with common re-usable data model designs with automation and data quality capabilities.
• Demonstrate proficiency at industry standard data modeling tools (e.g., Erwin, ER Studio, etc.).
• Integrate domain data knowledge into development of data requirements.
• Develop processing codebase using pySpark and implement medium to complex transformations, business logics.
• Look across multiple systems, understands the purpose of each system and defines data requirements by systems.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Lead other horizontal improvement initiatives to benefit technology and leap further on a problem area or Hackathon etc
• Establish and maintain as a trusted advisor relationship within GE Healthcare Data & Analytics (Finance Function)
• Establish and maintain close working relationships with teams responsible for delivering solutions to the businesses and functions
• Engage collaboratively with project teams to support project objectives through the application of sound data engineering principles
• Identify risks and assumptions for the in scope Data & Analytics solutions
• Work with the contract/vendor resources to deliver the solution and manage the technical resources work

Qualifications
• Bachelor's Degree in Computer Science, Information Technology or equivalent (STEM)
• A minimum of 6 year of similar experience working on Database(s), SQL, Python, Datawarehouse, Java, ETL and AWS cloud platform is required. AWS certifications would be added advantage
• Experienced in Deployment process on-prem and on-cloud using Kubernetes, Dockers, Jenkins
• Ability to drive projects in big data (structured/unstructured/machine/logs/streaming data types)
• 3+ Year of Data modelling & Data warehousing experience with MPP systems (Teradata, Netezza, Greenplum etc.)
• 3+ years in AWS Services Like Redshift, RDS, S3, Glue, Step Function, Lambda etc.
• Hands on experience in delivering analytics in modern data architecture (Massively Parallel Processing Database Platforms and Semantic Modelling)
• Demonstrable knowledge of ETL and ELT patterns and when to use either one; experience selecting among different tools that could be leveraged to accomplish this. (i.e. Informatica, HVR, Talend etc)
• Demonstrable knowledge of and experience with different scripting languages (python, shell)
• Understands data quality and solves for application-level needs
• Understanding of DaaS, Data management tools / solutions
• Strong verbal & written communication
• Experience working with solutions delivery teams using Agile/Scrum ore similar methodologies
• Added advantage if experienced in working on Finance data

Desired skills:
• Delivers results when working on shorter-term (weeks-months), outcome-focused service engagements
• Proactively learning new technology, predicts trends, and identifies new opportunities based on trends
• Leverages knowledge about technology trends, and changing business needs across the broad environment to bring new ideas to the team
• Articulates the value proposition of existing technology capabilities and maps them to customer requirements to minimize incremental cost of development
• Experienced in working with On-prem (Teradata) data warehouse – Dimensional and data modelling. Experienced in one of the ETL like Informatica.
• Identifies the customer’s business and strategic needs, concerns, and desires for the value delivery capabilities of the Product
• Functional understanding of finance - Close Book, Treasury, Cash, Controllership, Credit, Account Payables, Account Receivables, Cash Forecasting, Balance sheet exposure, Debt, Forex etc.

Inclusion and Diversity

GE Healthcare is an Equal Opportunity Employer where inclusion matters. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

We expect all employees to live and breathe our behaviors: to act with humility and build trust; lead with transparency; deliver with focus, and drive ownership – always with unyielding integrity.

Our total rewards are designed to unlock your ambition by giving you the boost and flexibility you need to turn your ideas into world-changing realities. Our salary and benefits are everything you’d expect from an organization with global strength and scale, and you’ll be surrounded by career opportunities in a culture that fosters care, collaboration and support.
#LI-Hybrid
#LI-GM2

Additional Information

Relocation Assistance Provided: Yes",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,False
Concinnity Media Technologies,Senior Data Engineer,"Preferred Experience:

• 8+ years’ experience building mobile, web and/or API-based applications

• Follow engineering standards and best practices

• Knowledge of databases: MySQL, PostgreSQL, SQL, etc...

• 4+ years of Python server development experience

• Django, Flask, Bottle, or similar framework experience

• 2+ years industry experience

• Knowledge of cloud deployment strategies using AWS, Azure, Rackspace, etc.

• Expertise working with and building RESTful APIs

• Ability to operate in Agile / Scrum development environments

• Understanding of OOP and Data Structures and know when to apply them in daily coding scenarios

• Knowledge in the following web-technologies:
• JavaScript
• HTML & HTML5
• CSS3
• JavaScript frameworks (React, Angular, Next.js, etc.)

• Understand the development of the following:
• Responsive Web Development
• Accessibility
• Secure web applications

• Message queue implementations (RabbitMQ, ZeroMQ, Kafka, etc.)

• Background task processing (Celery, etc.)

• Experience configuring container like systems (Vagrant, Docker, etc.)

• Container orchestration with Kubernetes

• Ability to self-organize with minimal guidance/competing priorities and work effectively within a team

• Ability to provide innovative, creative solutions to tasks/problems

• Ability to complete work following engineering standards and best practices

• Experience with GIT and Gitlab is a plus

• Experience with JSON is a plus",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
Hewlett Packard Careers,Data Engineer,"HP is the world’s leading personal systems and printing company, we create technology that makes life better for everyone, everywhere. Our innovation springs from a team of individuals, each collaborating and contributing their own perspectives, knowledge, and experience to advance the way the world works and lives.
We are looking for visionaries, like you, who are ready to make a purposeful impact on the way the world works.

At HP, the future is yours to create!

The Data Engineer will develop, test, and maintain Big Data solutions for a company. Gather large amounts of data from multiple sources and ensure that downstream users can access the data quickly and efficiently. Essentially, the company’s data pipelines are scalable, secure, and able to serve multiple users.

Job description
• Meeting with managers to determine the company’s Big Data needs.
• Developing Hadoop systems.
• Loading disparate data sets and conducting pre-processing services using Spark, Hive or Pig.
• Finalizing the scope of the system and delivering Big Data solutions.
• Managing the communications between the internal system and the vendor.
• Collaborating with the software research and development teams.
• Building cloud platforms for the development of company applications.
• Maintaining production systems.
• Training staff on data management.

Big Data Engineer Requirements:
• Bachelor’s degree in computer engineering or computer science.
• Previous experience as a big data engineer.
• In-depth knowledge of Hadoop, Spark, and similar frameworks.
• Knowledge of scripting languages is preferred .
• Knowledge of NoSQL and RDBMS databases including Redis and MongoDB.
• Familiarity with Mesos, AWS, and Docker tools.
• Excellent project management skills.
• Good communication skills.
• Ability to solve complex data, and software issues.

Education and Experience Required:
• Typically, 6+ years of progressive professional experience as a big data engineer.
• Bachelor’s degree in computer engineering or computer science.

We love our work environment. We think you will too:
• It’s a friendly atmosphere with supportive leaders to bring your creativity to the max.
• Work-life balance support including flex-time arrangements and work from home opportunities.
• Corporate Social Responsibility initiatives to help you make an impact to communities at large.

Sustainable impact is HP’s commitment to create positive, lasting change for the planet, its people, and our communities. This serves as a guiding principle for delivering on our corporate vision – to create technology that makes life better for everyone, everywhere.

HP is a Human Capital Partner – we commit to human capital development and adopting progressive workplace practices in India.

#LI-POST

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So
are we. We love taking on tough challenges, disrupting the status quo,
and creating what’s next. We’re in search of talented people who are
inspired by big challenges, driven to learn and grow, and dedicated to
making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is
respected and where people can be themselves, while being a part of
something bigger than themselves. We celebrate the notion that you can
belong at HP and bring your authentic self to work each and every day.
When you do that, you’re more innovative and that helps grow our bottom
line. Come to HP and thrive!",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Omnivio,Data Engineer - Full Time,"Job Profile

Omnivio is a startup in the Supply Chain and Logistics domain. We help retailers deliver an 'Amazon like' shopping experience to their customers. We optimize and manage delivery times and cost, inventory, geo-distributed stores etc. One of the core pieces of our infrastructure is the data engineering required to get data from various upstream systems into a data model that we use for intelligence. If you've worked in data engineering before, you might imagine that this has a good number of challenging engineering problems. We are looking for junior to mid level data engineers to join our team.

Experience / Skills required

We are looking for previous experience in data engineering for this role. Here's a list of tools and technologies that you can expect to be working with in this job. The listed examples are not necessarily all a part of our stack, but they are solid indicators of your skills being a good fit for the job.
• Relational Databases, both OLTP and OLAP, such as MySQL, Postgres, Redshift, BigQuery, CLickhouse etc
• Solid software engineering fundamentals, and experience with one or more general purpose programming languages such as Python, Typescript
• Data engineering programming libraries such as Pandas, NumPy etc
• Building data engineering pipelines using orchestration tools such as Airflow, Airbyte, Temporal, or other commercial offerings
• Transformations using dbt, or a similar alternative
• Experience with AWS's data engineering stack is definitely a plus
• Deployment/operating experience with any of these tools would be really interesting too

Work culture
• No ego anywhere in the team, including higher management.
• Remote, asynchronous, flexible work timings.
• Collaboration and team-thinking. No single person owns the failure.
• Ample time and attention to help developers level up.

Work Ex - 3+ years

Omnivio focuses on Supply Chain Management, Logistics, Cloud Infrastructure, Logistics Software, and Logistics / Transportation / Shipping. Their company has offices in Noida. They have a small team that's between 11-50 employees. To date, Omnivio has raised $400k of funding; their latest round was closed on July 2022 at a valuation of $5M.

You can view their website at https://omnivio.io or find them on LinkedIn.",,True,False,True,False,False,False,False,False,False,False,False,False,True,True,True,False
Mercedes-Benz Research and Development India Private Limited,Big Data Engineer,"AufgabenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team playerQualifikationenBackground

Mercedes Benz cars are equipped with Advanced Driver Assistance Systems (ADAS) and continuously new features/technologies are added in these systems to add more intelligence in them. The development or the customer cars then collect various vehicle, sensor and environment data. These vehicles at the end of their test drives or upon satisfying some trigger conditions will upload different sensor and vehicle data that are logged in binary formats (in TBs) into a Big Data cluster. The data engineering team has the responsibility to provide a 24x7 pipeline, which will ingest the data to the cluster, extract and transform to readable format. This data is further analysed and the performance of the ADAS systems are monitored.

Skill Requirements:
Essential Skills and experience:
- Excellent Programming skills in Python with object oriented design
- Experience in designing solution for business applications and deployment of the same.
- Hands on experience in working with relational SQL and noSQL DB, postgres, mongoDB etc.
- Hands on experience on developing ETL solutions on Big Data Clusters like MAPR Clusters
- Hands on experience in workflow schedulers like Airflow etc.,
- Good knowledge on microservices is desired
- Experience with Docker containers is a plus.
- Experience in creation of CI/CD pipeline is an added advantage
- Working experience on the cloud environment Azure, AWS is an added advantage
- Excellent problem solving skills
- Experience in the automotive field and exposure of multi-cultural environment is an added advantage
Key Job Responsibilities:
- Architecting, developing and deploying end to end pipeline to ingest various ADAS systems data into the clusters
- Develop scalable solutions and improve efficiency and the reliability of the existing application
- Develop and deploy new features based on the business need
- Ideates and improvises on the requirements through continuous innovation and application of new techniques and methods
- Understand new features requirement from counterparts, breakdown of task and participate in planning
- Good communication skills and ability to work in & contribute as a good team player",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
Confidential,Data Engineer - SQL/Data Pipeline,"Job Description : : - Hands on working knowledge on building and optimizing 'big data' data pipelines, architectures, and data sets- Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases- Strong know-how on data ingestion tools and APIs to prioritize data sources, validate them, and dispatch data to ensure an effective ingestion process. Knowledge on data ingestion tools such as Apache Kafka/ Apache Storm/Apache Flume/Apache Sqoop/Wavefront, and more.- Working knowledge on data mining tools such as Apache Mahout/KNIME/Rapid Miner/Weka,- Strong know-how on ETL tools such as Talend/Informatica PowerCenter/AWS Glue/Stitch,- Ability to handle various types of data in the form of text, speech, image, video, or live stream from IoT/ Sensors/ Web- Ability to manipulate, process and extract value from large, disconnected datasets and articulate the same in business contextPreferred : - Skilled in the use of business intelligence and visualization tools, such as PowerBI, Tableau - Experience with stream-processing systems such as Storm, Spark-Streaming, etc.- Ability to leverage MLOps Platforms such as Teraform, Ansible, Kubeflow, Google AI Platform- Know-how on software development languages such as Python, Java, C++, Scala, etc (ref:hirist.com) IT",,True,False,True,True,False,True,False,False,False,True,False,False,False,False,False,False
NIRA,Sr Data Engineer/Architect (3y-7y),"About the job

Starting with credit, NIRA aspires to be the pre-eminent financial brand for the mass market or ""Middle India"". We already have customers in over 5,000 towns and cities, and we're growing quickly (15% MoM for the last 20 months!). It's a very exciting time to join us. We have over 200+ employees.

Currently, only 10% of Indians can use banks when they need credit: banks typically require a high credit score or collateral, something most people don't have. It need not be this way. Using a combination of traditional data and the vast amount of digital data available, it is now possible to score the unscored.

Today, we receive 15000 new loan applications daily from across 4000 cities in India, and we are growing 15-20% MoM. People reach us at their time of need, and we offer them credit via our app. Money reaches their bank account within 24 hrs of application.

We are addressing head-on a big challenge. It's also a great opportunity from both a commercial and societal impact perspective. We can improve lives for millions. It is no exaggeration to say that our addressable market will be 400mm within 5 years. It's pretty exciting, we think.

If our mission resonates with you, and you are a talented and hardworking individual that wants to commit yourself to an incredible challenge, then we want to hear from you.

As one of the senior data engineers on the team, you’ll be working on our core data platform and infrastructure powering business decisions and data science workloads.

Job Overview

We are looking for an experienced Data Engineer to join our engineering team. The hire will be responsible for building our data and data pipeline architecture. You will optimise our data flow starting with the collection of data for cross functional teams and purposes. You will support our key data science initiatives while maintaining consistency of data delivery architecture throughout ongoing projects. Ideal candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities

• Create and maintain optimal data pipeline architecture

• Assemble large, complex data sets that meet performance and business requirements

• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies

• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, credit risk, operational efficiency and other business KPIs.

• Create data tools for analytics and data scientist team members that assist them in building and scaling our core products

• Work with cross domain data and analytics experts to strive for stronger data driven outcomes for the business.

Qualification

• Advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

• Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

• We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science. They should also have experience using the following software/tools:

• Experience with big data tools: Hadoop, Spark / PySpark, Kafka.

• Experience with relational SQL and NoSQL databases.

• Experience with object-oriented/object function scripting languages: Python.

• Expertise in data modelling and buiding data driven systems.

• Well versed with Shell Scripting.

• Hands on experience on various AWS services like EMR,EC2,Glue.

• Deploy existing data projects using CICD pieplines.

• Knowledge on Docker is a plus.

What we offer:

• Competitive salary

• Medical Insurance

You can learn more about NIRA here:

Press:

https://yourstory.com/2019/05/startup-fintech-nira-entrepreneur-loans/

https://www.livemint.com/companies/news/muthoot-finance-partners-with-nira-to-provide-personal-loans-11620309871295.html

https://www.financialexpress.com/money/personal-loan-collection-rates-return-to-pre-covid-levels-data-from-nira-reveals/2313170/

NIRA focuses on Consumer Lending and Fin Tech. Their company has offices in Bengaluru. They have a large team that's between 201-500 employees. To date, NIRA has raised $3.1M of funding; their latest round was closed on April 2020.

You can view their website at https://www.nirafinance.com or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Referrals Only,Consultant-Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.
Job responsibilities• You will partner with teammates to create complex data processing pipelines in order to solve our clients' most complex challenges
• You will collaborate with Data Scientists in order to design scalable implementations of their models
• You will pair to write clean and iterative code based on TDD
• Leverage various continuous delivery practices to deploy, support and operate data pipelines
• Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
• Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
• Create data models and speak to the tradeoffs of different modeling approaches
• Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
• Assure effective collaboration between Thoughtworks' and the client's teams, encouraging open communication and advocating for shared outcomes
Job qualificationsTechnical skills• You have a good understanding of data modelling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
• You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
• Hands on experience in MapR, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
• You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
• Working with data excites you: you can build and operate data pipelines, and maintain data storage, all within distributed systems
• You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Professional skills• You're resilient and flexible in ambiguous situations and enjoy solving problems from technical and business perspectives
• An interest in coaching, sharing your experience and knowledge with teammates
• You enjoy influencing others and always advocate for technical excellence while being open to change when needed
• Presence in the external tech community: you willingly share your expertise with others via speaking engagements, contributions to open source, blogs and more
Other things to knowL&DThere is no one-size-fits-all career path at Thoughtworks: however you want to develop your career is entirely up to you. But we also balance autonomy with the strength of our cultivation culture. This means your career is supported by interactive tools, numerous development programs and teammates who want to help you grow. We see value in helping each other be our best and that extends to empowering our employees in their career journeys.
About ThoughtworksThoughtworks is a global technology consultancy that integrates strategy, design and engineering to drive digital innovation. For 28+ years, our clients have trusted our autonomous teams to build solutions that look past the obvious. Here, computer science grads come together with seasoned technologists, self-taught developers, midlife career changers and more to learn from and challenge each other. Career journeys flourish with the strength of our cultivation culture, which has won numerous awards around the world.

Join Thoughtworks and thrive. Together, our extra curiosity, innovation, passion and dedication overcomes ordinary.",Hyderabad,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True,False
ANI Calls India Private Limited,Senior Data Engineer,"Anicalls

Industry: IT
Total Positions: 2
Job Type: Full Time/Permanent
Gender: No Preference
Salary: 900000 INR - 1800000 INR (Annually)
Education: Bachelor′s degree
Experience: 5-10 Years
Location: Bengaluru, India
Candidate should have:
Worked collaboratively with cross-functional teams and stakeholders to achieve an organizational goal.
Worked in an agile environment and are comfortable running an agile process for the data and analytics team.
Strong experience in data pipelines, ETL design (both implementation and maintenance), data warehousing, and data modeling (preferably in dbt).
Implementation and tuning experience in the Big Data Ecosystem,
(such as Data Analytics (Dataproc, Airflow, Hadoop, Spark, Hive),
Google Cloud Platform AI and ML Services and Data Warehousing (such as BigQuery, schema design,
query tuning and optimization) and data migration and integration.
End to end hands-on to carry out complex POC, Pilot, Limited production rollout, assignments requiring the development of new or improved techniques and procedures.
Participated in deep architectural discussions to build confidence and ensure customer, success when building new, or migrating existing, applications, software, and services on the Google Cloud Platform.
advanced skills in SQL, data modeling, ETL/ELT development, and data warehousing.
Strong skills in Optimization - performance, pipeline, spark.
Experience on Pyspark.
5+ years of design & implementation experience with distributed applications.
5+ years of experience architecting/operating solutions built on Google Cloud Platform.
. Bachelor's degree.

Experience: 5.00-10.00 Years",,False,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
deloitte,Consulting- SAMA- A&C-Azure Data Engineer- AD,"JD:
• Location - Mumbai OR Pune
• Experience range:
• 12-15 yrs for AD
• Strong experience in Python programming and related skills like PySpark
• Strong SQL skills
• Strong experience with any of the data engineering platforms like Hadoop, Spark, Synapse, Databricks, Apache Airflow, etc.
• Preferred: Knowledge of any cloud platform like Azure/AWS/Google
• For AD level: Need technology leadership experience in terms of architecture/solutioning and team leading",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Comcast,"Data Engineer, Data Products Engineering","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary INTRODUCTION: At Comcast, we believe in the talent of our people. It's our passion and commitment to excellence that drives Comcast's vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It's what makes us uniquely Comcast. Here you can create the extraordinary. Join us. ABOUT THE ROLE: Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered Job Description Responsibilities Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc) 2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas. Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications. Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data. Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time. REQUIREMENTS: Bachelor's degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience. Strong Computer Science/Engineering/Information Systems background 3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies. Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas Strong problem-solving skills. Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences. PREFERRED QUALIFICATIONS: 1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills Ability to design concise and visually appealing reports, user interfaces, mockups and documentation Ability to read external API documentation and write pipelines to extract data from our partners systems Ability to write and stand up internal API endpoints to share data with other internal teams. Strong analytical focus, results-oriented and execution driven. Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures. Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed. Flexibility to adjust to changing requirements, schedules and priorities. Ability to work independently under minimum supervision and proactive in solving issues Energetic, committed and solution focused with the ability to perform under pressure and meeting targets Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Relevant Work Experience 2-5 Years Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality - to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,False,True
Versor Investments,Data Engineer,"India

Versor Investments (“Versor”) is a quantitative investment boutique headquartered in Midtown Manhattan. The Firm currently has an AUM of $1.8 billion*. Versor creates diversified sources of absolute returns across multiple asset classes. Within a scientific, hypothesis-driven framework, Versor leverages modern statistical methods and vast datasets to drive every step of the investment process. Alpha forecast models, portfolio construction, and the trading process rely on the ingenuity and mathematical expertise of 60+ investment professionals. Versor offers two categories of investment products – Hedge Funds and Alternative Risk Premia. Both are designed to provide superior risk-adjusted returns while exhibiting low correlation to traditional and alternative asset classes. Each invests in liquid, scalable markets. On average, Versor’s partners have spent over 20 years researching, investing and trading systematic alternative investment strategies.

Role Summary

The Data Engineer position will be based in Mumbai and be part of the Portfolio Analytics team. They will collaborate closely with senior researchers to design and develop a large-scale data lake. We are seeking candidates who have excelled in engineering (specifically computer science). Prior experience in investments and finance is beneficial but not mandatory.

This role is ideal for candidates who are passionate about technology and excited about building a data platform.

Responsibilities
• Design architecture for a data platform.
• Design data pipelines based on business and functional requirements.
• Extract, transform, and load logic to automate data collection and manage data processes/pipelines. This includes data quality and monitoring.
• Develop data access tools to allow researchers to access data seamlessly.
• Develop integration tools and analytical reports for the databases and data warehouse.
• Write and review technical documents. This includes requirements and design documents for existing and future data systems, as well as data standards and policies.
• Collaborate with analysts, support/system engineers, and business stakeholders to ensure data infrastructure meets constantly evolving requirements.

Requirements
• E., B.Tech., M.Tech., or M.Sc. in Computer Science, Computer Engineering or similar discipline from a top tier institute.
• 2+ years direct experience working as a data engineer.
• Experience in design, architecture and implementation of data lake, data pipelines and flows.
• Experience with developing software code and APIs in one or more languages such as Python and C#.
• Experience designing and deploying large scale distributed data processing systems with one or more technologies such as MS SQL Server, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, Hive, Teradata, or MicroStrategy.
• A high-level understanding of automation in a cloud environment (AWS experience preferred).
• Excellent communication, presentation, and problem-solving skills.
• Data as of December 31, 2022. AUM reflects regulatory AUM as per SEC definition for the purposes of Item 5.F on the Form ADV Part 1a.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tredence Inc.,Senior Data Engineer,"Associate Manager – Data Engineering (8-11 Years)

This position requires someone with good problem solving, business understanding and client presence.

Overall professional experience of the candidate should be above 8 years. A minimum of 4 years of experience in Data Engineering space. Should have good understanding of business operations, challenges faced, and business technology used across business functions.

The candidate must understand the usage of data Engineering tools for solving business problems and help clients in their data journey. Must have knowledge of emerging technologies used in companies for data management including data governance, data quality, security, data integration, processing, and provisioning. The candidate must possess required soft skills to work with teams and lead medium to large teams.

Candidate should be comfortable with taking leadership roles, in client projects, pre-sales/consulting, solutioning, business development conversations, execution on data engineering projects.

Role Description:
• Engages with Leadership of Tredence' s clients to identify critical business problems, define the need for data engineering solutions and build strategy and roadmap
• S/he possesses a wide exposure to complete lifecycle of data starting from creation to consumption
• S/he has in the past built repeatable tools / data-models to solve specific business problems
• S/he should have hand-on experience of having worked on projects (either as a consultant or with in a company) that needed them to –
• Provide consultation to senior client personnel
• Implement and enhance data warehouses or data lakes.
• Worked with business teams or was a part of the team that implemented process re-engineering driven by data analytics / insights
• Should have deep appreciation of how data can be used in decision making
• Should have perspective on newer ways of solving business problems. E.g. external data, innovative techniques, newer technology
• S/he must have a solution creation mindset. Ability to design and enhance scalable data platforms to address the business need
• Working experience on data engineering tool for one or more cloud platforms -Snowflake, AWS/Azure/GCP
• Engage with technology teams from Tredence and Clients to create last mile connectivity of the solutions -
• Should have experience of working with technology teams
• Demonstrated ability in thought leadership – Articles/White Papers/Interviews

Mandatory Skills

Program Management, Data Warehouse, Data Lake, Analytics, Cloud Platform

Job Location - Bangalore , Chennai , Pune , Gurugram.

Experience Level - 8-11 yrs.

Expected Joining Time - Immediate to Max 30 days.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
Mercedes-Benz Research and Development India Private Limited,Big Data CoE - Engineering.IT Data Engineer,"TasksOverview
A highly motivated and technically proficient professional, capable of delivering solution on Data Engineering in Cloud platform. He or she must be skilled in developing all rituals for Data Engineering especially using pyspark.
Job Responsibilities
Data Engineer:

· Development, Enhancement, testing and release of existing application.
· Develop and enhance end to end data pipeline.
· Interpret data to analyze results to a specific business problem or bottleneck that needs to be solved using statistical techniques.
· Ensure data efficiency and reliability.
Qualification
Mandatory:
· Bachelor's /post graduate degree in Computer Science, Computer Engineering or Data Science, Data Engineering with strong knowledge in below technologies,
· Pyspark
· Databricks,
· ADLS
· SQL
Desired:
· Work experience on Agile/SDLC process.
· Experience in building Knowledge Base (KB)
· Experience in taking over Knowledge Transfer
· Open to work/explore new technology areas.
· Automotive and Aviation domain is plus.Qualifications",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
D2C Ecommerce India Pvt Ltd,D2C Ecommerce - Data Engineer,"What is D2cecommerce :D2C Ecommerce is India's first multi-D2C brand online platform that sells its own homegrown brands across multiple home and lifestyle categories, including - apparel, cosmetics, beauty, jewelry, accessories, fitness, sports, shoes, bags, books, kitchen, food, auto accessories, electronics, kids and travel packages. Along with selling these products on its own portal, D2CEcommerce also has these items listed on leading e-commerce sites.Job Summary : We are seeking a highly motivated and skilled Data Engineer to join our team. As a Data Engineer, you will be responsible for designing, developing, and maintaining our databases and reports. You will work closely with our team of developers, data scientists, and analysts to ensure that our data systems are accurate, efficient, and scalable.What would be your responsibilities :- Design and develop databases that are scalable, efficient, and accurate- Develop and maintain ETL pipelines to move data from various sources into our databases- Create and manage database reports to provide insights to our team of data scientists and analysts- Collaborate with our team of developers to integrate our databases into our applications and services- Continuously monitor and optimize our databases for performance and security- Ensure that our data systems adhere to industry best practices and compliance regulations- Stay up-to-date with the latest database technologies and trendsWhat are we looking for in the candidate :- Ability to build things from scratch.- Self-starter and motivated individuals who can drive processes on their own- A bachelor's or associate degree in management information systems, computer science, or a related field- 0-1 years of experience in database management or a similar role- 0-1 years of experience designing, developing, and producing database reports- Proficiency in SQL and experience with relational databases such as MySQL, Postgre SQL, or Oracle- Experience with ETL tools and techniques- Understanding of data modelling concepts- Familiarity with database administration and management tools- Strong analytical and problem-solving skills- Excellent verbal and written communication skillsWhy you should join D2cecommerce :In addition to an attractive compensation package, you own a piece of the company through ESOPs. Also you will have the opportunity to take up a role in a rapidly growing, highly disruptive organization, and shape the face of ecommerce for the future.Interested? What to do next :If reading the details above excited you and made you feel you can help us take D2Cecommerece to the next level, all you have to do is let us know!",Gurugram,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Thoughtworks Inc.,Senior Consultant - Data Engineer,"Data Engineers develop modern data architecture approaches to meet key business objectives and provide end-to-end data solutions. You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems. On other projects, you might be acting as the architect, leading the design of technical solutions or perhaps overseeing a program inception to build a new product. It could also be a software delivery project where you're equally happy coding and tech-leading the team to implement the solution.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Aryng,Sr. Data Engineer,"Welcome You made it to the job description page

Aryng is looking for a cloud data engineer with experience in developing
enterprise-class distributed data engineering solutions on the cloud. We are seeking
an entrepreneurial and technology-proficient Data Engineer who is an expert in the
implementation of a large-scale, highly efficient data platform, batch, and real-time
pipelines and tools for Aryng clients. This role is based out of India. You will work
closely with a team of highly qualified data scientists, business analysts, and
engineers to ensure we build effective solutions for our clients. Your biggest strength
is creative and effective problem-solving.

Key Responsibilities:

● Should have implemented asynchronous data ingestion, high volume stream data
processing, and real-time data analytics using various Data Engineering
Techniques.
● Implement application components using Cloud technologies
and infrastructure.
● Assist in defining the data pipelines and able to identify bottlenecks to enable
the adoption of data management methodologies.
● Implementing cutting edge cloud platform solutions using the latest tools and
platforms offered by GCP, AWS, and Azure.

Requirements
• Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
5+ years of data engineering experience is a must.
• 2+ years implementing and managing data engineering solutions using Cloud solutions GCP/AWS/Azure or on-premise distributed servers
• 2+ years' experience in Python.
• Must be strong in SQL and its concepts.
• Experience in Big Query, Snowflake, Redshift, DBT.
• Strong understanding of data warehousing, data lake, and cloud concepts.
• Excellent communication and presentation skills
• Excellent problem-solving skills, highly proactive and self-driven
• Consulting background is a big plus.
• Must have a B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study
Good to have:
• Experience in some of the following: Apache Beam, Hadoop, Airflow, Kafka,Spark
• Experience in Tableau, Looker or other BI tools is preferred
This role requires mandatory overlap hours with clients in the US from 8 am - 1
pm PST.

Benefits
• Direct Client Access
• Flexible work hours
• Rapidly Growing Company
• Awesome work culture
• Learn From Experts
• Work-life Balance
• Competitive Salary
• Executive Presence
• End to End Problem Solving
• 50%+ Tax Benefit
• 100% Remote company
• Flat Hierarchy
• Opportunity to become a thought leader

Why Join Aryng: Click on the Youtube link",Chennai,True,False,True,False,False,False,False,True,False,True,False,False,True,False,True,True
Emerson,Data Engineer - Sustainability,"AS AN Data Engineer, YOU WILL:

· Architect and design platform solutions to meet and exceed expectations of Projects.

· Proactively evolve and apply DevSecOps methodologies, standards and leading practices

· Apply architectural standards/principles, security standards, usability design standards, as approprioate.

· Lead a project from delivery perspective, including giving periodic updates to all stakeholders.

Skills Requirements:

· Must be able to communicate fluently in English, both written and verbal

· Excellent interpersonal communication and organizational skills

· Able to distil complex technical challenges to actionable and explainable decisions

· Inspire DevSecOps teams by building consensus and mediating compromises when necessary

· Demonstrate excellent technical & architecture skills, service management and product lifecycle management

· Demonstrate ability to rapidly learn new and emerging technologies

· Operational abilities including early life support and driving root cause analysis and remediation

·Any Azure/Microsoft Big Data related certifications is highly preferred.

REQUIRED EXPERIENCE :

· Bachelor’s Degree or equivalency (CS, CE, CIS, IS, MIS, or engineering discipline)

· 10+ years overall IT industry experience

· 3+ years in a solution design role using service and hosting solutions such as private/public cloud IaaS, PaaS and SaaS platforms.

· Large scale design, implementation and operations of OLTP, OLAP, DW and NoSQL data storage technologies such as SQL Server, Azure SQL, Azure SQL DW, PostgreSQL, CosmosDB, RedisCache, Azure Data Lake Store, Hadoop, Hive, MongoDB, MySQL, Neo4j, Cassandra, HBase

· Creation of descriptive, predictive and prescriptive analytics solutions using Azure Stream Analytics, Azure Analysis Services, Data Lake Analytics, HDInsight, HDP, Spark, Databricks, MapReduce, Pig, Hive, Tez, SSAS, Watson Analytics, SPSS

· Design and configuration of data movement, streaming and transformation (ETL) technologies such as Azure Data Factory, HDF, Nifi, Kafka, Storm, Sqoop, SSIS, LogicApps, Signiant, Aspera, MoveIT, Alteryx, Pentaho, IDQ,

· Enablement of data reuse through Data Catalog/Marketplace, Metadata, Search and Governance technologies such as including Azure Data Catalog, Waterline, Apache Atlas, Apache Solr, Azure Search, Alteryx Connect, Datawatch Monarch Swarm, Collibra Catalog, Enigma Councourse, Adaptive, Cambridge Semantics, Data Advantage Group (DAG), Global IDs, Alation

· Experience with any of the following: Azure, O365, Azure Stack, Azure AD

· Delivery using modern methodologies especially SAFe Agile.

Requisition ID : 23000590

Emerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.",Pune,False,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Confidential,Fractal.ai - Azure Data Engineer - SQL/PySpark,"Mandatory Skills :- Azure Databricks (ADB)- Azure DataFactory (ADF)- Python or Pyspark- SQLResponsibilities :- Be an integral part of large scale client business development and delivery engagements- Develop the software and systems needed for end-to-end execution on large projects- Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions- Build the knowledge base required to deliver increasingly complex technology projectQualifications & Experience :- A bachelor's degree in Computer Science or related field with 3-12 years of technology experience- Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space- Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks- Database programming using any flavours of SQL- Expertise in relational and dimensional modelling, including big data technologies Exposure across all the SDLC process, including testing and deployment- Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.- Good knowledge of Python and Spark are required- Good understanding of how to enable analytics using cloud technology and ML Ops- Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus- Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)- Characteristics of a forward thinker and self-starter - Ability to work with a global team of consulting professionals across multiple projects- Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems- Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to success. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
LatentView,Principal Data Engineer,"About LatentView:
• LatentView Analytics is a leading global analytics and decision sciences provider, delivering solutions that help companies drive digital transformation and use data to gain a competitive advantage. With analytics solutions that provide 360-degree view of the digital consumer, fuel machine learning capabilities and support artificial intelligence initiatives., LatentView Analytics enables leading global brands to predict new revenue streams, anticipate product trends and popularity, improve customer retention rates, optimize investment decisions and turn unstructured data into a valuable business asset.
• We specialize in Predictive Modelling, Marketing Analytics, Big Data Analytics, Advanced Analytics, Web Analytics, Data Science, Data Engineering, Artificial Intelligence and Machine Learning Applications.
• LatentView Analytics is a trusted partner to enterprises worldwide, including more than two dozen Fortune 500 companies in the retail, CPG, financial, technology and healthcare sectors.

Job Description:
As a Manager - data engineer, they should build and maintain scalable, rock solid self-serve data pipelines for data analysts and data Scientists and support them by understanding the content and context of data and collaborating with them to figure out the best way to Extract, Transform, Load, and access it.
• Be an SME in DE and should know how to set up the process, requirement gathering for any movement or data ingestion and data platform creation
• Has to be the end-to-end project manager, allocating work, monitoring progress, providing feedback, and taking necessary steps to ensure agreed timelines are met
• Scripting skills: SQL and Python or PySpark
• Excellent understanding of Data Warehouse and its architecture
• Excellent understanding of Issue Tracking System like JIRA/ Dev-Ops
• Excellent experience in Version control like Github or Gitlab
• Ideal to have knowledge of other DE platforms like Azure databricks, Snowflake etc.

Education: UGEmployment Type: CONTRACTOR",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
CIEL HR Services,Data engineer,CTC-6LPA EXP-Freshers Area of Expertise - A) Data Hygiene B) Application of ML DL Tools If interested please call 9000338173 or forward cv to [Confidential Information],Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer - ETL,"Design, build, and maintain the data infrastructure that supports our data-
driven applications and services.• Develop and maintain ETL (Extract, Transform, Load) processes to ensure
data accuracy and completeness.
• Optimize data pipeline performance and scalability.
• Collaborate with data scientists, analysts, and developers to ensure that our
data pipeline meets their needs.• Implement data governance policies and procedures to ensure data quality
and consistency.
• Design and develop data models that support data-driven applications and
services.
• Troubleshoot and resolve issues related to the data pipeline.
• Participate in the evaluation and selection of data management and analytics
tools and technologies.
• Keep up to date with emerging trends and technologies in data engineering
and big data.

experience

8",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Swift,Data Engineer,"About us:

Swift is building a next generation checkout stack for India - a platform rolling up payments and logistics solution for all fulfillment needs. We give businesses the opportunity to provide a customer experience at par with the likes of Amazon and Flipkart, all the while saving money and time.

Its basically Amazon without the website listing - we let our sellers design their own sales channel :-)

We believe there are many things a seller or small business has to worry about when selling online, logistics/payments/etc shouldn't be one of them. With our solution, SMBs and D2C brands get access to technologies and services like next day delivery, same day delivery, live package tracking, Card/Cash on delivery, scheduled delivery etc, making parcel delivery just as simple as collecting payment.

We also provide robust APIs which makes it easy for developers to add shipping capabilities to their multichannel online store.

We want to be the #1 checkout platform that’s reliable, easy to use and affordable.

About you:

You have experience in working with data pipelines and ETL sets (programmatically and using tools) – say MongoDB, Spark streaming, Python/Java, Apache Beam, PARQR, Delta Lake, Airflow, etc. You are looking for challenges in growing a data backed company to deal with from hundreds to millions of visitors data points per month.

You like working with streaming/reactive architectures and have experience/interest in setting up data pipelines on cloud infra from scratch. You generally prefer to use a minimal set of simple tools to a diverse range of complex ones. We are looking to build a back-end cloud infrastructure (Google Cloud Platform preferably) which will be a fault-tolerant real-time stream processing system on the cloud - Our system will need to meet liveliness guarantees from a big data/ETL perspective.

You like to work on a variety of projects - at this job, you’ll be developing a complex ETL infra, a reactive streaming architecture and a cloud-native, highly available API for our customers.

You are someone who is:
• Experienced in any JVM based language or Python.
• Have worked on NoSQL (MongoDB)/ SQL databases.
• Have worked on creating data pipelines (both programmatically, say using spark streaming) or using tooling like Airflow, dataflow)
• Strong verbal and written communication skills and the ability to work well cross-functionally.
• We offer: *
• You to be a part of a small, but a super capable team.
• The opportunity to work closely with founders to define, scope, estimate and plan various aspects of the product.
• Being one of the first hires at Swift, you will be involved in both high and low-level decision making. This means a lot of ownership, which we cultivate by having a flat structure.

Swift focuses on E-Commerce, B2B, Small and Medium Businesses, Logistics, and D2C. Their company has offices in Bengaluru. They have a mid-size team that's between 51-200 employees. To date, Swift has raised $2.34M of funding; their latest round was closed on July 2021.

You can view their website at https://goswift.in or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,True,False
CarbyneTech India Pvt Ltd,CarbyneTech - Azure Data Engineer - IoT,"- Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions - Azure Synapse/Azure SQL DWH, Azure Data Lake, Azure Blob Storage, Spark, HDInsights, Databricks, CosmosDB, EventHub/IOTHub.- Experience in migrating on-premise data warehouses to data platforms on AZURE cloud.- Designing and implementing data engineering, ingestion, and transformation functions- Azure Synapse or Azure SQL data warehouse- Spark on Azure is available in HD insights and data bricks- Good customer communication.- Good Analytical skill",Hyderabad,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Brillio,GCP Data Engineer - R01523647,"About Brillio:
Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022

GCP Data Engineer
Primary Skills

• BigQuery, Cloud Logging, Cloud Storage, Cloud Trace, Composer, Data Catalog, Data Modelling Fundamentals, Data Warehousing, Dataflow, Datafusion, Dataproc, ETL Fundamentals, Modern Data Platform Fundamentals, PLSQL, T-SQL, Stored Procedures, Python, SQL, SQL (Basic + Advanced)

Specialization

• GCP Data Engineering Basic: Senior Data Engineer

Job requirements

• Job description below. • 6 years of experience in software design and development • 5 years of experience in the data engineering field is preferred • 3 years of Hands-on experience in GCP cloud data implementation suite such as Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage, • Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms. • Mandatory 1 year of software development skills using Python • Extensive hands-on experience working with data using SQL and Python • Cloud Functions. Comparable skills in AWS and other cloud Big Data Engineering space is considered. • Experience in DevOps(CI/CD) pipeline facilitating automated deployment and testing • Experience with agile development methodologies • Excellent verbal and written communications skills with the ability to clearly present ideas, concepts, and solutions • Bachelor's Degree in Computer Science, Information Technology, or closely related discipline

Know what it’s like to work and grow at Brillio: Click here",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,True,True,False
Latent View Analytics Private Limited,Data Engineer,"Job Title :
Data Engineer Experience : 2.5-5 Location : INDIA
• Chennai Job Description: Experience working with Cloud Data Platforms, especially AWS and its services, must be strongly experienced in building data pipelines.
Experience with big data tools like Python, Pyspark, and Spark SQL. Focus on scalability, performance, service robustness, and cost trade-offs.

A continuous drive to explore, improve, enhance, automate, and optimize systems and tools to best meet evolving business and market needs.
Attention to detail, coupled with the ability to think abstractly. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product. Keen to learn new technologies and apply the knowledge in production systems.

AWS skills:
AWS Glue AWS EMR (any two AWS services)

Data Engineer Skills:
Python Pyspark Spark SQL HIVE HQL Scala Good to have: Snowflake querying Databricks AWS API Gateway AWS Lambda",Chennai,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,True
Whiteforce,Data Engineer - Java,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20246

LocationIndia

Job Descriptions

Job Purpose and Primary Objectives : Develop and Deploy data and analytics-led solutions on GCP Key responsibilities (please specify if the position is an individual one or part of a team): Data engineering solution on GCP using Cloud Bigquery, Cloud Dataflow, Pu-Sub, Cloud BigTable and AI/Ml solutions Key Skills/Knowledge : - Good Experience in GCP. - Python/Java, PySpark/Spark Java. - GCP BigQuery. - GCP Pub-Sub. - Secondary Skill - DataFlow, Compute Engine, Cloud Fusion. (ref:hirist.com)

Skills",,True,False,False,True,False,False,False,False,False,False,False,False,False,True,False,False
Confidential,Big Data Engineer - Python/AWS,"JOB_DESCRIPTION :- Has experience in the following #Python, #AWS_Athena, #Glue #Pyspark, #EMR, #DynamoDB, #Redshift, #Kinesis, #Lambda, #Snowflake.- Proficient in #AWS_Redshift, #S3, #Glue, #Athena, #DynamoDB.- Design, build and operationalize large-scale enterprise data solutions and applications using one or more of #AWS_data and analytics services in combination with 3rd parties - #PySpark, #EMR, #DynamoDB, #RedShift, #Kinesis, #Lambda, #Glue, #Snowflake. - Analyze, re-architect, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services. - Design and build production data pipelines from ingestion to consumption within a big data architecture, using Python/PySpark. - Design and implement data engineering, ingestion, crawling, manipulation and curation functions on AWS cloud using AWS native or custom programming. - Must have strong knowledge of databases like Postgres, MySQL, MongoDB, Cassandra, etc. - Must have experience in using AWS SDKs and libraries for interacting with different AWS services. - Experience in building REST APIs. (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,True
Varite India Private Limited,Data Engineer,"snowflake- Data Engineer Data engineering, integration, and data modeling experience . Can write scalable/performant pipelines, queries, and summaries of data . Has worked with various data systems and tools . Understands analytics and data science workflows and common use cases that leverage their work . Python . SQL . Datawarehouse experience . AWS experience . Data QA / validation skills (to check their work) . Snowflake experience (MUST) . Matillion, DBT, or other Data tech experience (ideal) . Marketing technology experience (ideal)",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
deloitte,Consulting-SAMA- A&C-Data Engineer/Architect-Associate Director,"Location: No Preference

Years of Exp: 10-15 Years

Hybrid: Yes

Mandatory Skill: MS Azure, Analytics, Delivery Management & Operations, People Management

Responsibilities:
• 10 - 15 years of deep delivery experience in cloud data engagements, with proven experience to lead teams sizes of 10+ resources
• Experience with platforms such as Azure Cloud Services, Solution Design & Review, Data Management, Data Visualization Tools
• Deep experience handling large volumes of data across multiple data sources like csv, relational data, SAP, json, parquet, flat files, streaming data, etc.
• Strong conceptual understanding of data warehousing, data modeling principles
• Strong Experience with visualization / reporting solutions
• Good understanding of data quality, data management and data governance principles
• Depth in data transformation / data modernization / ETL and ELT based data pipeline development
• Hands on with Agile/Scrum Methodology based implementations and end to end software delivery lifecycle
• Exposure working with international clients / geographies
• Excellent client handling, team handling and interpersonal skills
• Excellent written and spoken communication skills

Ability to understand and appreciate business / domain context and develop data and analytics solutions",Hyderabad,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False
Wipro Technologies,Data Engineer,"Share resume to akshara.raju@wipro.com

Location - Bangalore , Chennai , Pune , Hyd

JD :
• Azure data factory
• Azure data bricks with PySpark coding experience
• Experience in Snowflake
• Good to have knowledge in data visualization tool Tableau or PowerBI
• Good to have knowledge in data warehousing & data analytics

Data Analyst / Data Engineer
• 5 yrs. experience in working with large, complex data sets
• Create reports for internal teams and/or external clients
• Hands on Experience on Azure Data Factory and Azure
• Need to be able to code the data pipelines from ADF standpoint / Data Ingestion.
• Collaborate with team members to collect and analyze data
• Knowledge of Snowflake will be a big plus
• Need to ensure they can work on Data Mapping / Data Enrichment and Data Transformations.
• Use graphs and other methods to visualize data
• Establish KPIs to measure the effectiveness of business decisions
• Structure large data sets to find usable information
• Reporting and Data Visualization skills
• Experience in Data Mapping, data cleansing.",Bengaluru,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True
ANI Calls India Private Limited,Lead Snowflake Data Engineer,"Anicalls

Industry : IT

Total Positions : 3

Job Type : Full Time / Permanent

Gender : No Preference

Salary : 900000 INR - 1800000 INR (Annually)

Education : Bachelor s degree

Experience : 5-10 Years

Location : Noida, India

Candidate should have :

Knowledge and experience in Big data and Data Vault methodology

Experience in power shell, shell scripting, and python

Exposure to data modeling for Snowflake

Experience in working with agile / scrum methodologies.

Experience in building data pipelines for large volumes of data across disparate data sources

Experience in DBT for Snowflake

Good experience on Azure Databricks

experience in Confluent cloud platform

Experience in building pipelines through Confluent Kafka and Knowledge of Azure Kubernetes Service

Good communication and presentation skills

Expertise in building data pipelines for Snowflake using Snowpipe, Snowpark, SnowSQL's and stored procedures.

Azure experience must be focused on Azure Data Factory, Azure storage solutions (such as Blob and Azure Data lake Gen2), and Azure data pipelines

Good experience on Snowflake and Snowflake architecture

7+ years of total experience in data projects with a focus on data integration and ingestion

3+ years of experience working primarily on Snowflake",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Impetus Technologies India Pvt. Ltd,GCP Data Engineer,"Role : GCP Data Engineer

Job Description :

- The candidate should have extensive production experience in GCP, Other cloud experience would be a strong bonus.

- Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.

- Exposure to enterprise application development is a must.

Roles & Responsibilities :

- 6-10 years of IT experience range is preferred.

- Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.

- Strong experience in Big Data technologies Hadoop, Sqoop, Hive and Spark including DevOps.

- Good hands on expertise on either Python or Java programming.

- Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.

- Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.

- Ability to drive the deployment of the customers workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.

- Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.

- Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.

- Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.

- Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.

,",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Stantec Technology International,Senior Data Engineer,"Description
Grow with the best. Join a smart, creative, and inspired team that works behind the scenes to support operational excellence. As part of the Innovation Office, the Digital Technology & Innovation team is composed of digital experts who conduct research and development to keep our teams and our client's projects ahead of the technological curve. They implement established technologies and find emerging solutions for all business lines (Buildings, Energy & Resources, Environmental Services, Infrastructure, and Water), bridging existing knowledge domains and facilitating the integration of powerful tools and methods. The team's goal is to make projects more efficient and help provide higher-quality results to our clients. The ideal candidate will be a self-starter, a critical thinker, and highly interested in the application of new technologies and methods. The candidate will become a member of the Innovation Office, however, he or she will also be accessible to Stantec's project teams to support project work as needed.

Your Opportunity
The Innovation Office's Digital Technology & Innovation (DTI) team has an opportunity for a Senior Data Engineer. This position requires a person who is technically savvy, experienced in data engineering, and enjoys working with data to solve business problems, shaping & creating solutions, and helping to champion implementation. As a member of the Innovation Office, the Digital Technology Development group, as part of the DTI team, also engages in research & development and provides guidance and oversight as a center of excellence for the business. This group also engages in new product research and testing and the incubation of new ideas. The candidate will be responsible for the delivery of professional services and will recommend solutions to achieve complex strategic objectives across our large global team spanning Stantec IT, Business Lines, and the Office of Innovation.

Your Key Responsibilities
Serve at the direction of the Digital Technology Development Leader to:
- Take ownership of the project, work independently in a team environment, and mentor others as needed.
- A passion for solving problems and providing workable solutions, flexible to learn new technologies to meet the business needs.
- Translate complex functional and technical requirements into detailed designs.
- Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining.
- Implement access governance of production data systems to ensure compliance with our privacy and security policies.
- Oversee, design, and develop algorithms for real-time data processing within the business units and to create the frameworks that enable quick and efficient data acquisition.
- Build and maintain best practices to support the Continuous Integration and Delivery (CI/CD) of data engineering solutions.
- Build, test and operate stable, scalable data pipelines that cleanse, structure and integrate disparate data sets (batch and stream data) into a readable and accessible format for end-user facing reports, data science, and ad-hoc analyses.
- Build and maintain reliable and scalable ETL on big data platforms as well as work with varied forms of data infrastructure inclusive of relational databases and NoSQL databases.
- Work collaboratively with DTI's Data & Analytics group, Stantec's internal business units, and clients to define problem statements, collect data and define solution approaches.
- Deliver highly reliable software and data pipelines using Software Engineering best practices like automation, version control, continuous integration/continuous delivery, testing, security, etc.
- Possess excellent time-management skills, a thorough understanding of task assignments and schedules, and efficient use of time and available resources.
- Perform other miscellaneous tasks associated with being a member of the Digital Technology & Innovation team and those typical of a data engineer.",Pune,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
TensorGo Technologies,TensorGo Technologies - Senior Data Engineer - Python,"Skillset : Python, PySpark, Kafka, Airflow, Sql, NoSql, API Integration,Data pipeline, Big Data, AWS/ GCP/ OCI/ AzureRequirements :Understanding our data sets and how to bring them together.Working with our engineering team to support custom solutions offered to the product development.Filling the gap between development, engineering and data ops.Creating, maintaining and documenting scripts to support ongoing custom solutions.Excellent organizational skills, including attention to precise detailsStrong multitasking skills and ability to work in a fast-paced environment3+ years experience with Python to develop scripts.Know your way around RESTFUL APIs.[Able to integrate not necessary to publish]You are familiar with pulling and pushing files from SFTP and AWS S3.Experience with any Cloud solutions including GCP / AWS / OCI / Azure.Familiarity with SQL programming to query and transform data from relational Databases.Familiarity to work with Linux (and Linux work environment).Excellent written and verbal communication skillsExtracting, transforming, and loading data into internal databases and HadoopOptimizing our new and existing data pipelines for speed and reliabilityDeploying product build and product improvementsDocumenting and managing multiple repositories of codeExperience with SQL and NoSQL databases (Casendra, MySQL)Hands-on experience in data pipelining and ETL. (Any of these frameworks/tools: Hadoop, BigQuery, RedShift, Athena)Hands-on experience in AirFlowUnderstanding of best practices, common coding patterns and good practices aroundstoring, partitioning, warehousing and indexing of dataExperience in reading the data from Kafka topic (both live stream and offline)Experience in PySpark and Data framesResponsibilities :You'll :Collaborating across an agile team to continuously design, iterate, and develop big data systems.Extracting, transforming, and loading data into internal databases.Optimizing our new and existing data pipelines for speed and reliability.Deploying new products and product improvements.Documenting and managing multiple repositories of code.",,True,False,True,False,False,False,False,True,False,False,False,False,True,True,True,False
Confidential,Data Engineer (Data Bricks) Manager,"EXL (NASDAQ: EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXL's proprietary Business EXLerator Framework™, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients' decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries. Please visit www.exlservice.com for more information about EXL Analytics. Location - Bangalore/ Gurgaon Note- Currently Remote & expecting candidates to relocate to either Bangalore/Gurgaon once office reopens.Requirements: 7+ years of Data engineering exp with 3+ years hands on Databricks (DB) experience. Should be able to create New Clusters, Cluster Pools and attach existing clusters to pool in DB. Should have some pool management experience. Should be good in Datalakehouse concepts. Should have good experience in Data Engineering in Databricks Batch process, Streaming is good to have. Should have good experience in creating Workflows & scheduling the pipelines. Should have good exposure on how to make packages or libraries available in DB. Should have good experience in Databricks default runtimes, Photon & Light is good to have. Some experience in Databricks SQL / DW in Databricks. Delta Live Tables experience is good to have. IT Services and IT Consulting",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Streamline Digital,Sr. Data Engineer- Azure/ Snowflake/ Databricks,"Sr. Data Engineer

Who We Are

At Streamline, we are experts in Enterprise Mobility, Product Engineering, and IT Transformation. We help organizations navigate the constantly evolving landscape of IT. Our sole focus is ensuring that our client’s organization is armed with the strategies, products and solutions that are transformative to their business. Streamline works closely with our clients, takes pride in developing genuine relationships and embraces open communication and collaboration with our clients. We become a part of our client’s team, working together to achieve short-term goals and enable long-term success. Our team is comprised of world-class strategists, architects, engineers, and developers.

In our new flagship product, iEnterprise, we are taking things to the next level, using our collective experience and customer input to create new enterprise mobility management products that reduce operational costs, prevent issues before they happen, and resolve issues faster than with traditional tools and approaches.
Role Summary

This position is full time remote position. The Data Engineer will work with a team of other software developers to define, design, develop, integrate, and re-engineer the Enterprise Data warehouse. Data Marts, Data Virtualization and Data Visualization components in different environments which meet customers’ analytical and business intelligence requirements, scales easily and supports deployment in highly available environments. The Senior engineer acts as the development and technical lead and individual contributor on complex projects, contributing to strategic vision and technical decisions, participating in vendor analysis and selection projects, introducing process improvements, completing proof-of-concept projects for the introduction of changes to our architecture, and providing oversight on the work of other technical staff.

Role Responsibilities
• Build data-intensive solutions that are highly available, scalable, reliable, secure, and cost-effective
• Create and maintain highly scalable data pipelines using Databricks, Azure, AWS, Kafka.
• Design and build ETL/ELT data pipelines to extract and process data from a variety of external/internal data sources.
• Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery
• Build data insights, data analytics, ML models, fraud and anomaly detection using Snowflake
• Build and deploy modern data solutions.
• Define, implement and build jobs to populate data models.
• Relational and NoSQL Database management
• DevOps building CI/CD pipelines

Technical System Expertise: Understands data warehouse development practices and processes including ETL design, Dimensional models, slowly changing dimensions, Data Security components, Data quality methods, how MPP databases operate, and data flows. Aware of current technology benefits. Expected to independently develop full stack ETL solution from source to staging to presentation layers. Understands the building blocks, interactions, dependencies, and tools required to complete BI Data warehousing software and automation work. An Independent study of current technology is expected. Interact with system engineers to define continuous delivery and/or DevOps solutions, data security, and/or necessary requirements for automation.

Technical Engineering Services: Supports BI Data Warehousing and Enterprise Data Solutions projects by analyzing, designing, and developing ETL solutions; conducting tests and inspections; creating BI reports and virtualization components. Create interface jobs/APIs for data sharing with internal and external stakeholders Ensure we use accurate and secure methods to extract data. Analyze Big Data and MPP Databases to discover trends and patterns. Participates in reviews (walkthroughs) of technical specifications and program code with other members of the DevOps team. Expected to supervise associate engineers on occasion.

Innovation: Presents new ideas which improve existing BI Data Warehousing systems/processes/services. Presents new ideas which utilize new frameworks and reusable components to improve existing ETL jobs/processes/services. Express new perspectives based on an independent study of the industry. Review current company processes to highlight questions that may drive process refinement and optimization.

Technical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the engineer level. Develop application support documentation as required by the application support teams for acceptance of systems changes into production.

Technical Leadership: Collaborates with other engineering, development teams and utilizes data engineering/analyst expertise to deliver technical solutions. Continuously learn and mentor on new technologies.
Qualifications & Skills
• Bachelor’s degree in CS, Statistics, Information systems or equivalent experience.
• Strong expertise working with relational databases. Exceptional ability to author and optimize complex SQL queries/workflows Strong analytical skills to work with unstructured data.
• Experience building and optimizing highly scalable data pipelines, architectures, and data sets.
• A successful history of manipulating, processing, and extracting value from large, disconnected datasets and provide valuable insights.
• Proficiency in workflow orchestration (Databricks, Azure data factory).
• Experience working with streaming data solutions such as Kafka, IoT hub and Spark Streaming.
• Working experience building insights, ML models, fraud and anomaly detection using Snowflake/Databricks.
• Experience working with NoSQL databases like MongoDB, Cassandra, Cosmos DB
• Proficiency in Java/Python/Scala, pandas, pySpark, NumPy
• 8+ years of ETL Development experience using Azure Databricks, Azure data factory, SSIS, Talend.
• 8+ years Professional experience in designing and building cost-effective large scale data marts, data warehousing, distributed big data processing MPP Systems (Databricks, Spark, Snowflake)
• 5+ years of Expertise in Logical and Physical Data Model design using various modelling Tools like Erwin 7.3 and Power Designer.
• 5+ years of experience with Azure and AWS cloud stack
• 3+ years BI Visualization experience developing reports using Snowflake, PowerBI, Grafana, Qlik and analytic cubes utilizing SQL Server Analysis Services (SSAS).

Preferred Skills
• Certifications
• DevOps experience
• Bash, Golang scripting experience
• Docker/Kubernetes experience
• CI/CD pipelines

Powered by JazzHR",Hyderabad,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,True
Whiteforce,Data Engineer - ETL/,"Employment Information

Industry Data Engineer -

Job level

Salary -

Experience -

Pay-Type

Close-date

JOB-IDJB-20453

LocationIndia

Job Descriptions

Key Responsibilities - ETL pipeline design and implementation - Streaming and batch data processingStorage optimization - Storage optimization - DataOps / MLOps - Preparing, cleaning, structuring data for statistical modeling / machine learning - Deployment, packaging, versioning of Machine Learning models that operate on data Educational Qualification & Experience - BE/BTech in Computer Science or Information Systems - (Preferred) ME/MTech in Computer Science or Information Systems - Minimum 5 years total experience working in the industry - Minimum 3-5 years experience in Data Engineering Knowledge and Skills Required - Experience with Big Data technologies: Presto / Trino / Hive / Hadop - Cloud data storage, query and analytics technologies, esp. on AWS - S3 / Athena / Glue or Elastic Stack (specifically for data/ML workflows) - Message passing / data broker systems: Kafka / RabbitMQ

Skills",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Apple,Cloud Data Engineer,"Summary

The people here at Apple don’t just build products — they build the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that inspires the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Imagine what you could do here. Are you passionate about handling large & complex data problems, want to make an impact and have the desire to work on groundbreaking big data technologies? Then we are looking for you. At Apple, phenomenal ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Business Intelligence team is looking for passionate, technical savvy, energetic leader who like to think creatively. Someone who is self motivated and ready to lead team of most hardworking engineers building high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse team deals with Petabytes of data catering to a wide variety of real- time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet Services, enabling business drivers to make critical decisions. We leverage a diverse technology stacks such as Snowflake, AWS, Teradata, HANA, Vertica, Single Store, Dremio, Hadoop, Kafka, Spark, Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.

Key Qualifications

High expertise in modern cloud data lakes and implementation experience on any of the cloud platforms like AWS/GCP/Azure - preferably AWS.

Good Experience in cloud based data warehouse - Snowflake.

Hands on Experience in developing and building data pipelines on Cloud & Hybrid infrastructure for analytical needs- Preferably having Cloud certifications.

Experience in designing and building dimensional data models to improve accessibility, efficiency and quality of data.

Database development experience with Relational or MPP/distributed systems such as Teradata/ SingleStore/ Hadoop

Experience working with data at scale (peta bytes) with big data tech stack and sophisticated programming languages is a plus e:g Python, Scala.

Description

As a Cloud Development Engineer you will design, develop and implement modern cloud data warehouse/ datalakes and influence overall data strategy for the organization

Translate sophisticated business requirements into scalable technical solutions meeting data warehousing/analytics design standards

Strong understanding of analytics needs and proactive-ness to build solutions to improve the efficiency along with that help execute leading data practices & standards

Collaborate with multiple multi-functional teams and work on solutions which has larger impact on Apple business

Ability to communicate effectively, both written and verbal, with technical and non- technical multi-functional teams

You will engage with many other group’s & internal/external teams to deliver best-in-class products in an exciting constantly evolving environment

Education & Experience

Bachelors or Masters Degree in Computer Science or equivalent in Engineering.

Role Number: 200154652",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True
SecureKloud,Data Engineer,"The candidate will work in a cloud development team using Informatics Data Architecture and Data Engineering concepts to deliver foundational data capabilities such as Data Lakes and Master Data Management. The candidate will be part of the DevOps Team and responsible for building data platforms, test cases, deployment documentation, and support documentation in accordance with internal and industry standards. This is a highly technical and hands-on role.

Responsibilities
• Defining database design, data flows, and data integration techniques
• Design and build data solutions ensuring data quality, reliability, and availability
• Create data processing routines for managing enterprise master data throughout the data lifecycle (capture, processing, and consumption)
• Maximize business outcomes using Mobile Device Management via improved data integrity, visibility, and accuracy
• Manage and evolve global data lakes platforms
• Develop data ingestion, data enrichment, data deidentification routines, and RESTful APIs for consumption of data lakes data",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Varite India Private Limited,Data Engineer II (India - Contract),"Description: Location: Gurgaon / Bangalore Contract Duration: 8 months (extension and conversion based on project and performance basis) Shift: Early US (over lapping with India) Please provide 2-3 values or traits that are important to this role: Strong data warehouse and modeling skills You are a self-starter who is highly organized and communicative Deals well with ambiguity Please list 3-4 functional activities the resource should be capable of: Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) What technical skills will successful candidates possess Bachelor's or Master's degree in Computer Science or Engineering with 6+ years of proven experience in related field Experience in building and managing the data warehouse, data pipelines, and data products with data from event streams, on distributed data systems Experience building low-latency data product APIs You value strong pull request reviews, understand when to stand your ground and when to let go You are a self-starter who is highly organized, communicative, quick learner, and team-oriented Successful background as a technical leader driving cross-organizational data initiatives to completion Hands-on coding experience with commonly used programming languages and frameworks for building data pipelines, products, and platform services Deep expertise in data access patterns, data validation, data modeling, database performance, and cost optimization Hands-on knowledge with BI tools, modern OLAP engines such as Presto, Clickhouse, Druid, Pinot, and data processing frameworks such as Spark and Flink Experience establishing and applying standards for operational excellence, code quality, and software engineering best-practices Effective verbal and written communication skills with the ability to think strategically about technology decisions and manage relationships with stakeholders and senior leadership Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage Experience with BI tools like Tableau, DataDog Good understanding of data modeling, schema design patterns and modern data access patterns (including API, stream, data lake) in cloud (AWS preferable) Experience working with SQL & NoSql databases along with programming experience in Python and/or JAVA Strong business acumen with an understanding of business drivers and of how to drive value by supporting data discoverability across the organization Experience building low-latency data product APIs Experience working in an agile environment Tell us about your team: Clientis seeking a data engineer to join the Business Enablement Technology team. As we challenge the status quo and take the CTO's agenda to the next level, your focus will be on building a data platform to enable analytics and reporting capabilities across Product & Technology. This is an exciting, high-impact, and cross-functional role. This role requires a highly inventive individual with strong technical and analytical skills, execution drive, and self-motivation. Is there any industry specific experience that would separate one candidate from another Experience migrating data/apps from on-prem to cloud, engineering degree, tech industry experience",Gurugram,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
ITC Infotech India Ltd,Azure Data Engineer,"• Azure Data Engineer
• Ability to understand the functional & technical specification to develop the Notebooks
• Perform Data Loads and Transformations
• Schedule the Jobs with the Azure Data Factory and monitoring the jobs
• Strong in write complex SQL queries",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
bp,Senior Data Engineer - dataWorx,"Job Profile Summary
Role Synopsis:
As part of bp “reinvent”, we have created a major new business line called “Innovation & Engineering” (I&E). One key remit of this group is to drive the transformation of the company through its use of digital and data. A major digital sub-team within I&E is Digital Production & Business Services (DP&BS). DP&BS are responsible for all digital and data initiatives and operations across the following areas of the bp business:
• Production & Projects including Health, Safety, Environment & Carbon
• Refining & Operations
• Wells & Subsurface
• Business Services including Finance, Procurement, People & Culture, Performance Management
• Strategy & Sustainability
• “DataWorx” is the name of the data team that is responsible for all data within these areas and we are developing deep data capabilities to transform the access, supply, control and quality to our vast and ever growing data reserves that are measured in Petabytes. The DataWorx team covers many data sub-disciplines, including data science, data analytics, data engineering and data management as well as specialist areas such as geospatial, remote sensing, knowledge management and digital twin. The DataWorx team works with a wide variety of data from structured data to unstructured data & we also work on Real-time streaming data processing along with Batch data processing. Key Responsibilities :
• Architects, designs, implements and maintains reliable and scalable data infrastructure
• Leads the team to write, deploy and maintain software to build, integrate, manage, maintain, and quality-assure data
• Architects, designs, develops, and delivers large-scale data ingestion, data processing, and data transformation projects on the Azure cloud
• Mentors and shares knowledge with the team to provide design reviews, discussions and prototypes
• Leads customer discussions from a technical standpoint to deploy, manage, and audit best practices for cloud products
• Leads the team to follow software & data engineering best practices (e.g. technical design and review, unit testing, monitoring, alerting, source control, code review & documentation)
• Leads the team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline
• Leads the team in following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Actively contributes to improve developer velocity
• Part of a cross-disciplinary team working closely with other data engineers, software engineers, data scientists, data managers and business partners in a Scrum/Agile setup
• responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
• Work closely with other data engineers, software engineers, data scientists, data managers and business partners

Job Advert
Job Requirements :
Education :
Bachelor or higher degree in computer science, Engineering, Information Systems or other quantitative fields

Experience :
• Years of experience: 8 to 12 years with minimum of 5 to 7 years relevant experience
• Deep and hands-on experience (typically 5+ years) designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
• Hands on experience with:
• Databricks and using Spark for data processing (batch and/or real-time)
• Configuring Delta Lake on Azure Databricks
• Languages : Python, Scala, SQL
• Cloud platforms : Azure (ideally) or AWS
• Azure Data Factory
• Azure Data Lake, Azure SQL DB, Synapse, and Cosmos DB
• Data Management Gateway, Azure Storage Options, Stream Analytics and Event Hubs
• Designing data solutions in Azure incl. data distributions and partitions, scalability, disaster recovery and high availability
• Data modeling with relational or data-warehouse systems
• Advanced hand-on experience with different query languages
• Azure Devops (or similar tools) for source control & building CI/CD pipelines
• Understanding Data Structures & Algorithms & their performance
• Experience designing and implementing large-scale distributed systems
• Deep knowledge and hands-on experience in technologies across all data lifecycle stages
• Stakeholder management and ability to lead large organizations through influence

Desirable Criteria :
• Strong stakeholder management
• Continuous learning and improvement mindset
• Boy Scout mindset to leave the system better than you found it

Key Behaviours :
• Empathetic: Cares about our people, our community and our planet
• Curious: Seeks to explore and excel
• Creative: Imagines the extraordinary
• Inclusive: Brings out the best in each other

Entity
Innovation & Engineering

Job Family Group
IT&S Group

Relocation available
Yes - Domestic (In country) only

Travel Required
Yes - up to 10%

Time Type
Full time

Country
India

About BP
INNOVATION & ENGINEERING
Join us in creating, growing, and delivering innovation at pace, enabling us to thrive while transitioning to a net zero ‎world. All without compromising our operational risk management.

Working with us, you can do this by:
• deploying our integrated capability and standards in service of our net zero and ‎safety ambitions
• driving our digital transformation and pioneering new business models
• collaborating to deliver competitive customer-focused energy solutions
• originating, scaling and commercialising innovative ideas, and creating ground-breaking new ‎businesses from them
• protecting us by assuring management of our greatest physical and digital risks

Because together we are:
• Originators, builders, guardians and disruptors
• Engineers, technologists, scientists and entrepreneurs‎
• Empathetic, curious, creative and inclusive

Experience Level
Intermediate",Pune,True,False,True,False,False,False,False,True,False,False,True,False,False,False,False,False
Domnic Lewis Private Limited,Big Data Engineer,"we are hiring for Big Data Engineer with the experience in spark , python , SQL , AWS glue Big Data Engineer: Spark, Python, SQL, AWS Cloud (Glue, Lambda, Athena) Hyderabad Location A big data engineer is an information technology (IT) professional who is responsible for designing, building, testing and maintaining complex data processing systems that work with large data sets.",Secunderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Invsto,Data Engineer Intern (2024/2025 graduates),"You are
• A self-starter who is fueled by a desire to improve customer experiences in the moments that matter most, approaching your work with a bias toward accountability, decision-making and action
• Inquisitive and creative, with an ability to listen to identify customer needs and dig deeper to understand the reasons behind those needs
• Able to transform conceptual thinking into deliverables that generate excitement, feedback and alignment among stakeholders
• Thrive in a collaborative environment, partnering across the company with business experts, software developers, data engineers, and marketers

You have
• Enthusiasm to take the initiative to tackle problems and work with others to expand on your experience and expertise
• Experience with Python, AWS and databases such as Postgres
• Know-how to build data engineering pipelines using services such as Airflow

You will

Work closely with and learn from a team of engineers to contribute to a build a variety of features and infra.

Must-Have skills
• Python, Database experience
• Django (in lieu of Django, an equivalent tech stack)

Qualification and Experience:
• 0-2 Years software engineering
• Education: BE/BTech or equivalent (in lieu of academics, equivalent software developer experience required)

Benefits
• Fully remote, forever
• Annual retreats

About Us

Invsto is building the future of financial engineering.

We believe in hiring the best and providing complete autonomy to our employees to build stuff that they think would make a difference to the world

How to Apply

Does this role sound like a good fit? Email us at [hello@invsto.com].
• Include the role's title in your subject line.
• Send along links that best showcase the relevant things you've built and done (Github, Behance, Dribbble etc)

Invsto focuses on Hedge Funds and Stock Exchanges. Their company has offices in Bangalore Urban. They have a small team that's between 11-50 employees.

You can view their website at https://invsto.com/ or find them on LinkedIn.",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False
Everyday Health Group,Data Engineer,"Description

Everyday Health Group (EHG) is a recognized leader in patient and provider education and services attracting an engaged audience of over 74 million health consumers and over 890,000 U.S. practicing physicians and clinicians. Our mission is to drive better clinical and health outcomes through decision-making informed by highly relevant information, data, and analytics. We empower healthcare providers, consumers and payers with trusted content and services delivered through Everyday Health Group’s world-class brands.

Health eCareers, a property of Everyday Health Group, is looking for a Data Engineer to support our growing business.

Key Responsibilities
• Understand overall data collection strategies and implement them in data tables and connections.
• Program Python-based APIs and web services per implementation schema, such as creating applications to access Facebook, Twitter, Salesforce, and other data sources.
• Use SQL Server, MySQL, HDFS and AWS to design, develop and deploy data processing.
• Analyze and organize raw data from various ETL tools.
• Monitor/Forecast computing resources usage (data lakes, AWS, EC2, etc.).
• Collaborate with ETL developers located at various offices (US and India)
• Implement coding standards, procedures and techniques, concluding writing technical code base.
• Detect, repair, prevent data pipeline failures; identify systematic weaknesses and provide pre-emptive remedies with programming or processes.
• Build and optimize data storage systems.
• Prepare data for prescriptive and predictive modeling.
• Recommending and implement emerging database technologies.
• Create automation for repeating database tasks.

Job Qualifications
• 4+ years of programming experience with an emphasis on data processing.
• Coding skills: Python, SQL.
• Ability to create object-oriented programming and data architectures
• Knowledge of new, leading technology strategy in data engineering and management
• Understanding of industry technologies in scalability, performance, delivery pipeline and maintenance of these tools and systems.
• Hands-on experience with SQL database design, AWS or other cloud systems.
• 2+ year’s experience in Linux environments.
• Experience with SQL Server, MySQL or other popular database management tools.
• Good/Expert knowledge of API services
• Familiarity with Agile process
• Clear documentation requirements and specifications
• Bachelors or Advanced Degree in Information Management, Computer Science or related field.

Desirables:
• Experience in AWS, Tableau
• Experience in developing custom data pipelines in HDFS
• Experience working with social, Double Click, Adobe APIs
• Working with teams across multiple locations
• Skills: Pig; Hive; Spark; Impala; EMR

Our Culture and Values

We created our values together to guide our collective purpose and pursuits. We are collaborators and problem solvers. We empower one another to make informed decisions and to be enabled towards action. We embrace success. We recognize that innovation can spark and be born from any of us no matter our individual role or background. We encourage open mindedness and sensitivity to each other and our environment. Our personal and professional passions get ignited, nurtured and supported. We value that doing is greater than talking as the most measurable means of impact. Our collective purpose to deliver enlightened audience experiences with trusted brands is what drives the success of our business and our professional satisfaction.

Life at Everyday Health

At Everyday Health Group, a division of Ziff Davis, we work in a culture of collaboration and welcome those who desire to join our growing global community. We believe in careers versus jobs and people versus employees. We seek enthusiastic individuals with an entrepreneurial spirit looking for an environment that rewards your best work.

#HealtheCareers",,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Unusual Hire,Data Engineer,"Job type: Partial Onsite

Expert Level

Project detail

General:

– Ability to architect Data Science solutions

– Ability to lead a team

– Ability to gather requirements

Must have :

– 6-8 Years Data Science Experience

– At least 4 Data Science solutions

– At least 2 NLP / ML Solutions

– At least 1 solution with Deep learning framework Deep

– Python / PySpark

– Knowledge in Statistical Algorithms like classification, Regression , recommendation and Clustering , Neural Networks

– Azure ML , Databricks , Delta Lake , Data science workspace

Industry Categories

Data Scientist

Languages required

English

₹200 - ₹500

Cost

Location

India

Project ID: 00002514",,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Autodesk,Data Engineer,"Job Requisition ID # 22WD66509 Job Description Position Overview In this role, you will be part of a highly energetic team of engineers working on Autodesk Enterprise Integration Platform to assemble, enrich and enhance the data as per the business needs. You would be part of the team working on providing advisory services and support to business in deriving sales strategies. you will be working to build robust and scalable self-service platform for Autodesk while using innovative technologies in big data space. Responsibilities Design and develop components of end-to-end Enterprise Integration Platform Work with the team to make the Enterprise Integration Platform an efficient, robust and scalable platform Enthusiastic and passionate member of a highly skilled and motivated agile development team Contribute to a team culture that values quality, robustness, and scalability while fostering initiatives and innovation Minimum Qualifications BS or MS in computer science or a related field with 5+ years of experience. 3+ years of experience with cloud ETL/ELTs Working experience in Agile methodologies Strong knowledge and experience in AWS technologies Experience in Matillion and Redshift Strong programming skills in either Java, Scala or Python Experience with Hive and one relational DB Strong problem-solving skills Strong communication skills Ability to learn new technical skills Preferred Qualifications Experience with Matillion Experience with Python, Scala Experience with SQL (Redshift) Experience with Kanban methodology At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. Are you an existing contractor or consultant with Autodesk Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact .",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Finxera,Data Engineer,"Company Description

Finxera, Powered by Priority Technology Holdings, Inc. (NASDAQ: PRTH), is headquartered in Alpharetta, Georgia USA. Our India office is located in Chandigarh, where our dynamic team builds state of the art, sophisticated Fintech products & solutions.

We are an emerging payments powerhouse that offer a single unified platform for Banking & Payments powering modern commerce.

Finxera offers a unique family of products which integrate into SMB Payments, B2B Payments and Enterprise Payments to help businesses thrive. We are on a mission to offer an industry agnostic platform that enables businesses to collect, store and send money using various new age payment methods.

Finxera is an employee-first organisation and we continually strive to ensure their professional and personal success supported by employee friendly policies and a positive work environment built on mutual respect and professionalism. We offer a dynamic work environment, with continuous growth & learning opportunities. We believe in growing together and our people are the driving force behind our success.

Summary Requirement

We are looking for a Data Engineer for a position in the Data and Analytics team.We need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively.

Responsibilities

1. Data Warehousing experience

2. Develop ETL pipelines against traditional databases and distributed systems to allow our team to remain agile in data requirements, and to flexibly produce data back to the business and analytics teams for analysis.

3.Dimensional Modelling experience

4.Database Skills - SQL / PLSQL

5.Python / Spark / Hive / Nifi Knowledge

6.Elasticsearch / Kafka knowledge would be plus

7.Data visualization tools knowledge

8.Participate in design reviews and code reviews

9.Trouble shoot and resolve production issues

10.Resolve performance related issues

11.Knowledge on Data Science / ML / Analytics would be plus

12.Build processes that analyze and monitor data to help maintain data accuracy and completeness.

13.Ability to work with colleagues across global locations remotely

14.Independent and able to work with little supervision

15.Able to mentor and supervise junior team members

16.Agile Methodology Experience

Required Skills & Qualifications

l **Should have scored 70% & above in 10th and 12th grade.

l SQL/PLSQL

l ETL/ETL

l Python

l Pyspark (Optional)

l Any BI Tool like Si Sense, Looker, Quick Sight, Tableau, Power BI, etc (Preferred LOOKER)

l Dimension Modelling (Kimbal) / Data warehousing

l Redshift (Good to have)

l OLAP Cubes",Chandigarh,True,False,True,False,False,False,False,False,True,True,False,False,True,False,False,False
Confidential,Associate - Data Engineer,"JOB DESCRIPTIONRole and responsibilitiesResponsible for overall data analysis (e.g wrangling, cleansing), tools, and technology implementationBuild data systems, pipelines, and workflowsAbility to organize structure/unstructured raw data sources of many types with the ability to combine into useable dataDevelop data wrangling and analytical applicationsCollaborate with data scientists and analyst to management data pipelinesExplore opportunities to enhance quality of data and processesManage a team of data analysts, MIS, and operations resourcesProactive communication with project manager, ensuring all client requirements are met and reports are submitted on timeOversee implementation of tools and technology to build efficiency and consistencyResolve and escalate issues with tools and applicationsDesired candidate profile3-5 years of experience as a data engineer and/or developerStrong background within the role of a data engineer for capabilities such as knowledge of various programming language with a strong emphasis on Python, SQL, Power Query and VBA/MacrosExcellent knowledge in Microsoft Excel and knowledge of advance functionsExperience with SQL set-up and advanced queries developmentStrong technical knowledge in data mining, wrangling, and structuringManaging large volumes of data with the ability to scale as neededAbility to develop and design end-to-end solutions with operations/analyst groups for deliverablesCreating custom scripts and applications to perform wrangling, cleansing, and storingTrouble shoot data issues at any level of project/structure pipeline(s)Presenting data/reports as neededExcellent people management skillsPassionate to drive business metrics - Productivity, Quality, and other key deliverablesAbility to prioritize between multiple complex projects/timelinesExcellent written and Verbal communicationHigh level of positive attitude with good listening skillsAbility to priorities between multiple complex projects/timelinesStrong attention to detail and the ability to conduct root cause analysisCandidates with demonstrated experience in Data Breach Response, or Incident Response will be preferredKnowledge and hands-on experience in breach notification and privacy laws around data breach scenarios is desirable but not must.UnitedLex is committed to preserving the confidentiality, integrity, and availability of all the physical and electronic information assets throughout the organization. Consistent with the UnitedLex ISMS policy and the ISO 27001 standard, every employee is responsible for complying with UnitedLex information security policies and reporting all security concerns, weaknesses, and breaches. Legal Services",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Oil Field Instrumentation (India),DATA ENGINEER,"Experience :

4+ years of Mud Logging experience as Data Engineer in India and abroad, both onshore and offshore rig operations. Deep Water experience will be an advantage. Proficient with real-time data monitoring and data management, WITSML transmission, gas detection and analysis, MLU maintenance.

Qualification :

Graduate or above in Geology, Applied Geology, Geoscience, Petroleum Technology or related, or B.E. in Petroleum Engineering.",Mumbai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
News Corp,Senior Data Engineer,"One of the most innovative and high-profile teams in News Corp is looking for a seasoned data engineer to help accelerate its vision. The role will involve combining all of News Corp’s data across categories, countries and organizations into a singular view of a customer enabling engagement, measurement and targeting. The Dow Jones Senior Data Engineer will partner closely with product, data & engineering teams to help make this data available and accessible to our businesses, teams and partners in order to drive revenue and improve the customer experience.

Responsibilities
• Design, implement and maintain high performance big data infrastructure/systems & big data processing pipelines scaling to billions of structured and unstructured events daily
• Design, implement and maintain deep integration with up-stream systems
• Monitor performance of the data platform and optimize as needed
• Monitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)
• Support products with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
• Data analysis, understanding of business requirements and translation into logical pipelines & processes
• Conduct timely and effective research in response to specific requests (e.g. data collection, summarization, analysis, and synthesis of relevant data and information)
• Evaluate and prototype new technologies in the area of data processing
• Think quickly, communicate clearly and work collaboratively with product, data, engineering, QA and operations teams
• High energy level, strong team player and good work ethic

Technologies we use
• AWS (emr,ec2,glue,athena,redshift,lambda,s3,dynamodb,sns)
• Python, Spark (PySpark)
• Kubernetes, Docker
• Airflow
• Git for source code management
• Jira

Qualifications
• BS in Computer Science or other technical discipline
• 5+ years of experience designing and developing big data processing systems using distributed computing
• Fluency in Python and Spark
• Expert knowledge in optimizing complex SQL queries
• Experience with job orchestration tools like Airflow
• An affinity for automation
• Experience working with cloud platforms such as AWS & GCP
• Familiarity with networking and network application programming, including HTTP/HTTPS, JSON, and REST APIs
• Experience with at least one object oriented language (ex: Java)
• Strong OO design, data structure, and algorithm design skills
• Strong interest in emerging technologies: Hadoop, Hive

Nice to Have
• Experience in the digital advertising and marketing industry
• Contribution to Open Source projects",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,True,False,True,False
Saasvaap,Sr Data Engineer,"JOB DESCRIPTION
• Job Description (Data Engineer) ROLE AND RESPONSIBILITIES You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
• You will support our product, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
• You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
• You will be excited by the prospect of optimizing or even re-designing Peer Data architecture pipeline to support our next generation of products and data initiatives.
• Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implergent internal process improvements: automating manual processes, optimizing data delivery, re-designing hastructure for greater scalability, etc.
• Buld the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS/Azure big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep our data separated and secure across national boundaries through multiple data centers and cloud regions.
• Create data tools for analytics assist in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.
• QUALIFICATIONS AND EDUCATION REQUIREMENTS • 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field PREFERRED SKILLS Experience with Big Data platforms such as Apache Hadoop and Apache Spark Deep understanding of REST, good API design, and OOP principles Experience with object-oriented/object function scripting languages: Python, C#, Scala, etc.
• Experience with relational SQL and NoSQL databases, including Postgres, Cosmos and Cassandra.
• Experience with data pipeline and workflow management tools: Keboola, Stitch, Azkaban, Luigi, Airflow, etc.

SKILLS REQUIRED

My SQL, AWS

EXPERIENCE

4-10

LOCATION

Kochi, Trivandrum

WORK TYPE

FullTime

TIME SHIFT

Day",,True,False,True,False,False,False,False,False,False,False,False,True,False,False,True,False
Confidential,Chistats - Senior Data Engineer - ETL/Data Pipeline,"Day-to-Day Responsibilities :- Create and manage ETL pipelines and job schedulers.- Handle unstructured data and work to automate data ingestion and validation.- Develop APIs with Flask and Python for data mining, transformation and ingestion to AWS.- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.- Write clean, performant code to develop functional applications; build reusable code and libraries- Collaborate with team to evaluate technologies we can leverage, including open-source frameworks, libraries, and tools.- Support Business and application teams with respect to data-related requirements.- Build proactive data validation automation to catch data integrity issues.- Troubleshoot and resolve data issues using critical thinking.Required Qualifications :- Minimum of 3 years of relevant data platform experience (structured/non-structured database, data extraction, data ingestion)- A Degree discipline in Computer Science, Computer Information Systems, or other Engineering Disciplines.- Experience in at least one of these databases: MS SQL, mySQL, PostGreSQL.- Experience in AWS would be ideal and basic cloud infrastructure knowledge.- Experience in Python is required for Web Scraping.- Strong fundamentals in data mining & data processing methodologies- Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.- Build processes supporting data transformation, data structures, metadata, dependency and workload management.Good to have Critical Skills :- Ability to thrive in challenging situations and solve complex problems- Strong bias for action, and see yourself as an 'initiator' and 'problem solver'- Analytical and problem-solving skills- Customer centricity and Good communication (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Calix,Senior Data Engineer - Snowflake,"This position is based in Bangalore, India.

Calix is undergoing a growth transformation, and we are looking for the best and brightest engineers for our Data Engineering team. Our team is facilitating Calix’s transformation into a more data centric enterprise with our business operational leaders. We partner with our operational teams to identify the key points of decision. We create decision support tools that enable optimal data driven selection of business actions and we build out & maintain these decision support tools on a modern data technology stack using DataOps processes. We are building out the data foundations for the next phase of our growth journey. This is a great opportunity to join a rapidly scaling enterprise with a lot of opportunity for personal growth.

The Data Engineering team is seeking a Lead Data Engineer who will be an extraordinary addition to our growing team. You will build and maintain our cloud-based enterprise data platform. Key areas that you will own include data architecture, data modeling, data pipeline flow, data warehousing, security and governance protocols, data integrity processes, and data QA best practices. You will lead buildout of the end-to-end ETL/ELT data environment, integrating with new technologies, and the development of new processes to support the creation and deployment of trusted, accurate and secure decision support tools to the Calix operational business units.

The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

Responsibilities and Duties:
• Lead data modeling, data ingestion, ELT/ETL, and data integration development using our cloud-based tooling including Snowflake, AWS, Fivetran, dbt, Airflow and GitHub.
• Establish and maintain a DataOps approach for our data pipeline infrastructure and processes.
• Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
• Deploy production machine learning pipelines into business operations analytic tooling.
• Ensure data quality throughout all stages of acquisition and processing.
• Create and maintain secure and governed access to the enterprise data warehouse and reporting tools.
• Evaluate new technologies for continuous improvement in data engineering.
• Participate in project meetings, providing input to project plans and providing status updates.
• A desire to work in a collaborative, intellectually curious environment.
• Highly motivated self-starter with a bias to action and a passion for delivering high-quality data solutions.

Qualifications
• 5+ years of experience in related field; preferably experience building and delivering data pipelines, data lakes and ELT solutions at scale.
• Expert knowledge of data architecture, data engineering, data modeling, data warehousing, and data platforms.
• Experience with Snowflake, BigQuery, Redshift, AWS, and pipeline orchestration tools (Fivetran, Stitch, Airflow, etc.).
• Coding proficiency in at least one modern programming language (Python, Java, Ruby, Scala, etc.).
• Deep SQL expertise.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, operations, and technical documentation.
• Excellent verbal and written communication skills and technical writing skills.
• Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership to gain alignment, and drive progress.
• Bachelor’s degree or equivalent experience in Computer Science, Engineering, Management Information Systems (MIS), or related field.

Preferred Qualifications
• Experience with dbt SQL development environment.
• Experience developing and deploying machine learning models in a production environment.
• Experience with Power BI and/or SFDC Einstein/Tableau.
• Experience with Oracle ERP and Oracle Data Cloud tools.

Location:
• Bangalore, India",Bengaluru,True,False,True,True,False,False,False,True,True,True,False,False,True,True,True,True
Paradise Placement Consultancy,Azure Data Engineer (Mercedes-Benz)(BSL),"Job Description : Job Description: Job Description, Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, ETL,Big Data. Responsibilities: Touch base with customers to collect the requirements and analyze them Design and build end-to-end data pipelines to get the data for customers Unit testing of the pipelines and UAT support Deployment and post production support Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents. Adapt changes to the existing scripts, codes and pipelines. Reviewing design, code and other deliverables created by your team to guarantee high-quality results Capable enough to own the PoCs and deliver the results in reasonable time Accommodate and accomplish any ad-hoc assignments Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users. Job Qualifications Qualifications: Bachelor's or Master's degree in Computer Science, Information Technology or equivalent work experience 2+ years of full time data engineering experience 2+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks Skilled in ETL/ELT process. Good working knowledge with ETL tools. Experienced with Hadoop/Big data eco systems Data migration experience from on premise to cloud Expertise in structured query language and PL/SQL Exposure to log analytics and debugging Good to have DevOps, Continuous Deployment and testing techniques Agile development experience Fluent English in spoken and written Mercedes-Benz Research and Development India Private Limited. Preferred Qualifications: Microsoft Azure Certifications Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics Hands-on experience with Databricks notebooks Hands-on experience with Kafka/eventhubs Working experience in an international team or abroad.",,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Kaplan,Senior Data Engineer (Hybrid),"Job Title

Senior Data Engineer (Hybrid)

Job Description

For more than 80 years, Kaplan has been a trailblazer in education and professional advancement. We are a global company at the intersection of education and technology, focused on collaboration, innovation, and creativity to deliver a best-in-class educational experience and make Kaplan a great place to work.

Our offices in India opened in Bengaluru in 2018. Since then, our team has fueled growth and innovation across the organization, impacting students worldwide. We are eager to grow and expand with skilled professionals like you who use their talent to build solutions, enable effective learning, and improve students’ lives.

The future of education is here and we are eager to work alongside those who want to make a positive impact and inspire change in the world around them.

Job Impact and Scope Summary

The Senior Data Engineer at Kaplan North America (KNA) within the Analytics division will work with world class psychometricians, data scientists and business analysts to forever change the face of education. This role is a hands-on technical expert who will own the design and implementation of an Enterprise Data Warehouse powered by AWS RA3 as a key feature of our Lake House architecture.

The perfect candidate is an expert in data warehousing technical components (e.g. data modeling, ETL, reporting). You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be able to work with business customers in a fast-paced environment understanding the business requirements and implementing data & reporting solutions. Above all you should be passionate about working with big data and someone who loves to bring datasets together to answer business questions and drive change.

Responsibilities
• Hands-on technical leader. Continually raises the bar for the data engineering function.
• Leads the design, implementation, and successful delivery of large-scale, critical, or difficult data solutions. These efforts can be either a new data solution or a refactor of an existing solution and include writing a significant portion of the “critical-path” code.
• Sets an example through their code, designs and decisions. Provides insightful code reviews and take ownership of the outcome. (You ship it, you own it.)
• Proactively works to improve data quality and consistency by considering the architecture, not just the code for their solutions.
• Makes insightful contributions to team priorities and overall data approach, influencing the team’s technical and business strategy. Takes the lead in identifying and solving ambiguous problems, architecture deficiencies, or areas where their team bottlenecks the innovations of other teams. Makes data solutions simpler.
• Leads design reviews for their team and actively participates in design reviews of related development projects.
• Communicates ideas effectively to achieve the right outcome for their team and customer. Harmonizes discordant views and leads the resolution of contentious issues.
• Demonstrates technical influence over 1-2 teams, either via a collaborative development effort or by increasing their productivity and effectiveness by driving data engineering best practices (e.g. Code Quality, Data Quality, Logical and Physical Data Modelling, Operational Excellence, Security, etc.).
• Actively participates in the hiring process and is a mentor to others - improving their skills, their knowledge, and their ability to get things done.
• Hybrid Schedule: 3 days remote / 2 days in the office
• 30-day notification period preferred

Requirement:
• In-depth knowledge of the AWS stack (RA3, Redshift, Lambda, Glue, SnS).
• Expertise in data modeling, ETL development and data warehousing.
• 3+ years’ experience with Python, or Java, Scala
• Effective troubleshooting and problem-solving skills
• Strong customer focus, ownership, urgency and drive.
• Excellent verbal and written communication skills and the ability to work well in a team.

Preferred Qualification:
• 3+ years’ experience with AWS services including S3, RA3.
• Ability to distill ambiguous customer requirements into a technical design.
• Experience providing technical leadership and educating other engineers for best practices on data engineering.
• Familiarity with Airflow, Tableau & SSRS.

#LI-Remote

#LI-AK1

Location

Bangalore, KA, India

Additional Locations

Employee Type

Employee

Job Functional Area

Data Analytics/Business Intelligence

Business Unit

00092 Kaplan Health

Kaplan is an Equal Opportunity Employer. All positions with Kaplan are paid at least $15 per hour or$31,200 per year for full-time positions. Compensation for specific positions are based on job level, skills, years of experience, and education, among other factors. Additionally, certain positions are bonus or commission eligible. Information regarding benefits can be found here
.

Diversity & Inclusion Statement:

Diversity inspires innovation and growth in the Kaplan community. Kaplan strives to be a model employer for inclusiveness. Not only does Kaplan value its employees for their professionalism and skills, but also for the unique viewpoints they bring to the Organization. Kaplan's employees bring diverse perspectives, ideas, and backgrounds that give Kaplan a competitive edge in anticipating and exceeding our students' needs in today's global market. Learn more about our culture
.",Bengaluru,True,False,False,True,False,False,False,True,False,True,False,False,True,False,True,False
"Planview, Inc.",Data Engineer,"Overview

Planview is looking for a passionate data engineer to join our team innovating tools for connected work. You will work closely with other data engineers, data scientists and individual product teams to specify, validate, prototype, scale, and deploy features with a consistent customer experience across the Planview product suite.

Responsibilities
• Ability to work in a fast paced start up mindset. Should be able to manage all aspects of AI/ML activities
• Develop platforms that make data across applications/application deployments available for AI/ML-driven feature prototyping, proofs-of-concept, and general availability
• Refine data pipelines for analysis, while refining, automating, and scaling as needed for the use-case at hand
• Work on various aspects of the AI/ML ecosystem – data modelling, data pipelines, data observability, data documentation, scaling, deployment, monitoring and maintenance etc.
• Work closely with Data scientists and MLOps Engineers to come up with scalable system and model architectures for enabling real-time ML/AI services

Qualifications

Required qualifications
• Masters or equivalent experience in Informatics, CS, Data Science or a related field
• 2+ years of experience as a data engineer or data scientist with a focus on data engineering for ML applications
• Strong Python and SQL coding skills
• Familiar with AWS Data and ML Technologies (AWS Sagemaker, Data Pipeline, Glue, Athena, Redshift etc)

Preferred Qualifications
• Demonstrated experience building data and analytics pipelines/services that efficiently scale for cloud application usage, meeting a product team’s SLA for performance and resilience
• Exposure to database queries and strong in SQL
• Exposure to any of the libraries and frameworks in data science (Pandas, Numpy, Dask, PySpark etc)
• Exposure to data version control (DVC) and orchestration tools (Airflow, etc)
• AWS Certification is a plus
• Skilled at working as part of a global, diverse workforce of high-performing individuals
• AWS Certification is a plus",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,False
Full Potential Solutions,Lead Data Engineering,"Overview:

About The Company

Full Potential Solutions (FPS Perch) is a global product company headquartered in the US. We help contact centers around the world better engage with their customers by combining our deep domain expertise with cutting-edge technology. Our AI enabled, cloud-scalable analytics products enhance the end-to-end customer journey via omnichannel engagement (voice, email, SMS, chat and conversational AI) and optimize agent performance. Our offices are spread across the US, India and Philippines.

Some exciting initiatives they are working on are:
• Designing and development amazing user experiences across data visualization and analytics in the customer engagement / contact center industry
• Salesforce based solutions for omni-channel customer engagement leveraging AI to enhance contact strategy and routing, guide agents on next best action (NBA) enable dynamic scripting, capture voice of customer (VOC)
• Developing standardized AI/ML models to optimize customer engagement and agent performance using various Python frameworks and AWS services. (Transcribe, Comprehend, Sagemaker, etc.)
• Developing a comprehensive Data, Analytics and Reporting platform to integrate, measure and analyze millions of daily customer interactions/metrics across all our clients.
• Building web/mobile apps to improve agent/manager performance across thousands of agents (metrics, gamification, collaboration).
• Deploying cutting-edge, tech-enablement services using the AWS ecosystem, including microservices and serverless architecture, CI/CD pipelines, event notification & search services, multi-tenant architecture ec.,

Our team has successfully built and scaled analytics-focused products at companies such as Salesforce, Demandware, and IBM. We have strong backgrounds in Computer Science, Math and Engineering from top universities such as IIT, Harvard and Yale. We strongly believe that empowered people are the key to building a great company, and our development process focuses on iteratively improving our products as well as ourselves as individuals and as a team. Our mission as a company is to create an environment where the people THRIVE.

Explore more at: FPS Website

Leadership Team: Explore here

About The Team

Our analytics team helps our clients leverage data to optimize their performance and results. Our Data Analysts possess a unique skill of understanding the data, making a clear sense of it, and telling a story to the business. We work closely with the Analytics Leaders and the rest of the Analytics product team to analyze and understand customer needs, explore granular data to identify key drivers that influence each KPI, measure the level of expression of attributes that move the KPI, and execute in a way that brings the best solutions to life. Most importantly our team exhibits our core values: Integrity, Excellence, Accountability and Grace.

Responsibilities:

Functional Requirements
• Data exploration skills to unravel data sources and identify the granularity, attributes & measure
• Discover data sources and determine the best EL approach and tools to bring data into the D
• Determine the data cleansing rules based on source data and ensure data replication accurac
• Coordinate & collaborate with data architects and BI engineers to ensure timely project deliver
• Understanding of data warehouse concepts and implementation methods of warehouse object
• Translate the data models from the ER diagram into physical models coded into data warehous
• Innovate reusable and configurable frameworks for data replication to build the bronze laye
• Well-versed with the data warehouse modeling concepts (star/snowflake/denormalized structures
• Define data quality rules, modeling standards, business metrics, standard processes and test case
• Understand the domain, business cases, objectives, and KPIs that need to be reported and analyse
• Participate in data discovery phase and capture the data sources, required KPIs with the formulae

Qualifications:

Technical Requirements
• Bachelor’s degree in a Technical/Quantitative subject such as Computer Science (B.E/B.Tech/MCA
• 7 - 9 years of relevant experience in the field of Data Engineering and Data Warehouse/BI solution
• Proficient in SQL, query optimization, coding stored procedures, and ad-hoc data analysis using SQ
• Develop data pipelines using some combination of ETL/ELT tools and data processing framework
• Develop DBT models as per the data model and implement the data transformations & test case
• Orchestrate data integration & transformation processing from sources to ssots in data warehous
• Design, develop, monitor and re-engineer database objects and processes/pipelines/schedules
• Use AWS services for data storage, access and retrieval and application setup and monitorin
• Design & document the data processing workflows and setup systems as per solution architecture
• Design the multi-dimensional data model in data warehouse in order to meet BI reporting needs
• Setup software engineering best practices for data pipelines with data-associated documentation
• CI/CD implementation with source code version control, QA checks and deployment automation
• Hands-on with languages like Spark, Python, Scala, R, Bash/Shell Scripting or stored procedures
• Must have worked with RDS or data lakes (MySQL, PostgreSQL, Oracle, Redshift, Snowflake, etc.)
• Hands-on experience in using ETL/ELT tools like Talend, Informatica, SSIS, Airbyte, Alteryx, Stitch etc.,
• Experience working with orchestration tools such as Airflow, Astronomer, Control-M, Prefect etc.,
• Experience working with DevOps tools such as Bitbucket, Github, Gitlab, Jenkins, CircleCI etc.,
• Experience working with AWS Cloud services such as S3, EC2, ECS, EFS, Lambda, Docker, Kubernetes
• Hands-on experience in using data-modeling tools like Erwin, DBSchema, MySQL Workbench, ER Studio

Leadership Responsibilities
• Lead a squad of data engineers to implement data warehouse solution across various projects
• Ensure adherence to the best practices of data engineering and testing across all projects
• Drive sprint planning/review and lead daily Kanban meetings in alignment with project plan
• Allocate work smartly and efficiently manage/ coach the team while monitoring the progress
• Develop systematic training plan for onboarding new joiners and upskilling existing resources",,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Kyndryl,Azure Data Engineer,"Why Kyndryl Kyndryl is a market leader that thinks and acts like a start-up. We design, build, manage, and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl We are always moving forward - always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers, and our communities. We invest heavily in you - not only through learning, training, and career development, but also through the flexible working practices and stellar benefits that help you grow and progress long-term. And we give back - from planting 90,000 trees in our first 3 months as part of our One Tree Planted initiative to the Corporate Social Responsibility and Environment, Social and Governance practices embedded within everything we do, we are committed to powering human progress in an ethical, sustainable way. Your Role and Responsibilities Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Create and maintain clear and complete pipeline documentation, e.g., architecture diagrams and data transformations. Integrates data solutions with business processes. Translates business problems to data problems. Communicates effectively -verbally and in writing -across levels and organizations, e.g., junior vs. executive, technical vs. business, and internal vs. customer. Rigorously visualizes data in order to both preserve and clarify the messages within. Provide actionable insights via compelling storytelling to drive business outcomes. Microsoft Certified: Azure Data Engineer Associate Required Technical and Professional Expertise Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Preferred Technical and Professional Experience Data Engineers design and build data platforms. Ensure availability of clean and normalized data sets and adhere to regulations and policies. Using a well-defined methodology, critical thinking, domain expertise, consulting, and software engineering. Required Education Associate's Degree/College Diploma",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Role: AWS Data Engineer

Ex- 4 to 8 YRS

Locations- Delhi/NCR, Gurgaon, Bangalore, Ahmedabad, Pune, Indore, Mumbai, Kolkata

Must Have –
• Working on EMR, good knowledge of CDK and setting up ETL and Data pipeline
• Coding - Python
• AWS EMR, Athina, Supergule, Sagemaker, Sagemaker Studio
• Data security & encryption
• ML / AI
• Pipeline
• Redshift
• AWS Lambda

Expectations/Responsibility
• Industry experience in Data Engineering on AWS cloud with glue, redshift , Athena experience.
• Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python.
• 3+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
• Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.
• Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
• AWS engineer provides comprehensive systems administration functions on Amazon Web Services (AWS) infrastructure to include support of AWS products such as: AWS Console root user administration, Key Management, EC2 Compute, S3 Storage, Relational Database Service (RDS), AWS Networking & Content delivery (VPC, Route 53, ELB, etc.) Identity & Access Management, CloudWatch, CloudTrail, Cloud Formation, Auto Scaling, Cost and Usage Reports, and more.
• Train and guide the company’s HR engineering team on developing with aforementioned AWS tools, while also executing on specific deliverables (ingestion, Storage, integration, warehouse, visualization)
• Coach and mentor other technical resources on the team on AWS technologies
• Create ETL piplelines that are highly optimized with very large data sets
• Solve issues with data models and come up with solutions
• Developing and directing software system testing and validation procedures, programming and documentation
• Analyzing user needs and requirements to determine feasibility of design within time and cost constraints
• Provide technology expertise, direction, coordination, and consultation, in the development, integration, launch, scaling, and maintenance of new and existing products and solutions
• Establishes infrastructure technology architectures, standards, test plans, design templates and governance
• Works with the team to define standards and frameworks with regards to coding, programming, and the general development of applications for multiple platforms
• Work with business teams to understand customer issues and to investigate, prototype and deliver new and innovative solutions

FRESHERS DO NOT APPLY.",New Delhi,True,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False
Mercede,Azure Data Engineer-Karnataka-Bangalore,"Azure Data Engineer

Keywords: Microsoft Azure, Azure Data Factory, ADLS, Log Analytics, ELT, Big Data, Azure DevOps, Azure Boards, VSTS Git

Hey, do you want to change the world? Build #TheNextBigThing? Which means implementing ideas that are said to be unachievable? Like it was stated at first about smart chatbots or artificial intelligence. At MBRDI you are dealing with questions, for which there are no answers. Not yet.

About Us

The Corporate Center of Excellence (CoE) for AI, Advanced Analytics and Big Data is working on #TheNextBigThing. Besides conducting cool use cases in the field of Data Analytics and AI, we are inventing, building and running eXtollo a data analytics cloud platform based on Microsoft Azure for Daimler teams around the world. Our users trust in the cross-divisional platform to create secure and scalable cloud applications based on Machine Learning, Artificial Intelligence Big Data technologies. In addition to the provisioning and utilization of the technology we are also responsible for Daimler s data treasure the eXtollo Data Lake.

Role

We are searching you as a Data Engineer to continuously improve eXtollo within an agile working environment. You are expected to work directly with customers to understand their data demands and build end to end pipelines to get the data as they wish. Also be able to understand the current codes, pipelines and scripts which are already in production in a short time and support operations/changes.

Experience of data migration into Azure from various cloud and on-prem is a plus. You are expected to have good exposure to structured query language preferably MS Sql Server. Should be able to do PoC for the new adaptions and work independently with minimal guidance. Data warehouse/data engineering experience is appreciated. Most importantly the candidate should have real hands-on experience rather bookish/training experience.

Responsibilities
• Touch base with customers to collect the requirements and analyze them
• Design and build end-to-end data pipelines to get the data for customers
• Unit testing of the pipelines and UAT support
• Deployment and post production support
• Understand the current codes, pipelines and scripts in production and fix the issues in case of incidents.
• Adapt changes to the existing scripts, codes and pipelines.
• Reviewing design, code and other deliverables created by your team to guarantee high-quality results
• Capable enough to own the PoCs and deliver the results in reasonable time
• Accommodate and accomplish any ad-hoc assignments
• Building CI/CD pipelines in Azure DevOps to deliver our services into various data centers worldwide
• Analyzing and solving incidents in productive environment while avoiding data loss and minimizing service outages
• Challenging current practices, giving feedback to colleagues and encouraging software development best-practices to build the best solution for our users
Job Qualifications

Qualifications
• Bachelor s or Master s degree in Computer Science, Information Technology or equivalent work experience
• 3+ years of full time data engineering experience
• 3+ years of work experience with Azure and Azure Data Factory, Storage accounts and Notebooks
• Skilled in ETL/ELT process. Good working knowledge with ETL tools.
• Must be experienced with Hadoop/Big data eco systems
• Data migration experience from on premise to cloud
• Expertise in structured query language and PL/SQL
• Experienced in Powershell scripting for orchestration
• Exposure to log analytics and debugging
• Good knowledge in DevOps, Continuous Deployment and testing techniques
• Agile development experience
• Fluent English in spoken and written

Preferred Qualifications
• Microsoft Azure Certifications
• Working experience with Data Factory, Data Lake Storage, Role Based Access Control and Log Analytics
• Hands-on experience with Databricks notebooks
• Working experience in an international team or abroad
,

This job is provided by Shine.com",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
HARMAN International,Data Engineer,"What You Will Do :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis What You Need :

Bachelors Master&rsquos degree in information technology/Computer Science, Engineering (or equivalent) ? 7-10 years of experience in Azure Data Engineering.
• Experience working with Hadoop or other Big Data platform and building solutions that deliver value through leveraging data as an asset
• ETL technologies: at least one of the following: SSIS, Talend, Pentaho, Informatica, etc.
• End-to-end Microsoft BI & Data stack: Analytics Platform Server, HD Insight, Stream Insight, SharePoint BI, PowerPivot, Power View
• Applied experience in Reporting Tools like Power BI , Tableau etc.
• Applied experience and skills with API Development
• Applied experience with Cloud platforms including AWS and/or Azure
• Experience working with various types of data sources such as SAP, JDE, legacy ERP systems

Mandatory Skills
• Big Data stack: Spark, Hadoop, Sqoop, Pig, Hive, Hbase, Flume, Kafka, Storm
• Develop data pipelines using Azure Data Factory.
• Develop data storage layer using Azure SQL DB & Azure Data Lake.
• Develop dashboards & Reports using Power BI.
• Knowledge of developing real-time data ingestion pipelines.
• Design & develop data platform using Azure Databricks to process data.
• Develop notebooks & jobs using Python language.
• Understanding of messaging infrastructure like event hub, service bus & notification hub.
• Should have good understanding on Azure Cosmos DB, Azure DW Duties and Responsibilities
• Work with the application development team to implement data strategies, build data flows
• Should have expertise across the database. Responsible for gathering and analyzing data requirements and data flows to other systems
• Lead multiple scrum teams Preferred Interpersonal Skills
• Good verbal and written communication skill
• Team Player
• Team-working &ndash being able to work with others in groups and teams
• Problem solving and decision-making &ndash working with others to identify, define and solve problems, which includes making decisions about the best course of action
• Strong conceptual strength, strategic thinking, problem solving, technical, and analytical skills.
• Ability to ask next level questions anticipating business inquiries and performing root cause analysis",Bengaluru,True,False,True,False,False,False,False,False,True,True,False,False,False,False,False,False
GSPANN Technologies,Azure Data Engineer,"Should have experience in ADLS (Azure Data Lake storage) Experience implementing Azure Data Factory Pipelines using latest technologies and techniques Experience in working on Azure HDInsight Experience in working with Storage Strategy Azure developer should be able to ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements Expert in Azure Data Factory, Azure Data Lake Azure SQL Data Warehouse, Azure Functions, Databricks · Comfortable working with Spark, Python, and PowerShell Excellent problem solving, Critical and Analytical thinking skills Strong t-SQL skills with experience in Azure SQL DW DevOps, CI/CD, and Automation experience strongly preferred Able to interact with team members collaboratively Experience handling Structured and unstructured datasets Experience in Data Modelling and Advanced SQL techniques",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Uber,Data Engineer II,"What You'll Do
• Responsible for defining the Source of Truth (SOT), Dataset design for multiple Uber teams.
• Identify unified data models collaborating with Data Science teams
• Streamline data processing of the original event sources and consolidate them in source of truth event logs
• Build and maintain real-time/batch data pipelines that can consolidate and clean up usage analytics
• Build systems that monitor data losses from different sources and improve the data quality
• Owns the data quality and reliability of the Tier-1 & Tier-2 datasets including maintaining their SLAs, TTL and consumption
• Devise strategies to consolidate and compensate the data losses by correlating different sources
• Solve challenging data problems with cutting-edge design and algorithms.

What You'll Need
• 4+ years of extensive Data engineering experience working with large data volumes and different sources of data.
• Strong data modeling skills, domain knowledge, and domain mapping experience.
• Strong experience in using SQL language and writing complex queries.
• Experience with using other programming languages like Java, Scala, Python
• Good problem-solving and analytical skills
• Good communication, mentoring, and collaboration skills.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Cardinal Health,"Sr. Data Engineer, Data Engineering","Job function:

IT Quality Control is responsible for owning and implementing software testing and certification strategies for the enterprise. Debugs problems with software through standard tests and recommends solutions. Conducts defect trend analysis and continuous process improvement. Demonstrates knowledge of requirement and risk based testing principles, theories, concepts and techniques. Establishes internal IT service quality control standards, policies and procedures.

Job duties:

Create Test Strategy, define quality standards for Google Cloud Platform and define metrics to measure the efficiency and testing for applications on Google BigQuery, Dataflow and Airflow. Develop and maintain test automation frameworks, build regression test strategy and continuous testing process.

Skills:
• Ability to create test strategy balancing manual and automated testing
• 8+ years of experience with designing and implementing test frameworks in cloud
• Technical skills – SQL, Java/Python
• Good communication and collaboration skills across teams and business SMEs
• Should exhibit continuous testing mindset
• Knowledge on Devops and CI/CD process
• Develop automated test cases to be used in performance testing or as part of testing
• Identify and implement performance metrics to be measured
• Collaborate with functional and technical teams to identify test data or create through UI and database
• Generate automation or performance testing reports from execution
• Maintain record of test discrepancies, using designated QA tools
• Provide feedback of test results to development and infrastructure teams for resolution
• Review Business Requirements Documents and Functional and Technical Specifications towards determining test data scope
• Oversee defects from initial identification through post-deployment analysis
• Coordinate with other QA engineers, leadership, system administrators, architects and developers

What is expected of you and others at this level:
• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects
• Participates in the development of policies and procedures to achieve specific goals
• Recommends new practices, processes, metrics, or models
• Works on or may lead complex projects of large scope
• Projects may have significant and long-term impact
• Provides solutions which may set precedent
• Apply design thinking mindset
• Independently determines method for completion of new projects
• Receives guidance on overall project objectives
• Acts as a mentor to less experienced colleagues

Cardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,True,True,False
Siemens Energy,Data Engineer (PMK Project),"A Snapshot ofYour Day

A Our team ofdata engineers supports several business teams to get needed data, prepare itproperly for the particular use cases and make it available in an efficientway. You are integrated in the business team and also work on business topics.

One strongexample is the Prescriptive Marketing team. They create forecasts for energymarkets about power prices, demand and other key factors. This is used to helpcustomers to improve their plants and thus their revenues. It is essential alsofor the data engineers to understand the big picture while fulfilling the datarequests.

You discussthe scope of the data and how to make it available, followed by implementingand maintaining data pipelines, normally Python based, with generic setups thatcan be re-used easily to scale up the market models. There is a wide range oftasks, from doing research about data sources up to presenting the data indashboards.

How You’ll Makean Impact
• As a data engineer you willsupport the team by using, building and maintaining ETL tools and pipelines forcollecting, transferring and preparing data for several use case like internalanalytics or consumer facing applications.
• You should be able to deliverrapidly in a reliable manner with the highest quality standards.
• You should be curious,passionate about problem solving, building and self-improving.

WhatYou Bring
• Master’s / bachelor’s degree in computer science or similar with a minimum 5 years of experience in the field of Software Development and Architecture.
• Deliver rapidly in a reliable manner with the highest quality standards
• Curious, passionate about problem solving, building and self-improving
• Experience in Agile development
• Fluent English Skills
• Experience working in and with international teams and stakeholders with different cultures

Technical Skills:
• Highly experienced in relational database setups and data warehouse environments
• Snowflake knowledge is a plus
• Solid experience with building ETL pipelines
• Strong SQL skills (aggregations, windows functions, pivoting etc.)
• Python knowledge
• Solid experience in software development, incl. design patterns is a plus
• Experience with OOP concepts (e. g. Java/C# background) is a plus

Working experience:
• version control systems (Gitlab) and tools like Confluence and Jira
• deployments with CI/CD pipelines
• AWS cloud environments
• Time series data
• integrating and managing structured and unstructured data in cloud based data management systems for example with PostgreSQL or Redshift, Snowflake is a plus
• Tableau dashboards is a plus

About The Team

""Let’s make tomorrowdifferent today"" is our genuine commitment at Siemens Energy to allcustomers and employees on the way to a sustainable future.

Ourteam belongs to the Software Engineering andProduct Development Function within Siemens Energy. Our missionis to grow the digital software business and develop solutions and products forour internal and external customers. This covers from edge data acquisition,data lake and processing up to development of digital twins and apps aroundcustomer assets.

Who is Siemens Energy?

At SiemensEnergy, we are more than just an energy technology company. We meet thegrowing energy demand across 90+ countries while ensuring our climate isprotected. With more than 92,000 dedicated employees, we not only generateelectricity for over 16% of the global community, but we’re also using ourtechnology to help protect people and the environment.

Ourglobal team is committed to making sustainable, reliable, and affordable energya reality by pushing the boundaries of what is possible. We uphold a 150-yearlegacy of innovation that encourages our search for people who will support ourfocus on decarbonization, new technologies, and energy transformation.

Our Commitment to Diversity

Lucky for us, we are not all the same.Through diversity we generate power. We run on inclusion and our combinedcreative energy is fueled by over 130 nationalities. Siemens Energy celebratescharacter – no matter what ethnic background, gender, age, religion, identity,or disability. We energize society, all of society, and we do not discriminatebased on our differences.

Rewards/Benefits
• The opportunity to engage inan exciting environment on challenging projects
• Strong professional supportand working with colleagues around the world
• Professional developmentopportunities within the company
• To be part of a growingfunction with a dynamic, informal, and inspiring working environment in aposition that entails a large responsibility
• Medical benefits
• Remote/Flexible work
• Time off/Paid holidays
• Parental leave
• Continual learning throughthe Learn@Siemens-Energy platform

https://jobs.siemens-energy.com/jobs",Gurugram,True,False,True,True,False,False,False,False,False,True,False,False,True,False,False,True
Snowflake,Data Engineer,"Build the future of data. Join the Snowflake team.

Snowflake started with a clear vision: make modern data warehousing effective, affordable, and accessible to all data users. Because traditional on-premises and cloud solutions struggle with this, Snowflake developed an innovative product with a new built-for-the-cloud architecture that combines the power of data warehousing, the flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions.

In addition, Snowflake’s culture was built on the following values that are even more important to us today:
• Put Customers First. We only succeed when our customers succeed
• Integrity Always. Be open, honest, and respectful
• Think Big. Be ambitious and have big goals
• Be Excellent. Quality and excellence count in everything we do
• Get It Done. Results matter!
• Own It
• Make Each Other the Best
• Embrace each others’ Differences

Job Description
• Interface with data scientists, product managers, and business stakeholders to understand data needs and help build data infrastructure that scales across the company
• Drive the design, building, and launching of new data models and data pipelines in production
• Build data expertise and own data quality for allocated areas of ownership
• Align to Product roadmap in building tools for data platform users
• Mature requirement and follow design develop and communicate model using Agile methodology for data ingestion and data tooling.

MINIMUM QUALIFICATION
• Expertise in SQL statements and modeling concepts.
• Must be aware of the cloud environment from data ingestion and modeling perspective.
• Must be strong in python
• Experience with Apache Airflow is highly desirable.
• schema design and dimensional data modeling.
• custom ETL design, implementation and maintenance.
• object-oriented programming languages.
• Understanding of API and connectors is highly desirable
• analyzing data to identify deliverables, gaps and inconsistencies.
• Experience in data warehouse space.

PREFERRED QUALIFICATION :
• BE/BTECH in Computer Science, IT or other technical field.
• Experience with data ingestions and data analytics.
• 4 years experience using Python and SQL, .",Pune,True,False,True,False,False,False,False,True,False,False,False,False,False,False,True,True
PepsiCo,Data Engineer,"Overview

This role is with the Global business services arm of Media Center of Excellence (CoE) at PepsiCo.

In this role, you will play a key role in shaping the future of media measurement strategy for PepsiCo, esp. with focus on building an understanding role of media as key Growth Driver thru ROI and related analytics.

You will be laying down strong data foundation through implementation of process & technology.

This role is the backbone of media measurement agenda at Pepsico and is responsible for building data systems and pipelines to feed into prescriptive and predictive modeling by establishing and enhancing process around data capture, storage, quality and reliability.

You will work with Media, Data engineering, Data Science and IT teams globally and within AMESA sector to identify best practices in the industry and across all PepsiCo’s brands, providing support to codify and scale best in-class methods that inspire continuous improvement in marketing effectiveness and ROIs.

Responsibilities
• Collect, structure, analyze, organize and maintain RAW data from various data sources needed for creating predictive models in structured databases in order to ensure faster model building
• Partner with PepsiCo functional teams, agencies and third parties to build seamless process for acquiring, tagging, cataloging and managing all media, Nielsen and internal data periodically in structured format as needed for measurement statistical models
• Design, build and codify data structures in efficient way to periodically feed in raw data from various internal and external sources and also manage and house model outputs for quick input to businesses;
• Build data systems and pipelines as per business needs and objectives, in this case prepare data to feed specifically to MMM and media measurement models or any descriptive or prescriptive analysis
• Promote data consistency globally to support common standards and analytics
• Establish periodic data verification processes to ensure data accuracy
• Build new technologies and algorithms to optimize any business process around creation and maintenance of databases/data lakes running of batch processes for data updation

Qualifications
• 3-6 years of experience in the field of data structures, building and managing data lakes
• Hands on experience in building database/Data Management Solutions
• Mandatory experience Python, Data modelling, data pipeline set up and meta data management
• Experience in relational databases as well as unstructured data streams
• Experience with schema design and dimensional data modeling (for ex: using data vault/snowflake/star schema)
• Hands on experience, Airflow (or any other Orchestration tools)
• Hands on experience in AWS (or any other cloud operator)
• Good to have experience on technologies like, DBT
• Good to have experience in data engineering teams in consulting or ecom/telecom sector
• Desirable - Experience optimizing larger applications to increase speed, scalability, and extensibility
• Educational Background- BE/B T ECH/ MS in computer science or related technical field",Peeramcheru,True,False,False,False,False,False,False,True,False,False,False,False,False,False,True,True
HTC Global Services,Data Engineer-GCP,"Greetings from HTC Global Services

We are hiring Data Engineer- GCP

Skills Required:

Data Engineer- GCP

Experience with Big query, Terraform ,Hive

Experience:

3+ Years

Location:

Chennai

Notice:

Looking for candidates who can join within 15 days.

Interested candidates please drop your resume to sunitha.manohar@htcinc.com

Regards

Sunitha",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
HP,Senior Data Engineer,"The Company

HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations and dreams. We apply new thinking and ideas to create simpler, more valuable and trusted experiences with technology, continuously improving the way our customers live and work.

Position background

In the GTM analytics COE our mission is to deliver impact by building machine learning products to optimize pricing and marketing investments and provide guidance to our sales organization.

As a Big Data Engineer, you will be in a unique position to support the development one of our internal assets. You will work together with the project and asset team to understand the end state in which the data must be delivered and you will model the data using Big Data technologies like Spark.

We offer an international experience, collaborative culture, top rate experience in AI and ML and opportunity to create significant real-world impact.

What You Will Do

Create / Maintain ETL pipelines.

Ensure that processes are optimized.

Use Spark to model big volumes of data.

Contribute to the database architecture, design and implementation.

What You Will Need

Bachelor’s in computer engineering, Computer Science, Electrical Engineering, Robotics or a related field

2+ years on a similar role.Ability to work independently under a fast-paced environment, comfortable to deliver results under pressure.

You Have Strong Problem-solving Skills.Agile Experience.

Experience with modern application lifecycle management tools (Git, Visual Studio, Intellij, Code Reviews).

Proficient in at least one of the following languages: Python, PySpark, Scala, Spark, SparkR.

Experience with SQL & NoSQL databases is preferred (PostgreSQL, MongoDB, Elastic Search).

Experience in working with DataIku DSS software.

Strong analytical skills with demonstrated problem solving ability.

Who We Are

At HP, we believe in the power of ideas. We use ideas to put technology to work for everyone. And we believe that ideas thrive best in a culture of teamwork. That is why everyone – at every level in every function, is encouraged to think big, have original ideas and express and share them. We trust anything can be achieved if you really believe in it, and we will invest in your ideas to change lives and the way people work. This vision is what sets us apart as a company. At HP, we work across borders and without limits. Global virtual teams share resources, pool their big ideas to solve our biggest business opportunities. Everyone is valued for the unique skills, experiences and perspective they bring. That’s how we work at HP. And this is how ideas and people grow.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
DataTheta,Azure Data Engineer,"Experience Required: 5-10 Years

Location: Noida/Chennai

Azure Data Engineer | Job Description
• Dev / Architect
• 2+ years of experience in Azure cloud data stack such as Synapse/DW, Azure SQL DB, Azure Blob Storage
• 1+ years of experience in Logic Apps
• 3+ years of experience in Python
• 5+ years of experience in SQL
• 3+ years of experience in Databricks
• 2+ years of experience in Azure/AWS Lambda Functions
• 2+ years of experience in Microservices (REST) architecture
• Ability to project manage and work within an agile, flexible environment.
• Performs peer reviews for other data engineers’ work.
• Ensuring adherence to programming and documentation policies, software development, testing and release.
• Develop modeling, design, and coding practices.
• Experience with Lean / Agile development methodologies
• Positive attitude with great collaboration and communication skills",Noida,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Tech Mahindra,GCP Data Engineer,"Job Role - GCP Data Engineer
• Looking only GCP Data Engineers. (GCP Certification is not mandatory)
• Experience should be 4-8 Years Max.
• Candidates should be aware about BigQuery, Data Flow, Composures.
• GCP Services, SQL, Migration Process
• Migration Tools ( Plate spin, Cloud Physics, Stratozone)
• Work Location Pune/Bangalore/Noida/Chennai",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False,False
MOURI Tech,Sr. Data Engineer,"Hi Folks,

Greetings from Mouritech!!

We are hiring for Sr. Data Engineer

Mandatory Skill: Data Engineer, Python, SQL, DWH & GCP

Location: Gurgaon (Hybrid)

Exp: 4+ Yrs to 12 Yrs

Notice Period: Immediate to 30 Days Serving.

If interested pls share your profile on below mail id

surabhim.in@mouritech.com.",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
NAB,Senior Data Engineer [T500-7047],"Essential Skills:
• Advanced SQL skills (or equivalent database querying language), Database SQL skills (MySQL preferably), Able to write complex queries (subquery, window function, CTE, removing duplication), Able to analyse query bottleneck by “Explain” command.
• Construct RMDB database-Database modelling skills, Able to operate DDL (stored procedure, indexing, trigger, table, materialised view, view), Understand database modelling (normalisation)
• Develop, maintain and Implement Power BI dashboards,
• Understand batch job process, able to modify it, and solve batch job issue- Python (ODBC DB connection, Pandas, XML handling), PowerShell (General PS command), ETL experience.
• Extract meaningful insights out of the data.
• AWS experience -AWS EC2, RDS monitoring, parameter store
• Aware of GitHub skills- Merge/pull request/resolving conflicts, Pull/push.
• Translate business information requirements into a meaningful set of SQL queries.
• Develop and maintain key reporting metrics that drive business performance.
• Creating Views, Stored Procedures and Materialised Views in MySQL.
• Ability to parse XML tags into SQL human readable tables.

Job Requirements:
• Advanced SQL skills (or equivalent database querying language)
• 6+ years’ experience working in a similar role.
• 6+ years of experience working with BI tools such as Power BI
• Proficient in Python language
• Data Science enthusiast
• Understand Jenkin pipeline, Jira & Confluence
• Advanced MS Office skills, including Excel, PowerPoint, Access etc.
• Ability to deal with ambiguity, solve complex problems, and navigate large, global organisations.
• Self-motivated, assertive, analytical, and comfortable working in a fast-paced environment
• Stakeholder management

Department : Group Security

Sub Department : Cyber Security

Job code : AAZA01",Gurugram,True,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
SID Global Solutions,Senior Data Engineer(8+yrs),"Skillset: SQL, AWS Stack, Python, Redshift/MYSQL

Roles & Responsibilities:

Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift

Hands on Python programming.

Working knowledge on S3

AWS certification is a nice to have

Must be punctual and follow deadlines and deliver on time.

Must be able to clearly communicate with stakeholders and team",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
InVisions Ltd.,Data Engineer,"Hello people,

We are happy to assist the product and service company “Saras Analytics” in welcoming new Data Engineers to the team.

Now, a little bit about the company and the product:

Saras Analytics is a rapidly growing data management and analytics advisory firm with offices in Austin, USA and Hyderabad, India. We are a group of engineers and analysts focused on accelerating growth for e-commerce and digital businesses by setting up or transforming their data (analytics & BI) ecosystems and providing further analytics services. We are laser focused on providing the best ROI for our clients and leave no stone unturned in our quest to provide the best results for our customers.

We are an employee-centric organization and, to meet the ever-growing demand for our services, are looking for individuals who share our passion to make a difference and would be great additions to our analytics and growth consulting practice.

How Saras Analytics describes your role:

As a Data Engineer at Saras Analytics, you will be responsible for building and maintaining large-scale data pipelines as well as create and data pipelines that deal with large volumes of data.

You will deal with:
• Database programming using multiple flavors of SQL and Python.
• Understand and translate data, analytic requirements and functional needs into technical requirements.
• Build and maintain data pipelines to support large scale data management projects.
• Ensure alignment with data strategy and standards of data processing.
• Deploy scalable data pipelines for analytical needs.
• Big Data ecosystem - on-prem (Hortonworks/MapR) or Cloud (Dataproc/EMR/HDInsight).
• Work with Hadoop, Pig, SQL, Hive, Sqoop and SparkSQL.
• Experience in any orchestration/workflow tool such as Airflow/Oozie for scheduling pipelines.
• Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow.
• Understand and execute IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, writing optimized queries in Spark.
• Hands-on experience in using Spark Streaming, Kafka and Hbase.

What you bring with you:
• 4 to 6 years of experience in building data processing applications using Hadoop, Spark and NoSQL DB and Hadoop streaming. Exposure to latest cloud ETL tools such as Glue/ADF/Dataflow is a plus.
• Expertise in data structures, distributed computing, manipulating and analyzing complex high-volume data from variety of internal and external sources.
• Experience in building structured and unstructured data pipelines.
• Proficient in programming language such as Python/Scala.
• Good understanding of data analysis techniques.
• Solid hands-on working knowledge of SQL and scripting.
• Good understanding of in relational/dimensional modelling and ETL concepts.
• Understanding of any reporting tools such as Looker, Tableau, Qlikview or PowerBI.
• Degree: Bachelor of Engineering - BE, Bachelor of Science - BS, Master of Engineering - MEng, Master of Science – MS or equivalent work experience.

Eligibility:
• Significant technical academic course work or equivalent work experience.
• Excellent communication and interpersonal skills.
• Willingness to work under labor contract, B2B contract is an option too.
• Dedicate 40 hours/weekly to Saras Analytics.

Let’s connect and check if we match!

You can state your interest by sending your CV and we will get in touch with the short-listed candidates.

We treat your personal information with respect and confidentiality, guaranteed and protected by the professional ethics, the Bulgarian and European law.

“InVisions” agency license № 2420 from 19.12.2017.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,True,False
LTIMindtree,Specialist Data Engineer- AZURE-ADF/ADB,"• Job Title- Specialist Data Engineer
• Primary skill- Azure+Databricks (ADF+ADB+Pyspark)
• Locations- Pune, Mumbai, Chennai, Hyderabad, Kolkata, Coimbatore, Bangalore
• Experience- 5 to 12 Years
• Notice Period- 0 to 30 Days
• Job Description-

Primary Skills

• 5+ years of experience in Python and Databricks.

• Deep understanding of data modelling techniques for analytical data (i.e. facts, dimensions, measures)

• Experience developing and managing reporting solutions, dashboards, etc. Design and architecture experience in data transformation.

• Should have experience with data platforms and in data transformation and extraction: some combination of ETL/ELT, table and database design, query design, performance analysis and optimization

• Worked as a data engineer or related specialty (Software Engineer/Developer, BI Engineer/Developer, DBA)

Secondary Skills

• Experience in Azure Data Factory and Azure Storage

• Hands on experience with handling of large amount of data using SQL, Azure Data Factory, Spark, Azure Cloud architecture

• Knowledge of cloud architecture and data solutions

• Proficiency in Snowflake would be added advantage.

• Excellent written and verbal communication skills",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,True
Rently,Data Engineer,"Must have:
• Overall experience of 4+ years
• Experience in AWS Cloud services & solutions
• Experience working with enterprise data warehouse
• Experience as an ETL/ELT Developer using various ETL/ELT tools
• Experience in SQL/NoSQL/DWH databases across SQL DB, Managed instance & Data warehouse
• Experience in AWS platform services such as S3, EMR, RedShift, Glue, Kinesis, OpenSearch, Athena, QuickSight
• Working on SnowFlake and pipeline tools like Fivetran or Matillion.
• Experience in Apache Spark, Databricks
• Experience in creating data structures optimized for storage and various query patterns like Parquet
• Experience in building secured visualization reports and dashboards with access controls
• Experience in working in an Agile SDLC methodology
• Experience in DevOps Services using Git Repos, deployment artifacts and release packages for Test & production environment
• Experience in building end-end scalable data solutions, from sourcing raw data, and transforming data to producing analytics reports
• Should have experience in developing a complete DWH ETL lifecycle
• Experience in Data Analysis, Data Modelling and Data Mart design
• Should have experience in developing ETL processes - ETL control tables, error logging, auditing, data quality, etc. - using ETL tools.
• Experience in Data Integrator Scripts, workflows, Dataflow, Data stores, Transforms, and Functions.
• Should have worked on at least 2 end-to-end implementations
• Worked on Change Data Capture on both SOURCE and TARGET levels and a good understanding of Slowly changing Dimension (SCD)
• Should be able to implement reusability, parameterization, workflow design
• Should have experience in interacting with customers in understanding business requirement documents and translating them into ETL specifications and Low/High-level design documents
• Strong database development skills like complex SQL queries, complex stored procedures
• Able to work in Agile Framework Should have exposure to Scrum meetings.

Good to have:
• Exposure to other ETL/ELT, DWT technologies
• Hands-on with Data visualization tools like Power BI, Tableau, Qlik, QuickSight etc.
• Exposure to Python on ETL and Data Visualization libraries

Additional Skills:
• Good Communication Skills.
• Able to deliver independently.
• Team player.

Professional Commitment:

Being a product based company we heavily invest in developing functional/ technology/ leadership skill sets in our team members. So candidates who are willing to commit to a minimum of 2 years need to apply.",Coimbatore,True,False,True,False,False,False,False,False,True,True,False,True,True,False,False,True
Embibe,Data Engineer,"Requirements
• Should have knowledge in Coding: Preferred Java.
• Good to have - ( Python / Scala).
• Should have Knowledge in Technologies: Spark, spark streaming, scala spark/py spark.
• Good to have Knowledge of Messaging buses like Apache Kafka/ Rabbit MQ.
• Good to have Knowledge of NoSQL databases like - MongoDB, ElasticSearch, Cassandra, Hive, Impala, ADX, Synapse, Redshift, Athena, etc.
• Should have Knowledge in building Microservices with Spring boot/ Fast-Api.",Bengaluru,True,False,True,True,True,False,False,False,False,False,False,False,True,False,False,False
Concentrix,Data Engineer Big Data,"Job Title:

Data Engineer Big Data

Job Description

Data Engineer Big Data

Keywords: RDBMS SQL & Spark/Hive SQL, Performance tuning, Modeling Design

Job Description

Develops and maintains scalable data pipelines

Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.

Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.

Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.

Designs data integrations and data quality framework.

Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.

Qualification:

Bachelor's Degree in Computer Science or related field

3+ years of work experience

Strong experience in SQL ( include complex SQL query , SQL performance tuning , Index , Lock )

Experience with schema design and dimensional data modeling

Experience with Hive SQL , Spark(Spark SQL, DataFrame)

Experience with near-realtime data warehouse (10-30 mins level)

Experience with data quality check

Experience in Java or Python or Shell Script

#CSS

Location:

India Bangalore - Divyashree

Language Requirements:

Time Type:

Full time

If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents

R1357932",Bengaluru,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
CirrusLabs,AWS Data Engineer / Data Engineer / Lead AWS Data Engineer,"Job Role: Aws Data Engineer

Location: Bangalore / Hyderabad

Type: Fulltime

JOB DESCRIPTION

Data Engineer/Operational Support with Snowflake

Must-Have:
• 7+ years of experience in an Oracle/Informatica environment with knowledge of views, packages, stored procedures, functions, constraints, cursors, indexes, and table partitions.
• 7+ years of experience with an ETL tool such as SSIS, Azure Data Factory, or AWS Glue
• Strong background in a data warehouse, data management, and data analytics
• Monitor ETL production batch schedules to meet predefined SLAs.
• Resolve functional and system errors as identified by Business Partners
• Coordinate activity between Business Units and EIS to drive open action items to closure.
• Work with other technical teams to resolve infrastructure-related problems.
• Maintain a good relationship with other technology teams within the client enterprise.
• Generate, Control, and Resolve incident tickets relating to Production batch and Data availability issues.
• Serve as senior contact for production support issues and escalations.
• Enterprise L3 support to resolve production support issues in a timely manner.
• Candidate is expected to exude a take-charge attitude toward problems and thrive for excellence. This is a hands-on, delivery-focused role.
• Attempt to isolate, reproduce, and resolve problems using available systems and tools, and investigate potential workarounds for verified defects.
• Participate in the creation of Knowledge Base articles, solutions, and other related support collateral.
• To interface with Subject Matter Experts, where the problem cannot be resolved at a frontline support level.
• Good to have:
• Excellent written and verbal communication skills
• Detail-oriented; Analytical with problem-solving abilities",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Tata Technologies,Data Engineer,"Job Title : Data Engineer

Job Location : Thane(Mumbai)

Domain Knowledge:

Should be capable of carrying out the following operations on the data with any application.

• Familiarity with data loading tools like Flume, Sqoop.

• Analytical and problem-solving skills applicable to Big Data domain

• Proven understanding with Hadoop, PySpark, Hive, Hadoop

• Good aptitude in multi-threading and concurrency concepts",Thane,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Trademo,Data Engineer,"Position : Data engineer - Full Time and Intern

Role: Python/ Data scraping with Algo and Data structures with Automation

Experience: 0-1 years

Location: Gurgaon (Work from office)

About Trademo

Trademo is a Global Supply Chain Intelligence SaaS Company, headquartered in Palo-Alto, CA. Trademo collects public and private data on global trade transactions, sanctioned parties, trade tariffs, ESG and other events using its proprietary algorithms. Trademo analyzes and performs advanced data processing on billions of data points using technologies like Graph databases, NLP and Machine Learning to build end-to-end visibility on Global Supply Chains. Trademo's vision is to build a single truth on global supply chains to help large and small businesses - discover new commerce opportunities, ensure compliance with trade regulations and build operational resilience. Trademo last closed its $12.5 mn Seed Round from marquee Silicon Valley VCs.

Trademo has been founded by Shalabh Singhal who is a third-time tech entrepreneur. Shalabh last co-founded ZipLoan. ZipLoan is a leading fintech lending startup in India. He earlier founded Credence, a Data-driven Digital Marketing, CRM Product and Sales Solutions company. Shalabh is an Alumni of Goldman Sachs, IIT BHU, CFA Institute USA and Stanford GSB SEED. Trademo has recently closed a $12.5 mn Seed round from some of the marquee investors in Silicon Valley.

Website

https://www.trademo.com

Location

Gurgaon (Work from office)

Technical Skills Required
• Python 3.6+ version, Pandas
• Scraping → Selenium, Beautiful Soap
• Knowledge NOSQL/MYSQL Database
• Knowledge how to tackle the problems with optimal Solution
• Individual contributor role with eagerness to learn new technologies - Elasticsearch , BigData etc.
• Knowledge of Basics DS and algorithms",Gurugram,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Pracemo Global Solutions,Data Engineer,"We are looking for an experienced data engineer to join our team. You will use various methods to transform raw data into useful data systems. For example, you’ll create algorithms and conduct statistical analysis. Overall, you’ll strive for efficiency by aligning data systems with business goals.

To succeed in this data engineering position, you should have strong analytical skills and the ability to combine data from different sources. Data engineer skills also include familiarity with several programming languages and knowledge of learning machine methods.

If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Responsibilities
• Analyze and organize raw data
• Build data systems and pipelines
• Evaluate business needs and objectives
• Interpret trends and patterns
• Conduct complex data analysis and report on results
• Prepare data for prescriptive and predictive modeling
• Build algorithms and prototypes
• Combine raw information from different sources
• Explore ways to enhance data quality and reliability
• Identify opportunities for data acquisition
• Develop analytical tools and programs
• Collaborate with data scientists and architects on several projects

Requirements And Skills
• Previous experience as a data engineer or in a similar role
• Technical expertise with data models, data mining, and segmentation techniques
• Knowledge of programming languages (e.g. Java and Python)
• Hands-on experience with SQL database design
• Great numerical and analytical skills
• Degree in Computer Science, IT, or similar field; a Master’s is a plus
• Data engineering certification (e.g IBM Certified Data Engineer) is a plus
• Self-motivated with a results-driven approach
• Aptitude in delivering attractive presentations
• High school degree
Skills: data warehousing,etl,sql,python,java,hadoop,hive,spark,nosql databases,cloud computing,aws,azure,gcp,data modeling,data mining,data visualization,communication,project management,data lake,data quality,data architecture",Pune,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Thompsons HR Consulting LLP,Lead Data Engineer,"We are looking for Lead Data Engineer

with Strong experience in Python, Development, Business Intelligence (BI tools), AWS, Mysql

Experience: 10+ years

It is a Remote opportunity.

If interested, please share your resume at deepika.ashok@thompsonshr.com",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Anonymous,Data Engineer - Partime / Freelance,"Required skills: Pyspark, AWS-cloud, Hive
Good to have: streamsets, CICD
Experience: 2 - 5yr
Timing : 8pm to 12am on weekdays",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
New Era India,Data Engineer/Sr. Data Engineer/Lead Data Engineer - Data Axle,"Data Engineer / Sr. Data Engineer / Lead Data Engineer (Pune)

About Data Axle

Data Axle Inc. has been an industry leader in data, marketing solutions, sales and research for over 45 years in the USA. Data Axle has set up a strategic global center of excellence in Pune. This center delivers mission critical data services to its global customers powered by its proprietary cloud-based technology platform and by leveraging proprietary business & consumer databases. Data Axle is headquartered in Dallas, TX, USA.

Roles And Responsibilities
• Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
• Creation and support of real-time data pipelines built on AWS technologies including Glue, Redshift/Spectrum, Kinesis, EMR and Athena
• Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency.
• Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
• Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering, and machine learning.
• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

Basic Qualifications
• 3 to 12 years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
• Demonstrated strength in data modeling, ETL development, and data warehousing.
• Experience using big data processing technology using Spark.
• Knowledge of data management fundamentals and data storage principles
• Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, Power BI etc.)

Preferred Qualifications
• Degree/Diploma in computer science, engineering, mathematics, or a related technical discipline
• Experience working with AWS big data technologies (Redshift, S3, EMR, Spark)
• Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience working with distributed systems as it pertains to data storage and computing.
• Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.",Pune,False,False,True,False,False,False,False,True,True,True,False,False,True,False,False,False
Quadratyx,Lead Data Engineer,"About Quadratyx

We are a product-centric insight & automation services company globally. We help the world’s organizations make better & faster decisions using the power of insight & intelligent automation. We build and operationalize their next-gen strategy, through Big Data, Artificial Intelligence, Machine Learning, Unstructured Data Processing and Advanced Analytics. Quadratyx can boast of more extensive experience in data sciences & analytics than most other companies in India. We firmly believe in Excellence Everywhere.

Purpose of the Job/ Role:

As a Lead Data Engineer, your work is a combination of hands-on contribution, customer engagement and technical team management. Overall, you’ll design, architect, deploy and maintain big data solutions.

Key Requisites:

• Expertise in Data structures and algorithms.

• Technical management across the full life cycle of big data (Hadoop) projects from requirement gathering and analysis to platform selection, design of the architecture and deployment.

• Scaling of cloud-based infrastructure.

• Collaborating with business consultants, data scientists, engineers and developers to develop data solutions.

• Leading and mentoring a team of data engineers.

• Hands-on experience on test-driven development (TDD).

• Expertise in No SQL like Mongo, Cassandra etc., preferred is Mongo and strong knowledge of relational database.

• Good knowledge of Kafka and Spark Streaming internal architecture.

• Good knowledge of any Application Servers.

• Extensive knowledge on big data platforms like Hadoop; Hortonworks etc.

• Knowledge of data ingestion and integration on cloud services such as AWS; Google Cloud; Azure etc.

Skills/ Competencies Required

Technical Skills

• Strong expertise (9 or more out of 10) in at least one modern programming language, like Python, Java.

• Clear end-to-end experience in designing, programming, implementing large software systems.

• Passion and analytical abilities to solve complex problems.

Soft Skills

• Always speaking your mind freely.

• Communicating ideas clearly in talking and writing, integrity to never copy or plagiarize intellectual property of others.

• Exercising discretion and independent judgment where needed in performing duties; not needing micro-management, maintaining high professional standards.

Academic Qualifications & Experience Required

Required Educational Qualification & Relevant Experience

• Bachelor’s or Master’s in Computer Science, Computer Engineering, or related discipline from a well-known institute.

• Minimum 7 - 10 years of work experience as a developer in an IT organization (preferably Analytics /

Big Data/ Data Science / AI background.

Quadratyx is an equal opportunity employer - we will never differentiate candidates based on religion, caste, gender, language, disabilities or ethnic group.",Hyderabad,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Persistent Systems,Data Engineer (Immediate joiner),"About Persistent

We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above.

We are experiencing tremendous growth, with $701.1 million in trailing 12-month revenue, representing 29.8% year-over-year growth. Along with that growth, we onboarded over 4,500 new employees in the past year, bringing our total employee count to over 16,500 people located in 18 countries across the globe.

At Persistent, our values are more than a list of ideals to improve our corporate image. We’re dedicated to building an inclusive culture that reflects what’s important to our employees and is based on what they value. As a result, 95% of our employees approve of the CEO and 83% recommend working at Persistent to a friend.

For more details please login to www.persistent.com

About Position
• 4+ years of strong technology experience in the field of transactional data and analytics systems
• Lead client conversations and data discovery sessions
• Should understand and be able to command architecture design for transactional and analytics systems.
• Strong SQL skills
• Hands on experience in building end to end data / orchestration pipelines using Python.
• Cloud Experience- Should have experience with any cloud data products (AWS, GCP, Azure)
• Experience in Agile Methodologies
• Familiarity with source repositories (Git, BitBucket etc.)
• Excellent communication skills",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Niftel Resources,Senior Data Engineer,"Responsibilities:

 Design and build reusable components, frameworks and libraries at scale to support analytics

products

 Design and implement product features in collaboration with business and Technology

stakeholders

 Anticipate, identify and solve issues concerning data management to improve data quality

 Clean, prepare and optimize data at scale for ingestion and consumption

 Drive the implementation of new data management projects and re-structure of the current data

architecture

 Implement complex automated workflows and routines using workflow scheduling tools

 Build continuous integration, test-driven development and production deployment frameworks

 Drive collaborative reviews of design, code, test plans and dataset implementation performed by

other data engineers in support of maintaining data engineering standards

 Analyze and profile data for the purpose of designing scalable solutions

 Troubleshoot complex data issues and perform root cause analysis to proactively resolve product

and operational issues

 Mentor and develop other data engineers in adopting best practices

Qualifications:

Primary skillset:

 Experience working with distributed technology tools for developing Batch and

Streaming pipelines using SQL, Spark, Python [3+ years], Airflow [2+ years], Scala [1+

years].

 Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc.

 Able to quickly pick up new programming languages, technologies, and frameworks.

 Strong skills building positive relationships across Product and Engineering.

 Able to influence and communicate effectively, both verbally and written, with team members and

business stakeholders

 Experience with creating/ configuring Jenkins pipeline for smooth CI/CD process for Managed

Spark jobs, build Docker images, etc.

 Working knowledge of Data warehousing, Data modelling, Governance and Data Architecture

Good to have:

 Experience working with Data platforms, including EMR, Airflow, Databricks (Data Engineering &

Delta Lake components, and Lakehouse Medallion architecture), etc.

 Experience working in Agile and Scrum development process

 Experience in EMR/ EC2, Databricks etc.

 Experience working with Data warehousing tools, including SQL database, Presto, and

Snowflake

 Experience architecting data product in Streaming, Server less and Microservices Architecture

and platform.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,True,True
"Giant Eagle, Inc.",Senior Data Engineer,"Job Summary

As a Sr Data Engineer on the Marketing Data Platforms team, you will be working on a team to bring customer-centric personalization to life. In this role, you will be empowered to develop data solutions in support of analytics, data science, and business partners to understand capability requirements and develop data solutions based on priorities. This leading technical and architecture role will collaborate with product managers, architects, technology teams, analysts, marketing operations specialists, and monetization business partners to understand capabilities and that will be brought to life for Giant Eagle’s 4M+ customers. The ideal candidate will have experience within multiple technology platforms (e. g. GCP, Engagement Platforms, Customer Data Platform, Ad-Tech, etc.) while providing the vision and design for integrating customer data. Additional key skills and qualifications below

Job Description
• Primary Job Responsibilities:
• 5+ years of relevant technical experience working with various data engineering methodologies such as data integration and data pipelines (ETL/ELT) to activate against data at scale.
• 3+ years of experience of data modeling for analytic projects activities that include design, curation, and management of large datasets
• 3+ years of experience adeveloping on big data technologies with Spark and Hive, preferably leveraging such as DataBricks, Juypter notebooks, or GCP, AWS, and Azure equivalent technology.
• 2+ years of experience data solution design for data engineering pipelines
• Strong Experience building event driven systems using cloud technology: storage, Pub/Sub, cloud functions, API’s, and DataProc
• Expertise with databases experience such as BigQuery, Snowflake, and Synapse designing schema and dimensional data modeling
• Experience leveraging RESTful web services to collect and publish data.
• Experience in software engineering development and testing life cycles using but not limited to Python, R, Linux, Java, JavaScript, Lambda, and SQL programming
• Bachelor's degree in Computer Science, Mathematics, or other technical field or equivalent work experience. Advanced degree a plus
• Experience with Retail Media Networks and Ad Tech preferred

Role Requirements:
• Architect, develop and implement end-to-end complex data projects and technical solutions through translating business requirements into technical solutions and data-flow architectures.
• Architect, build and automate data pipelines that clean, transform, and aggregate unorganized data into data sources that are ready for analysis.
• Use expertise to apply various analytic methods to discover and interpret information about customer behavior from multiple data sources to implement analytics solutions
• Use expertise in database design to implement, operate stable and scalable dataflows from multiple marketing platforms into a cloud data lake for Ad Tech
• Experience building data visualization tools Tableau, PowerBI, and Looker with data modeling and Looker ML preferred
• Design, implement and deploy data applications and mechanisms using big data technology
• Provides subject matter expertise for multiple projects concurrently through all phases of the development lifecycle.
• Develop, enhance, govern, and administer for data platform to: collect data, transform, enrich, unify, segment, and integrate data
• Strong adherence data ethics rules around PII data sets
• Work collaboratively with IT teams, Performance Marketing team, and business leaders to ensure actionable is provided key stakeholders
• Research and analyze customer behavior data to improve customer experience
• Experience with agile or other rapid application development methods a plus
• Retail industry experience a plus

About The Company

Since our founding in 1931, Giant Eagle, Inc. has evolved into one of the top 40 largest private corporations in the U. S. and one of the country’s largest food retailers and distributors. With more than 37,000 Team Members and $9.7 billion in revenue, we are committed to investing in people, technology, and data to elevate our customer’s experience across multiple touchpoints. It helps us follow on our commitment to serving others and improving our communities.

About Giant Eagle Bangalore

The Giant Eagle GCC in Bangalore is our global capability center. Our team of more than 370 members at the GCC enables us to expand internal capabilities in the areas such as data analytics, merchandising and eCommerce, quality engineering, and automation to generate insights for faster decision-making and helping us accelerate our business strategy. Our team in India plays a pivotal role in helping the company transition to new ways of working by redefining the food and grocery shopping experience for over 4.6 million customers.

About Us

At Giant Eagle Inc., we’re more than just food, fuel and convenience. We’re one giant family of diverse and talented Team Members. Our people are the heart and soul of our company. It’s why we strive to create a nurturing environment that offers countless career opportunities to grow. Deep caring and solid family values are what makes us one of the top work places for jobs in the Greater Pittsburgh, Cleveland, Columbus and Indianapolis Areas. From our Warehouses to our GetGo’s, our grocery Stores through our Corporate home office, we are working together to put food on shoppers' tables and smiles on their faces. We’re always searching for the best Team Members to welcome to our family. We invite you to join our Giant Eagle family. Come start a lasting career with us.",,True,False,True,True,False,False,True,False,False,True,False,False,False,True,False,True
TMRW House of Brands,Data Engineer-III,"Responsibilties:

Create, implement, and operate the strategy for robust and scalable data pipelines for business intelligence and machine learning.

Develop and maintain core data framework and key infrastructures

Create and support the ETL pipeline to get the data flowing correctly from the existing and new sources to our data warehouse.

Data Warehouse design and data modeling for efficient and cost-effective reporting

Collaborate with data analysts, data scientists, and other data consumers within the business to manage the data warehouse table structure and optimize it for reporting.

Constantly striving to improve software development process and team productivity

Define and implement Data Governance processes related to data discovery, lineage, access control, and quality assurance

Perform code reviews and QA data imported by various processes

Qualifications

6-10 years of experience.

At least 3+ years of experience in data engineering and data infrastructure space on any of the big data technologies: Hive, Spark, Pyspark(Batch and Streaming), Airflow, and Delta Lake.

Experience in product-based companies or startups.

Strong understanding of data warehousing concepts and the data ecosystem.

Strong Design/Architecture experience architecting, developing, and maintaining solutions in AWS.

Experience building data pipelines and managing the pipelines after they’re deployed.

Experience with building data pipelines from business applications using APIs.

Previous experience in Databricks is a big plus.

Understanding of Dev Ops would be preferable though not a must

Working knowledge of BI Tools like Metabase, and Power BI is plus

Experience in architecting systems for data access is a major plus.",Bengaluru,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False
Impetus,GCP Data Engineer,"Qualification
• The candidate should have extensive production experience (3-5 Years ) in GCP, Other cloud experience would be a strong bonus.
• Strong background in Data engineering 4-5 Years of exp in Big Data technologies including, Hadoop, NoSQL, Spark, Kafka etc.
• Exposure to enterprise application development is a must

Role
• 6-10 years of IT experience range is preferred.
• Able to effectively use GCP managed services e.g. Dataproc, Dataflow, pub/sub, Cloud functions, Cloud composer, Big Query, Big Table, - At least 4 of these Services.
• Strong experience in Big Data technologies – Hadoop, Sqoop, Hive and Spark including DevOPs.
• Good hands on expertise on either Python or Java programming.
• Good Understanding of GCP core services like Google cloud storage, Google compute engine, Cloud SQL, Cloud IAM.
• Good to have knowledge on GCP services like App engine, GKE, Cloud Run, Cloud Built, Anthos.
• Ability to drive the deployment of the customers’ workloads into GCP and provide guidance, cloud adoption model, service integrations, appropriate recommendations to overcome blockers and technical road-maps for GCP cloud implementations.
• Experience in architecting and designing technical solutions based on industry standards using GCP - IaaS, PaaS and SaaS capabilities.
• Extensive, real-world experience designing technology components for enterprise solutions and defining solution architectures and reference architectures with a focus on cloud technologies.
• Act as a subject-matter expert OR developer around GCP and become a trusted advisor to multiple teams.
• Coach and mentor engineers to raise the technical ability of the rest of the team, and/or to become certified in required GCP technical certifications.",इन्दौर,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Newell Brands,Cloud Data Engineer,"Job Title: Cloud Data Engineer

Report To: Sr. Manager, Data Engineering

Job Location: Guindy, Chennai, India

Job Duties
• Participates in the full lifecycle of cloud data architecture (Preferably Azure cloud) from gathering, understanding end-user analytics and reporting needs.
• Migrate On-Prem applications and build CI/CD pipeline in Cloud platform.
• Design, develop, test, and implement on Cloud platform (Ingestion, Transformation and export pipelines that are reliable and performant) .
• Ensures best practices are followed and business objectives are achieved by focusing on process improvements.
• Quickly adapt by learning and recommending new technologies and trends.
• Develop and Test Data engineering related activities on Cloud Data Platform.
• Work with dynamic tools within a BI/reporting environment.

Job Requirements
• B.E/B.Tech, M.Sc/MCA.
• 5+ years experience in Rapid development environment, preferably within an analytics environment.
• 3+ years experience with Cloud experience (Preferably Azure cloud but not mandatory).
• DB : T-SQL, SQL Scripts, Queries, Stored Procedures, Functions and Triggers
• Language : Python / C# or Scala
• Cloud: Azure / AWS / Google cloud
• Frameworks: Cloud ETL/ELT framework

Preferred
• Azure Data Factory, Azure Synapse Analytics, Azure SQL, Azure Data lakes, Azure Data bricks, Airflow and Power BI
• Data warehousing principles and frameworks.
• Knowledge in Cloud DevOps and CI/CD pipelines would be an added advantage.

Newell Brands (NASDAQ: NWL) is a leading global consumer goods company with a strong portfolio of well-known brands, including Rubbermaid, FoodSaver, Calphalon, Sistema, Sharpie, Paper Mate, Dymo, EXPO, Elmer's, Yankee Candle, Graco, NUK, Rubbermaid Commercial Products, Spontex, Coleman, Campingaz, Oster, Sunbeam and Mr. Coffee. Newell Brands' beloved, planet friendly brands enhance and brighten consumers lives at home and outside by creating moments of joy, building confidence and providing peace of mind.",Chennai,True,False,True,False,False,False,False,False,True,False,False,False,False,False,True,False
Zupee,Lead Data Engineer,"About Zupee

Zupee is India’s fastest growing Technology backed Behavioral Science company. We are innovating Skill-Based Gaming with a mission to become the most trusted and responsible entertainment company in the world. We have been constantly focusing on innovation of indigenous games to entertain the mass.

Our strategy is to invest in our people & user experience to drive profitable growth and become the market leader in our space. We have been experiencing phenomenal growth since inception and running profitable at EBT level since Q3, 2020. We have closed Series B funding at $102 million, at a valuation $600 million.

The company also announced a partnership with Reliance Jio Platforms, post which Zupee games will distribute its content across all customers using Jio phones. The partnership now gives Zupee the biggest reach of all gaming companies in India, transforming it from a fast-growing startup to a firm contender for the biggest gaming studio in India.

About The Job

Lead Data Engineer

We are looking for someone to develop the next generation of our Data platform

collaborating across functions like product, marketing design, growth, strategy, customer

experience and technology.

Core Responsibilities

●Understand, implement and automate ETL and data pipelines with up-to-date

industry standards

●Hands-on involvement in the design, development and implementation of optimal and

scalable AWS services

What are we looking for?

●S/he must have experience in Python

●S/he must have experience in Big Data – Spark, Hadoop, Hive, HBase and Presto

●S/he must have experience in Data Warehousing

●S/he must have experience in building reliable and scalable ETL pipelines

Qualifications and Skills

●6-12 years of professional experience in data engineering profile

●BS or MS in Computer Science or similar Engineering stream

●Hands-on experience in data warehousing tools

●Knowledge of distributed systems such as Hadoop, Hive, Spark and Kafka etc.

●Experience with AWS services (EC2, RDS, S3, Athena, data pipeline/glue, lambda, dynamodb etc.
•",Gurugram,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
UST Product Engineering,Data Engineer,"Job Description :

- 4-8 Years experience in data warehousing , ETL processes, and data analytics.

- Good experience in developing, maintaining, and testing infrastructures for data generation, verification and transformation.

- Good understanding database concepts (SQL, Cloud DBs)

- Strong SQL query, profiling and troubleshooting skills

- Good understanding AWS Data related concepts like big data, big query etc.

- Basic understanding AWS (or supported) ETL tools - Glue, Airflow etc etc. would be an added advantage

- Good Understanding of python programming

- Basic understanding of programming language like C# or similar

- Basic knowledge of working in scrum/agile teams and tools like JIRA, confluence etc.",Pune,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.

Apply for this job",Mumbai,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
ThousandEyes,Cloud Application and Data Engineer,"Cloud Application and Data Engineer

Who We Are

The name ThousandEyes was born from two big ideas: the power to see what’s not ordinarily possible, and the ability to collect intelligence from vantage points as diverse and global as the Internet. As organizations depend on cloud services, the Internet has become their defacto network connecting cloud applications to users. Our Internet and cloud intelligence platform is like a ‘Google maps of the Internet’, providing the only collectively powered view of digital experiences end-to-end. We enable our customers made up of the world’s largest and fastest-growing brands, to identify problems before they impact revenue, brand reputation, or employee productivity.

In August 2020, Cisco Systems completed the acquisition of ThousandEyes, which now forms the ThousandEyes Business Unit within Cisco’s Network Services Business Group,and is a foundational component of Cisco’s growing Observability business.About The Team

Digital experiences rely on a vast ecosystem of ISPs, cloud providers, SaaS applications, individual configurations, unique devices, and many other external services that are critically dependent on the Internet. Trying to identify the root cause of a problem or a deviation from normal is like finding a needle in a haystack. This leads to long downtimes and poor customer or employee experience.

The AI Analytics team at ThousandEyes is leveraging machine learning at scale, while working across several different business units, to our help customers answer tough questions like:
• What is normal in my network and how do I catch deviations from this normal?
• How do I understand the root cause of a problem in my network or application stack?
• How do I ensure that devices joining my network are who they say they are?
• How do I know when my networking gear is about to break?

The goal of the AI Analytics Team is to leverage different machine learning techniques to deliver actionable insights for our customers to solve real world problems in their complex environments.

What You’ll Do

You will be part of our data and platform team. A worldwide distributed team responsible for data collection, ingestion, processing, and quality. You will play an important role in helping to deliver new ML powered features to our customers as well as monitoring and improving the existing features for performance and quality. You will work in the AI Cloud hosted on AWS with Python, Go, Spark, Hive, Open Search, and other cutting-edge technologies.

Responsibilities
• Collaborating closely with ML engineer to bring new features to production.
• Create and maintain an optimal data pipeline architecture.
• Contribute and operate data quality tooling.
• Monitor and optimize compute and query performances.
• Troubleshoot and debug issues across our applications and services.

Who you are

Agile, pragmatic and hardworking. You also love to interact with data scientists, machine learning engineers and software engineers to develop pipelines that scale seamlessly at huge volumes of data. You love technology, innovation and building products at scale.

You hold a degree in computer science, or a related field and you can demonstrate a consistent track record in the following areas:
• At least 4 years of software development experience.
• 2 years of experience building and developing data-intensive systems at industrial scale.
• Dimensional data modeling and schema design for both SQL and NoSQL databases.
• Prior exposure to data science, machine learning or statistics is a plus.
• Previous experience developing applications running on a public cloud infrastructure is a plus.
• Strong Communication and documentation skills in English.
• Strong sense of ownership, drive, attention to detail and ability to work in a distributed team.

We are looking for candidates based in Bangalore to work hybrid

Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis. Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.

Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference powering an inclusive future for all.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (36 years strong) and only about hardware, but we’re also a software company. And a security company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do –you can’t put us in a box! But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)Day to day, we focus on the give and take. We give our best, give our egos a break, and give of ourselves (because giving back is built into our DNA.) We take accountability, bold steps, and take difference to heart. Because without diversity of thought and a dedication to equality for all, there is no moving forward. So, you have colourful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us.

We recognize that diverse teams make the strongest teams, and we encourage people from all backgrounds to apply.

Cisco COVID-19 Vaccination Requirements

The health and safety of Cisco's employees, customers, and partners is a top priority. Our goal is to protect and mitigate the spread of COVID-19 infection for strong business resiliency during the pandemic. Therefore, Cisco may require new hires to be fully vaccinated against COVID-19 if the role requires business-related travel, meeting with customers/partners (including visiting third-party sites on behalf of Cisco), attending trade events, and Cisco office entry, unless otherwise prohibited by applicable law, and in countries where COVID-19 vaccination is legally required. The company will consider legally required accommodations/exceptions for medical, religious, and other reasons as per the requirements of the role and in accordance with applicable law. Additional information will be provided to candidates about the requirements and accommodation process at the offer time based on region.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Verizon,Principal Engineer - Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

You will be expected to architect solutions for business projects, work with enterprise architects to align application & system architecture to enterprise strategy and deliver individually and/or with the help of a team. You need to have passion to learn and educate fellow associates/subordinates and guide them to follow best practices. Principal consultant to the team that develops, maintains and enhances the service delivery and management for NS applications
• Architecting/Developing solutions for the application which deals with big data platforms.
• Engaging with Enterprise Architects on HLAs and defining new solutions that adhere to big data volume processing.
• Driving a Culture of Innovation: Champion a culture of innovation and drive as an example.
• Supporting customers with major platform issues and coordinating triage efforts to solve them.
• Identifying and aligning project requirements and conducting impact analysis.
• Working closely with the business team, and other internal IT teams to deliver projects on time.
• Preparing presentations and reports to internal and external customers, as well as internal Executives.
• Evaluating various new technical products based on changing business needs and making product recommendations to management keeping in mind the architecture of the entire list of applications supported.
• Providing technical leadership and business-related subject matter expertise on large, highly complex projects.
• Guiding the team on best practices for efficient and streamlined delivery of software to production. Guiding teams on maintaining security posture and code quality of applications keeping the tech debt in check.
• Identifying chronic production issues, pain points of customers by evaluating feedback and monitoring the NPS to maintain it above the required threshold.
• Working with Quality Assurance, UAT & Production Support teams to support releases, troubleshoot progression/regression issues, integration & E2E testing and implement deliverables as per the targeted timelines.
• Working with infrastructure teams to implement DevOps capabilities that help streamline the CICD process. Leverage innovative technologies to build proof-of-concepts that help build customer experiences, reduce pain points in the current experience, and provide a delight factor to customers.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You view technology through a lens of making things better and more effective. Understanding and building continual improvements to the digital value chain is something you flourish with. You enjoy the process of solving complex issues while empowering the team around you to do the same.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Experience in Hadoop, Hive, Pyspark , Spark Scala, PIG, Java, GCP, AWS, CICD (Jenkins/ Gitlab).
• Experience in Big query, Composer, Cloud Functions & Java script.
• Experience with any of RDBMS, Druid and MongoDB.
• Experience in Devops & automation.
• Experience in Docker/K8s & SRE Practice.
• Experience in Agile & SAFe methodologies.

Even better if you have one or more of the following:
• A Master's degree.
• Ability to design products which can scale up for large volumes of data.
• Knowledge in Wireless Domain.
• Knowledge of Security Vulnerabilities.
• Strong written and verbal communication skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False
lululemon India Tech Hub,Data Engineer - SQL & Python,"We are looking to hire dynamic Data Engineers for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.

Responsibilities
• Uses structured tools for analysis and presentation of concepts and models to enhance the BRD
• Develop, maintain and deliver training materials to the supply chain end-users
• Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs
• Support day to day reporting needs where required
• Support production issues as relate to application functionality and integrations
• Excellent spoken and written communication skills (verbal and non-verbal)
• Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)
• Solid scripting capability for analysis and reporting (ANSI SQL)
• Solid experience in RDBMS and NoSql technologies
• Strong analytical skills to support BAs.
• Strong problem-solving skills (Math skills required for data modeling)
• Ability to work as an integration / data engineer.
• Ability to manage and complete multiple tasks within tight deadlines
• Possess expert level understanding of software development practices and project life cycles.
• Working experience with Java batch spring boot/ python.
• Working Experience with cloud-native technologies
• Must have: Working experience in dealing with big data and data manipulation.
• Desired: Familiarity with Retail planning / merchandising systems/ supply chain.
• Desired: Familiarity with DevOps practices like CICD pipeline
• Desired: Retail experience is a plus. (fashion retail experience would be ideal)
• Must Have: Working experience with cloud platforms namely AWS
• Must Have: Working experience with large data sets (at least 80 – 100 GB data)

Requirements
• name : lululemon India Tech Hub
• location : Bengaluru, IN
• experience : 5 - 8 years
• Primary Skills: SQL or RDBMS or NoSQL,Python,AWS,Springboot,ETL",Bengaluru,True,False,True,True,False,False,False,True,False,False,False,False,False,False,False,False
Splunk,Senior Data Engineer - 27505,"The Senior Data Engineer will be involved in building data pipelines at a large scale to enable business teams to work with data and analyze metrics that support and drive the business. You will work as part of an evolving Enterprise Data Management (EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency.

The team is looking for a Senior Data Engineer who can architect and build solutions across multiple data sources to deliver metrics/reporting use cases. This position is responsible for building and scaling the data platform that works to provide business analytics. The role involves ownership and technical delivery, working closely with other members (BI engineers and Infrastructure teams plus other data roles, including Data Governance, Quality, and Architecture Stewards). Strong technical experience within enterprise software is essential.

Responsibilities:
• Responsible for developing and supporting data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
• Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand enterprise data requirements of analytics projects
• Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
• Identify and document standard methodologies, standards, and architecture guidelines
• Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery
• Identify ways to improve Data Reliability, Data Efficiency and Data Quality

Required Qualifications, Skills & Experience:
• 7+ years of data engineering related experience such as data analysis, data modeling, and data integration.
• Experience with Sales Operations, Partner Operations and customer success business processes and applications
• Experience in custom ETL design, implementation, and maintenance
• Strong knowledge of programming languages (e.g. Python and Object Oriented Programming)
• Hands-on experience with SQL database design
• Experience working on CI/CD processes and source control tools such as GitHub and GitLab
• Experience working in Snowflake and relational databases
• Extensive hands-on experience in leading large-scale full-cycle cloud enterprise data warehousing (EDW) implementations like Snowflake
• Strong knowledge and experience with Agile/Scrum methodology and iterative practices in a service delivery lifecycle
• Experience with or exposure to data governance & quality principles and practices
• Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
• Passionate about data solutions, technologies, and frameworks
• Experience in Data Visualization tools such as Tableau

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Knowledge of enterprise systems such as Salesforce, Workday, SAP etc.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,True,False,False,False,False,False,True
Reverate,Senior Data Engineer - Remote,"Reverate Tech is a product and service-based start-up, working with International Clients. Our services include Data Engineering, Web Development, BI/Data Warehousing, Enterprise Application Implementation (ERP/CRM), and NetSuite. Our product portfolio has business apps in the domain of ERP, Auto Service, and Personal Safety.

This is an exciting opportunity to work as Senior Data Engineer for our client SellerX.

SellerX is the 3rd fastest growing company in the EU evaluated at more than 1 Billion Euros. It has an ambitious goal: to become a leading global acquirer and operator of a new generation of eCommerce businesses.

Your Job:
• You are responsible for all types of data management processes (collection, storage, cleansing, preparation, maintenance, accumulation, transfer to business reporting).
• You optimize and develop existing and new data warehouse applications using tools for data ingestion and data modeling
• You design and document new data models and best-practice solutions.
• You are responsible for prototyping and implementing new ETL jobs and modeling approaches.
• As a data engineer, you will deal with python programs and their configurations in order to create or improve automated data engineering tasks.
• In addition to technical project management, you advise our other tech teams in Data Management aspects.
• Ensuring data security (e.g. encryption) and improving the backup strategy.
• You work hand in hand with data architects, data analysts, and data scientists.
• You ensure that quality, stability, and robustness along the entire process chain meet our high standards.

Your Background:
• You have a bachelor's degree with a focus on software engineering.
• 5+ years of experience in data engineering.
• Strong with Algorithms and have worked on scaling pipelines/solutions
• Hands-on experience with data ingestion tools like Fivetran, Daton, Stitch, or Data Virtuality.
• Hands-on experience with ETL and orchestration tools like Apache Airflow or similar.
• Object-oriented Python programming is more than just a plus.
• Expert knowledge in the areas of data modeling and ETL processes on the SQL level (e.g. using DBT), as well as experience working with REST APIs, is beneficial.
• DevOps experience
• In addition to your ""hands-on"" mentality, you score points with a high technical affinity and a strong analytical mindset.
• Your working standards do not suffer in terms of quality, even in hectic times.

Benefits:
• Compensation: up to 40 LPA
• 100% remote-working;
• Flexible working hours;
• Development of your personal strengths in a dynamic environment;
• An attractive and varied job with a high level of personal responsibility;
• A collegial togetherness and a modern management style/startup;

Interested? Join us and start your learning and growth journey.

Reverate focuses on Software Engineering. Their company has offices in India. They have a small team that's between 11-50 employees.

You can view their website at https://reverate.tech/",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
"6221, Roche",Senior Data Engineer,"The Position

Roche sequencing solution is developing the next generation sequencing based on nanopore technology. This has the potential to make sequencing based diagnostics cheaper, faster and more accurate enabling precision medicine and early diagnosis of many diseases improving the health outcome.

As part of Data Science Automation group, you will get to work on key software technologies enabling research and development of sequencer. You will solve complex problems related to processing terabytes of data coming out sequencer and deriving useful insights from the data. This requires massively parallel computation locally on GPU as well as in the cloud. You will gain exposure to latest and greatest in data engineering and data pipeline tools and technologies. You will also work with advanced data visualization problems involving millions of data points.

You also will get to collaborate with multidisciplinary team of scientists and engineers working in fields ranging from protein engineering, bio chemistry, biophysics, stats modeling, bioinformatics and deep learning.

If you are excited to become part of the next generation sequencing research and development and revolutionize the healthcare, we have a rare opportunity for you to come and work with us.

We need an experienced Data/Workflow Engineer with a strong background in designing and developing highly scalable data management solutions and workflow pipelines. You will work across a variety of problems and application spaces involved in high availability systems, for data management and compute systems, at a very large scale. You will be working with a hardworking team of engineers and data-scientists who are passionate about building creative and novel solutions at the forefront of Sequencing research.

Required Qualifications:
• BS in CS or similar and 10+ years professional experience, or MS with 7+ years of experience in building highly scalable, performant software systems, in a Linux environment.
• Strong, hands on experience building and supporting Enterprise level Workflow management systems such as airflow, nexflow, kubeflow etc. Experience with building performant airflow pipelines with a large number of DAGs and dynamic DAGs is desirable.
• Working experience in deploying and managing airflow platforms, knowledge of Terraform, Kind, Helm etc. is a huge plus.
• At least 3+ years’ experience of developing solutions using container and cloud technologies. Preferably Kubernetes, Docker.
• Experience building with cloud native technologies (e.g. GCP, AWS), blob stores and knowledge of various data compression formats.
• Demonstrated skill with software development following current software engineering best practices using languages such as: Python, Java and BASH scripting.
• Have a strong understanding of modern software development practices and tools, including: version control systems (e.g., Git), issue trackers, and test frameworks.
• Experience building and using automation tools, CI/CD, unit testing.

You 'll go above and beyond our required requirements if you...
• Possess a PhD/MS in Computer Science, Computer Engineering, or another related, technical discipline.
• Have at least ten years of relevant experience in the development of software systems ideally in a Linux environment.
• Have experience using modern frontend and backend software frameworks for software applications.
• Knowledge of challenges involved in large-scale, high-availability data platforms. Experience with designing and implementing platforms providing secured access to large datasets.
• Have experience applying software expertise to full project lifecycles, including requirements analysis, design, implementation, and testing.

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,True,False,False,True,False,False,False,False,False,False,False,False,False,False,True,False
Koch,Senior Data Engineer,"Description

Position Description/ Requirements

The Data Engineer will be a part of an international team that designs, develops and delivers Data Pipelines and Data Analytics Solutions for Koch Industries. Koch Industries is a privately held global organization with over 120,000 employees around the world, with subsidiaries involved in manufacturing, trading, and investments. Koch Global Solution India (KGSI) is being developed in India to extend its IT operations, as well as act as a hub for innovation in the IT function. As KSGI rapidly scales up its operations in India, it’s employees will get opportunities to carve out a career path for themselves within the organization. This role will have the opportunity to join on the ground floor and will play a critical part in helping build out the Koch Global Solution (KGS) over the next several years. Working closely with global colleagues would provide significant international exposure to the employees.

The Enterprise data and analytics team at Georgia Pacific is focused on creating an enterprise capability around Data Engineering Solutions for operational and commercial data as well as helping businesses develop, deploy, manage monitor Data Pipelines and Analytics solutions of manufacturing, operations, supply chain and other key areas.

A Day In The Life Could Include:

(job responsibilities)
• Partner/collaborate with Business stakeholders and build high-quality end-to-end data solutions.
• Build a data architecture for ingestion, processing, and surfacing of data for large-scale applications in the cloud (AWS/ Azure)
• Create and maintain optimal data pipeline architecture.
• Follow best practices of Agile and DevOps focusing on the delivering of high-quality products and providing the ongoing support to meet the customers' needs
• Implement processes for Continuous integration, Test automation and Deployment (CI/CD Pipelines)
• Provide quality documentation of your design (process and workflows) and implementation including experiment tracking / logs.
• Provide on-call support on an as-needed basis
• Handle support cases to ensure issues are recorded, tracked, resolved, and follow-ups finished in a timely manner.

What You Will Need To Bring With You:

(experience & education required)
• Bachelor’s degree in Engineering (preferably Analytics, MIS or Computer Science). Master’s degrees preferred.
• 6+ years of IT experience.
• In depth knowledge of Data Engineering concepts and platforms - SQL based systems, Hadoop, Spark, Distributed computing, In-memory computing, real time processing, pub-sub, orchestration, etc.
• Expertise of building data pipelines using (Pyspark based) and Databricks utilising techniques in Azure or AWS.
• 4-5 years of experience in DevOps and CI/CD using tools like Git, Terraform, Jenkins, Ansible.
• 5+ year of experience in Data modeling, SQL, Data Warehouse skills are a MUST.
• A passion and fearlessness for learning new technologies and methods in the areas of Administration
• Ability to thrive in a team environment and juggle multiple priorities.
• Excellent written and verbal communication skills.

What Will Put You Ahead:

(experience & education preferred)
• In depth knowledge of entire suite of services in AWS/Azure Cloud Platform.
• Strong coding experience using Pyspark.
• Experience of designing and implementing ETL process using SSIS.
• Cloud Data Anaytics/Engineering certification.

Other Considerations:

(physical demands/ unusual working conditions)
• Some work may involve hours outside of normal KGS works hours.

Koch is proud to be an equal opportunity workplace",Bengaluru,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Tata Consultancy Services,Data Engineer,"Greetings from TCS !!!

TCS India presents excellent opportunities for IT professionals.

Role :- Data Engineer

Experience:- 7 to 10 years

Location- Bangalore / Mumbai / Chennai / Bhubaneswar

Required Technical Skill Set- Data Engineer – Big Data, Hadoop, Hive, Spark, Yarn

Must-Have:-

1. 4-8 Yrs of hands-on development experience

2. Experience leveraging big data technologies (One or more of Hadoop, Python, Spark) is mandatory.

3. Experience working with various data exchange formats (JSON, CSV, XML etc.).

4. Solid understanding of relational and dimensional database design and knowledge of logical and physical data models is preferred.

5. Excellent knowledge of SQL and Linux shell scripting.

6. Experience with job scheduling (TIDAL, CAWLA, Oozie) and file transfer (e.g. SFTP)

Good-to-Have:-

1. Experience building real-time data pipelines using Kafka or spark streaming is preferred.

2. Exposure to Microsoft Azure (or other cloud) platforms is preferred.

3. Experience with Agile methodologies for project development.

4. Excellent diagnostic, analytical and problem-solving skills are preferred.

5. Experience with continuous delivery tools (Jenkins, Bamboo, Circle CI), and an understanding of the principles and pragmatics for build pipelines, artefact repositories, zero-downtime deployment, etc. is preferred

TCS Eligibility Criteria:
• BE/B.Tech/MCA/M.Sc/MS with minimum 3 years of relevant IT-experience post Qualification.
• B.Sc Graduates with minimum 4+ years of relevant IT-experience post qualification.
• Only Full Time courses would be considered.
• Consistent academic records class X onwards (Minimum 50%)
• Candidates who have attended TCS interview in the last 3 months need not apply.

Interested candidate can share their resumes with the mandatory details mentioned below.

Please update the details:

1. Total years of Exp:

2 Email ID :

3. Present Company:

4. Current & Preferred Location:

5. Mobile No.:

6. Current CTC:

7. Expected CTC:

8.Notice Period:

9: Working With TCS /CMC ( Direct Payroll) earlier (Yes/ NO):

10. No Of job change-

Interested candidate can share their resumes with hiba.fathima@tcs.com",Bengaluru,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LTIMindtree,GCP Data Engineering POD Lead,"Primary Skill – GCP Data Engineering POD Lead

Total Exp – 3 to 14 Years

Notice Period – 0 to 30 Days

Job Location – Kolkata, Bangalore, Mumbai, Pune, Chennai, Hyderabad

Job Description:

Job Description:

TPrimary Skill – GCP

Secondary Skill – Python, Big query

Overall, more than 8+ Yrs of experience in Data Science Statistical Modeling and Projects to Develop and Deliver Data Science work Strong understanding of Machine Learning Statistics fundamentals Technology Skill Set Python R Pandas Scikit Learn R s

Desired Candidate Profile Technology & Engineering Expertise

• 5+ years of experience in implementing data solutions using GCP/SQL programming

• Proficient in dealing data access layer, RDBMS | NO-SQL.

• Experience in implementing and deploying Big data applications with GCP Big Data Services.

• Good to have SQL skills.

• Experience with different development methodologies (RUP | Scrum | XP) Soft skills

• Able to deal with diverse set of stakeholders

• Proficient in articulation, communication, and presentation

• High integrity

• Problem solving skills & learning attitude

• Team player Key Responsibilities

• Implement data solutions using GCP and need to be familiar in programming with SQL/python.

• Ensure clarity on NFR and implement these requirements.

• Work with Client Technical Manager by understanding customer’s landscape & their IT priorities

• Lead performance engineering and capacity planning exercises for databases",,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Arcadis,Azure Data Engineer,"ARCADIS is looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in “Everything Digital, Digital Everything”. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.

Technology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.

Role accountabilities:
• Possess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment
• You are excited about working with Azure Data Platform
• challenges while building the next wave of software engineering solutions
• Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform
• Leading the craftsmanship, security, availability, resilience, and scalability of your solutions
• Very strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.
• Strong experience in
• T-SQL, SSIS, SSAS, SSRS
• Azure Data Factory
• Azure Data Lake Store
• Azure Data Lake Analytics (Good to have, not mandatory)
• Azure SQL DB
• Azure SQL DW
• Azure Analysis Services, DAX
• Azure Data Bricks with Python/Scala
• Experience in building end to end solution using Azure data analytics platform.
• Experience in building generic framework solution which can be reused for upcoming similar use cases.
• Experience in building Azure data analytics solutions with DevOps (CI/CD) approach.
• Experience in using TFS, Azure Repos.
• Mentor peers to gain expertise on Azure data platform solutions skills.
• Experience in developing, maintaining, publishing, and supporting dashboards using Power BI.
• Strong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded

Qualifications & Experience:

Basic Qualifications:
• Bachelor in Engineering/Math/Statistics/Econometrics or related discipline
• Should have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.
• Preferred Qualifications:
• Master’s or Minor in Computer Science
• 3+ years of experience developing Data Engineering solutions
• Architecture, design experience with good knowledge of data model design & their implementation.

Why Become an Arcadian?

Our work with clients has a direct impact on people’s lives and on the planet. We make moving, living and belonging in cities safer, more resilient and more sustainable. By partnering with our clients as responsible custodians of our earth's resources, we can create a sustainable planet.

We continue to think of new ways to make positive impacts and create better experiences for people; data driven and digital solutions have become part of the Arcadis DNA. Working together with clients and using techniques like design thinking, we can get to the heart of our clients’ most pressing challenges and work together to solve them.

As a global business, we have committed to support five of the UN’s Sustainable Development Goals to ensure that our projects contribute to a better and more sustainable future for all. But it’s not just the work that we do on client projects that benefits communities and our planet. As a global business, we are committed to making a positive impact to society by supporting local communities where we operate.

To help protect our planet, we monitor and measure non-financial information to inform business decisions and reduce our own environmental impact as part of our commitment to be net zero carbon as a global company by 2030.

Our Commitment to Equality, Diversity, Inclusion & Belonging:

We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.

In accordance with the Colorado Equal Pay Transparency Rules:

Arcadis offers benefits for full time positions. These benefits include medical, dental, and vision coverage along with a 401K plan, STD and LTD, and Life Insurance as well as some additional optional benefits. Full time positions also come with annual PTO days and at certain levels a bonus program may apply. The Salary range for this role is $61,360 - $95,000 for Colorado based positions only. Other locations will vary in salary range

Transform Your World",Hyderabad,True,False,True,False,False,False,False,True,True,False,True,False,False,False,False,False
Dolby Laboratories,Data Engineer,"Join the leader in entertainment innovation and help us design the future. At Dolby, science meets art, and high tech means more than computer code. As a member of the Dolby team, you’ll see and hear the results of your work everywhere, from movie theaters to smartphones. We continue to revolutionize how people create, deliver, and enjoy entertainment worldwide. To do that, we need the absolute best talent. We’re big enough to give you all the resources you need, and small enough so you can make a real difference and earn recognition for your work. We offer a collegial culture, challenging projects, and excellent compensation and benefits, not to mention a Flex Work approach that is truly flexible to support where, when, and how you do your best work.

Play a key role as part of Dolby's new R+D Center in Bangalore as a Data Engineer in our Advanced Technology Group ""ATG"". ATG is the research and technology arm of Dolby Labs. It has multiple competencies that innovate technologies in audio, video, AR/VR, gaming, music, and movies. Many areas of expertise related to computer science and electrical engineering, such as AI/ML, computer vision, image processing, algorithms, digital signal processing, audio engineering, data science & analytics, distributed systems, cloud, edge & mobile computing, natural language processing, knowledge engineering and management, social network analysis, computer graphics, image & signal compression, computer networking, IoT are highly relevant to our research.

Responsibilities:

As a Data Engineer, you’ll be a part of a growing engineering team building and designing our core data infrastructure for our internal technology research and development efforts. You’ll have the chance to partner closely with our research and data science teams to understand data and functional requirements. We are looking for an experienced data professional who is a problem solver, logical thinker and passionate about everything relating to data and analytics. Your responsibilities include:
• Create and maintain optimal data pipeline architecture for data coming from different sources, in various formats and of different content type (text, audio, video etc.) allowing to standardize, clean and ingest data.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Design and develop solutions which are scalable, generic and reusable. Be responsible for collecting, storing, processing, and analyzing huge sets including, but not limited audio, video, and metadata.
• Develop techniques to analyze and enhance both structured/unstructured data and work with big data tools and frameworks.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Databricks, and AWS ‘big data’ technologies.
• Create data tools for research and data scientist teams.

What You Bring To The Role
• BsC/Msc degree in CS or EE. Work experienced desired, but not required.
• Experience building and optimizing streaming big data pipelines, architectures, and data sets.
• Deep understanding data pipeline frameworks including Databricks and Fivetran.
• Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
• Experience or solid theoretical understanding of data workflows including:
• Ingestion
• Batch and stream processing
• Storage and archiving
• Visualization/Reporting and Dashboards
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Understanding of the current state of infrastructure automation, continuous integration/deployment - CI/CD, SQL/NoSQL, security, networking, and cloud-based delivery models.
• In-depth understanding of:
• NoSql databases (Kafka, HBase, Spark, Hadoop ,Cassandra, MongoDb etc). SQL development and any procedural extension language (T-SQL, PL/SQL, Pg/PLSQL etc.)
• Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Distributed data processing frameworks like Apache Spark, Apache Flink
• Scalable ML pipelines for image, video and audio modalities with tools such as Flyte, MLflow, Prefect, or AirFlow
• Data collection, labeling, cleaning, and generation tools such as LabelBox, SuperAnnontate, Scale Ai, or V7
• Scripting abilities with two or more general purpose programming languages including but not limited to Java, C/C++, C#, Objective C, Python, JavaScript.
• Data modeling and extraction of data from different sources
• Strong documentation skills, communication and client facing Experience
• Experience supporting and working with cross-functional teams in a dynamic environment.

Build your career profile, also within the Careers tab in Employee Central to open the possibility of new opportunities finding you. Express your interest. If you want to express your interest in a specific opportunity and be contacted by a recruiter, click the apply button associated with the relevant job description. The Recruiter is the only one who will see your application.

Please refer to the recruiting website for more information: https://jobs.dolby.com/careers

]]>",Bengaluru,True,False,True,True,False,True,True,True,False,False,False,True,False,False,True,False
Mercede,Positions for Data Engineer,"Technical Skills Competencies
• Deep hands-on expertise in Databricks (Scala or Python).
• Experience in Design and implementation of Big Data technologies (Apache Spark, Hadoop ecosystem, Apache Kafka, NoSQL databases) and familiarity with data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture).
• Experience in working as a Big Data Engineer: query tuning, performance tuning, troubleshooting, and debugging Spark and other big data solutions.
• Familiarity with a full range of data engineering approaches, covering theoretical best practices and the technical applications of these methods.
• Experience building and deploying a range of data engineering pipelines into production, including using automation best practices for CI/CD.
• Very good experience in writing SQL queries.
• Hands-on experience with any of the cloud providers such as AWS or Azure.
• Familiarity with databases and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Ability to juggle and prioritize multiple tasks within a collaborative team environment
• Desire to learn and grow both technical and functional skill sets, and drive team s potential
• Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience And Skills

Microsoft Azure and AWS Certifications
• Certified Databricks Associate Developer (Scala or Python) is highly preferable.
• Trained in Data Factory, Delta lake, Data bricks Notebooks
• Working experience in SAFe - Scaled agile framework
• Working experience in an international team environment
,

This job is provided by Shine.com",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
HuQuo,Interesting Job Opportunity: Azure Data Engineer - ETL/MDM,"Job Description
• To collaborate with various teams/regions in driving facilitating data design, identifying architectural risks and key areas of improvement in data landscape, and developing and refining data models and architecture frameworks
• Technical experience and knowledge in Cloud Data Warehousing, data migration and data transformation
• Develop and test ETL components to high standards of data quality and performance as a hands-on development lead
• Familiarity with Data Lakes, Data Warehouses, MDM, BI, Dashboards, AI, ML
• Design data architecture patterns and ecosystems including data stores (operational systems, data lakes, data warehouses, data marts), ingress patterns (API, streaming, ETL/ELT), and egress patterns (analytics/decision tools, BI tools). Lead, consult or oversee multiple architectural engagements
• Oversee and contribute to the creation and maintenance of relevant data artifacts (data lineages, source to target mappings, high level designs, interface agreements, etc.) in compliance with enterprise level architecture standards
• Experience in leading and delivering data centric projects with concentration on Data Quality and adherence to data standards and best practices.
• Experience in data modeling, metadata support, development and testing for enterprise wide data solutions
• Azure cloud experience is a must have with familiarity of the services: Azure Databricks, Azure Datafactory, Azure Datalake, Spark SQL, PySpark, Airflow, SQL server and Informatica MDM.
• Additional exposure to GCP and AWS is good to have.

Key Skill: Azure Databricks, ADF, ETL, Pipeline Dev, SQL, DWH, ADLS.

(ref:hirist.com)",Gurugram,False,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
AXA XL,Data Engineer,"Gurgaon, Haryana, India

The Application Developer plays a critical role within the Data and Analytics SDC as this person is responsible for designing and implementing data structures to support current and future analytical projects. We are looking for candidates that have experience working with data from a raw, unprocessed state and organizing it intuitively. Building this data pipeline enables our partners to analyze data better and faster – ultimately leading the organization in optimizing the decision-making process.

DISCOVER your opportunity

What will your essential responsibilities include?
• Candidates for this role should have experience developing data processes with source data in a variety of formats (structured / unstructured, databases, APIs) into a target state. This will involve building proper data pipelines to support initial exploration and real-time integration.
• Data development using appropriate tools and techniques to process data required for advanced analytics. A candidate would be expected to interact with Data Engineering Leads and Data Scientists to understand requirements and would be responsible for the development of the solution.
• Providing the right context of data required for a given analysis. This would require the candidate to work with data modelers/analysts to understand the business problems they are trying to solve and create data structures to feed into their analysis.
• Build upon learnings of internal and external data to become more proactive. This includes thinking ahead of what modelers will anticipate with their data needs and designing structures that are intuitive to use.
• Making sure quality and understanding of analytical data. This would require hands-on data experience to look into data issues and seek resolution or acceptance. Create the appropriate amount of documentation, leverage standards, and build upon them. Data should be reconciled and documented at various stages for integrity.
• Take part in developing governance and rigor of data management practice within the Data and Analytics SDC. This will also include partnering with enterprise IT groups and involvement in enterprise data-related functions.
• You will report to Data Manager/Principal Data Engineer.

SHARE your talent

We’re looking for someone who has these abilities and skills:
• Demonstrated ability to work through data complexities which include a variety of sources, formats, and structures. Robust preference for experience in the Insurance domain.
• Ability to see through ambiguous concepts and break down complex problems into manageable components.
• Detail-orientated, proven ability to recognize patterns in data.
• Demonstrated ability to incorporate data quality standards into data development.
• Possesses natural curiosity. Seek to understand the world around you, and question when appropriate.
• Robust SQL Skills required.
• 2-4 years of development experience using data development (visual ETL or coded) / analysis tools (ex. SAS, SPSS, R, Microsoft SSIS/SSAS, Informatica, DataStage, AbInitio).
• Experience in .NET, Python, or Java development is a plus.
• Experience in web extraction, unstructured data, advanced text parsing, machine learning, and NLP a plus.
• Familiarity with developer support tools (TFS/GIT, Jenkins) is a plus.
• College Degree in MIS, Information Technology, Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, or equivalent.

FIND your future

AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks. For mid-sized companies, multinationals, and even some inspirational individuals we don’t just provide re/insurance, we reinvent it.

How? By combining an effective and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business − property, casualty, professional, financial lines, and specialty.

With an innovative and flexible approach to risk answers, we partner with those who move the world forward.

Learn more at axaxl.com

Inclusion & Diversity

AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic.

At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success. That’s why we have made a strategic commitment to attract, develop, advance, and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential. It’s about helping one another — and our business — to move forward and succeed.
• Five Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability, and inclusion with 20 Chapters around the globe
• Robust support for Flexible Working Arrangements
• Enhanced family-friendly leave benefits
• Named to the Diversity Best Practices Index
• Signatory to the UK Women in Finance Charter

Learn more at axaxl.com/about-us/inclusion-and-diversity. AXA XL is an Equal Opportunity Employer.

Sustainability

At AXA XL, Sustainability is integral to our business strategy. In an ever-changing world, AXA XL protects what matters most for our clients and communities. We know that sustainability is at the root of a more resilient future. Our 2023-26 Sustainability strategy, called “Roots of resilience”, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations.

Our Pillars
• Valuing nature: How we impact nature affects how nature impacts us. Resilient ecosystems - the foundation of a sustainable planet and society – are essential to our future. We’re committed to protecting and restoring nature – from mangrove forests to the bees in our backyard – by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans.
• Addressing climate change: The effects of a changing climate are far reaching and significant. Unpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption. We're building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions.
• Integrating ESG: All companies have a role to play in building a more resilient future. Incorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business. We’re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting.
• AXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL’s “Hearts in Action” programs. These include our Matching Gifts program, Volunteering Leave, and our annual volunteering day – the Global Day of Giving.

For more information, please see axaxl.com/sustainability

Flexible Work Eligible

None

AXA XL is an Equal Opportunity Employer.

Location

IN-HR-Silokhera Gurgaon

Job Field

IT

Schedule

Full-time

Job Type

Standard",Gurugram,True,False,True,True,False,False,False,False,False,False,True,False,False,False,False,False
Inference Labs,Data Engineer,"Responsibilities for the job Key Responsibilities: - Data Model Designing, Developing and maintaining Data pipelines on cloud (AWS Platform) Translate business needs to technical specifications and framework Maintain and support data mart, data analytics platforms & application. Perform quality assurance to make sure the data correctness Develop sub-marts using SQL and OLAP function to fulfil immediate/ad-hoc need of the business users basis the comprehensive marts Monitoring of the performance of ETL and Mart Refresh processes, understand the problem areas and open a project to fix the performance bottlenecks. Other Responsibilities (If Any):- Availability during month-end Deck generation, may be sometime during week-end/holidays. Eligibility Criteria for the Job Education B.E/B.Tech in any specialization, BCA, M.Tech in any specialization, MCA Work Experience Data Engineer: 4+ years of experience in data engineering on cloud platforms like AWS, Azure, GCP Exposure with working on BFSI domain / big data warehouse project Exposure to manage multiple source of the information, both structured / unstructured data Manage data lake environment for point in time analysis (SCD Type 2), multiple refresh during the day, event based refresh Should have exposure on Managing environment having real time dashboard, data mart requirement. Primary Skill Must have orchestrated using any of the cloud platforms Expert in writing complex SQL Command using OLAP Working experience on BFSI Domain Technical Skills Must have orchestrated at least 3 projects using any of the cloud platforms (GCP, Azure, AWS etc.) is a must. Must have worked on any cloud PaaS/SaaS database/DWH such as AWS redshift/ Big Query/ Snowflake Python/Java Hands - on Exp from data engineering perspective is a must Experience with any of the object-oriented/object function scripting languages: Python, Java, Scala, Shell, .NET scripting, etc. is a must Experience in at least one of the major ETL tools (Talend + TAC, SSIS, Informatica) will be added advantage Management Skills Ability to handle given tasks and projects simultaneously in an organized and timely manner. Soft Skills Good communication skills, verbal and written. Attention to details. Positive attitude and confident.",,True,False,True,True,False,False,False,False,False,False,False,False,True,False,False,True
PwC,Data Engineer-Manager-P&T Labs,"Line of Service
Internal Firm Services

Industry/Sector
Not Applicable

Specialism
IFS - Internal Firm Services - Other

Management Level
Manager

Job Description & Summary
A career in National Special Functions, within Internal Firm Services, will provide you with the opportunity to support service, sector, and market leaders deliver the unique PwC client experience to our clients. You’ll play an important part in continuously innovating and improving Firm operations so that we can continue to provide the highest quality of services to our current and prospective clients.

Our team focuses on representing data as a strategic business asset to help serve our clients. You’ll focus on using data and information across PwC to drive change and improvements in data related operations to help enable the business as well as provide insights related to attendant risks.

Preferred Knowledge/Skills:

Demonstrates intimate knowledge and/or a proven record of success in the following areas:
• Understanding architectural design and data platform delivery in technologies that include, but are not limited to cloud, ETL, data streaming, data storage, data modeling, APIs/microservices, automation, continuous integration/continuous deployment;
• Showcasing work experience as a Data Engineer, Data Architect or similar role;
• Showcasing data engineering knowledge around complex efforts within established Software Development Lifecycles and methodologies including agile, scrum, iterative and waterfall;
• Showcasing technical knowledge that spans multiple platforms and portfolio of applications with demonstrated knowledge of the business strategic priorities in order to resolve complex problems;
• Utilizing IT processes and frameworks including, but not limited to, Identity Access Management (IdAM), Enterprise Application Integration, Data Warehousing, Business Intelligence, Reporting, Mobility, Master Data Management, and Search;
• Understanding of database structure principles;
• Showcasing advanced experience building and maintaining optimal data pipeline architecture and data streaming and integrations using tools such as ADF, SSIS, Informatica, API Management, Enterprise Service Bus (preferably Kafka);
• Showcasing advanced SQL knowledge and experience working with relational databases and performance optimization;
• Demonstrating data mining and segmentation techniques;
• Exhibiting knowledge in relational SQL, NoSQL and Big Data technologies;
• Understanding Data Federation/Virtualization technologies, such as PowerBI, Tableau, D3.js, and implementing Cloud based solutions;
• Assessing and analyzing system requirements;
• Showcasing analytical skills and a problem-solving attitude;
• Demonstrating virtual leadership and motivational skills;
• Recommending and participating in activities related to the design, development and maintenance of the Enterprise Data Architecture;
• developing internal relationships and PwC brand;
• Demonstrating time management skills with the ability to handle multiple projects simultaneously;
• Leveraging business knowledge and interpersonal skills to build, maintain, and influence relationships with leaders throughout the business and IT.

Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:

Degrees/Field of Study preferred:

Certifications (if blank, certifications not specified)

Required Skills

Optional Skills

Desired Languages (If blank, desired languages not specified)

Travel Requirements
Not Specified

Available for Work Visa Sponsorship?
No

Government Clearance Required?
Yes

Job Posting End Date
May 10, 2023",Hyderabad,False,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Mindera,Data Engineer,"We are looking for an experienced Data Engineer to join our team.

Here at Mindera, we are continuously developing a fantastic team and would love for you to join us.

As a Data Engineer, you will be responsible for building, maintaining, scaling, and integrating big data-based platforms. you will also be engaged with the Data Science team in setting up and automating the Data Science models/algorithms for production use.

This is a fantastic opportunity for someone who is passionate about Data and is excited to use different tools to provide data insight for further analysis and drive business decisions.

National and international expected travelling time varies according to project/client and organisational needs: 0%-15% estimated

Requirements

You’re great at
• Python
• AWS like (Glue, S3, EMR, Athena and ECS/Fargate)
• SQL
• Airflow
• Data Modelling
• Pyspark

It also would be cool if you have
• Exposure to DBT would be preferable
• Experience working with modern data platforms such as redshift or snowflake would be preferable
• Experience working with Airflow, Docker, Terraform and CI/CD would be preferable
• Experience working with docker, Scala, and Kafka would be an added advantage

What You Will Be Doing
• Implement/support new data solutions in the data lake/warehouse built on the snowflake
• Develop and design data pipelines using python.
• Design and Implement Continuous Integration/Continuous Deployments pipelines.
• Perform Data Modelling using downstream requirements.
• Develop transformation scripts using advanced SQL and DBT.
• Write test cases/scenarios to ensure incident-free production release.
• Collaborate closely with data analysts and different domains such as Finance, risk, compliance, product and engineering to fulfil requirements.
• Debug production and development issues and provide support to colleagues where necessary.
• Perform data quality checks to ensure the quality of the data exposed to the end users.
• Build strong relationships with team, peers and stakeholders.
• Contributes to overall data platform implementation.

Benefits

We offer
• Flexible working hours (self-managed)
• Competitive salary
• Annual bonus, subject to company performance
• Access to Udemy online training and opportunities to learn and grow within the role

At Mindera we use technology to build products we are proud of, with people we love.

Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.

We partner with our clients, to understand their products and deliver high-performance, resilient and scalable software systems that create an impact on their users and businesses across the world.

You get to work with a bunch of great people, and the whole team owns the project together.

Our culture reflects our lean and self-organisation attitude.

We encourage our colleagues to take risks, make decisions, work in a collaborative way and talk to everyone to enhance communication. We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean and collaborative environment.

Check out our Blog: http://mindera.com/ and our Handbook: http://bit.ly/MinderaHandbook

Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | Chennai, India | Bengaluru, India",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
Rishabh Software,Big Data Engineer,"Job Description:

Roles and Responsibilities:

1) Work with BigData Practice Tech lead to Execute BigData Projects

2) Work with Techlead , helping him in Solutioning, Architecture and Technical Design

3) Analyze requirements and prepare low level design

4) Hands on implementation of Data ingestion, data processing and Data storage code and algorithms

5) Team management under Techlead guidance - including work distribution and delivery

6) Participate in potential client meetings and demos

Required Skills:

Any one programming Language - Java or Scala

Good Experience with Apache Spark

Any one Data integration platform - Kafka or similar

Any one No Sql data storage - S3 or No Sql Database

One live BigData project - Data ingestion , processing and Storage

Basic Cloud Exposure - AWS preferred

Excellent Analytical and problem solving skills.

Excellent Communication Skills",Vadodara,False,False,True,True,False,False,False,True,False,False,False,True,False,False,False,False
Cloud Software Group,Senior Data Engineer,"About Cloud Software Group

Cloud Software Group combines the capabilities of both Citrix and TIBCO, creating one of the world’s largest cloud software providers, serving more than 100 million users around the globe. When you join Cloud Software Group, you are making a difference for real people, each of whom count on our suite of cloud solutions to get work done – from anywhere. Members of our team will tell you that we value diverse lived experiences, varied perspectives, and having the courage to take risks. Our teams are encouraged to learn, dream, and build the future of work. We are on the brink of another Cambrian leap - a moment of immense evolution and growth. And we need your expertise and experience to do it. Now is the perfect time to move your skills to the cloud.

Position Summary

This is an individual contributor role with responsibility for supporting all data warehouse processes including technical analysis, design, development, implementation, and support of ETL solutions. The ideal candidate needs to have at least 5 years of experience developing with Microsoft SQL applications in an implementation and support role of a business intelligence organization.

Primary Duties / Responsibilities

Responsibilities will include, but are not limited to:

• Supporting the designs, tasks, and continuous improvements to maintain a scalable data warehouse

• Analyzing and validating data to ensure that business requirements are satisfied

• Creating data flow diagrams to depict business logic relating to data transformations

• Creating conceptual, logical, and physical data models for relational and dimensional solutions

• Breaking down, estimating, and executing increments of work

• Developing ETL packages of high complexity to fulfill all the business requirements

• Supporting deployment and delivery of defined technical solutions

• Communicating accurate and timely project status, issues, risks, and scope changes

• Performing root cause analysis of data discrepancies

• Creating data dictionaries and business glossaries to document data lineages, data definitions and metadata for all business-critical data domains

• Documenting all work (both technical and procedural) and ensuring that co-workers understand how to support system from an operational perspective

• Working in a highly collaborative team environment following the Agile Framework for planning and executing deliverables

Qualifications (include knowledge, skills, abilities, and related work experience)

• Bachelor’s degree in computer science or related field, or equivalent combination of education and recent, relevant work experience

• Minimum 5 years of experience in developing T-SQL Queries, Stored Procedures, and ETL packages with Microsoft SQL databases

• Strong understanding of data warehouse design and report development principles

• Experience in creating data flow diagrams and data models pertaining to business intelligence

• Experience in analyzing and developing reporting output such as Power BI, Tableau, or SSAS

• Strong interpersonal and problem resolution skills

• Strong teamwork and customer support focus

• Strong written (technical documentation) and verbal communication skills

• Ability to handle numerous conflicting priorities in a professional manner

Cloud Software Group is firmly committed to Equal Employment Opportunity (EEO) and to compliance with all federal, state and local laws that prohibit employment discrimination on the basis of age, race, color, gender, sexual orientation, gender identity, ethnicity, national origin, citizenship, religion, genetic carrier status, disability, pregnancy, childbirth or related medical conditions, marital status, protected veteran status, and other protected classifications.",Bengaluru,False,False,True,False,False,False,False,False,True,True,True,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"Roles and responsibilities:
• Mandatory: Strong in Azure, ADF, Data Lake, Databricks, Pyspark
• Hands-on-experience in developing data lake solutions using Azure (Azure data factory for ingestion, Data Lake gen 2 and Azure SQL server for storage, Azure analysis service for transformations, Azure data bricks)
• Implement a robust data pipeline using Microsoft Stack.
• Create reusable and scalable data pipelines.
• Development and deployment of new data platforms.
• Leverage Azure BI services for development of Big Data Platforms.
• Work closely with the Product Owners and Architects to develop Azure Data Platforms.
• Work with the leadership to set the standards for software engineering practices within the team and support across other disciplines.
• Produce high-quality code that allows us to put solutions into production.
• Refactor code into reusable libraries, APIs, and tools.",Chennai,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Affine,Data Engineer,"Company Description

About Company

http://www.affine.ai

""AFFINE"" cited by GARTNER as a SPECIALIST MIDSIZE CONSULTANCY in ANALYTICS and MACHINE LEARNING solutions and services. Click to Read More ""

Affine is a provider of high-end analytics services to solve complex business problems with offices in NJ, USA & Bangalore, India. We combine data driven algorithmic analysis with heuristic domain expertise to provide actionable solutions that empower organizations make better and informed decisions. Affine's value proposition is enabling clients to implement and realize ROI of the recommendations.

Affine has a group of people with significant experience in Analytics industry along with solid pedigree, deep business understanding and strong problem solving acumen. Our group primarily consists of Statisticians, Operations Researchers, Econometricians, MBAs and Engineers. Our employees have experience of working for many Fortune 500 companies.

Job Description

What the candidate will do:
• Contribute to adoption of cloud & cloud-based technologies and good design practices, while finding opportunities to simplify and scale
• Resolve problems and roadblocks as they occur with peers and help unblock junior members of the team. Follow through on details and drive issues to closure
• Define, develop, and maintain artifacts like technical design or partner documentation
• Drive for continuous improvement in Data engineering process within an agile development team
• Own and deliver assigned sprint tasks and help drive the team forward.
• Communicate and work effectively with geographically distributed cross functional teams

Experience

4 to 6 Years in Deploying models, Sage Maker or TensorFlow

Required skillset.
• Big Data: Spark, Kafka, Hadoop, Hive, SQL and NoSQL
• Cloud: AWS, EMR, Qubole/Databricks, VPC
• Devops: Docker containers and Jenkins. Spinnaker is preferred but not required.
• Programming languages: Scala and Pyspark is mandatory
• Agile and scrum experience and working with a remote team (nice to have, not required)

Must Have Skills
• Spark, AWS, Scala/Python, SQL, Java
• ML ops tools:Tensorflow or Sagemaker

Additional Information

Others
• Quick learner
• Excellent written and oral communication skills
• Excellent interpersonal & organizational skills
• Good listening and comprehension skills",Bengaluru,True,False,True,True,True,False,False,True,False,False,False,False,False,False,False,False
DISH Careers,Data Engineer,"About DISH:

DISH Network Technologies India Pvt. Ltd is a technology division of DISH. In India, the technology division is located in Bengaluru and Hyderabad. These centers were established in the market to provide opportunities to the world’s best engineering talent, and to further boost innovation in multimedia network and telecommunications development. The Bengaluru center is a state-of-the-art facility, which plays a crucial role in fostering innovation. One of DISH’s largest development centers outside the U.S., we have a growing team of over 600 dynamic professionals, who are committed to delivering our vision to change the way the world communicates. With multidisciplinary expertise of our engineers, we have filed for over 200 patents in the market

Job Duties and Responsibilities:
• Actively engage with other data warehouse engineers representing business needs and shepherding projects from conception to production
• Creation and optimization of data engineering pipelines for analytic projects
• Strong analytic capability and the ability to create innovative solutions
• Participate in the Unit Testing, defect resolution, and root cause analysis of data sources as well as actively engaged in the identification and resolution of PROD broke issues
• Provide technical guidance to L1 team members and help to resolve ETL related issues
• Need to work as on call-support

Skills, Experience and Requirements:
• Engineering degree with 3 to 6 years of experience in development and production support of large Enterprise Data Warehouse in cloud data environment
• Experience in developing/debugging and fixing data ingestion pipelines both real time and batch
• Should have knowledge on AWS services - S3 bucket, EC2 , CloudWatch , Athena, lambda, Cloudtrail, Dynamodb
• Experience in transforming/integrating data in Redshift/Snowflake
• Strong in writing complex SQLs to ingest data into cloud data warehouses
• Good hands on experience in shell scripting or python
• Experience with scheduling tools - ControlM, Airflow , StepFunction
• Troubleshooting of ETL jobs and addressing production issue and suggest job enhancements
• Perform root cause analysis (RCA) for failures
• Good Communication skills – written and verbal with the ability to understand and interact with the diverse range of stakeholders
• Capable of working without much supervision",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,True,False,True,True
MPOWER Financing,"Data Engineer - Data and Analytics - Bangalore, India","THE COMPANY

MPOWER’s borderless loans and scholarships enable students from around the world to realize their full academic and career potential by attending top universities in the U.S and Canada.

As a mission-oriented fintech/edtech company, we move extremely quickly and leverage the latest technologies, global best practices, and heavy analytics to tackle one of the biggest challenges in financial inclusion. We’re backed by over $150 million in equity capital from top global investors, which enables fast growth and provides our company with financial stability and a clear path to an IPO over the coming years.

Our global team is composed of former management consultants, financial service and technology professionals, and other experts in their respective fields. We work hard, have fun, and believe strongly in our cause. For us, MPOWER’s mission is personal.

As a member of our team, you’ll be challenged to think quickly, act autonomously, and constantly grow creatively in an environment where fast change and exponential growth are the norm. Ideation and implementation happen very quickly. We value feedback and emphasize personal and professional development by providing the resources you need to further your skills and grow with the company. MPOWER is committed to cultivating your strengths and curiosity and helping you make an immediate impact.

MPOWER has been named one of the best fintechs to work for by American Banker for 2018, 2019, 2020, and 2021. We pride ourselves on being a “growth company for grown-ups,” where there are no pool tables but rather great health, education, and maternity/paternity benefits instead. Our team diversity has been recognized as well; we’re one of the most diverse workforces in the world in terms of nationality, gender, religion, age, sexual orientation, and educational background.

THIS IS A FULL-TIME POSITION, BASED IN OUR BANGALORE OFFICE

THE ROLE

You will be tasked with building and maintaining MPOWER’s data infrastructure. You’ll also play a key role in acquiring, organising and analysing data to provide insights that enable the company in making sound business decisions. This includes, but is not limited to:
• Maintaining MPOWER’s database and building on the existing database infrastructure
• Establishing the needs of different users and monitoring user access and security
• Capacity planning and refining the physical design of the database to meet system storage requirements
• Creating efficient queries and tools to obtain data for different business needs
• Building data models to identify, analyze and interpret trends or patterns in data sets that inform business decisions and strategy
• Working with various internal and external stakeholders to maintain and develop enhanced data collection systems
• Performing periodic data analyses, creating and presenting findings and insights
• Performing scheduled data audits in order to locate and correct code errors and maintain data integrity
• Collaborating with MPOWER’s global tech team to build data collection and data analysis tools

THE QUALIFICATIONS
• Undergraduate degree in computer science; advanced degree preferred
• 5+ years of experience in database programming, database administration and data analysis
• Must have prior experience in building high quality databases in accordance with end users information needs and views
• Proficiency in Big Data and Hadoop ecosystems.
• Deep familiarity with database design and documentation
• Hands-on expertise and exposure to at least one database technology (MySQL, PostgreSQL)
• Advanced knowledge of R/Python, PySpark, or Scala is a plus
• Prior experience building data pipelines and data orchestration is a plus.
• Superior analytical and problem solving skills
• Proven ability to create and present comprehensive reports
• Ability to multitask and own several key responsibilities at a given time
• Passion for excellence: constantly striving to improve professional skills and business operations

A passion for financial inclusion and access to higher education is a must, as well as comfortable working with a global team across multiple-time zones and sites!

In addition, you should be comfortable working in a fast growth environment, meaning a small agile team, fast-evolving roles and responsibilities, variable workload and tight deadlines, a high degree of autonomy, and 80-20 everything.

MPOWER Financing focuses on Financial Services, Finance, Finance Technology, Higher Education, and Education Technology. Their company has offices in New York City, Washington DC, and Washington. They have a small team that's between 11-50 employees. To date, MPOWER Financing has raised $7.291M of funding; their latest round was closed on October 2016.

You can view their website at http://www.mpowerfinancing.com/ or find them on Twitter, Facebook, and LinkedIn.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,False,False,False
Cortex Consultants LLC,Data Engineer,"Hi,

Welcome to Cortex

Job Title: Data Engineer

Job Description

2+ years of Data Engineer experience in Snowflake (on Azure Cloud Preferred).

Strong knowledge of SQL to build queries and Optimization techniques.

Strong Knowledge of the ETL process using SSIS / ADF (Azure Data Factory) / Matillion

Experience of Python programming is an added advantage.

Location: Chennai

Work type-Hybrid

Immediate joiners

Interested candidates share your resume to

Deepak.g@cortexconsultants.com

Contact No: 9080100600",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Roche,Data Engineering Manager,"The Position
Engineering Manager is a critical leadership role in our Data Engineering team. This is a people management role that needs the ability to hire and grow top engineering talent and to manage multiple teams. It includes responsibility to deliver and operate high quality, scalable, and extensible products & solutions, including making appropriate design and technology choices. The role requires strong strategic thinking and making build/buy/partner decisions for technical capabilities. Effective Communication is critical, as you will be working closely with a variety of stakeholders to understand and address their needs. A healthcare background with experience in integrating healthcare IT systems would be good to have.

KEY RESPONSIBILITIES
• Manage team of Data Engineers working on multiple data analytics products.
• Work with different agile product teams, understand and fulfill their staffing needs.
• Work with business stakeholders to develop high level project plans and roles and responsibilities.
• Prepare training and development plans for the team.
• Understand and create a career path for the team members.
• Evolve and develop a long-term roadmap for team and projects.
• Apply data engineering best practices in terms of quality, security, scalability and maintainability.
• Participate in how the budget and staff is allocated for the projects.
• Maintain project time frames, budget estimates and status reports.
• Create management, communication plans and processes. Analyze and develop process for management and technical duties.
• Foster team bonding and trust within the team. Responsible for hiring, growing and motivating engineers on your team, ensuring you recruit and retain top talent.

REQUIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• BS degree in Computer Science, Computer Engineer or a related technical discipline with 10+ years of IT industry experience.
• At least 4-6 years of proven managerial experience developing a high-performing team.
• Experience in Agile Solution Delivery and Operations Management and people management.
• Quick learner with the ability to understand complex workflows and develop and validate innovative solutions to solve difficult problems.
• Strong communication, with the ability to explain complex technical problems to non-technical audiences and the ability to translate customer requirements to technical designs.
• Strong interpersonal skills, with proven ability to navigate complex corporate environments and influence stakeholders and partners.

DESIRED EXPERIENCE, SKILLS & QUALIFICATIONS
• Proven work experience in AWS or other cloud related technologies.
• Experience of working in product based organization
• Proven work experience as an Engineering Manager or similar role
• Communication skills for overseeing staff and working with other management personnel
• Organizational skills for keeping track of various budgets, employees, and schedules simultaneously
• Leadership, team-building, and mentoring skills
• Personnel and project management skills
• Ability to work on multiple projects in various stages simultaneously
• Experience in the Healthcare Laboratory domain is a plus.

EDUCATION

Bachelor’s degree in Engineering

Who we are

At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.

Roche is an Equal Opportunity Employer.",Pune,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Data.Ai,DNA Team - Data Engineer,"data.ai is the mobile standard and the trusted source for the digital economy. Our vision is to be the first Unified Data AI company that combines consumer and market data to provide insights powered by artificial intelligence. We passionately serve enterprise clients to create winning digital experiences for their customers.

We care deeply about our high-performance culture and operate as a global team. We put our customers at the center of every decision [Customer First], follow through with what we say we are going to do [Own It & Deliver] and propose solutions, not just issues [Challenge, Them Commit] to Win As A Team.

We are a remote-first company and we trust our people to get it done from the location that works for them.

What can you tell your friends when they ask you what you do?

As a DNA Team Data Engineer, I’ll be a key contributor to DNA team data services. I’ll help the DNA team to build and enhance internal processes of data production and transaction/transformation, as well as internal tools. And help colleagues from other teams and/or external clients to better experience the DNA team services.

You will be responsible for and take pride in…
• Exciting Projects using technical expertise across Python, SQL, Spark, DataBricks
• Build data pipeline across different data sources/databases such as AWS S3, PG database, and Snowflake
• Produce and maintain relevant documentation
• Support internal and external customers
• Becoming better at what you do every day

You should recognize yourself in the following…
• Bachelor’s degree in Computer Science, Engineering, or equivalent experience
• At least 5 years of related work experience in building data pipelines
• Strong skills in Python and PL/SQL
• Deep understanding and experience in building data pipelines across different data sources/databases such as AWS S3, PG database, and Snowflake
• Experience in data processing such as ETL
• Knowledge of machine learning and AI is preferred
• Familiarity with specific app markets (e.g.: Gaming, Entertainment, Finance, etc.) is a big plus
• Strong problem-solving, analytical, and troubleshooting skills
• A self-starter who identifies and solves problems before anyone has noticed
• Fluent in English, both written and oral

data.ai are in the process of establishing an entity in India, in the interim the employees will be on the rolls of Leap 29 our Global Employer of Record",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,True
Danske Bank,Senior Data Engineer-ETL Datastage,"Experience 5-8Years

The ideal applicant should have the following skills:

- Strong technical experience in Data Warehousing and Experience in working with ETL tools (Datastage, Informatica etc) for the purpose of creating data marts for analytical purposes

- Strong understanding of relational database concepts & technology. Exposure to Big Data technologies is an added advantage.

- Strong analytical and problem solving skills with the ability to collect, organize, analyse and process large volumes of data in a complex environment

- Good written and verbal communication skills with the ability to communicate and articulate one's thought process clearly.

- Be self driven and work closely with business stakeholders, in a global environment, to gather enough context to translate the business
objective into an analytical solution.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Splunk,Data Engineer - 27516,"As a Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have a deep understanding of the architecture for enterprise-level data lake solutions using multiple platforms (RDBMS, AWS, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.

What you'll do: Yeah, I want to and can do that.
• As a Data Engineer, you will be responsible for engineering data pipelines for Splunk’s enterprise data platform, democratizing datasets, enabling advanced analytics capabilities, integrating data from various systems, and applications. You will work as part of an evolving Enterprise Data Management(EDM) - Data Engineering Team to rapidly design, secure, build, test and release new data enablement capabilities. The role will collaborate closely with other specialists, Product Managers & key stakeholders across the company.
• Build large-scale batch and real-time data pipelines using the cloud data technologies, such as Snowflake, Matillion, Kubernetes, Python, Apache Airflow and Apache Kafka
• Serve as a resource for data management implementations on other technology teams and collaborate with data owners, business owners, and leaders.
• Supports the design and development of framework based data integration and interoperability across multiple Splunk Business applications.
• Advanced level skills in Python, SQL, data integration, data modeling and data architecture.

Requirements: I’ve already done that or have that!
• A minimum of 5 years of related experience
• 3+ years of experience as a Data Warehouse Architect or Data Engineer.
• 2+ years of experience driving adoption and building automation of data management services and tools.
• 2+ years of experience with API based ELT automation framework, data management, or interface design, development and maintenance.
• Large scale design, implementation and operations of Cloud data storage technologies such as AWS Redshift, Snowflake, Kubernetes, etc.
• 3+ years of experience with programming scripting and data science languages such as Python, SQL, etc.
• Experience in building data models, including conceptual, logical, and physical for Enterprise Relational, and Dimensional Databases.
• Advanced knowledge of Big Data concepts in organising both structured and unstructured data

Preferred knowledge and experience: These are a huge plus.
• Knowledge of Splunk products
• Knowledge of DBT
• Experience with Sales Operations, Partner Operations and customer success business processes and applications

Education: Got it!
• Bachelor’s degree preferably in Computer Science, Information Technology, Management Information Systems, or equivalent years of industry experience.

We value diversity, equity, and inclusion at Splunk and are an equal employment opportunity employer. Qualified applicants receive consideration for employment without regard to race, religion, color, national origin, ancestry, sex, gender, gender identity, gender expression, sexual orientation, marital status, age, physical or mental disability or medical condition, genetic information, veteran status, or any other consideration made unlawful by federal, state, or local laws. We consider qualified applicants with criminal histories, consistent with legal requirements.",Hyderabad,True,False,True,False,False,False,False,True,False,False,False,False,True,False,True,True
Verizon,Manager-Data Engineering,"When you join Verizon

Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect across the globe. We’re a diverse network of people driven by our shared ambition to shape a better future. Here, we have the ability to learn and grow at the speed of technology, and the space to create within every role. Together, we are moving the world forward – and you can too. Dream it. Build it. Do it here.

What you’ll be doing...

As a Manager for Data Engineering team, you will be managing data platforms and implementing new technologies and tools to further enhance and enable data science/analytics, focus to drive scalable data management and governance practices. Leading the team of data engineers & solutions architects to deliver solutions to business teams.
• Driving the vision with leadership team for data platforms enrichment covering the areas like Data Warehousing/Data Lake/BI across the portfolio.
• Defining and executing on a plan to achieve that vision.
• Building a high-quality Data engineering team and continue to drive to scale up.
• Ensuring the team adheres to the standard methodologies on data engineering practices.
• Building cross-functional relationships with Data Scientists, Data Analysts and Business teams to understand data needs and deliver data for insight solution.
• Driving the design, building, and launching of new data models and data pipelines.
• Driving data quality across all data pipelines and related business areas.

Where you’ll be working…

In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.

What we’re looking for...

You are curious and passionate about Data and highly scalable data platforms. People count on you for your expertise in data management in all phases of the software development cycle. You create environments where teams thrive and feel valued, respected and supported. You enjoy the challenge of managing resources and competing priorities in a dynamic, complex and deadline-oriented environment. Building effective working relationships with other managers across the organization comes naturally to you.

You’ll need to have:
• Bachelor’s degree or four or more years of work experience.
• Six or more years of relevant work experience.
• Two or more years of experience in leading the team and tracking the end-to-end deliverables.
• Experience in end-to-end delivery of Data Platform Solutions and working on large scale data transformation.
• Knowledge of Spark, Hive, Scala, Pig, Kafka, Pulsar, Nifi, Python, Shell scripting.
• Knowledge of Google Cloud Platform/BigQuery.
• Knowledge of Teradata.
• Experience in working with DevOps tools like Bitbucket, Artifactory, Jenkins.
• Knowledge of Data Governance and Data Quality.
• Experience in building / mentoring the team.

Even better if you have one or more of the following:
• Master’s degree.
• Experience in data engineering, big data, hadoop and DevOps technologies.
• Certifications in any Data Warehousing/Analytical solutioning.
• Certification in program/project management.
• Experience in technical leadership in architecture, design, implementation and support of large-scale data and analytics solutions that are highly reliable, flexible, and scalable.
• Ability to meet tight deadlines, multi-task, and prioritize workload.
• Experience in collaborating with cross-functional teams and managing stakeholder expectations.
• Experience in working with globally distributed teams.
• Good Communication and Presentation skills.

If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.

Diversity & Inclusion

We're proud to be an equal opportunity employer. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.

COVID-19 Vaccination Requirement

Please note, in countries where there is a COVID-19 related government order or rule, Verizon is required to ensure that all employees accessing our workplace comply with these mandatory requirements. If you work in one of these locations, you will be required to provide us with your vaccination status prior to joining. If this, or any other COVID related requirement applies in your location, we will notify you about this before you start work.",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False
FairMoney,Senior Data Engineer,"About FairMoney

FairMoney is a credit-led mobile bank for emerging markets. The company was launched in 2017, operates in Nigeria & India, and raised close to €50m from global investors like Tiger Global, DST & Flourish Ventures. The company has offices in France, Nigeria, and India.

Role and responsibilities

At FairMoney, we are making a lot of data driven decisions in real time: risk scoring, fraud detection as examples.

Our data is mainly produced by our backend services, and is being used by data science team, BI team, and management team. We are building more and more real time data driven decision making processes, as well as a self serve data analytics layer.

As a senior data engineer at FairMoney, you will help building our Data Platform:

• Ensure data quality and availability for all data consumers, mainly data science and BI teams.
• Ingest raw data into our DataWarehouse (BigQuery / Snowflake)
• Make sure data is processed and stored efficiently:
• Work with backend teams to offload data from backend storage
• Work with data scientists to build a machine learning feature store
• Spread best practices in terms of data architecture across all tech teams
• Effectively form relationships with the business in order to help with the adoption of data-driven decision-making.

You will be part of the Datatech team, sitting right between data producers and data consumers. You will help building the central nervous system of our real time data processing layer by building an ecosystem around data contracts between producers and consumers.

Our current stack is made of

• Batch processing jobs (Apache Spark in Python or Scala)
• Streaming jobs (Apache Flink deployed on Kinesis Data Analytics - Apache Beam deployed on Google Dataflow)
• REST apis (Python FastApi)

Our tool stack

• Programming language: Python, SQL
• Streaming Applications: Flink, Kafka
• Databases: MySQL, DynamoDB
• DWH: BigQuery, Snowflake
• BI: Tableau, Metabase, dbt
• ETL: Hevo, Airflow
• Production Environment: Python API deployed on Amazon EKS (Docker, Kubernetes, Flask)
• ML: Scikit-Learn, LightGBM, XGBoost, shap
• Cloud: AWS, GCP

Requirements

You will work on a daily basis with the below tools, so you need working experience on

• Languages: Python and Scala.
• Big data processing frameworks: all or one of Apache Spark (batch/streaming) - Apache Flink (streaming) - Apache Beam.
• Streaming services: Apache Kafka / AWS Kinesis.
• Managed cloud services: one of AWS EMR / AWS Kinesis Data Analytics / Google Dataflow.
• Docker.
• Building REST APIs.

Ideally, you have experience with:

• deployment/management of stateful streaming jobs.
• the Kafka ecosystem: Kafka connects mainly.
• infrastructure as code frameworks (Terraform).
• architecture around data contracts: Avro Schemas management, schema registries (Confluent Kafka / AWS Glue).
• Kubernetes.

Overall experience required for this role: 6+ Years.

Benefits

• Training & Development
• Family Leave (Maternity, Paternity)
• Paid Time Off (Vacation, Sick & Public Holidays)
• Remote Work

Recruitment Process • A screening interview with one of the members of the Talent Acquisition team for 30 minutes.
• Takeaway assignment to be done at home.
• Technical design interview for 60-90 minutes.",Bengaluru,True,False,True,False,False,False,False,False,False,True,False,True,False,True,True,True
Boston Consulting Group,IT Senior Data Engineer,"WHAT YOU'LL DO
Under the general supervision of senior management and the Data Engineering Chapter Lead in the Enterprise Data Tribe, you will be working with key customers to deliver timely and accurate data engineering pipelines in a secure manner. You are expected to provide guidance on proper engineering design ensuring that our architectural guidelines are met, and the appropriate support model is in place for production deployments. This role will work in a multi-functional agile squad and support the product owner. You will also be supporting the Chapter Lead and other team members of the Data Engineering chapter in proof-of-concept activities and other Data Engineering chapter related work.
YOU'RE GOOD AT
You have experience in data warehousing, data modelling, and the building of data engineering pipelines. You are well versed in data engineering methods, such as ETL and ELT techniques through scripting and/or tooling. You are good in analysing performance bottlenecks and providing enhancement recommendations; you have a passion for customer service and a desire to learn and grow as a professional and a technologist.
• Viewed as subject matter expert for stakeholders, possessing in-depth knowledge and specialized technical skill set
• Able to work independently with minimal supervision
• Proactively identify and independently solve non-routine problems by applying expertise
• Perform research of viable technical and/or non-technical solutions
• Develop internal network with senior leaders within the chapter and key stakeholders in the tribe.
• Develop strategies for data engineering in Snowflake using DBT and Talend.
• Architect, design, and implement data pipelines to feed data models for subsequent consumption
• Actively monitor and resolve user support issues, working closely with your assigned squad and other squads as part of the chapter.
• Develop and maintain architectural standards, best practices, and measure compliance

YOU BRING (EXPERIENCE & QUALIFICATIONS)
You bring to us experience in data engineering technologies, database development, and data model design; both in IaaS and PaaS Cloud (AWS and/or Azure) environments.
• Minimum of a Bachelor's degree in Computer science, Engineering or a similar field
• 5-7+ years of project experience, preferably as a Data Engineer/Developer and minimum of 3 years of agile project experience is a must (preferred tool - JIRA)
• Essential: Must have exposure to technologies such as DBT, Talend and Apache airflow
• Essential: SQL is heavily focused. An ideal candidate must have hands-on experience with SQL database design
• Essential: Extremely talented in applying SCD, CDC and DQ/DV framework
• Essential: Experience in data platforms: Snowflake, Oracle, SQL Server, PostgreSQL, and MySQL
• Essential: Lead R&D efforts to find solutions for data engineering requirements not addressed by existing technology standards
• Essential: Demonstrate ability to write new code i.e., well-documented and stored in a version control system (we use GitHub & Bitbucket)
• Essential: Develop metrics that illuminate the flow of data across the organization
• Essential: Experience in data modelling and relational database design
• Preferred: Experience in AWS and Azure data platforms.
• Preferred: Experience in Qlik Compose, Fivetran and HVR
• Preferred: Strong programming/ scripting skills (Python, Powershell, etc.)

YOU'LL WORK WITH
As part of the Enterprise Data Tribe, you don t have to fit into a mould at BCG. We seek people with strong drive, relentless curiosity, desire to create their own path, ability to work collaboratively, and the passion and leadership to make an impact. You ll collaborate on challenging projects with team members from many backgrounds and disciplines, increasing your understanding of complex business problems from diverse perspectives and developing new skills and experience to help you at every stage of your career. You ll be able to experience business on a genuinely global scale and learn how to bring together people from different cultures to uncover insights that challenge the status quo. As a member of the Product Engineering Group, you will work closely with a cross functional team that is collaborative, passionate and that holds themselves to a high standard.",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
General Mills,Data Engineer,":

India is among the top ten priority markets for General Mills, and hosts our Global Shared Services Centre. This is the Global Shared Services arm of General Mills Inc., which supports its operations worldwide. With over 1,300 employees in Mumbai, the center has capabilities in the areas of Supply Chain, Finance, HR, Digital and Technology, Sales Capabilities, Consumer Insights, ITQ (R&D & Quality), and Enterprise Business Services. Learning and capacity-building is a key ingredient of our success.

Job Description:

Job Overview

The Enterprise Data Development team is responsible for designing & architecting solutions to integrate & transform business data into Data Lake to deliver data layer for the Enterprise using cutting edge technologies like Big Data - Hadoop. We design solutions to meet the expanding need for more and more internal/external information to be integrated with existing sources; research, implement and leverage new technologies to deliver more actionable insights to the enterprise. We integrate solutions that combine process, technology landscapes and business information from the core enterprise data sources that form our corporate information factory to provide end to end solutions for the business.

This position will develop solutions for the Enterprise Data Lake & Data Warehouse. You will be responsible for developing data lake solutions for business intelligence and data mining.

Job Responsibilities

70% of time Create, code, and support a variety of Hadoop, ETL & SQL solutions

Experience with agile techniques or methods

Work effectively in a distributed global team environment.

Works on pipelines of moderate scope & complexity

Effective technical & business communication with good influencing skills

Analyze existing processes and user development requirements to ensure maximum efficiency

Participates in the implementation and deployment of emerging tools and processes in the big data space

Turn information into insight by consulting with architects, solution managers, and analysts to understand the business needs & deliver solutions

20% of time Support existing Data warehouses & related jobs.

Job Scheduling experience (Tidal, Airflow, Linux)

10% of time Proactive research into up to date technology or techniques for development

Should have automation mindset to embrace a Continuous Improvement mentality to streamline & eliminate waste in all processes.

Desired Profile

Education:

Minimum Degree Requirements: Bachelors

Preferred Degree Requirements: Bachelors

Preferred Major Area of Study: Engineering

Experience:

Minimum years of Hadoop experience required: 2 years

Preferred years of Data Lake/Data warehouse experience: 2-4+ years

Total Experience required : 4-5 years

Specific Job Experience or Skills Needed

Skills Level: Beginner  Intermediate Expert  Advance

HDFS, Map reduce

Beginner

Hive, Impala & Kudu

Beginner

Python

Beginner

SQL, PLSQL

Proficient

Data Warehousing Concepts

Beginner

Other Competencies:
• Demonstrate learning agility & inquisitiveness towards latest technology
• Seeks to learn new skills via experienced team members, documented processes, and formal training
• Ability to deliver projects with minimal supervision
• Delivers assigned work within given parameter of time and quality
• Self-motivated team player and should have ability to overcome challenges and achieve desired results",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,False
Fidelity India Careers,Lead - Software Engineering - Data Engineering,"Job Description:

Job Title – Lead Data Engineer [Data CoE]

The Purpose of This Role

At Fidelity, we use data and analytics to personalize incredible customer experiences and develop solutions that help our customers live the lives they want. As part of our digital transformation, we have significant investments to create innovative big data capabilities and platforms. One of them is to build various enterprise data lakes by gathering data across Business Units. We are looking for a hands-on data engineer who can help us design and develop our next generation, cloud enabled data capabilities.

The Value You Deliver
• You will be participating in end to end development which includes design, development, testing and deployment.
• You will be working closely with Technical Lead/Architects to ensure that solutions are consistent with IT Roadmap.
• You will be participating in technical life cycle processes, which include impact analysis, design review, code review, and peer testing.
• You will be participating in hands on development of application framework code in Oracle PL-SQL, pySpark, Python, NiFi, Informatica Power Center, along with Control-M and UNIX shell scripts.
• You will be troubleshooting and fixing any issues reported on data issues and performance.
• You will be presenting the findings and outcome to Senior Leadership teams and provide insights from the data to the business.
• You will be helping business teams optimize their current tasks and increase their productivity.

The Skills that are Key to this role

Technical / Behavioral
• You must be an expert in using SQL and PLSQL on Oracle or Netezza with UNIX shell scripting skills.
• You should be having working knowledge in Hadoop, HDFS, Hive, Spark, NoSQL DBs,
• Good knowledge on Python, JavaScript, Java and Scala
• You should have experience of using AWS services like RDS, EC2, S3, EMR and IAM to move data onto cloud platform
• Experience/Knowledge on Kubernetes, Containerization and building applications in Containers
• Knowledge of Logging, Telemetry and Data Security on AWS / Azure
• Understanding of data modeling and Continuous Integration (e.g. Jenkins, GIT, Concourse) tools
• Experience of query tuning and optimization in one of the RBMS (oracle or DB2)
• You should be having experience in Control-M or similar scheduling tools.
• You should have proven analytical and problem-solving skills
• You should be strong in Database and Data Warehousing concepts.
• You must be able to work independently in a globally distributed environment
• You should have clear understanding of the business needs and incorporate these into technical solutions.

The Skills that is good to have for this role
• Experience in performance tuning and optimization techniques on SQL (Oracle and Netezza) and Informatica Power Center.
• Having strong inter-personal and communication skills including written, verbal, and technology illustrations.
• Having adequate knowledge on DevOps, JIRA and Agile practices.

How Your Work Impacts the Organization

Cloud Enablement and Data Model ready for Analytics.

The Expertise we’re looking for
• 3+[SE] / 7+ [Lead] years of experience in Data Warehousing, Big data, Analytics and Machine Learning
• Graduate / Post Graduate

Location: Bangalore , Chennai

Shift timings: 11:00 am - 8:00pm

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following:

For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2
For roles based in Ireland: Contact AccommodationsIreland@fmr.com
For roles based in Germany: Contact accommodationsgermany@fmr.com

Fidelity Privacy policy

Certifications:

Company Overview

At Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. Fidelity India has been the Global Inhouse Center of Fidelity Investments since 2003 with offices in Bangalore and Chennai. For information about working at Fidelity, visit India.Fidelity.com.

Fidelity Investments is an equal opportunity employer.",Bengaluru,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
EMERSON,Data Engineer - Sustainability,"JOB DESCRIPTION AS A PROFFESSIONALYOU WILL: Work closely with key stakeholders to understand business needs and translate them into technical requirements that would feed into developing effective data analytics solutions Design and implement end-to-end data solutions in collaboration with other technical and functional teams. Review and revise existing software development lifecycle andcode standards. Work closely with the data Architect onproduct roadmaps. Work on SharePoint and Power BI tools to manage, analyse and deduce data insights. Act as a point of escalation for complex operational issues to ensure optimal performance of analytics systems. WHO YOU ARE: You anticipate customer needs and provide services that are beyond customer expectations. You understand interpersonal and group dynamics and react in an effective manner. You encourage others to learn and adopt new technologies. You show a tremendous amount of initiative in tough situations and are exceptional at spotting and seizing opportunities. You promote high visibility of shared contributions to goals. REQUIRED EDUCATION, EXPERIENCE, & SKILLS: Bachelor's degree in Computer Science/Information Technology or equivalent Must have a minimum of 6+ years of experience in a Engineering role with experiences with: SharePoint Online and Power BI Experience in Visualization and Interpreting Data in various forms Technical expertise in data modelling, data mining, and segmentation techniques Experience with building new and troubleshooting existing data pipelines using Experience with batch and real-time data ingestion and processing frameworks Experience with languages such asPython andJava Knowledge of additional cloud-based analytics solutions Hands-on experience working on Linux and Windows systems Using Agile development methods Ability to work in a large, global corporate structure Ability to lead, manage and deliver large scale projects Advanced English level Demonstrated ability to clearly isolate and define problems, effectively evaluate alternative solutions, and make decisions in a timely manner Good decision-making ability, ability to operate in ambiguous situations, and high analytical ability to judge pros/cons of approaches against objectives PREFERRED EDUCATION, EXPERIENCE, & SKILLS: Expert level knowledge of data analytics and warehousing frameworks, including Snowflake and Cloud-based data integration solutions Experience with DevOps andCI/CD development practices Advanced level of software development knowledge",Chandigarh,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,True
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 24 2023

Come join us as we supercharge GSK’s data capability!

At GSK we are building a best-in-class data and prediction powered team that is ambitious for patients.

Scientific Digital and Tech’s goal is to power the discovery, development and supply of medicines and vaccines to patients. This means new tools to discover new medicines and vaccines, predictive capability for pre-clinical research, accelerated CMC and supply chain and an improved day-to-day laboratory experience for our scientists. Our Digital & Tech solutions will automate workflows and speed up decisions; freeing hands and releasing minds to focus on science.

As R&D enters a new era of data driven science, we are building a data engineering capability to ensure we have high quality data captured with context and aligned data models, so that the data is useable and reusable for a variety of use cases.

GSK R&D and Digital and Tech’s collective goal is to deliver business impact, including the acceleration of the discovery and development of medicines and vaccines to patients. The R&D Digital and Tech remit has expanded over the past 2 years, and to position GSK for the future, The change will strengthen R&D Tech, to provide more strategic impact, focus, accountability, and improved decision making in the use of Digital, Data and Analytics (DDA) to strengthen the pipeline.

Job Purpose

This role contributes to the construction of the development data fabric and data strategy. This role will interact with architects, engineers, data modelers, product owners as well as other team members in Clinical Solutions and R&D. This role will actively participate in creating technical solutions, designs, implementations & participate in the relentless improvement of R&D Tech systems in alignment with agile and DevOps principles.

The Data Engineer demonstrates both depth and breadth across key data engineering competencies e.g. Software Development, Testing, DevOps, Data Science/Analytics, and cloud. Can collaborate with experts from other subject domains. Primary responsibilities include using Azure cloud services and GSK data platform tools to ingest, egress, and transform data from multiple sources.

In addition, the role will demonstrate core engineering knowledge/experience of industry technologies, practices, and frameworks such as data fabric and scaling data platforms, containerization, cloud-based platforms, data analytics, machine learning, and data streaming. Examples of technologies include Java/C#/Python, Denodo, GIT, Azure Devops, Data Bricks, Presto, Spark, Azure Data Factory, ADLS V2, Kafka, Selenium, JUnit/NUnit, SAFe, Kanban, Docker, AI/ML, Azure/GCP Cloud Architecture including networking principles and scaling applications.

The Data Engineer, Clinical Solutions role is a senior technical role and will provide you the opportunity to lead key activities to progress your career. These responsibilities include the following:
• Working with other teams that are defining devops and data platform practices to meet the requirements of clinical solutions.
• Supporting engineering teams in the adoption and creation of data fabric best practices.
• Conducting PoCs of new technologies and helping to embed them in product teams
• Being part of a cutting-edge team creating the Development Data Fabric
• Ensures that technical delivery is fully compliant with GSK Security, Quality and Regulatory standards
• Ensures use of relevant R&D Tech / central services and collaborating with service partners in identification and delivery of service improvements
• Maintains best practices for engineering and architecture on our Confluence site. This requires hands on experience with cutting edge technology.
• Pro-actively engages in experimentation and innovation to drive relentless improvement
• Provides leadership, technical direction and GSK expertise to architecture and engineering teams composed of GSK FTEs, strategic partners and software vendors.

Why you?

Basic Qualifications:

Are you ready to work in an environment where you are continuously expected to work on projects with new technology and expected to use this technology to deliver real business value?

We are looking for professionals with these required skills to achieve our goals:
• Total 15+ years of experience and proficient with at least 3 of the below skills and can demonstrate knowledge and value with relevant experience in all the following competencies:
• Must have experience in Spark, Python and Databricks
• Software development, architecture design & technology platforms/frameworks
• Data Platforms and Domain-driven design
• Agile, DevOps & Automation [of testing, build, deployment, CI/CD, etc.]
• Data science (e.g. AI/ML), data analytics & data quality/integrity
• Testing strategies & frameworks
• Role requires:
• Demonstrated skill in delivering high-quality engineered data products
• Knowledge of industry standards and technology platforms aligned to GSK and R&D roadmaps
• Excellent communication, negotiation, influencing and stakeholder management skills
• Customer focus and excellent problem-solving skills
• Computer Science or related bachelor’s degree – MS in Computer Science is preferred
• Familiarity and use of various open-source ecosystems including JavaScript, Bigdata, java, python etc.
• Good understanding of various software paradigms: domain-driven, procedural, data-driven, object-oriented, functional
• Familiar with .Net Core (C#), Java, Python
• Demonstrable knowledge depth in more than one area of software engineering and technology

Preferred Qualifications:

If you have the following characteristics, it would be a plus:
• Experience in agile software development and DevOps, relevant technology platforms [e.g., Kubernetes] and frameworks [e.g. Docker] including cloud technologies & data structures (i.e. information management), data models or relational database design
• Subject matter expertise in clinical development
• R&D Tech requires Engineers with understanding of the relevant technical and scientific domains. Able to deliver continuous change to meet rapidly evolving R&D strategy and ambition.
• Experience with agile development methods, with security strategies and best practices, data integration mechanisms, architectural design tools, delivering and integrating COTS applications, areas of Service Oriented Architecture (SOA), Application Integration, Business Process Management and Data Quality.
• Experience in applying AI/ML, data curation, virtualization, predictive modelling, workflow, and advanced visualization techniques to enable decision support across multiple products and assets to drive results across R&D business operations.

At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.
#LI-GSK

GSK is a global biopharma company with a special purpose – to unite science, technology and talent to get ahead of disease together – so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns – as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it’s also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We’re committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in “gsk.com”, you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,True,False,False,True,False,False,True,True,False,False,False,False,False,False,False,False
Bloom Consulting Services,Data Engineer,"Data Engineer ( Job ID : 815310498 )

data engineer

NA

Contract

Experience

06.0 - 08.0 years

Offered Salary

10.00 - 14.00

Notice Period

Not Disclosed

Job Description

Total Experience6 to 8 years

Min Relevant Experience: 3 to 5 years

Location :Bangalore

JD: Data Engineer

Role Description:

In this role, you will be part of a growing, global team of data engineers, who collaborate in DevOps mode, in order to enable business with state-of-the-art technology to leverage data as an asset and to take better informed decisions.

The Life Science Data Engineering Team is responsible for designing, developing, testing, and supporting automated end-to-end data pipelines and applications on Life Science’s data management and analytics platform (Palantir Foundry, Hadoop and other components).

The Foundry platform comprises multiple different technology stacks, which are hosted on Amazon Web Services (AWS) infrastructure or own data centers. Developing pipelines and applications on Foundry requires:
• Proficiency in SQL / Java / Python (Python required; all 3 not necessary)
• Proficiency in PySpark for distributed computation
• Familiarity with Postgres and ElasticSearch
• Familiarity with HTML, CSS, and JavaScript and basic design/visual competency
• Familiarity with common databases (e.g. JDBC, mySQL, Microsoft SQL). Not all types required

This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.

Roles & Responsibilities:
• Develop data pipelines by ingesting various data sources – structured and un-structured – into Palantir Foundry
• Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application
• Acts as business analyst for developing requirements for Foundry pipelines
• Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline
• Document technical work in a professional and transparent way. Create high quality technical documentation
• Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)
• Deploy applications on Foundry platform infrastructure with clearly defined checks
• Implementation of changes and bug fixes via change management framework and according to system engineering practices (additional training will be provided)
• DevOps project setup following Agile principles (e.g. Scrum)
• Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java
• Work closely with business users, data scientists/analysts to design physical data models

Education
• Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields

Professional Experience
• 5+ years of experience in system engineering or software development
• 3+ years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.

Required Knowledge, Skills, and Abilities

Data engineer",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,False,False
"Cognizant India, Cognizant Technology Solutions",Azure Data Engineer,"• Experience with Azure Data Bricks, Data Factory
• Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics
• Experience in Python/Pyspark/Scala/Hive Programming.
• Experience with Azure Databricks/ADB
• Good understanding of SQL queries, joins, stored procedures, relational schemas
• Experience with NoSQL databases, such as HBase, Cassandra, MongoDB",Chennai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Genpact,Data Engineer,"With a startup spirit and 90,000+ curious and courageous minds, we have the expertise to go deep with the world's biggest brands--and we have fun doing it! We dream in digital, dare in reality, and reinvent the ways companies work to make an impact far bigger than just our bottom line. We're harnessing the power of technology and humanity to create meaningful transformation that moves us forward in our pursuit of a world that works better for people.

Now, we're calling upon the thinkers and doers, those with a natural curiosity and a hunger to keep learning, keep growing. People who thrive on fearlessly experimenting, seizing opportunities, and pushing boundaries to turn our vision into reality. And as you help us create a better world, we will help you build your own intellectual firepower.

Welcome to the relentless pursuit of better.

In this role, resource will be expert in designing, building and maintaining data infrastructure. Work will help people with unmet medical needs, including those who wish to quit smoking, those with major depression disorder, and those with schizophrenia--ultimately improving lives through engineering. Help design and build a data infrastructure using state-of-the-art technologies with data security at utmost importance and employ elegant solutions to help ensure Client's data products meet compliance needs (e.g., GDPR and HIPAA) in different regions of the world.

Responsibilities!
• Design, build and maintain analytical data infrastructure which includes both data processing and data reporting.
• Onboarding data from both internal and external systems.
• Collaborate with Product, Engineering, Science, Data analysts and Data scientists to implement rich and re-usable datasets/metrics.
• To make data infrastructure and applications scalable, reliable, and secure.
• Strong attitude towards automating routine tasks via coding/scripting.
• Research on security and privacy requirements and provide solutions.

Qualifications we seek in you!
• B Tech/M Tech/BCA/MCA
• Experience in building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
• Experience writing complex, highly optimized SQL queries.
• Experience with reporting to enable explanatory and exploratory analytics.
• Python development experience.
• Have experience with dbt, Airflow, Snowflake and AWS infrastructure.
• Have experience implementing APIs to share data with internal / external vendors.
• Experience implementing streams.
• Understanding of privacy and security regulations (e.g., GDPR, HiTrust, HIPA)",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,True,True
Vanderlande Careers,Lead Data Engineer,"Lead Data Engineer at DSF

Vanderlande provides baggage handling systems for 600 airports around the globe, capable of moving over 4 billion pieces of baggage around the world per year. For the parcel market our systems handle 52 million parcels per day. All these systems generate data. Do you see a challenge in building data-driven services for our customers using that data? Do you want to contribute to the fast growing Vanderlande Technology Department on its journey to become more data driven? If so, then join our Digital Service Factory team!

Your Position

As a lead data engineer you will be leading the data engineering efforts in a product team. You will work together with product/solution architecture to provide technical necessities to design and develop end-to-end data ingestion pipelines and well tested and monitored data services. You will assess the technical dependency between different functional components and define a resolution. You will also provide technical guidance and coaching to the junior/medior data engineers in the team, set technical standards and best practices.

Your responsibilities:
• You will be designing, developing, testing, and documenting the data collection framework. The data collection consists of (complex) data pipelines with data from (IoT) sensors and low/high level control components to our Digital Service and Data Science platform.
• You will build monitoring solutions for data pipelines which enable data quality improvement.
• You will develop scalable data pipelines to transform and aggregate data for business use, following software engineering and Data Mesh best practices. For these data pipelines you will make use of the best and most applicable frameworks available for data processing.
• You develop our data services and data products for customer sites towards a product, using (test & deployment) automation, componentization, templates, and standardization to reduce delivery time of our projects for customers. The product provides insights in the performance of our material handling systems at customers all around the globe.
• You design and build a CI/CD pipeline, including (integration) test automation for data pipelines. In this process you strive for an ever-increasing degree of automation and high levels of security.
• You will work with infrastructure engineers to extend storage capabilities and types of data collection (e.g. streaming)
• You have experience in developing APIs.
• You will coach and train the junior data engineer with the state of art big data technologies.
• You will lead the Data Engineering Guild where passionate members discuss current trends, short term development, and solutions for ongoing issues that span multiple teams.

Your Profile
• Total experience of 10+ years (with at least 7+ years of programming exp)
• Experience programming in Python and/or Scala (Java programming exp is a plus)
• You are familiar with DevOps practices and have relevant experience in automation (CI/CD), measurement, applying lean practices and what DevOps culture entails
• You know how to achieve high performing secure pipelines, maintain and test them
• You are familiar with different storage formats (e.g. Azure Blob, SQL, noSQL)​
• Experience with scalable data processing frameworks (e.g. Spark)​
• Experience with event processing tools like Splunk or the ELK stack​
• Deploying services as containers (e.g. Docker and Kubernetes)​
• You have experience with streaming data platforms (e.g. Kafka )​ and messaging formats (e.g. Apache AVRO)
• Strong experience with cloud services (preferably with Azure)

Diversity & Inclusion

Vanderlande is an equal opportunity employer. Qualified applicants will be considered without regards to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Pune,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False
Visa,Sr. Data Engineer,"Company Description

Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.

When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.

Join Visa: A Network Working for Everyone.
Job Description

This position is ideal for an engineer who is passionate about solving challenging business problems and building applications that provide an excellent user experience. You will be an integral part of the Payment Products Development team focusing on design and development of software solutions that leverage data to solve business problems. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, development, and testing of new functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Responsible for the design, development, and implementation
• Work on development of new products iteratively by building quick POCs and converting ideas into real products
• Design and develop mission-critical systems, delivering high-availability and performance
• Interact with both business and technical stakeholders to deliver high quality products and services that meet business requirements and expectations while applying the latest available tools and technology
• Develop code to ensure deliverables are on time, within budget, and with good code quality
• Have a passion for delivering zero defect code and be responsible for ensuring the team's deliverables meet or exceed the prescribed defect SLA
• Coordinate Continuous Integration activities, testing automation frameworks, and other related items in addition to contributing core product code
• Present technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.
• Perform other tasks on R&D, data governance, system infrastructure, and other cross team functions, on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.
Qualifications

We are seeking team members that are passionate, visionary and insatiably inquisitive. Successful candidates frequently have a mix of the following qualifications:

• Bachelor’s Degree or an Advanced Degree (e.g. Masters) in Computer Science/ Engineering, Information Science or a related discipline
• Minimum of 3 years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies
• Extensive experience with SQL and Big Data technologies (Hadoop, Java, Spark, Kafka, Hive, Python) for large scale data processing and data transformation
• Deep knowledge of Unix/Linux
• Experience with data visualization and business intelligence tools like Tableau, or other programs highly desired
• Familiar with software design patterns
• Experience working in an Agile and Test-Driven Development environment
• Strong knowledge of API development is highly desired
• Strategic thinker and good business acumen to orient data engineering to the business needs of internal and external clients
• Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
• Previous exposure to financial services is a plus, but not required
Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",Bengaluru,True,False,True,True,False,False,False,True,False,True,False,False,False,False,False,False
Shell,"Senior Data Engineer- Azure (ADF, Data lake)","Join the number One Global Lubricants supplier in the world and be part of the team that helps in shaping up the digital and the IDT strategy which delights our customers in over 100 countries across every sector.

If you are looking for a place where you can gain hands-on exposure and have direct impact, then this is the place for you!

Where you fit

Shell's Projects and Technology (P&T) business exists to make the delivery of our strategies and the growth of our company possible. Our team develops the advanced products and technologies Shell needs to meet customer demand. Our solutions help our partners grow the LNG, Gas and Power businesses, deepen the integration of Manufacturing, Chemicals and Trading, and maximise the competitiveness of our Upstream business.

What's the role?

As a Data Engineer in Shell, you will create and maintain optimal data pipeline architecture and also will a ssemble large, complex data sets that meet functional / non-functional business requirements.

You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

More specifically, your role will include:
• Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.
• Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.

What we need from you

We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree and at least have a Seniority level in their previous workplace.

They should also have experience using the following software/tools:
• Experience with Azure: ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.
• Experience with relational SQL/NoSQL databases, file handlings and API integrations
• Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.
• Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODS

Skills - Nice to Have
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing data pipelines using ADF
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
• Strong analytic skills related to working with structured and unstructured datasets
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of transforming, processing and extracting value from large disconnected datasets
• Strong team player with organizational and communication skills
• Experience supporting and working with cross-functional teams in a dynamic environment",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Fibe India,Data Engineer - SQL,"Responsibilities:
• The candidate is expected to lead one of the key analytics areas end-to-end. This is a pure hands-on role.
• Ensure the solutions are built to meet the required best practices and coding standards.
• Ability to adapt to any new technology if the situation demands.
• Requirement gathering with business and getting this prioritized in the sprint cycle.
• Should be able to take end-to-end responsibility for the assigned task
• Ensure quality and timely delivery.

Requirements:
• Experience: 3- 6 years.
• Strong at PySpark, Python, and Java fundamentals
• Good understanding of Data Structure
• Good at SQL query/optimization
• Strong fundamental of OOPs programming
• Good understanding of AWS Cloud, Big Data.
• Nice to have Data Lake, AWS Glue, Athena, S3 Kinesis, SQL/NoSQL DB",Pune,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Data Engineer,"Role: Data Engineer Job Description
• Design, build, and maintain distributed batch and real-time data pipelines and data models.
• Facilitate real-life actionable use cases leveraging our data with a user- and product-oriented mindset.
• Be curious and eager to work across a variety of engineering specialties (i.e., Data Science, and Machine Learning to name a few).
• Support teams without data engineers with building decentralized data solutions and product integrations, for example around DynamoDB.
• Enforce privacy and security standards by design.
• Conceptualize, design and implement improvements to ETL processes and data through independent communication with data-savvy stakeholders.

Qualifications
• +3 years experience building complex data pipelines and working with both technical and business stakeholders.
• Experience in at least one primary language (e.g., Java, Scala, Python) and SQL (any variant).
• Experience with technologies like BigQuery, Spark, AWS Redshift, Kafka, or Kinesis streaming.
• Experience creating and maintaining ETL processes.
• Experience designing, building, and operating a DataLake or Data Warehouse.
• Experience with DBMS and SQL tuning.
• Strong fundamentals in big data and machine learning.

Preferred Qualifications
• Experience with RESTful APIs, Pub/Sub Systems, or Database Clients.
• Experience with analytics and defining metrics.
• Experience with measuring data quality.
• Experience productionalizing a machine learning workflow; MLOps
• Experience in one or more machine learning frameworks, including but not limited to scikit-learn, Tensorflow, PyTorch and H2O.
• Language ability in Japanese and English is a plus (We have a professional translator but it is nice to have language skills).
• Experience with AWS services.
• Experience with microservices.
• Knowledge of Data Security and Privacy.

experience

6",Hyderabad,True,False,True,True,False,False,False,False,False,False,False,False,True,True,False,False
deloitte,Consulting - BO - Cloud Engineering - Manger - Azure Data Engineer,"What impact will you make?

Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organisations, Deloitte is where you will find numerous opportunities to succeed and realise your full potential.

The team

Deloitte is working with global customers on cloud technologies to help unlock growth, stability, and sustainability by enabling them to spot unseen business trends through curation, transformation, and blending of data. In our endeavors for continued expansion, we’re searching for like-minded individuals to help us ‘take it to the next level’.

In this exciting opportunity for an experienced developer, you will join a team delivering a transformative cloud hosted data platform for some of the world’s biggest organizations. The candidate we seek, needs to have a proven track record in implementing data ingestion and transformation pipelines on Microsoft Azure. Deep technical skills and experience with working on Azure Databricks. Familiarity with data modelling concepts and exposure to Synapse.

You will also be required to participate in stakeholder management, highlight risks, propose deliver plans and estimate for time and team size based on requirements. Hence, adequate levels of communication skills and relevant experience in handling such situations is desired.

Scope of work

Your main responsibilities will be:
• Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks
• Delivering and presenting proofs of concept of key technology components to project stakeholders.
• Developing scalable and re-usable frameworks for ingesting and enriching datasets
• Integrating the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
• Working with event based / streaming technologies to ingest and process data
• Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
• Evaluating the performance and applicability of multiple tools against customer requirements
• Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.

Qualifications
• Strong knowledge of Data Management principles
• 9+ years of total years of experience
• Experience in building ETL / data warehouse transformation processes
• Direct experience of building data pipelines using Azure Data Factory and Apache Spark (preferably Databricks).
• Experience using Apache Spark and associated design and development patterns
• Microsoft Azure Big Data Architecture certification is an advantage.
• Hands-on experience designing and delivering solutions using Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
• Experience with Apache Kafka / Nifi for use with streaming data / event-based data (Nice to have but not mandatory)
• Experience with other Open Source big data products Hadoop (incl. Hive, Pig, Impala)
• Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)
• Experience working in a Dev/Ops environment with tools such as Microsoft Visual Studio Team Services, Terraform etc.

Your role as a leader

At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters.

In addition to living our purpose, managers across our organisation:
• Develop self by actively seeking opportunities for growth, share knowledge and experiences with others, and act as a strong brand ambassadors
• Understand objectives for clients and Deloitte, align own work to objectives and set personal priorities
• Seek opportunities to challenge self
• Collaborate with others across businesses and borders to deliver and take accountability for own and team results
• Identify and embrace our purpose and values and put these into practice in their professional life
• Build relationships and communicate effectively in order to positively influence peers and other stakeholders

Professional growth

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn.From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits

At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Our Purpose

Deloitte is led by a purpose: To make an impact that matters.

Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and work—always striving to be an organisation that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world",Bengaluru,False,False,True,False,False,False,False,False,False,False,False,True,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description

insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.

Job Description
• Develops and maintains scalable data pipelines for bulk data movement between systems of record and systems of reference
• Develops and maintains scalable application to application integrations
• Aligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
• Implements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes
• Writes appropriate unit or integration tests to implement test-driven development
• Continually contributes to and enhances data team documentation
• Performs data analysis required to troubleshoot and resolve data related issues
• Works closely with a team of frontend and backend engineers, product managers, and analysts
• Defines company data assets, artifacts and data models

Qualifications

Required qualifications:
• 5 years of Data Engineering and Data Integration
• 5 Years of Data Warehousing
• 3 Years of Data Architecture and Modeling
• 2 years of Cloud Data Engineering
• Agile Methodologies

Preferred skills:
• AWS or Azure Data Certifications
• Experience with databricks, spark, python
• Experience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)
• Experience with Salesforce

Additional Information

All your information will be kept confidential according to EEO guidelines.
• * At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. **

insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Northern Tool + Equipment, India",Senior Data Engineer,"Are you an individual who wants to play a game changing role and make an impact in a fast-growing organization? We at Northern are waiting for you. Join us and unleash your potential!!

We are hiring <>!!

Join the core group of founding members at the NTE India to build an organization from the ground up.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Role Objective

PRIMARY OBJECTIVE OF POSITION:

We are looking for an Experienced Data Engineer who will partner with a specific business function and understand the requirements, builds data model, creates data pipelines and stored procedures. Also work with Data Analysts/Modelers, Data Visualization Engineers to deliver high performing analytics.

MAJOR AREAS OF ACCOUNTABILITY:
• SME for data structures and data models for specific line of business.
• Analyze and understand various source systems and related data structures.
• Build and automate creation of ETL pipelines and stored procedures to move data from source system to consumption layer using variety of ETL methods.
• Collaborate with Data Analysts/Data Architect/Data Visualization Engineers to provide them with Data mapping documents and ensure adherence to a common data model.
• Responsible for administration and security of data and analytics assets in Azure.
• Works collaboratively and effectively communicates with others across departments in order to perform and complete necessary tasks and projects.
• Follows established Software Development Life Cycle (SDLC) to enable CI/CD in relevant areas.
• Follow established change control, release management and incident management processes.
• Responsible for performance and tuning, scaling of Azure resources to optimize costs
• Builds and maintains relationships cross-functionally in order to stay current with the needs and operations of the business functional areas supported.
• Supports the day-to-day operation of the reporting and analytic solutions by troubleshooting ETL and other errors encountered during data processing.
• Keeps manager informed of important developments, potential problems, and related information necessary for effective management. Coordinates and communicates plans and activities with others, as appropriate to ensure a coordinated work effort and team approach.

Job Description

Performs related work as apparent or assigned.

QUALIFICATIONS:
• To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
• Bachelor’s Degree in Computer Science, Statistics, Mathematics, Business or related field.
• At least 6 years relevant work experience in Data and Analytics field.
• In-depth understanding of Data warehousing concepts.
• Hands-on experience in writing complex, highly optimized SQL queries across large data sets
• Hands-on experience in building performance optimized data pipelines (ETL/ELT)
• Experience in configuring, deploying, and provisioning of IaaS, PaaS with Terraform and PowerShell using Azure DevOps and GIT.
• Specific familiarity with the Microsoft Azure Data Stack - ADF, Azure SQL DB, Azure Synapse (SQL Data Warehouse), Azure Data Lake, Azure Storage and Analysis Services.
• Azure Security & Identity: Azure Active Directory App Permissions, Key Vaults.
• Hands-on experience in creating user groups, creating security policy and implementation of Row-Level Security(RLS) to restrict the data access to the users.
• Hands-on experience with data cataloging and data profiling concepts.
• Diversity of perspective for various tools and technologies like Azure Stream Analytics, Azure Databricks, NoSQL databases, read or write optimized databases to advocate for their appropriate adoption at Northern Tool.
• Basic programming experience using .NET, Python, or any scripting language.
• Must be willing to work as a team and possess the skills to work independently.
• Demonstrated ability to take initiative and utilize creativity on assigned projects.
• Must possess strong analytical, problem-solving, and technical design skills.
• Demonstrates Northern Tool + Equipment’s 12 Core Competencies.
• Sounds interesting? Here’s your chance to join our family at Northern.

About the Company

Northern Tool + Equipment is a retailer and manufacturer that specializes in offering superior quality tools at great prices, along with the knowledge and support needed to help customers get the job done right.

They’ve been in business for over 40 years, recently reaching revenues over $1.5 billion. The company not only supplies over 100,000 tools from the top brands in the industry but also designs, manufactures, and tests an extensive lineup of premium private label products that customers can’t get anywhere else.

Northern Tool’s far-reaching customer base includes handy men and women, weekend hobbyists, serious do-it-yourselfers, full-fledged contractors, trade professionals, and more. The company’s products can be found in over 120 retail stores in the USA, on its comprehensive international website, and via numerous catalogs throughout the year. Recently Northern Tool has expanded operations to offices in India to serve its global distribution better.

We are recently named as one of the Top Workplaces for MidSize Employers by Forbes in the US.

We have also been recognized as the “Top GCC to work for in AI and analytics” and our India HR team as the “Top HR Professionals in AI and Analytics” by 3AI which is a professional firm associated with analytics within India.

About NTE India

Northern Tool is making a significant investment in business transformation. We are committed to providing our customers with an exceptional experience. The team in India will enable Northern Tool to expand its internal capabilities in Finance, Merchandising, Product Engineers, Manufacturing Ops, Marketing, Contact Center, and Information Technology.

Why Northern?

True Northern: We know that our strength is our people. The distinct abilities they bring into the system are the key to our success. We seek talented people who wish to share their initiative, ideas, and expertise; we develop and support our teams, and we put them in a position to succeed. We know our customer; we provide value, and we act with integrity. We are True Northern.

Build Lasting Relationships: At Northern Tool + Equipment, we’re far more interested in building relationships than we are in simply making transactions. Our purpose is building a long-lasting relation with our customers and employees.

We care for our customers, employees and society. Our customer base is exceptionally loyal because customers know that we will give them the right solution.

Accelerate Decision Making: by collaborating with the brightest minds, bring ideas to life across our value chain of business operations across our vast network of over 120 stores across the US.

Lead with Innovation: Join us to elevate our customer experience?with cutting-edge products, technology, and business processes and?drive our business forward.

We are Family: As a family-owned business, we have respect for personal lives; wherever possible, we strive for flexibility in work schedules, and we maintain a relaxed, professional atmosphere.

Does this sound interesting?? Be an early applicant!!

Northern Tool is an Equal Opportunity Employer. We encourage and empower everyone and support diversity in experience, and point of view. We are pledged to a fair and a transparent hiring process with no discrimination of race, color, ancestry, religion, gender, national origin, age, citizenship, marital status, disability, or veteran status.

Requirements
• name : Northern Tool + Equipment, India
• location : Hyderabad, IN
• experience : 6 - 9 years
• employmentType : Full-Time
• Primary Skills: ETL or ELT,Python,Data Warehousing,Azure Data Lakes or Data Factory,SQL",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
"Comcast India Engineering Center I, LLP",Data Engineer 3,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast. Job Summary About Sky We’re Sky, Europe’s biggest entertainment brand. Think top-quality shows. Breaking news. Innovative tech. Must-have products. Careers here mean the freedom and support you need to make an impact – pushing boundaries, creating solutions, hitting targets. And as part of our close-knit team, you’ll enjoy plenty of benefits. Plus, experiences you’ll only find at Sky. We love telling the world we work at Comcast . We’re fans too. We move fast and embrace pace. We have the freedom to be brilliant. And we work collaboratively because together we can. This is how we work at Comcast and why we love it. Responsible for planning and designing new software and web applications. Analyzes, tests and assists with the integration of new applications. Documents all development activity. Assists with training non-technical personnel. Has in-depth experience, knowledge and skills in own discipline. Usually determines own work priorities. Acts as a resource for colleagues with less experience. Job Description Core Responsibilities Create and maintain an optimal data pipeline architecture focussed upon network data, including real-time and batch data sources. Assemble large, complex data sets that meet functional and non-functional business requirements. Build batch/streaming ELT/ETL solutions from a wide variety of data sources in varying formats (SQL, JSON, AVRO, HTTP, API, etc.) using the right blend of tools. Keep our data compliant, relevant and secured across multiple data centres and regions. Identify, design, and implement internal process improvements: automating manual processes, optimising data delivery and evolving current solutions whilst ensuring continuity of service. Create data tools for data scientist team members that assist them in building and optimizing into an innovative industry leader. Guide and collaborate with data consumers on analytics, tooling and platform related queries. Employees at all levels are expected to: Graduate degree BSc in Computer Science, Electrical Engineering or similar. Strong communications skills. SQL knowledge and experience working with relational databases. Strong analytic skills related to working with structured and unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large, disconnected datasets. Hands-on experience in building scalable data platforms. Awareness of security practices and privacy concerns when working with data across both in-house and cloud platforms. Ideally an awareness of network technologies and concepts or an insatiable desire to learn. Key technologies Apache Airflow / NiFi / Kafka / ZooKeeper Confluent ecosystem (Connect / Schema Registry / ksqlDB) GCP (BigQuery, Dataflow, Pub/Sub, IAM) Linux / Terraform / Ansible Python / Docker (Nice to have). Experience: 5 Years to 7.5 Years Location: Chennai Disclaimer: This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications. Comcast is an EOE/Veterans/Disabled/LGBT employer. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That's why we provide an array of options, expert guidance and always-on tools that are personalized to meet the needs of your reality—to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the benefits summary on our careers site for more details. Education Bachelor's Degree While possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience. Certifications (if applicable) Relative Work Experience 5-7 Years Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other basis protected by applicable law. At Comcast , you have the power to connect the world. Your career options are endless as you grow in your career. Explore your future with access to a variety of teams, locations, and resources in an expanding network. You can also explore additional opportunities at our company, NBCUniversal.",,True,False,True,False,False,False,False,False,False,False,False,False,False,True,True,False
Token Metrics,Crypto Data Engineer (India Remote),"Token Metrics is seeking a multi-talented Big Data Engineer to facilitate the operations of our Data Scientists and Engineering team. The Big Data Engineer will be responsible to employ various tools and techniques to construct frameworks that prepare information using SQL, Python, R, Java and C++. The Big Data Engineer will be responsible for employing machine learning techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field. During various aspects of this process, you should collaborate with coworkers to ensure that your approach meets the needs of each project.

Responsibilities
• Liaising with coworkers and clients to elucidate the requirements for each task.
• Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
• Reformulating existing frameworks to optimize their functioning.
• Testing such structures to ensure that they are fit for use.
• Building a data pipeline from different data sources using different data types like API, CSV, JSON, etc.
• Preparing raw data for manipulation by Data Scientists.
• Implementing proper data validation and data reconciliation methodologies.
• Ensuring that your work remains backed up and readily accessible to relevant coworkers.
• Remaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.

Requirements
• Bachelor's degree in Data Engineering, Big Data Analytics, Computer Engineering, or related field.
• A Master's degree in a relevant field is an added advantage.
• 3+ years of Python, Java or any programming language development experience
• 3+ years of SQL & No-SQL experience (Snowflake Cloud DW & MongoDB experience is a plus)
• 3+ years of experience with schema design and dimensional data modeling
• Expert proficiency in SQL, NoSQL, Python, C++, Java, R.
• Expert with building Data Lake, Data Warehouse or suitable equivalent.
• Expert in AWS Cloud.
• Excellent analytical and problem-solving skills.
• A knack for independence and group work.
• Capacity to successfully manage a pipeline of duties with minimal supervision.

About Token Metrics

Token Metrics helps crypto investors build profitable portfolios using artificial intelligence-based crypto indices, rankings, and price predictions.

Token Metrics has a diverse set of customers, from retail investors and traders to crypto fund managers, in more than 50 countries.",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,True
Revolo Infotech,Data Engineer - SQL/Python,"Job Description :

- Design, develop, and maintain data pipelines and architecture for data storage, processing, and analysis

- Work with cross-functional teams to understand and implement data requirements

- Build and optimize data pipelines using various cloud-based technologies such as AWS, Azure, or Google Cloud Implement data visualization solutions using cloud-based tools such as Tableau, Power BI, or Looker Monitor and troubleshoot data pipeline issues, and implement solutions to improve performance and scalability

- Collaborate with data scientists and analysts to ensure data is accurate, complete, and accessible for analysis

- Stay up-to-date with the latest technologies and industry trends in data engineering and data visualization

Requirements :

- 2+ years of experience as a data engineer with a focus on cloud-based data pipelines and visualization

- Strong experience with cloud-based technologies such as AWS, Azure, or Google Cloud

- Experience with data visualization tools such as Tableau, Power BI, or Looker

- Strong knowledge of SQL and programming languages such as Python or Java

- Familiarity with big data technologies such as Hadoop, Spark, or Hive

- Strong problem-solving and analytical skills

- Experience working in an Agile development environment Bachelor's degree in Computer Science or related field.

Preferred Qualifications :

- Experience with data warehousing concepts and technologies

- Experience with data governance and data management best practices.

- Experience with machine learning and AI technologies Strong communication and teamwork skills.

Job Types : Full-time, Regular / Permanent, Contractual / Temporary

Salary : 1,000,000.00 - 1,200,000.00 per year

Benefits :

- Health insurance

- Internet reimbursement

- Paid sick time

- Paid time off

Schedule :

- Day shift

- Monday to Friday

Power BI: 2 years (Preferred)

Tableau: 2 years (Preferred)

AWS: 2 years (Preferred)
(ref:hirist.com)",Navi Mumbai,True,False,True,True,False,False,False,False,True,True,False,False,False,False,False,False
MediaMath,Data Engineer,"About Us

MediaMath is the leading technology pioneer on a mission to make advertising better. We deliver outstanding results through powerful ad tech, partnership and a curiosity for what’s next. We help more than 3,500 advertisers solve complex marketing problems so they can deepen their customer relationships across screens and around the world.

Key Responsibilities

MediaMath’s Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by Analytics to discover and deliver insights that drive value for our clients. The Analytics team fulfils customers’ advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. As the Data Engineer on the Analytics Engineering team within the Analytics team, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:
• Become an expert in MediaMath data flows and the Analytics data infrastructure.
• Build, maintain, and own scalable data pipelines to support client data integration.
• Become a team SME in data munging and automated ETL processes.
• Work with Analysts to understand and leverage big data to solve client problems and needs.
• Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
• Be a team player, and bring the team and company forward by solving team and company priorities.

You are:
• Experienced in writing readable, re-usable code SQL and Python (our entire team uses Jupyter Notebook and Pandas!)
• Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
• Experienced in implementing data pipeline health monitoring, alerting
• Experienced with data infrastructure troubleshooting and working with system logs
• Experienced developing data flow schematics/blueprints
• Advocate for automation and building efficient, scalable solutions
• Self-driven, with a hunger to learn and spread knowledge by teaching others
• Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:
• Bachelor’s Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
• 3 - 5 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
• Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
• Experience partnering with client-facing teams to understand client needs and translate them to technical requirements

Nice-to-have’s:
• Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)
• Experience working with REST APIs, web services, object-oriented technologies like Java, C++
• Public GitHub repos or notebooks that illustrate the way you think about data
• Exposure to ad-tech, digital marketing, or e-commerce industries

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are three values that provide a framework for how we approach our work and the world: Win Together, Obsess Over Growth, and Do Good, Better. These values inform how we energize one another and engage with our clients. They get us amped to come to work.

Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath is recognized as a Leader in the Gartner 2020 Magic Quadrant for Ad Tech and has won Best Account Support by a Technology Company for two years in a row in the AdExchanger Awards.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristic as established by law.

MediaMath focuses on Digital Media, Internet, Advertising, Software, and Marketing. Their company has offices in New York City, San Francisco, Chicago, Durham, and Singapore. They have a large team that's between 501-1000 employees. To date, MediaMath has raised $617.877M of funding; their latest round was closed on July 2018.

You can view their website at http://www.mediamath.com or find them on Twitter, Facebook, and LinkedIn.",,True,False,True,True,False,True,False,True,False,False,False,False,False,False,False,False
"Atlassian, Inc.",Senior Data Engineer,"Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.

Working at Atlassian

Atlassian can hire people in any country where we have a legal entity. Assuming you have eligible working rights and a sufficient time zone overlap with your team, you can choose to work remotely or from an office (unless it’s necessary for your role to be performed in the office). Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

The Data Engineering team is responsible for building and managing data pipelines and data visualizations that powers analytics, machine learning, and AI across Atlassian, including finance, growth, product analysis, customer support, sales, marketing, and people functions. We maintain Atlassian's Enterprise data lake and build a creative, reliable, and scalable analytics data model that provides a unified way of analyzing our customers, our products and drive growth and innovations.

You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to partners across the company.

What you'll do
• Partner with Product Manager, analytics, and business teams to review and gather the data/reporting/analytics requirements and build trusted and scalable data models, data extraction processes, and data applications to help answer complex questions.
• Design and implement data pipelines to ETL data from multiple sources into a central data warehouse.
• Design and implement real-time data processing pipelines using Apache Spark Streaming.
• Improve data quality by leveraging internal tools/frameworks to automatically detect and mitigate data quality issues.
• Develop and implement data governance procedures to ensure data security, privacy, and compliance.
• Implement new technologies to improve data processing and analysis.
• Coach and mentor junior data engineers to enhance their skills and foster a collaborative team environment.

Your background
• A BE in Computer Science or equivalent with 6+ years of professional experience as a Data Engineer or in a similar role
• Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools.
• Experience with Databricks and its APIs.
• Experience with modern databases (Redshift, Dynamo DB, Mongo DB, Postgres or similar) and data lakes.
• Proficient in one or more programming languages such as Python/Scala and rock-solid SQL skills.
• Champion automated builds and deployments using CICD tools like Bitbucket, Git
• Experience working with large-scale, high-performance data processing systems (batch and streaming)

Great to have, not mandatory
• Experience working for SAAS companies
• Experience with Machine Learning
• Committed code to open source projects
• Experience building self-service tooling and platforms

Our perks & benefits

To support you at work and play, our perks and benefits include ample time off, an annual education budget, paid volunteer days, and so much more.

About Atlassian

The world’s best teams work better together with Atlassian. From medicine and space travel, to disaster response and pizza deliveries, Atlassian software products help teams all over the planet. At Atlassian, we're motivated by a common goal: to unleash the potential of every team.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To learn more about our culture and hiring process, explore our Candidate Resource Hub.",,True,False,True,False,False,False,False,False,False,False,True,True,True,False,True,False
Motilal oswal,Data Engineer,"Job Description : Strong AWS Data Engineering skills. Exposure to SSIS, SSRS, SSAS will be an advantage,Handson experience working with S3, Redshift, Glue, EMR, RDS, Athena, Aurora,Strong development skills and experience coding with SQL, Pyspark, Python,High on ownership and accountability,Comfortable with change, initial hiccups and small failures,Experience with understanding designs, creating low level designs, unit test cases, unit testing and assisting with Integration and User acceptance testing,Experience of 2-6yrs with AWS Data technologies.",,True,False,True,False,False,False,False,False,False,False,True,False,True,False,False,False
Fisker Inc.,Data Engineer,"Responsibilities
• Work with leaders, engineering and data scientists to understand data needs.
• Design, build and launch efficient and reliable data pipelines to best utilize connected vehicle data for real-time systems and within data warehouses.
• Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
• Help insure that best practices are followed when storing, retrieving and accessing data.

Qualifications
• 3+ years of Python development experience.
• 3+ years of SQL experience.
• 3+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
• 3+ years experience with Data Modeling.
• Experience in organizing queries, tables and pipelines with proper indexing, partition and sharding.
• 3+ years experience in custom ETL design, implementation and maintenance.
• Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Clickhouse, Spark, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).
• Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.

Preferred Qualifications:
• Experience with more than one coding language, ideally Go or C++ and java.
• Experience with designing and implementing real-time pipelines.
• Experience with data quality and validation.
• Experience with SQL performance tuning and E2E process optimization.
• Experience with notebook-based Data Science workflow.
• Experience with Airflow.
• Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.",Hyderabad,True,False,True,True,False,True,False,False,False,False,False,False,True,True,True,False
Poshmark,"Software Developer, Data Engineering","The Big Data team is a central player in the Poshmark organization. Our mission is to build a world-class big data platform to bring value out of data for us and for our customers. Our goal is to democratize data, support exploding business, provide reporting and analytics self-service tools, and fuel existing and new business critical initiatives.

The Data Engineering team at Poshmark is looking for an experienced software engineer to take care of Poshmak’s growth data, ensuring real-time access to quality data for all the stakeholders. The role requires strong understanding of software engineering best practices and excellent software development skills to build and maintain real-time and batch data pipelines with a focus on scalability and optimizations. In addition, the role also requires collaborating with Data Science, Analytics and other Engineering teams to build newer ETLs analyzing terabytes of data.

The role also requires being able to write clean and scalable code to pull datasets from disparate sources involving External APIs, S3 transfers, Web Scraping. You will work with cutting edge technologies and frameworks like Scala, Ruby, Apache Spark, Airflow, Redshift, Databricks, Docker. You will also manage the growth data infrastructure comprising ETL pipelines, Hive tables, Redshift tables, BI tools. We are looking for a software engineer who can help us define the next phase of growth data systems in terms of scalability and stability.

Responsibilities
• Design, Develop & Maintain growth data pipelines and integrate paid media sources like Facebook and Google to drive insights for business.
• Build highly scalable, available, fault-tolerant data processing systems using AWS technologies, Kafka, Spark, and other big data technologies. These systems should handle batch and real-time data processing over 100s of terabytes of data ingested every day and a petabyte-sized data warehouse.
• Responsible for architecting/designing/developing critical data pipelines at Poshmark.
• Productionizing ML models in collaboration with the Data Science and Engineering teams.
• Maintain and support existing platforms and evolve to newer technology stacks and architectures.
• Participate and contribute to constantly improving best practices in development.

Desired Skills & Experience
• Excellent technical problem solving using data structures and algorithms, with emphasis on optimization and code quality.
• 1-3 years of relevant software engineering experience using object oriented programming languages like Scala / Java / Ruby / Python / C++ etc.
• Expertise in architecting and building large-scale data processing systems using Big Data technologies like Spark, Hadoop, EMR, Kafka/ Kinesis, Flink, Druid.
• Expertise in SQL with knowledge on any existing data warehouse technology like Redshift
• Expertise in Google Apps Script, Databricks or API Integrations is a plus.
• Be self-driven, take complete ownership of initiatives, make pragmatic technical decisions and collaborate with cross-functional teams.",Chennai,True,False,True,True,True,True,False,True,False,False,False,True,True,False,True,False
Confidential,Data Engineer - AWS/ETL,"Role and responsibilities :- The Data Engineer will be responsible for leading design, development, transformation, deployment, and maintenance of Data Warehousing stack on AWS- Work with BI and dev team to build data pipelines using AWS Glue and similar tools.- Develop custom data ingestion jobs and ETL scripts using Python/Spark scripts- Perform data modelling and schema design activities in Data Lake and Data Warehouse environments as per the standard practices- Advanced SQL knowledge and experience working with relational databases, able to write/debug complex SQL queriesYour profile must have :- 4+ years of experience in building data pipelines and data warehouse architectures on cloud platforms such as AWS- Exposure to agile methodology- Experience in developing Python or Spark jobs- Strong understanding of data modelling principles- Good communication and collaborative skillsExtra points if you have :- Built processes supporting data transformations, data structures and workload management in Database/Data Warehouse- Experience in performance tuning of Redshift databases and implement recommendations- Experience with sourcing data using APIs from external systems- Experience working with teams across the globe, in a fast-paced, high-tech and customer-obsessed environment- Exposure to Shopify, Amazon and Syndicated data (ref:hirist.com) IT",,True,False,True,False,False,False,False,False,False,False,False,False,True,False,False,False
ANI Calls India Private Limited,Azure Data Engineer with Big Data,"Anicalls Industry:

IT
Total Positions: 3

Job Type:
Full Time/

Permanent Gender:
No Preference Salary: 900000 INR - 1400000 INR ( Annually )

Education:
Bachelor′s degree Experience: 8 -12

Years Location:
Hyderabad, India . Azure Data Factory . Azure Databricks . Python, Scala, PySpark, Spark . HIVE / HIVE LLAP / HBASE / CosmoDb . Azure Active Directory Domain Services . Apache Ranger / Apache Ambari . Azure Key Vault . Expertise in HDInsight ( Minimum 2 -3 years ' experience with multiple implementations ) . Expertise in Cloud Native and Open Cloud Architecture",Hyderabad,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
DAZN,Senior Data Engineer,"Are you an engineer who loves to make things that just work better? Do you love to work with cutting edge technologies and think about how can this run faster, be deployed quicker or fail less and deliver killer streaming applications that add business value and stick with customers?

DAZN is a tech-first sport streaming platform that reaches millions of users every week. We are challenging a traditional industry and giving power back to the fans. Our new Hyderabad tech hub will be the engine that drives us forward to the future. We’re pushing boundaries and doing things no-one has done before. Here, you have the opportunity to make your mark and the power to make change happen - to make a difference for our customers. When you join DAZN you will work on projects that impact millions of lives thanks to your critical contributions to our global products

This is the perfect place to work if you are passionate about technology and want an opportunity to use your creativity to help grow and scale a global range of IT systems, Infrastructure and IT Services. Our cutting-edge technology allows us to stream sports content to millions of concurrent viewers globally across multiple platforms and devices. DAZN’s Cloud based architecture unifies a range of technologies in order to deliver a seamless user experience and support a global user base and company infrastructure.

Join us in India’s beautiful “City of Pearls” and bring your ambition to life.

Benefits will include access to DAZN, an annual performance related bonus, family friendly community, free access for you and one other to our workplace mental health platform app (Unmind), learning and development resources, opportunity for flexible working, and access to our internal speaker series and events.
As our new Data Engineer, you'll have the opportunity to:

• Support building real-time user-facing analytics and data driven operations applications
• Be responsible with the rest of the team for the availability, performance, monitoring, emergency response, and capacity planning
• Use your love of big data systems, thinking about how to make them run as smoothly and securely as possible, support operational endpoints
• Have a strong sense of teamwork and put team’s / company’s interests first

You'll be set up for success if you have

• 5+ years’ experience writing clean, robust and testable code, preferably in Typescript
• Experience building high performant, low latency and high velocity data pipelines
• Working knowledge in AWS services, such as Kinesis, EventBridge, SQS, SNS Topic, S3, Lambda, Kinesis, EKS, Firehose
• Experience with infrastructure-as-code (preferably Terraform) and CI/CD processes
• Comfortable building & maintaining production level data pipelines; streaming or event driven.
• Strong analytical and communication skills.

Even better if you have:

• Exposure to streaming technologies such as Apache Kafka / Google PubSub, Apache Beam, Google Dataflow.
• Having worked in an agile environment with scrum / kanban delivery methodologies

At DAZN, we bring ambition to life. We are innovators, game-changers and pioneers. So if you want to push boundaries and make an impact, DAZN is the place to be.

As part of our team you'll have the opportunity to make your mark and the power to make change happen. We're doing things no-one has done before, giving fans and customers access to sport anytime, anywhere. We're using world-class technology to transform sports and revolutionise the industry and we're not going to stop.

If you're ambitious, inventive, brave and supportive, then you're the kind of person who's going to enjoy life at DAZN.

We are committed to fostering an inclusive environment, both inside and outside of our walls, that values equality and diversity and where everyone can contribute at the highest level and have their voices heard. For us, this means hiring and developing talent across all races, ethnicities, religions, age groups, sexual orientations, gender identities and abilities. We are supported by our talented Employee Resource Group communities: proud@DAZN, women@DAZN, disability@DAZN and ParentZONE.

If you’d like to include a cover letter with your application, please feel free to. Please do not feel you need to apply with a photo or disclose any other information that is not related to your professional experience.

Our aim is to make our hiring processes as accessible for everyone as possible, including providing adjustments for interviews where we can.

We look forward to hearing from you.",Hyderabad,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Wavicle Data Solutions,Sr. Data Engineer,"• Deep object-oriented programing skills (Python preferred, Java or C#) in developing and maintaining various microservices.
• Experience writing and testing code, debugging programs and integrating with Event Hub/Kafka and NoSQL Database.
• Experience developing server-side logic and able to test and package standalone python modules.
• Strong experience developing APIs and has written API documentation using Swagger or similar tool.
• Preferred experience with: Azure CLI deployment; Azure DevOps, Azure Bicep, Azure CosmosDB and python virtual environment set-up and interaction.
• Must be familiar with Unit Testing framework including but not limited to JUnit, .Net equivalent, Pytest framework.",,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False
IBM,Data Engineer: Enterprise Content Management,"Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As Enterprise Content Management, you will be working as an application developer on projects in OpenText Process suite BPM. Your role would also involve in playing a critical role in design of a new system

Responsibilities:
• As a Business Process Management (BPM) Developer, you will manage asset services and application development while collaborating with global team in harmonizing the development of asset management applications.
• You will focus on improving corporate performance by managing business processes.
• Identification and driving of related service quality improvements and engineering deliverables.
• Management and progression of Action items on time with prompt response
• Automation and process improvement, if applicable
• Client communication

Required Technical and Professional Expertise
• Minimum 4 years of core development experience as OpenText Process Suite Developer
• Proficient in OpenText Process Suite BPM and having knowledge to design and develop the workflow
• Experience in Xform, HTML5, Angular JS. Javascript, & SQL
• Working knowledge of Core Java, Web Services & Ws APP integration is an added advantage
• Knowledge on Rest API's and SOAP' API's

Preferred Technical and Professional Expertise
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications

About Business UnitIBM Consulting is IBM's consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients' businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date and proof of vaccination status will be required before your start date. During the Onboarding process you will be asked to confirm your vaccination status, in case you are unable to get vaccinated for any reason, you can let us know at that stage. Please let us know if you are unable to be vaccinated due to medical or religious reasons. IBM will consider such requests on a case by case basis subject to submission of required proof by the candidate before a stipulated date.

Your Life @ IBMIn a world where technology never stands still, we understand that, dedication to our clients success, innovation that matters, and trust and personal responsibility in all our relationships, lives in what we do as IBMers as we strive to be the catalyst that makes the world work better.

Being an IBMer means you'll be able to learn and develop yourself and your career, you'll be encouraged to be courageous and experiment everyday, all whilst having continuous trust and support in an environment where everyone can thrive whatever their personal or professional background.

Our IBMers are growth minded, always staying curious, open to feedback and learning new information and skills to constantly transform themselves and our company. They are trusted to provide on-going feedback to help other IBMers grow, as well as collaborate with colleagues keeping in mind a team focused approach to include different perspectives to drive exceptional outcomes for our customers. The courage our IBMers have to make critical decisions everyday is essential to IBM becoming the catalyst for progress, always embracing challenges with resources they have to hand, a can-do attitude and always striving for an outcome focused approach within everything that they do.

Are you ready to be an IBMer?

About IBMIBM's greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we're also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business. At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it's time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location StatementWhen applying to jobs of your interest, we recommend that you do so for those that match your experience and expertise. Our recruiters advise that you apply to not more than 3 roles in a year for the best candidate experience.

For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBMIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.",Bengaluru,False,False,True,True,False,False,True,False,False,False,True,False,False,False,False,False
Digital Mapout Solutions India Private Limited,Azure Data Engineer,"Role : Azure Data Engineer

Location : Bangalore / Hyderabad

Experience : 4+

M.O.H : Full Time

M.O.W : Work From Office

NP Immediate / 15 days

Education : ÂBE / BTech, ME / MTech / MCA.

Skillsets : Azure Data Factory+Azure Data Lake + Azure SQL+ Azure Synapse+PowerBI

JD : -

Azure data / Lead Engineer :

Mandatory Skill sets : T SQL, Data Warehousing (DW) , ADF, Synapse Analytics

Optional : Power BI-DAX,Data Bricks, Python,PySpark

experience : 3 to 15 years

Senior developer to leads
• Candidate must have a strong experience background in database, Data warehousing & ETL / ELT design and development
• Exposure to complex & large scale enterprise development environment
• Excellent communication & collaboration skills required. Ability to work with internal and external stakeholders is must.
• Good business acumen
• Good problem solving and analytical ability
• Ability & willingness to learn new skills

Core technical skills
• Azure data lake Gen 2
• Synapse Analytics
• Python / scala programming
• Synapse pipeline / Azure data factory
• Azure SQL
• T-SQL programming
• Power BI-DAX",Bengaluru,True,False,True,False,True,False,False,True,True,False,False,False,False,False,False,False
insightsoftware,Senior Data Engineer,"Company Description insightsoftware is a leading provider of reporting, analytics, and performance management solutions. Over 30,000 organizations worldwide rely on us to support business needs in the areas of accounting, finance, operations, supply chain, tax, budgeting, planning, HR, and disclosure management. We enable the Office of the CFO to connect to and make sense of their data in real time so they can proactively drive greater financial intelligence across their organization. Our best-in-class solutions provide customers with increased productivity, visibility, accuracy, and compliance. Learn more at insightsoftware.com.Job DescriptionDevelops and maintains scalable data pipelines for bulk data movement between systems of record and systems of referenceDevelops and maintains scalable application to application integrationsAligns initiatives between business teams and technical teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organizationImplements processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processesWrites appropriate unit or integration tests to implement test-driven developmentContinually contributes to and enhances data team documentationPerforms data analysis required to troubleshoot and resolve data related issuesWorks closely with a team of frontend and backend engineers, product managers, and analystsDefines company data assets, artifacts and data modelsQualificationsRequired qualifications:5 years of Data Engineering and Data Integration5 Years of Data Warehousing3 Years of Data Architecture and Modeling2 years of Cloud Data EngineeringAgile MethodologiesPreferred skills:AWS or Azure Data CertificationsExperience with databricks, spark, pythonExperience with Microsoft stack (SQL Server, SSIS, PowerBI, etc)Experience with SalesforceAdditional InformationAll your information will be kept confidential according to EEO guidelines.** At this time insightsoftware is not able to offer sponsorship to candidates who are not eligible to work in the country where the position is located. ** insightsoftware About Us: Hear From Our Team - InsightSoftware (wistia.com)",Hyderabad,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Factspan,Factspan Analytics - Azure Data Engineer - Big Data/Hadoop,"Job Description :- 7+ Years of deep experience with complex data systems and good instincts around data modelling and usage.- Knowledge of data engineering technologies, architecture, and processes. Specifically, Azure Data Lake, Hadoop ecosystem, Kafka, and common third-party integration and orchestration tools.- Good knowledge of multi-cloud data ecosystem and build scalable solutions on cloud (Azure)- Good knowledge of Big Data Ecosystem-Spark, Hadoop, Databricks- Work across 3-4 teams to develop practices which lead to the highest quality products and contribute transformation change within the cloud- Experience building large scale data processing ecosystems with real time and batch style data as input using big data technologies- Experience in any programming language like Scala or Python.- Exposure to agile methodology and proven ability to technically lead a team of engineers across geographies.- Implement Data Quality, Data Governance on Azure Cloud ecosystem- Good instincts around technical architecture, including metadata, Rest API Integrations, Data API and Solution design of NoSQL and File systems.- Willingness and ability to invest in engineering growth.- Strong communication skills and ability to coordinate across a diverse group of technical and non-technical stakeholders.",Bengaluru,True,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False
Randstad India,Big Data Engineer,"DATA ENGINEER - JD

The role will be part of the Data and Analytics Team responsible for expanding and optimizing AECOM’s data and data pipeline architecture, data flow, and collection for cross functional teams. The role will support software developers, database architects, data analysts, and data scientists on data initiatives and will ensure consistent optimal data delivery architecture throughout ongoing projects.

Responsibilities & Duties
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
• Keep AECOM’s data separated and secure across national boundaries through multiple data centres and regions.
• Create data tools for analytics and data scientist team members -to assist them in building and optimizing our product into an innovative industry leader.
• Collaborate with data and analytics experts to strive for greater functionality in our data systems.
• Escalate issues and recommend resolutions to the Team Lead for timely. May support junior members of the team in addressing routine issues within the assigned processes.
• Maintain the SOP/DTP of current processes and incorporate documentation updates as required.
• Perform moderately complex tasks in compliance with service level agreement, process, policies, and procedures.
• Propose alternatives in identified issues and assist in investigating and in resolving common and unusual issues.
• Contribute in various and simultaneous process improvement initiatives to streamline processes, improve customer experience, and increase productivity. This includes automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Contribute specialized expertise to different assigned projects and may provide key updates to Team Lead and Manager.

Qualifications & Requirements

Minimum Requirements:
• Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems, or relevant discipline in the quantitative field
• 6-10years
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Advanced working SQL/nosql, ADLS, Databricks, ADF, Azure DevOps
• Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency,and workload management.
• Demonstrated ability to manipulate, process, and extract value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
• Strong project management and organizational skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.

Preferred Qualifications
• Experience with big data tools: Hadoop, Spark, Kafka, etc.
• Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
• Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
• Experience with AWS cloud services: EC2, EMR, RDS, Redshift
• Experience with stream-processing systems: Storm, Spark-Streaming, etc.
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Attributes
• Demonstrated ability to champion and drive ideas/programs/solutions
• Excellent organizational and time management skills, able to work under pressure and prioritize effectively
• Able to demonstrate passion, energy and drive, especially in the face of resistance
• Ability to effectively communicate and collaborate within a varied audience and internal and external customers. (Communication)
• Ability to maintain good customer relationship with the ability to suggest ways to improve customer support customer experience (Customer Service)
• Ability to be thorough and meticulous in completing assigned tasks and identifying errors, duplicates, & discrepancies through defined methods. (Attention to Detail)
• Ability to identify and resolve simple to moderate with the ability to provide resolution alternatives by following defined policies and procedures. (Problem Solving)

experience

10",Bengaluru,True,False,True,True,False,True,False,True,False,False,False,False,True,False,True,False
Tiger Analytics India Consulting Private Limited,Senior Data Engineer - Denodo,"Job Title: Senior Data Engineer – Denodo

Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, our 2800+ tribe is solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Curious about the role? What your typical day would look like?
· Engage with clients to understand their business context.
· Translate business needs to technical specifications.
· Define Data Virtualization architecture, deployments, and standards.
· Support the development of data architecture principles, standards, and processes and applies these to deliverables
· Involving in data exploitation and the development of (advanced) analytical data models with multiple data sources using Denodo/Tibco or AtScale semantic layer.
· Developing integrated data solutions, modernizing, consolidating, and coordinating business needs across several applications.
· Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.",,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Mastercard,Senior Data Engineer,"Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Job Title

Senior Data Engineer

Senior Data Engineer, Delivery Engineering Platform
Delivery Engineering Platforms is part of MasterCard Data & Services group and one of the most rapidly growing organization in the space. Platform Teams provides cloud-based analytic software tools that enable large, consumer-focused businesses to seize the Big Data analytics opportunity by triangulating between business strategy, algorithmic math, and large databases to improve decisions.

100 of the largest corporations in the world uses these products. Test & Learn™ for Sites, Test & Learn™ for Customers, Test & Learn™ for Ads, and other similar products employ patented algorithms and workflow to design and interpret business experiments that evaluate, target, and refine proposed business programs

The Delivery Engineering Platform team is a core component to consulting services, managing the data acquisition, integration and transformation of client provided data within the Test & Learn platform for global engagements.

Role

The Senior Data Engineer will lead and participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace. A Senior Data Engineer will:
• Independently lead projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base
• Act as an expert technical resource within the team and region
• Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors
• Build trust-based working relationships with peers and clients across local and global teams
• Implement best practices and collaborate in the design of effective streamlined processes for a complex global solutions group
• Leverage industry best practices including proper use of source control, participation in code reviews, data validation and testing
• Plays a lead role where he/she oversees the activities of the data engineers and ensures the efficient execution of their duties
• Act as an advisor/mentor and helps in managing careers for junior team members
• Comply and uphold all MasterCard internal policies and external regulations

All about you:
• BE/BTech in a quantitative field (e.g., Computer Science, Statistics, Econometrics, Engineering, Mathematics, Operations Research). ME/MTech preferred
• Excellent English quantitative, technical, and communication (oral/written) skills; is an excellent listener
• Expertise with hands-on experience with RDMS technologies, preferably with Microsoft SQL Server, the SSIS Stack and .Net; Proficiency with at least one scripting language (VB Script, Perl, Python)
• Proven self-motivated leader with experience working in teams
• Demonstrate excellent skills in the ability to innovate, think critically and disaggregate problems. Able to provide oversight, validation and quality control to own and team work product
• Ability to easily move between business, data management, and technical teams; ability to quickly intuit the business use case and identify technical solutions to enable it
• Able to balance multiple projects and differing project priorities
• Flexible to work with global offices across several time zones

Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
LodgIQ,Data Engineer,"About LodgIQ

Headquartered in New York, LodgIQ delivers a revolutionary SaaS platform for Algorithmic Pricing and Revenue Management for the hospitality industry by incorporating machine learning and artificial intelligence. For more information, visit http://www.lodgiq.com.

Backed by Highgate Ventures and Trilantic Capital Partners, LodgIQ is a well-funded company, seeking for a motivated and entrepreneurial Developer to join its Product/ Engineering team. Qualified candidates will be offered an excellent compensation and benefit package.

Title: Data Engineer

Location: India

Requirements:
• In-depth knowledge of Python.
• Understanding of Django/Flask, Pandas.
• Familiarity with AWS Environment (EC2, S3, IAM, Athena).
• Working knowledge of NoSQL databases such as MongoDB.
• Proficiency in consuming and developing REST APIs with JSON data.
• Ability to perform data mining and data exploration with intuitive sense for problem solving and strong desire for craftsmanship.

Specific Job Knowledge, Skills & Abilities:
• Real world experience with large-scale data on AWS or similar platform.
• Must be a self-starter and an effective data wrangler.
• Intellectual curiosity and strong desire to learn new Big Data and Machine Learning technologies.
• Deadline driven, and capable of delivering projects on time under a fast paced, high growth environment.
• Willingness to work with unstructured and messy data.
• Bachelor’s degree or Masters degree in relevant quantitative fields (e.g. Computer Science, Statistics, Electrical Engineering, Applied Mathematics, etc).",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Edu Angels India Private Limited,Data Engineer (PySpark),"Responsibilities
• Develop process workflows for data preparations, modeling, and mining Manage configurations to build reliable datasets for analysis Troubleshooting services, system bottlenecks, and application integration.
• Designing, integrating, and documenting technical components, and dependencies of big data platform Ensuring best practices that can be adopted in the Big Data stack and shared across teams.
• Design and Development of Data pipeline on AWS Cloud
• Data Pipeline development using Pyspark, AWS, and Python.
• Developing Pyspark streaming applications

Eligibility
• Hands-on experience in Spark, Python, and Cloud
• Highly analytical and data-oriented
• Good to have - Databricks",Bengaluru,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
Zepto,Data Engineer III (Lead Data Engineer),"Responsibilities:
• Collaborate with Tech and Analytics team to build and maintain the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
• Oversee and govern the expansion of the current data architecture as the business grows and ensure best practices are followed.
• Design and build best-in-class architecture for data tables to ensure optimal querying performance in relational databases.
• Create and maintain connectors that expose the data securely for consumption by downstream systems and services in near real-time.
• Create and maintain data architecture docs to communicate data requirements that are important to business stakeholders and work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that.
• Build data governance and security protocols and ensure adherence from analytics, tech, and business teams.
• Build and mentor the data engineering team, recognize their strengths, and lead them to take ownership of end-to-end data architecture.
• Stay on top of the latest developments in the tech stack and propose potential upgrades to existing systems.

Requirements:
• 6 to 10 years of experience in Data Engineering - Designing databases, building data pipelines, and maintaining data governance protocols in cloud platforms.
• A visionary in technical architecture, with experience building and maintaining Data.
• Engineering Products, along with the demonstrated ability to take accountability for achieving results.
• Hands-on working experience with Python, ETL pipelines, and advanced SQL.
• Strong understanding of AWS Services - Redshift, Lambda, Glue, Athena, and security protocols.
• Experience in any Cloud DW Redshift/Snowflake/BigQuery/Synapse.
• Strong data Modelling and database design experience with Redshift or other relational databases.
• Experience working with Agile methodologies, Test Driven Development, and implementing CI/CD pipelines using Gitlab and Docker.
• Good understanding of ETL/ELT technology and processes.
• Experience in gathering and processing raw data at scale including writing scripts, web scraping, and calling APIs.",Bengaluru,True,False,True,False,False,False,False,False,False,False,False,False,True,True,False,True
Confidential,Data Engineer 3 - AWS & Python (Contractual),"IntroductionThe Economist Intelligence Unit (EIU) is a world leader in global business intelligence. We help businesses, the financial sector and governments to understand how the world is changing and how that creates opportunities to be seized and risks to be managed. At our heart is a 50 year forward look, a global forecast of the majority of the world's economies, we seek to analyse the future and deliver that insight through multiple channels and insights, allowing our clients to take better trading, investment and policy decisions. We're changing, embedding alternate data sources such as GPS and satellite data into our forecasting, products will increasingly be tailored to individual clients, driven by some of the most innovative data in the market. A highly collaborative team of Product Managers, Customer Experience and Product Engineering is being created with a focus on creating business and customer value driven by real time analytics alongside our traditional products. What will you experience At Economist Intelligence Unit (EIU) we believe having the right work-life balance is super important; striking balance between your personal and professional life is critical to wellbeing and happiness. We offer flexible working and have recently shifted to a 'remote first' working policy with a minimum expectation of coming to the office two days a month, however you can come in more often if you wish to. Accountabilities How you will contribute: Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS via Python, Glue and Lambda Support and Transition: Support and optimise our current desktop data tool set and Excel analysis pipeline to a transformative Cloud scale Big Data Architecture environment. Work in an agile environment: within a collaborative agile product team using Kanban Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements. Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives. Become a data and analytics evangelist: This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals. Experience, skills and professional attributesTo succeed in this role it would be an advantage if you possess: Experience with programing in Python, and Lambda functions Knowledge of building bespoke ETL solutions, and extracting data using Data APIs MS SQL Server (data modelling, T-SQL, and SSIS) for managing business data and reporting Prior experience in design and developing microservice architecture Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. A combination of IT skills, data governance skills, analytics skills and economics knowledge An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (postgraduation diploma or related) or a related quantitative field or equivalent work experience. Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. This employer is a corporate member of myGwork - LGBTQ+ professionals, the business community for LGBTQ+ professionals, students, inclusive employers & anyone who believes in workplace equality. PRB",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Axtria - Ingenious Insights,Data Engineer,"• 5-8 years of experience in data engineering, consulting, and/or technology implementation roles
• Expertise in the design, data modeling creation, and management of large datasets/data models
• Experience in building reusable and metadata driven components for data ingestion, transformation and delivery
• Good understanding of any one cloud platform – AWS, Azure or GCP
• Experience with Lambda, Python and Spark; Familiarity with S3, Kinesis, Glue and Athena
• Strong proficiency in SQL and database design, development and maintenance
• Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence
• Good understanding of modern architecture patterns like serverless and microservices
• Expertise with analytics and business intelligence solutions (e.g. 1 or more of Tableau, PowerBI, MicroStrategy, Qlik etc.)
• Experience of working in complete Software Development life cycle involving analysis, technical design, development, testing, trouble shooting, maintenance, documentation and Agile Methodology
• Experience working with some of the following marketing data sources
• Traditional > TV / Print / Email
• Digital > Social Media (Twitter/Facebook) / Display Ads / Search / Website data
• Experience leading project teams with members with different roles and skills
• Experience working in hybrid onshore/offshore team models
• Strong communication skills",New Delhi,True,False,True,False,False,False,False,False,False,True,False,False,False,False,False,False
Valiance Solutions,Big Data Engineer,"About Us

Valiance is a global AI & Data analytics firm helping clients build cutting-edge technology solutions for digital transformation. We work with some of the marquee brands across India, US and APAC to build transformative solutions for Credit Risk, Fraud, Predictive Maintenance, Quality Inspection, Data lake, IOT analytics etc. Our team comprises 150+ professionals across Machine Learning, Data Engineering & Cloud expertise.

We are looking to hire a Senior Data Engineer to help our customers create scalable data engineering pipelines and infrastructure for downstream analytics workloads. You should be good at understanding client data needs, the landscape of various heterogeneous data sources, identifying a set of services for data ingestion & transformation workloads, and timely execution of projects.

Roles & Responsibilities:
• As a data engineer with Pyspark & SQL skills you will be required to highly scalable, robust, and resilient data engineering pipelines .
• You will be working closely with business stakeholders & the data science team to understand their data requirements and underlying business logic.
• Deploy and monitor pyspark jobs on cloud infrastructure.
• Troubleshoot job failures and ensure system recovery at earliest.
• Attending regular client calls, communicating work status and pro-actively highlighting any delays to the product release.

Technical Skills :
• Hands on experience on pyspark for at least 3 years
• Solid programming experience in Python & SQL is required.
• Working experience of any one cloud platform; AWS, GCP or Azure
• Intermediate plus proficiency in shell scripting
• Experience deploying ML algorithms in production is preferred

Personal Skills :
• Excellent communication skills, both written & oral.
• Ability to learn new skills quickly, adjust to the changing needs of the project.
• You are highly enthusiastic about your work
• Ability to multi-task, manage high-pressure release scenarios occasionally.

Valiance Solutions focuses on Financial Services, Cloud Computing, Artificial Intelligence, Internet of Things, and Big Data Analytics. Their company has offices in Noida and Bengaluru. They have a large team that's between 201-500 employees.

You can view their website at http://valiancesolutions.com or find them on Twitter and LinkedIn.",,True,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
DarioHealth,Data Engineer - Hybrid,"About The Position

At Dario, Every Day is a New Opportunity to Make a Difference.

﻿We are on a mission to make better health easy. Every day our employees contribute to this mission and help hundreds of thousands of people around the globe improve their health. How cool is that? We are looking for passionate, smart, and collaborative people who have a desire to do something meaningful and impactful in their career.

DarioHealth is looking for an experienced Data Engineer who will join our team and create new data solutions, maintain existing solutions and be a focal point of all technical aspects of our data activity. As part of this position, you will develop advanced data and analytics solutions to support our analysts and production units with validated and reliable data.

Responsibilities
• Develop and maintain DarioHealth data infrastructure.
• Develop in-house applications for providing self-service tools.
• Develop real-time data applications for production.
• Provide analysts and data scientists technical support related to data infrastructure.
• Design, build and launch new data models and visualizations in production, leveraging common development toolkits.

Requirements
• At least 4 years of proven experience with Python - a must.
• Very high level of SQL and data warehouse modeling.
• Experience with 24/7 systems and real-time analytics.
• Experience developing data pipelines with Airflow or similar - a must
• Experience with big data solutions like Kinesis/Sparks - an advantage
• Experience with NoSQL databases like MongoDB/Redis.
• Experience with web development using Django/javascript/react - an advantage.
• Experience in the online industry.
• B.A./B.Sc. in industrial/information systems engineering, computer science, statistics, or equivalent.
• **DarioHealth promotes diversity of thought, culture and background, which connects the entire Dario team. We believe that every member on our team enriches our diversity by exposing us to a broad range of ways to understand and engage with the world, identify challenges, and to discover, design and deliver solutions. We are passionate about building and sustaining an inclusive and equitable working and learning environments for all people, and do not discriminate against any employee or job candidate.***",,True,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False
AlphaGrep Securities,Data Engineer,"About the Company

AlphaGrep is a quantitative trading and investment firm founded in 2010. We are one of the largest firms by trading volume on Indian exchanges and have significant market share on several large global exchanges as well. We use a disciplined and systematic quantitative approach to identify factors that consistently generate alpha. These factors are then coupled with our proprietary ultra-low latency trading systems and robust risk management to develop trading strategies across asset classes (equities, commodities, currencies, fixed income) that trade on global exchanges..

We are seeking bright and resourceful individuals for our Data team which is based out of our Mumbai office.

Roles & Responsibilities
• Build infrastructure tools and applications to support trading teams across the firm.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Coordinate with global teams to understand their requirements and work alongside them.
• Establishing programming patterns, documenting components and provide infrastructure for analysis and execution
• Set up practices on data reporting and continuous monitoring
• Write a highly efficient and optimized code that is easily scalable.
• Adherence to coding and quality standards.

Required Skills
• Strong working knowledge in Python.
• Strong working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external processes to answer specific business questions and identify opportunities for improvement.

Good to have
• Experience with web crawling and scraping, text parsing
• Experience working in Linux Environment
• Experience with Stock Market Data

Why You Should Join Us
• Great People. We’re curious engineers, mathematicians, statisticians and like to have fun while achieving our goals
• Transparent Structure. Our employees know that we value their ideas and contributions
• Relaxed Environment. We have a flat organizational structure with frequent activities for all employees such as yearly offsites, happy hours, corporate sports teams, etc.
• Health & Wellness Programs. We believe that a balanced employee is more productive. A stocked kitchen, gym membership and generous vacation package are just some of the perks that we offer our employees",Mumbai,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
Robert Bosch,Azure Data Engineer,"Job Description

Location : Bengaluru
Experience : 6 to 8 years
Requirements
• Overall 6+ IT experience out of which 3+ years of working experience in Azure, architecting data soultions with good experience on Azure SQL, Azure Data Lake, ADF, Azure DataBricks/Synapse etc.
• 5+ years experience with data modelling,implementing backends and data optimization.
• Good experience working with with Automotive domain usecases.
• Experience implementing compliances like GDPR,HIPAA etc.
• Enforcing data security at rest and in transit.
• Good experience implemeting data security in Azure storage systems.
• Ability to thrive in a fast-paced, dynamic, client-facing role where delivering solid work products to exceed high expectations is a measure of success
• Excellent leadership and interpersonal skills
• Eager to contribute in a team-oriented environment
• Ability to be creative and analytical in a problem-solving environment
• Effective verbal and written communication skills

Skills
• Must have skills : SQL,ADF,other Azure storage services.
• Good to have:Synapse,spark or any big data framework or data warehouse.
• Key Responsibilities : conceptualizaing ,modelling Optimizing databases.Designing data flows
• Knowledge or basic experience with Nosql,parquet,Predictive modelling etc
• Working knowledge, creating ETL packages and deploying them

Qualifications

BE,MCA,MSC,MS,MTech
Experience : 6 to 8 years

Additional Information

Additional information
• Nice to have : Power BI experience, Azure DevOps, Cost Monitoring, Azure AD, Azure Synapse.
• Ability to quickly ramp up on new Azure Components which comes in Azure Roadmap.
• Excellent communication skills (English)

Experience: 6.00-8.00 Years",,False,False,True,False,False,False,False,True,True,False,False,False,False,False,False,False
CX Customer Experience,Salesforce - Data Engineer,"Come create the technology that helps the world act together

Nokia is committed to innovation and technology leadership across mobile, fixed and cloud networks. Your career here will have a positive impact on people’s lives and will help us build the capabilities needed for a more productive, sustainable, and inclusive world.

We challenge ourselves to create an inclusive way of working where we are open to new ideas, empowered to take risks and fearless to bring our authentic selves to work.

The team you'll be part of

You will work as part of the CX Global Sales Operation team. You will work making our Data strategy come alive across CX. You will be involved with integrating data across multiple platforms and especially the integration of the Advanced Analytic platform and the use cases being developed on it. The Advance Analytic projects will enable the CX organization to increase speed and efficiency, and assure the right things are done in the right way to maximize Nokia business.

What you will learn and contribute to

Be responsible for the data engineering of the Advanced Analytics Platform and CX AI use cases

Data integration from different source data platform to the advanced analytics platform

Data integration from the advanced analytic platform to different platform with UI

Ongoing support for the integration

Potentially doing data cleaning and wrangling on the advanced analytics platform

Potential involvement in dashboard and UI development

The type of use case you will work on will center around one or more of the following:
• Machine learning prediction models,
• The use of machine learning to automate the currently manual business planning process
• Knowledge mining and recommendation systems to improve the likelihood to win new business.

Your skills and experience
• Deep experience end data engineering including but not limited to experience using SQL and other types of databases and database languages
• Proficiency developing software probably in python using jupyter notebooks
• Proven competency in agile and lean software development
• Competency in SCM (Git), Automation tools, infrastructure automation,
• Good knowledge about Azure cloud infrastructure, security and application development.
• Experience with Python / Spark and Delta Lake. Familiar with big data patterns like lake house.
• Experience with Azure DevOps, CI/CD pipelines, version control tools like GIT / VSTS. Familiar with IDE’s like Visual Studio
• Nokia Business and process understanding
• Bachelor’s degree or higher in information technology, data science or related disciplines
• Effective communication in English (written and verbal)

Nice to have:
• Experience in app development

What we offer

Nokia offers flexible and hybrid working schemes, continuous learning opportunities, well-being programs to support you mentally and physically, opportunities to join and get supported by employee resource groups, mentoring programs and highly diverse teams with an inclusive culture where people thrive and are empowered.

Nokia is committed to inclusion and is an equal opportunity employer

Nokia has received the following recognitions for its commitment to inclusion & equality:
• One of the World’s Most Ethical Companies by Ethisphere
• Gender-Equality Index by Bloomberg
• Workplace Pride Global Benchmark
• LGBT+ equality & best place to work by HRC Foundation

At Nokia, we act inclusively and respect the uniqueness of people.

Nokia’s employment decisions are made regardless of race, color, national or ethnic origin, religion, gender, sexual orientation, gender identity or expression, age, marital status, disability, protected veteran status or other characteristics protected by law.

We are committed to a culture of inclusion built upon our core value of respect.

Join us and be part of a company where you will feel included and empowered to succeed.

Additional Information",,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False
GSK,Senior Principal Data Engineer,"Site Name: Bengaluru Luxor North Tower
Posted Date: Apr 20 2023

Ready to help shape the future of healthcare?

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organization where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to impact the health of 2.5 billion people around the world in the next 10 years.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a place where people feel inspired, encouraged and challenged to be the best they can be. A place where they can be themselves - feeling welcome, valued and included. Where they can keep growing and look after their wellbeing. So, if you share our ambition, join us at this exciting moment in our journey to get Ahead Together.

The Senior Principal Data Engineer is a vital technical role in the successful design and delivery of Data and Analytics (D&A) initiatives for the GSK's Pharmaceutical and Vaccines Supply Chains. The primary purpose of this role is to ensure that D&A Products have an optimal solution design and that the technical development work to then deliver them into production and support is smooth and successful. This requires deep expertise in data and analytics platforms and technologies as well as domain understanding of Pharmaceutical & Vaccines manufacturing and quality processes. This also requires close collaboration with D&A Product Managers, D&A Development Squads and the D&A Platform & Architecture team as well as with business stakeholders and other Digital and Tech teams.

The MSAT & Quality D&A team currently has a portfolio of around 15 D&A products across 4 product groups with over 100 people (GSK employees plus contractors) working in agile squads to deliver these. The Sr Principal Engineer will oversee and be accountable for the technical success of all of these products.

Key Responsibilities:
• Accountable for optimal solution designs for D&A Products that facilitates an agile, product management approach, can be rapidly and cost-effectively delivered to meet the true business requirements and are robust, sustainable and supportable throughout their lifecycle
• Work closely with D&A Product Mangers, using deep technical expertise and domain understanding to effectively influence (and when needed challenge) business and architectural stakeholders to arrive at the right design
• Steer solution design through D&A Architecture Review process, aligning with enterprise platforms and architectural patterns by first intent
• Oversee technical work of development teams ensuring it is remains aligned with agreed design, is of high quality, complies with relevant standards and policies and will meet agreed business objectives
• Provide hands-on technical problem-solving expertise to address technical challenges during development and, where needed, during lifecycle support
• Act as mentor for more junior technical roles, supporting their development and promoting adoption of best practice across development teams
• Lead discovery / proof-of-concept activities to establish early technical feasibility of new Products or Product Features
• Input to, review and approve key technical documents (e.g. design spec, validation plan)
• Drive adoption by development teams of existing and future best-practice approaches from D&A Platform team (e.g. implementation of DevOps CI/CD pipelines, automated testing)
Why you?

Basic Qualifications:
• Computer Science or related Bachelor's degree
• 16+ years of experience and track record of engineering and delivery of flexible, scalable, and supportable data and analytics applications for large complex, global organizations
• End-to-end / 'full stack' D&A experience from data ingestion through transformation to user interaction (visualisation, analytics, etc.)
• Track record of designing and delivering solutions in a cloud environment using modern data architectures and engineering technologies
• Experience designing with DataOps and FinOps in mind to ensure solutions are flexible/future-proof and can scale to handle growing demand, while remaining cost effective
• Experience in Agile development
• Track record of designing and delivering solutions compliant with industry regulations and legislation
• Ability to oversee and matrix manage GSK and 3rd party technical resources
• Excellent communication, negotiation, influencing and stakeholder management skills.
• Customer focus and excellent problem-solving skills.
Preferred Qualifications:
• Computer Science or related Master's degree
• Microsoft Azure accreditation and experience
• SAFe (Scaled Agile) accreditation experience
• Experience developing and delivering GxP-validated solutions for the Pharma/Vaccines industry
• Experience with specific technologies in GSK stack: Talend, Databricks/DeltaLake, Azure Synapse, Snowflake, PowerBI, Azure Functions, Azure App Services,
At GSK we value diversity (Gender, LGBTQ +, PwD etc.) and treat all candidates equally. We aim to create an inclusive workplace where all employees feel engaged, supportive of one another, and know their work makes an important contribution.

#LI-GSK

GSK is a global biopharma company with a special purpose - to unite science, technology and talent to get ahead of disease together - so we can positively impact the health of billions of people and deliver stronger, more sustainable shareholder returns - as an organisation where people can thrive. Getting ahead means preventing disease as well as treating it, and we aim to positively impact the health of 2.5 billion people by the end of 2030.

Our success absolutely depends on our people. While getting ahead of disease together is about our ambition for patients and shareholders, it's also about making GSK a place where people can thrive. We want GSK to be a workplace where everyone can feel a sense of belonging and thrive as set out in our Equal and Inclusive Treatment of Employees policy. We're committed to being more proactive at all levels so that our workforce reflects the communities we work and hire in, and our GSK leadership reflects our GSK workforce.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

It has come to our attention that the names of GlaxoSmithKline or GSK or our group companies are being used in connection with bogus job advertisements or through unsolicited emails asking candidates to make some payments for recruitment opportunities and interview. Please be advised that such advertisements and emails are not connected with the GlaxoSmithKline group in any way.

GlaxoSmithKline does not charge any fee whatsoever for recruitment process. Please do not make payments to any individuals / entities in connection with recruitment with any GlaxoSmithKline (or GSK) group company at any worldwide location. Even if they claim that the money is refundable.

If you come across unsolicited email from email addresses not ending in gsk.com or job advertisements which state that you should contact an email address that does not end in ""gsk.com"", you should disregard the same and inform us by emailing askus@gsk.com, so that we can confirm to you if the job is genuine.",Bengaluru,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,True
Confidential,Data Engineer,"Job purpose We are hiring a Data Engineer to join our Enterprise Analytics team. As a Data Engineer, you will be responsible for building, maintaining, and optimizing the data infrastructure needed to support our company's data-driven initiatives. You will work closely with our data analysts, data scientists, and business intelligence developers to ensure that our data is accurate, complete, and secure.Job Responsibilities:Design, build, and maintain the data infrastructure needed to support our company's data-driven initiatives.Develop and maintain data pipelines and ETL processes that move data from source systems to our data warehouse.Implement data quality checks to ensure that our data is accurate, complete, and consistent.Work closely with our data analysts, data scientists, and business intelligence developers to understand their data needs and ensure that our data infrastructure meets those needs.Optimize our data infrastructure to ensure that it can handle large amounts of data and support complex queries.Develop and maintain documentation for our data infrastructure and processes.Stay up to date with the latest technologies and trends in data engineering and recommend new tools and techniques as appropriate.Collaborate with other members of the BI and Data team to ensure that our data infrastructure is aligned with our company's strategic goals.Background and experience:Bachelor's degree in Computer Science, Engineering, or a related field.3+ years of experience in data engineering or a related field.Competencies and skills:· Strong knowledge of SQL and experience working with relational databases.· Experience with data modeling and schema design.· Experience building and maintaining data pipelines and ETL processes.· Experience with cloud-based data warehousing technologies, such as Azure Data Lake, Data Factory, Synapse Analytics.· Strong problem-solving skills and attention to detail.Excellent communication and collaboration skills. Transportation, Logistics and Storage,IT Services and IT Consulting,Truck Transportation",,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False
Loop Health,Data Engineer - Remote,"About Loop

Looking for a great mission? Help build a customer focused healthcare company.

Loop wants to create an inspirational healthcare and insurance company. We believe in the transformative nature of empathetic primary care, proactive financial coverage and want to bring that to our members. We want to fundamentally change how healthcare assurance is designed and delivered. We believe in the power of incentives. We are successful when we deliver health outcomes — when our members and their families get healthier.

“Why exactly are we building a new revolutionary healthcare system? The obvious answer is India deserves better care for its people. Not enough of it around, and what exists can be tough to navigate.

Imagine if doctors were paid to actually make you better. What a concept! What if they didn't have to worry about finishing consults in 10 minutes to meet their daily quota. What if they could take their time, really understand the symptoms, the family history & the lifestyle to come up with a plan, rather than just a prescription.

Imagine if hospital admissions, treatments, billing and insurance were as easy as ordering food home and your care doesn't end when they send you home from the hospital. It goes till you are back on your feet. And further, now imagine if your family had access to this great care anytime they wanted. From serious conditions to the smallest questions. So that they live longer. Wouldn't you worry less?

At the end of it. It's not why you would build this system... Why wouldn't you?”

Here’s how we are going about it:

- We built a high quality concierge and primary care program that allows members and their families to access unlimited care when they need it.
- We use technology to deliver this through highly engaging care.
- We work with insurers to bring financial protection to our members so that they do not worry about their families’ well being.
- As a healthcare insurance broker, we provide companies with the best coverage and claims service for their employees and dependents.

Doing this will mean that we create great products and services that work on changing behaviors and mindsets. This will need a deep understanding of design of products and services through an empathetic lens of what members need for their health and technology will play a very pivotal role in enabling our members to use our programs . We are looking for folks in our ‘EngineeringTeams’ to work with us to take Loop to this future.

If you'd like to learn more about what we are building at Loop, there are tons of resources. Here are some of our favorites:
https://yourstory.com/2021/06/loop-health-insurance-plans-improve-
healthcare/amphttps://yourstory.com/2022/04/loop-health-raises-25m-elevation-capital-general-catalyst/amp

Join us in making healthcare simple, reliable, and human.

Roles and Responsibilities

• Work in collaboration with engineers and stakeholders to build a platform for enabling data-driven decisions.
• Build reliable, scalable, CI/CD driven streaming and batch data engineering pipelines.
• Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.
• Create a conceptual data model to identify key business entities and visualize their relationships.
• Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and relationships
• Storage (cloud data warehouse, S3 data lake), orchestration (Airflow), processing (Spark, Flink), streaming services (Kafka), BI tools, graph database, and real-time large scale event aggregation store are all examples of data architecture to design and maintain.
• Work on cloud data warehouses, data as a service, business intelligence, and machine learning solutions.
• Data wrangling in a diverse environment.
• Ability to provide data and analytics solutions that are cutting-edge.
• Identify strategic and Operational KPIs for the team and drive the team to deliver the committed targets.

Qualifications

• SQL knowledge, as well as programming skills in Scala or Python.5+ years of applicable data warehousing, data engineering, or data architecture experience
• Experience with the GCP stack (BigQuery, GCP Databricks) is a plus
• Ability to design data analytics solutions to meet performance and scaling requirements.
• Demonstrated analytical and problem-solving abilities, particularly in the context of large data.
• Data warehousing concepts and modern data warehouse/Lambda architecture are well-understood.
• Good understanding of the Machine Learning and Artificial Intelligence (AI) solution space.
• Communication and interpersonal skills at all levels of management
• You are a detail-oriented person with excellent communication skills and a strong sense of teamwork.

What you can expect from us

  ‍  ‍   Loop Family Healthcare Health insurance for you and your family for all medical emergencies.

   High agency You'll always have the agency to shape projects, processes and outcomes independently.

  Learning Budget If there's a workshop, book or event you think will help you learn, we'll cover your bill.

   Work from home setup We'll help you set up your office the way you want to with the best equipment around.",Bengaluru,True,False,True,False,True,False,False,True,False,False,False,False,False,True,True,False
Visa,Sr. Data Engineer - Big Data Testing,"This position is ideal for an engineer who is passionate about solving challenging business problems. You will be an integral part of the Payment Products Development team focusing on test automation. The candidate will be extensively involved in hands-on activities including POCs, design, documentation, and testing of new and existing functionality. Candidate must be flexible and willing to switch tasks based on team's needs.
• Develop systems and processes to refine efficiency of automated testing solutions
• Design and execute tests for applications and services
• Develop and maintain tools for automation tracking and reporting
• Review product requirements and specifications and recommend improvements to ensure product testability
• Recommend areas of applications and services where automation would be beneficial
• Present technical solutions, capabilities, considerations, and features in business terms
• Effectively communicate status, issues, and risks in a precise and timely manner
• Perform other tasks on data governance, system infrastructure, and other cross team functions on an as-needed basis

This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office two days a week, Tuesdays and Wednesdays with a general guidepost of being in the office 50% of the time based on business needs.",Bengaluru,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
PayPal,"MTS 1, Data Engineer","Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 375 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
The MTS 1 ? Data Engineer will directly report to and support Sr. Manger of Finance Technologies in the development and execution of strategic transformation programs & initiatives, strategic engineering architecture design, resource allocation, and platform performance monitoring. Ideal candidate is a technologist who believes that use of technology is in its infancy and the best is yet to come. The Regulatory Reporting Hadoop product owner (Business System Analyst) will be part of the Global Regulatory Reporting, and Merger & Acquisition Integration support. The nature of role is strategic, analytical and highly collaborative, working with team members across World and also as a liaison for Global projects.
• Lead, develop, and grow a high performance, multi-function team of talented and passionate professionals, who are results driven to take the business forward and demonstrate superior leadership in line with the PayPal values.
• Undergraduate/ Master degree in Computer Engineering or equivalent from a leading university.
• 11+ years of post-college working experience as a Business System Analyst and leading large scale projects end to end.?
• Minimum 4+ years? experience working with large data sets, experience working with distributed computing a plus (Map/Reduce, Hadoop, Hive, Spark, etc.)
• Experience in Data Analysis, Data Validation.
• Strong knowledge in writing complex queries for validation of ETL process.
• Preferred/Basic understanding of Payments/Finance/Accounting Industry Background.
• Must have demonstrably strong interpersonal and communication skills (both written and verbal), to include speaking clearly and persuasively in positive or negative situations.
• Experience with databases, systems integration, application development and reporting.?
• Works independently and able to make decisions quickly when necessary.
• Quick Learner with an ability to ramp up in technologies and modules to meet business needs.
• Works in an Agile environment and continuously reviews the business needs, refines priorities, outlines milestones and deliverables, and identifies opportunities and risks.
• Maintain, track and collaborate with dev teams to ensure project estimation for delivery.
• Experience using JIRA and Confluence, or similar User Story workflow and management tool is a must.
• Highlight the bugs and blockers and coordinate with the development and operations team to come up with the best solutions/fixes and document them.
• Work across internal team in various geo-locations across the world
• ?Drive For Results? - Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.
• ?Priority Setting? - Spends his/her time and the time of others on what’s important; quickly zeros in on the critical few and puts the trivial many aside; can quickly sense what will help or hinder accomplishing a goal; eliminates roadblocks; creates focus.
• Weekly and Monthly status reporting to leadership.",Chennai,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False
